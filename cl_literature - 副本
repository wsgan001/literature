% Encoding: UTF-8


@InProceedings{,
  Title                    = {Empir Software Eng (2010) 15:556?576 DOI 10.1007/s10664-010-9131-y},
  Author                   = {An empirical investigation into open source web and applications? implementation vulnerabilities },
  Year                     = {2010},

  Abstract                 = {Current web applications have many inherent vulnerabilities; in fact, in 2008, over 63% of all documented vulnerabilities are for web applications. While many approaches have been proposed to address various web application vulnerability issues, there has not been a study to investigate whether these vulnerabilities share any common properties. In this paper, we use an approach similar to the Goal-Question-Metric approach to empirically investigate four questions regarding open source web applications vulner- abilities: What proportion of security vulnerabilities in web applications can be considered as implementation vulnerabilities? Are these vulnerabilities the result of interactions between web applications and external systems? What is the proportion of vulnerable lines of code within a web application? Are implementation vulnerabilities caused by implicit or explicit data flows? The results from the investigation show that implementation vulnerabilities dominate. They are caused through interactions between web applications and external systems. Furthermore, these vulnerabilities only contain explicit data flows, and are limited to relatively small sections of the source code.
},
  Doi                      = {10.1007/s10664-010-9131-y},
  File                     = {:home/ccc/github/literature/article/An empirical investigation into open source web applications? implementation vulnerabilities.pdf:PDF},
  Keywords                 = {Empirical evaluation . Web applications . Security . Vulnerability . Injection},
  Review                   = {Empir Software Eng (2010) 15:556?576 DOI 10.1007/s10664-010-9131-y
An empirical investigation into open source web applications? implementation vulnerabilities
Toan Huynh & James Miller
Published online: 22 May 2010 # Springer Science+Business Media, LLC 2010 Editor: Bojan Cukic
Abstract Current web applications have many inherent vulnerabilities; in fact, in 2008, over 63% of all documented vulnerabilities are for web applications. While many approaches have been proposed to address various web application vulnerability issues, there has not been a study to investigate whether these vulnerabilities share any common properties. In this paper, we use an approach similar to the Goal-Question-Metric approach to empirically investigate four questions regarding open source web applications vulner- abilities: What proportion of security vulnerabilities in web applications can be considered as implementation vulnerabilities? Are these vulnerabilities the result of interactions between web applications and external systems? What is the proportion of vulnerable lines of code within a web application? Are implementation vulnerabilities caused by implicit or explicit data flows? The results from the investigation show that implementation vulnerabilities dominate. They are caused through interactions between web applications and external systems. Furthermore, these vulnerabilities only contain explicit data flows, and are limited to relatively small sections of the source code.
Keywords Empirical evaluation . Web applications . Security . Vulnerability . Injection .
Classification of vulnerabilities
1 Introduction
The Laws of Vulnerabilities 2.01 states that ?80% of vulnerability exploits are now available within single digit days after the vulnerability?s public release?. The 2008 Internet Security Threat Report2 from Symantec notes that web applications contain 63% of all documented
1http://www.qualys.com/research/rnd/vulnlaws/, last accessed August 16, 2009 2http://www4.symantec.com/Vrt/wl?tu_id=gCGG123913789453640802, last accessed January 29, 2010
T. Huynh : J. Miller (*) Department of Electrical and Computer Engineering, Electrical and Computer Engineering Research Facility, University of Alberta, Edmonton, AB T6G 2V4, Canada e-mail: jm@ece.ualberta.ca
T. Huynh e-mail: huynh@ece.ualberta.ca

}
}

@Article{Ping-pingHuiWen-siEtAl2014,
  Title                    = {Fuzzing test technology based on concolic symbolic execute},
  Author                   = {Lu Ping-ping and Li Hui and Mu Wen-si and Han Qing},
  Journal                  = {Application Research of Computers},
  Year                     = {2014},
  Number                   = {7},
  Pages                    = {2088--91,},
  Volume                   = {31},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Fuzzing test and symbolic execute are common techniques for vulnerability mining, but each are flawed. In order to improve vulnerability mining efficiency, this paper combined the advantages of both techniques, and designed and implemented a Fuzzing test tool based on concolic symbolic execution. Using code instrumentation technology, it recorded all primary code execute paths and contexts dynamically. Using offline concolic symbolic execute technology, it collected the path constraints while the code record was replayed, and solved by STP program. New test cases was generated with solved results and tested dynamically, meanwhile execute exception was monitored and code coverage was calculated. The test results verify that the tool can find exceptions effectively, and can be applied to large application testing, seven exceptions are discovered in Word 2003 software, the code coverage is also mostly improved than traditional Fuzzing test tools.},
  Doi                      = {10.3969/j.issn.1001-3695.2014.07.040},
  Groups                   = {Code Mining},
  Sn                       = {1001-3695},
  Tc                       = {0},
  Ut                       = {INSPEC:15079877},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{Yong-weiQingHuiEtAl2013,
  Title                    = {Loop vulnerability detection based on decompile},
  Author                   = {Li Yong-wei and Yin Qing and Shu Hui and Li Ji-zhong},
  Journal                  = {Application Research of Computers},
  Year                     = {2013},

  Month                    = may,
  Number                   = {5},
  Pages                    = {1508--10,},
  Volume                   = {30},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Improper circulatory buffer copy is one of the main reasons of buffer overflow, in order to improve the efficiency of such vulnerability detection, this paper presented a method which based on decompile to detect loop vulnerability. Firstly, the method decompiled target binary files and constructed the abstract syntax tree (AST) of functions, designed algorithm to extract circulation information of functions. Then, according to the characteristics of loop vulnerability, it built finite state automata to detect the loop vulnerability. This method had obvious advantages in non-source code vulnerability detection, could discover loop vulnerability in the software effectively, and improve the efficiency and automation of vulnerability mining.},
  Doi                      = {10.3969/j.issn.1001-3695.2013.05.058},
  Groups                   = {Code Mining},
  Sn                       = {1001-3695},
  Tc                       = {0},
  Ut                       = {INSPEC:14167267},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{International2014,
  Title                    = {Model Checking C++ with Exceptions},
  Author                   = {Proceedings of the and 14th International and Workshop on},
  Booktitle                = {Proceedings of the 14th International Workshop on Automated Verification of Critical Systems},
  Year                     = {2014},

  File                     = {:article\\Model Checking C++ with Exceptions.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {model chdecking,C++,execption handling.LLVM},
  Read                     = {未读},
  Review                   = {文中第一部分是将C++代码转换为LLVM IR，这个需要看看。}
}

@Article{Jian-junLe-changJing-juEtAl2011,
  Title                    = {Research for buffer overflow vulnerabilities based on multi-dimensional fuzzing technology},
  Author                   = {Xia Jian-jun and Sun Le-chang and Liu Jing-ju and Zhang Min and Cai Ming},
  Journal                  = {Application Research of Computers},
  Year                     = {2011},
  Number                   = {9},
  Pages                    = {3539--41},
  Volume                   = {28},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Buffer overflow( BOF) is always one of the most dangerous vulnerabilities to computer security. This paper proposed multi-dimentional Fuzzing of buffer overflow( MFBOF), which was based on multi-dimentional Fuzzing technology, combined the structure knowledge of target' s input,static binary code analysis and dynamic I/O analysis technique, generated test cases using adaptive simulated annealing genetic algorithm. The results of testing Libpng validate that MFBOF is effective. At last, this paper gave its further improvement directions.},
  Doi                      = {10.3969/j.issn.1001-3695.2011.09.095},
  Groups                   = {Code Mining},
  Sn                       = {1001-3695},
  Tc                       = {0},
  Ut                       = {INSPEC:12741930},
  Z8                       = {1},
  Z9                       = {1},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{,
  Title                    = {IZ},
  Author                   = {??? ??},
  Year                     = {4258},

  File                     = {:home/ccc/github/literature/article/??????????????????????????.pdf:PDF},
  Review                   = {IZ42586
??? ??
UDC

}
}

@InProceedings{,
  Title                    = {????-?? UCSB CS290C????},
  Author                   = {0 ?? and ?????????????????????????????????????? and ???????????????????Rice????Optimizing Compilers and for and Modern Architectures????????????????????????????? and ??????????????????????????????????????? and ????????????????????????(1) ????????????? and ??????????????????????????????????????? and ??(2) ????????Ben and Hardekopf???????????????????? and ??????????? and ??????????????????????????????????????? and ??????????????????????????????????????? and ??????????????????????????????????????? and ?????????????????????????????????????? and ??????????????????????????????????????? and ????????? and ??????????????????????????ppt(??????)???? and ???????????????ppt?????????????????????? and ??????????????????????????????????????? and ???????????????????????????????????? and ?????????? and 1 ?? and 2 ????? and 3 ????? and 4 ?????SSA and 5 ???? and 6 ????? and 7 ?????Andersen???? and 8 ?????Steensgaard????},

  File                     = {:home/ccc/github/literature/article/????-?????.pdf:PDF},
  Review                   = {????-?? UCSB CS290C????
0 ?? ?????????????????????????????????????? ???????????????????Rice????Optimizing Compilers for Modern Architectures????????????????????????????? ??????????????????????????????????????? ????????????????????????(1) ????????????? ??????????????????????????????????????? ??(2) ????????Ben Hardekopf???????????????????? ??????????? ??????????????????????????????????????? ??????????????????????????????????????? ??????????????????????????????????????? ?????????????????????????????????????? ??????????????????????????????????????? ????????? ??????????????????????????ppt(??????)???? ???????????????ppt?????????????????????? ??????????????????????????????????????? ???????????????????????????????????? ?????????? 1 ?? 2 ????? 3 ????? 4 ?????SSA 5 ???? 6 ????? 7 ?????Andersen???? 8 ?????Steensgaard????

}
}

@InProceedings{,
  author = {??????},
  title  = {????},
  year   = {2012},
  file   = {:home/ccc/github/literature/article/????????????????.pdf:PDF},
  review = {????
??????
????????????????
?????
?????????
??????????
????????
2012-05},
}

@InProceedings{,
  Title                    = {????????},
  Author                   = {?????? },
  Year                     = {2010},

  File                     = {:home/ccc/github/literature/article/??????????.pdf:PDF},
  Review                   = {????????
??????
??????????
??????
?????????
???????????
???????
20100601

}
}

@InProceedings{,
  Title                    = {??????},
  Author                   = {?????? },
  Year                     = {2007},

  File                     = {:home/ccc/github/literature/article/?????????????????????.pdf:PDF},
  Review                   = {??????
??????
?????????????????????
??????
?????????
??????????
????????
20070530

}
}

@InProceedings{,
  Author                   = {????????},
  Year                     = {1678},

  File                     = {:home/ccc/github/literature/article/????????.pdf:PDF},
  Review                   = {? !! ?? " ? ? ? ? ? ? ??? ? ? ? ?? 1678 !! *68 " #$$% ? " ? &?()*+, ?- .?*/&0 (*012)30.4?*+.()+, 3502*52? 9 &:;8 #$$%
????????
?9 ?????????
????? ????????????9 #$$$@#?
????????????????????????????????????8????????????? ??????8????????????????????????????????????????????? ???????????????????????????????????????????????????
???????????????????????????????????????????????????
?????8???????????????????????????????????????????? ????????????????8???????????????????????????????? ??8
??????????????????????
??????.K !!>9 9 9 9 9 9 ??????+9 9 9 9 9 9 9 9 ?????$#%! = !?<L?#$$%?$" = $M>! = $?
!"#$%& ?( )%*%+#,- ?. /0."*,+12(3 453?#21-6*
!"# $%&??()* +,-.,&??/)01 2,%3-4,&?
?NOPCQRBO;R 6S 56BP:ROQ 3EDO;EO C;T .OEG;676IH?.6;IUD (;DVOQFDRH?3GC;IGCD #$$$@#?5GD;C?
40*1#+,1?WOCF:QOF 6S P6RO;EH?QOFD7DO;EO?FROC7RG C;T OXOE:RD6; E6FR CQO D;RQ6T:EOT CR RGO YOID;;D;I8 .GO; C FH;6PFDF C;T C QOVDOZ 6S S6776ZD;I ECROI6QDOF 6S 6YS:FECRD;I RQC;FS6QBCRD6;F CQO IDVO;8 .GO SDQFR DF 7OXDEC7 RQC;FS6QBCRD6;8 .GO FOE6;T DF E6;RQ67 S76Z RQC;FS6QBCRD6;F D;E7:TD;I YQC;EG D;FOQRD6;?OXRO;TD;I 766P E6;TDRD6;F?E6;VOQRD;I C QOT:EDY7O S76Z IQCSR R6 C ;6; = QOT:EDY7O 6;O?QOB6VD;I 766PF?TOROQD6QCRD6; 6S E6;RQ67 S76Z C;T TCRCS76Z RQC;FS6QBCRD6;8 .GO RGDQT DF TCRC RQC;FS6QBCRD6;F D;E7:TD;I QOFRQ:ER:QD;I CQQCHF? QOE6;FRQ:ERD;I VCQDCY7OF?E6;VOQRD;I FRCRDE TCRC R6 PQ6EOT:QC7 TCRC C;T BOQID;I FEC7CQ VCQDCY7OF8 .GO S6:QRG DF E7CFF FRQ:ER:QO RQC;FS6QBCRD6;F D;E7:TD;I E7CFF E6C7OFED;I?E7CFF FP7DRRD;I C;T RHPO GDTD;I8 -:QRGOQB6QO? CRRCE[F CICD;FR 6YS:FECRD;I C7I6QDRGBF C;T TO6YS:FECRD6; CQO F:BBCQD\OT8 36BO F:IIOFRD6;F CY6:R S:R:QO Z6Q[ CQO PQ6P6FOT D; RGO O;T8 7%& 8?#9*?6YS:FECRD;I RQC;FS6QBCRD6;F?B6YD7O E6TO PQ6ROERD6;?D;RO77OER:C7 PQ6POQRH PQ6ROERD6;
9 9 ???????????????????? ???????????????8???&CVC ?? ??????????????8 ??????? ?????????????????????? ????? W6YD7O +IO;R???????????? ???????????&CVC ?????????
?????#$$< = $> = ># ???????????????????$>?>$">? ??????9 ??>@?? =?????????????8 2ABCD7?E;FGDHC;IJ HCG668 E6B8 E;
????

}
}

@InProceedings{,
  Title                    = {??????? ? ? ? ? ? ? ? ?????????? ???????? ?????????????????????????????? ???},
  Author                   = {??????????? },
  Year                     = {2345},

  File                     = {:home/ccc/github/literature/article/??????????.pdf:PDF},
  Review                   = {??????? ? ? ? ? ? ? ? ?????????? ???????? ?????????????????????????????? ????????
???????????
???????????? ????????? ????????????? ???????
???????????????????????????????????????????????? ???????????????????????????????????????????????? ????????????????????????????????????????????????? ????????????????????? ??????????????????????? ?????????????????????????????????????????????????????? ????????????????????????????????????????
????????????????????????????????????? ????????????????????????????? ????????????????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????????????????????? ????????????????????????????????????????????????????????????????????????????????????????????????????????????????? ????????????????????????????????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????
??????????????????????????? ??????????????????????????? ???????????????????????????? ???? ???????????????????????????? *++ "#$%
0/1/
,++
&'(
???????????????????????????? -++ .++
2345
??????????????????????????? /++ 0++
???????????????????????????? 1++ 678 )++
?????????????????????????????? 2++ +
9:()*
??????????????????????????? 3 4 1 0 / . - , * !+ !! !4 ! ;<=>?6@'
)
! " #$%&'
! "#$% &'(
?????????? ????????????????????? ABCD ()*+,-./
??????????????????????????? ??????? ??????????????????????????? ????????? ????????????? ??????
??????????? ????????????????????????????? !
??????????????????????????? ????????????????????????? ??????????????????????????? ??????????????????????????? ??????????????????????????? ?????????????????????????? ?????????????????????????? ????????? ??????????????????????????? ???????? ?????????????????????????? ????????????????????????? ???????????? ???????????????????????????? ???????? ?????????????????? ?????????????????????????? ????????????????????????? ??????????????????????????? ?????????????????????????? ??????????????????????????? ??????????????????????????? ??????????????????????????
????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????
?

}
}

@InProceedings{,
  Title                    = {? ? ? ? 2014??08? doi ?10.3969/j.issn},
  Author                   = {??????????? and ?????? },
  Year                     = {2014},

  Abstract                 = {Program obfuscation is a compiler that transfers the original program into an unintelligible form while preserving the functionality. The concept of obfuscation was first introduced in code obfuscation, which is used for software protection, digital watermarking, etc. However, it lacks formal analysis and security proof. Obfuscation for cryptographic purposes was proposed by Barak et al., and they gave the formal definition of `virtual black-box obfuscation and its security requirements. General obfuscation of cryptographic functions has important meaning in theoretical research and has close relation with other cryptographic primitives such as random oracle, fully homomorphic encryption, zero knowledge, etc. Besides, secure obfuscation of specific cryptographic functions has practical use in cloud computing and delegate computing. In recent years, secure program obfuscation has become one of the hottest topics in the progress of cryptographic research. As obfuscation of general function families was proved impossible under Barak?s standard definition, thus following researches are mainly focused on realizing secure obfuscation of specific families of functions, new definition models of obfuscation, and relations and applications of obfuscation in other cryptographic primitives. In this paper, we give an overview on the study of secure obfuscation, which includes constructions of secure obfuscation of specific cryptographic functions, studies on special models of obfuscation and generalization and applications of secure obfuscation. 
},
  Doi                      = {?10.3969/j.issn.1671-1122.2014.08.002},
  File                     = {:home/ccc/github/literature/article/???????????.pdf:PDF},
  Review                   = {? ? ? ? 2014??08? doi ?10.3969/j.issn.1671-1122.2014.08.002
??????????? ??????
???????????????????510006?
??? ???????????????????????????????????
????????????????????????????????????????
????????????????????????????????????? Barak
????????????????????????????????????????
????????????????????????????????????????
????????????????????????????????????????
??????????????????????????????????? Barak ??
????????????????????????????????????????
? 3 ??? ???????????????????????????????????
????????????????????????????????????????
????????????????????????????????
??? ???? ????? ???????
????? ?TP309 ????? ? A ???? ?1671-1122?2014?08-0006-11
An Overview on the Secure Program Obfuscation 
 CHENG Rong, ZHANG Fang-guo (School of Information Science and Technology, Sun Yat-sen University, Guangzhou Guangdong 510006, China)
Abstract: Program obfuscation is a compiler that transfers the original program into an unintelligible form while preserving the functionality. The concept of obfuscation was first introduced in code obfuscation, which is used for software protection, digital watermarking, etc. However, it lacks formal analysis and security proof. Obfuscation for cryptographic purposes was proposed by Barak et al., and they gave the formal definition of `virtual black-box obfuscation and its security requirements. General obfuscation of cryptographic functions has important meaning in theoretical research and has close relation with other cryptographic primitives such as random oracle, fully homomorphic encryption, zero knowledge, etc. Besides, secure obfuscation of specific cryptographic functions has practical use in cloud computing and delegate computing. In recent years, secure program obfuscation has become one of the hottest topics in the progress of cryptographic research. As obfuscation of general function families was proved impossible under Barak?s standard definition, thus following researches are mainly focused on realizing secure obfuscation of specific families of functions, new definition models of obfuscation, and relations and applications of obfuscation in other cryptographic primitives. In this paper, we give an overview on the study of secure obfuscation, which includes constructions of secure obfuscation of specific cryptographic functions, studies on special models of obfuscation and generalization and applications of secure obfuscation. 
Key words: cryptography; program obfuscation; virtual black-box property
???? ? 2014-07-21 ???? ????????? [61379154,U1135001]????????????????????? [20120171110027] ???? ????1987-?????????????????????????????????1972-??????????????????? ????????????????
6

}
}

@InProceedings{,
  Title                    = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jos@iscas?ac?cn JouronfaSloftware?2012?23(3)?700?711[doi?10?3724?SP?J?1001?2012?03h9t9t4p????wwwjos?org?cn ???????????????? Tel?Fax},
  Author                   = {???????????? },
  Year                     = {9825},

  Doi                      = {?10?3724?SP?J?1001?2012?03h9t9t4p????wwwjos?org?cn},
  File                     = {:home/ccc/github/literature/article/???????????.pdf:PDF},
  Review                   = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jos@iscas?ac?cn JouronfaSloftware?2012?23(3)?700?711[doi?10?3724?SP?J?1001?2012?03h9t9t4p????wwwjos?org?cn ???????????????? Tel?Fax?+08?66?21562563
????????????
???1r????1????1??????1?2+????2?3
1(??????????????????710127)
2(??????????????????????710127)
3(?????(??)???????100125)
EvaluaotfCiodnOebfuscTartainsgformation
ZHAYuO?Jiel?2Z?ThAaNnG?Yon91?N2i?lW?A2N?GFDAiNngG?Yil?2Yu+a?nG-UXian92?3
1(SchoofoIlnforSmcaiteianoncnTedechnology?UNnoirvtehrswietsy7t?1X0i1?a2n7?China)
2(NWU?INredtewtork?InfSoercmuarJtioitioyLnnatboratory(NI7S1L0)1?2X7i??Cahnina)
3(IrdAectcoeTseschnology(Beijing)1C0o0?1L2t5d??C?hBeiinjai)ng
+Correspaountdhoirn?gE-mail?dcyaf@nwu?edu
ZhaYoJ?TaZnYg?WaNn?gFaDngY?GYuX?Evaluoafctoidooenbfusctatriangsformationf?Journal
Software?2012?23(3)?700-711?http???www?jos?org?cn?1000?9825?3994?htm
AbstracCto?doebfuscaistciuornreontnleoyfthemosvtiabmlethfoodprsrevenrteivnegernsegineeatrtiancgks
Mankyindosfcodoebfuscattriaonnsfaorrewmisdeulsyeidnsoftwparoetection?Howareesvteilrnl,othere
suffictiheenotrtioesvalutahteeffectivoefonbefsusscattrainosnforfma?cItn?mfewasureamreanvtaislable thaptrovidneformatbiooutnthecapabiolfiotbyfuscattorieodnuactetackers?effifceiwexnicsyt,ianngd theories?dwrhaiwucsphocnomplemxeittryifcrsosmoftweanrgeineericnogn?vairencinpga?pTeuhrsiesas
differweantyoevalutahtedeiffictuhlatatyttackhearvsienunderstandmiondgifoybifnugscsaotfetdware
throustgahtaincalysis?dyenbaumgiogcfirnevgeresnegineeritnhge?tnoaanbdstrsaoctmmeetrtiocqsuanttiofy
whaetxtetnhtactodoebfusciasatbilotenomakaettamckosrdeiffitcoubletperformed?
Kewyords?codoebfuscatione?nrgeivneeresreing?evaplluuagt�inosn??cfIolDnoAtfwrloalttening
??? ?????????????????????????????????????????????
???????????????????????????????????????????????????
??????????????????????????????????????????????????
??????????????????????????????????????????????????? ??????????????????????????????????
???? ?????????????IDA????????
???????TP309 ??????A
�?????????????(61070176?61170218)??????????????(2010JC24)??????????
(CXY1l)0?1???????????????????(NISL?2009TR01)????????????????(10YZZl6)
?????2010?09?09??????2011-01?21
????

}
}

@InProceedings{,
  Title                    = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jOS@iscas?ac?cn JouronfaSloftware?2013?24(12)?2767?2781?doi?10?3724?SP?J?1001?201h3t?t0p4?3?8?5w]wwjos?org?cn ???????????????? Tbl?Fax},
  Author                   = {???????????????? },
  Year                     = {9825},

  Doi                      = {?10?3724?SP?J?1001?201h3t?t0p4?3?8?5w]wwjos?org?cn},
  File                     = {:home/ccc/github/literature/article/???????????????.pdf:PDF},
  Review                   = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jOS@iscas?ac?cn JouronfaSloftware?2013?24(12)?2767?2781?doi?10?3724?SP?J?1001?201h3t?t0p4?3?8?5w]wwjos?org?cn ???????????????? Tbl?Fax?+86?10?62562563
????????????????
??1??????1,2????1,2
1(???????????????(????)?????210093)
2(??????????????????210093)
?????????E-mail?zqk@nju?edu?cn
??? ?????????????????????????????????????????????
?????????????????????????????????????????GCC????????
???DRIVER(adnerdtuen?ctticmheecinkteger-vublanserdabwiliitithniefsormaftlioown)???????????
?????????????????? ???? ???????????????? ???????TP31 1 ??????A
???????????????????????????????????????2013?24(12)?2767?2781?http???www?jos org?cn?1000-9825?4385?htm
???????SuHn?LHiP?ZeQnKg?Statidceatlealcnytdrim?tcimheeicnktegerv-ubianseerdabwiliitithnifesormfaltoiwon RuaJinaXnuBeao?JouorfnSaolftware?2013?24(1C2h)i?n2e7se6)7??h2t7t8pl??(?iwnww?jos?org?cn?1000-9825?4385?htm
StaticDaeltleyacntRdun-TCihmeeIcnktegerV-uBlanseerdabwiiftiIhtnifeosrmFatlioown
SUHNa01?_H?LuIi?Pen91?Q?inZgE?NKaGil?2
1(StaKteLyaboraftoNroroyvSeolftwTaercehnologyU(nNiavnerjsiintgy)2?1N0a0n9j3i?nCghina)
2(DeparotfCmoemntpuStceireanncTdeechnologyU?nNiavnejrisnigty?21N0a0n9j3i?nCghina)
Correspaountdhiongr?QZiEngN-GKai?E?mail?zqk@nju?edu?ca
Abstracatp?pArontaodcehtecitnitneggerv-ublanseerdabislpirtoipeosbsaesdoendinformatiaonna-lfyislniosrwdteorimprtohvee
run-tpiemreformatnhcieas?pIpnroacht?houenlsyaifneteogpeerratoinotanisnitnedformaftliopowanths?wChaibncehcontroblyled userasnidnvolivnesdensitoipveerationtso?bneeiendstrumweintrheudn?ticmheeckode?tShOabtotthhedensiotfsytatic
instrumeantnapdteirofnoromvaenrcheaerareedduced?0B1at1hsieasdpproapcrho?taostyspetceamllDeRdIVER(adnerdtuen?cttime cheicnktegerv-ublanseerdabwiilitithnifeosrmafltoiwoi)nimsplemaesannteexdtentsoitohGne CcCompialnetdrestoendanumboefr
real�waoprlpdlicatioenxsp?eTrhiemreenstuslatlhsotwhathiaspproiasecfhfective?scalablea?nlcdiagphatob-flwleoicgahttihneg
rooctause?
Kewyords?integveurln-ebraasbeidlity?filnofwo?rtmaanitaniltoynsis?instrumentation
??????????????????????????????????????????????
????????????????????????????C???????????????????
????????????????????????????????????????????????
?????????????????????????????????????????????????
???????????????????????Apache?OpenSSH?Sendmail?Snort?BSD???Linux
�?????????????(61170070?90818022?61021062)?????????(2012BAK26801)???????????
?(863)1(A20A1lA202)
?????2012?08?3l??????2012?12?03??????2013?02?04
????

}
}

@InProceedings{,
  Title                    = {ISS1N000?9825R?CUOXDEUNEW E�mail?jos@iscas?ac?cn JouronfaSloftware?"C01?21?N2o0?120??FPePb?r1ua7r9y-193 hap???www?jos?org?cn doi?10?3724?SPA?1001?2010?03785 TeL?Fax?+86?10?62562563 obyInstiotfuSteoftwaCrhe?itnAheecseadoefmSyciencreisg?hrAtelslerved},
  Author                   = {????????????????????? },
  Year                     = {9825},

  Abstract                 = {?aTphepirseseanntasutomattesitcimnegthod?DAIDTau(tdoymnaitanitmceigcer-overflow detecatintodensting)f?ifnodrintgeogevrerfflatoabwlugisnbinacroyde?DCAaIltlDhToroutgeshtlhybeinary
},
  Doi                      = {rn?gE-mail?superligen@Igmail?com},
  File                     = {:home/ccc/github/literature/article/????????????????????.pdf:PDF},
  Review                   = {ISS1N000?9825R?CUOXDEUNEW E�mail?jos@iscas?ac?cn JouronfaSloftware?"C01?21?N2o0?120??FPePb?r1ua7r9y-193 hap???www?jos?org?cn doi?10?3724?SPA?1001?2010?03785 TeL?Fax?+86?10?62562563 obyInstiotfuSteoftwaCrhe?itnAheecseadoefmSyciencreisg?hrAtelslerved?
?????????????????????
??????+??????
(??????????????????410073)
High?-Trusted�-SoftAwaurteo�?mOTaretisietncfitoenIrdgnteOgveerrfBluogws
LUXi?ChenGg?eLnI+?KLaUi?ZHYAinNgG
(SchooflComputer?NationalUniversityofDefenseTechnology?Changsha410073?China)
+Correspaounthdoirn?gE-mail?superligen@Igmail?com
LuXC?LGi?Llu(?ZhaYn?gHigh-Trusted-Sofatuwtaormeta?etOsirtcifnoegirnntteedgoevrerfbluogsw?
JouronfaSloftware?2010?21(2)?179?193?http???www?jos?org?cn?1000�9825?3785?htm
Abstractp?aTphepirseseanntasutomattesitcimnegthod?DAIDTau(tdoymnaitanitmceigcer-overflow detecatintodensting)f?ifnodrintgeogevrerfflatoabwlugisnbinacroyde?DCAaIltlDhToroutgeshtlhybeinary
codaendautomatificnaudlnlkynionwtneogevrerfbluogwsithoneuctesskarniolwyinsgy?mebthoabllesi?sIt
formaplrloyvientdhipsaptehraDtAIcDaTntheoretidceatlealc1yt1thehigh-riinstkeogevrerfbluogwsitnho
falpsoesitaivnednsofalsne gativeasd?dIintionbalu?gfasinnybdyDAIcDaTbnereplayedde?mTonsthreate
effectioveftnhietshseory,InhtaHbsueneitnmeprlementhaesfd?oIut4nndehwigrhisiknteogveerrfbluogiwsn
thelatersetleaosfetshreheigh-traupspteldicatiMoincsr(otsWwoIofNtsServiicneWsind2o0w0sa0nd2003
Server,HBiIanisdtuManetssagteers)teibnaygcfho2r4hours?Tohftrheeebseugasllaorwbiucaordeyexecution
andhavreeceicvoendfivrumlenderabinluimtbiesrs?CVE?2009-192f3r?oCMmViEc�r2oSs0eo0cf9ut-r1i9ty24 RespoCnesnetaenrCdVE�2008?f6r4oB4ma4idu?
Kewyords?integoevrerflow?ionvteergfvelruoiwnerabili?a?udtyonmaatmetisictc88geeneration?taint
analysis?seyxmebcoultiicon
??? ????????????????????????????????(dynaamuitocmatic integer-odveetrefclatoniwodtnesting???DAIDT)?????????????????????????????
?????????????????????????????????????????????????? ????????????????????????????????IntHonter?????IntHunter?3????
?????????(????Wind2o0w0s3?2S0e0r0ver?WINS????????????H?i?)BaiDu
?????24?????????4????????????3?????????????????????? ??????????CVE?2009?1923?CVE?2009?1924???????????????CVE?2008?6444?
???????????????????????????????????????
???????TP3l1 ??????A
�SuppobryttehNdeatioHniaglh�TReeesheaarncDdheveloPplmaoenfntChuindaGeraNnot?2007AA010301(???????
????(863))N?atthieoBnaaslRiecseaPrrcohgorfaCmhuindaGerraNnot?2005CB321801(????????????(973)) Recei2v0e0d9-06?15?2R0e0v9i?s0e9�d11?A2c0c0e9p�t1e2d?07
 ????

}
}

@InProceedings{,
  Title                    = {????ISS1N000?9825R?ICo?DUEENW E-mail?jos@iscas?ac?cn ????w???5?7?�W??20l?0?2)2?(21438?24503?[3d7o2j4P???Js?l01 01?210?l03888? bllp???wwwjos?o??cn @??????????????? Tel?Fax},
  Author                   = {????????????????????? },
  Year                     = {9825},

  Doi                      = {ncretahseaeccuroafcoymp?asnoadnnti-ja?c?anpgabil1i0?0tpahsree},
  File                     = {:home/ccc/github/literature/article/????????????????????.pdf:PDF},
  Review                   = {????ISS1N000?9825R?ICo?DUEENW E-mail?jos@iscas?ac?cn ????w???5?7?�W??20l?0?2)2?(21438?24503?[3d7o2j4P???Js?l01 01?210?l03888? bllp???wwwjos?o??cn @??????????????? Tel?Fax?+86?10?62562563
?????????????????????
??1?3+????1????1????1?2
1(???????????????????????? lool90)
2(???????????????????????100049)
3(??????????????????? 100190)
DependencMya-lBwaSasiremeidIaCr0imtyparMiestohnOd
YANYiGl?3+?SUPu?RuilY? INLiGng?Y?1F?ENDeGng?Gu01?2
1(stateKeyLaboratsoercyuroiftI)n?f?oImnastitoInlteofso?are?TheChin1e0s0e1A90c?acdheimnyao)fsciences?Be?ing 2(staKteLy?oratoofIrnyfo?mSteiocn?ty??audunaitverscih?iTnhAecseadoefsmcyiences?Bleoq0i0?4g9?China)
3(NatiEongalineR?cnsge?CcehntfeorrInfo?Saectlilornity?1B0e0?1in9g0?China)
+co?espoanutdhionr?gE?mail?y?gyi@is?iscas???cn
Ya?Yg?SPuR?YiLnYg?FeDnGg?Dependenmcayl-wBsairsmieldacroimtpyariso?method???bH??? ????20ll?22(10)?2438?2453?ht????www?jos?org?cn?1000?9825?3888?h? AbstracMta?lwsairmeilacroitmyparisiosnoenftllbeasiwcoriknsmalwanraelysdiest?edction?Presently?
mosstimilari?commeptahrnoi?edsamostnalwasCreFoGrbehavsieoqruences?Maluwsaeorbmes?ctaetrison? packerost?hmederaIolfstechnitoqcuoenfuse?adsiitmiiolnc?aotlmyparmiestohnods?paTphpeirsopoases neawppmianicdhenti?tlhisenigmilarbietitewsmeaelnwsaarmeples?rwehloiynchontrdoelpendednactea?d
d?endence?Fidrsytn??tithaceillatnalyissipscIfb?toeodbtacionn?0d1ependreenlcaetiaonndsata dependreenlcaetions?cNoenxttrd?oealpendgern?chaendatdaependgernacpaerheconstIucted?Similarity
infomaitsoibotnaibnyecdompatrhiensge?t7y0peosfgraph?oIrndetortal?efulladvanotfatgheinherent behavoifmoarlicicoudsaenstdoincretahseaeccuroafcoymp?asnoadnnti-ja?c?anpgabil1i0?0tpahsree
recuetdh?nedlbbissrhemobvyemdeaonftshdeependgernacprhee?processirnegd?uwtchhiececsohmplexi? ofthesimil?cto)rmparailsgoondatnhdimprotvhepserfornolfathnecaelgorithpmr?oTphoepsreodto?pe systheamsbeeanppliteowdilmdalwacrolelectiorness?uTlshthseotwhattheaccllroaftchyemethod?d
comparciaspaobnilainthiaeVsaenobViaoduvsantage?
Kewyords?malware?sicmoimlaprairtiyson?dynamicp?raolpyasgiast?itoanint
??? ?????????????????????????????????????????????
??????????????????????????????????????????????????
?????????????????????????????????????????????????? ??????????????????????????????????????????????????? ???????????????????????????????????????????????????
�?????????????(60703076)????????????(863)(2007AAOIz45l?2009?OIZ435) ?????20lO?01-08??????2010?03?30??????20lO?05-14
????

}
}

@InProceedings{,
  Title                    = {???? ISSN 1000-9825, CODEN RUXUEW E-mail: jos@iscas.ac.cn Journal of Software,2015,26(2):348?363 [doi: 10.13328/j.cnki.jos.004786] http://www.jos.org.cn �??????????????. Tel},
  Author                   = {??????????????????????? and ??? and ??? and ??? and ??? and 1(????????????????(???????????) and ?? ?? 214083) and 2(???????????????(????) and ?? ?? 210023) and ????: ??? and E-mail: ganshuitao@gmail.com},
  Year                     = {1000},

  Abstract                 = {This article proposes a clone detection method based on a program characteristic metrics. Though analyzing the syntax and semantic characteristics of vulnerabilities, this detection method abstracts certain key nodes which describe different forms of vulnerability type from syntax parser tree, and expands four basic types of code clone to auxiliary classes. The characteristic metrics of the code then is finalized by obtaining the number of key nodes which are calculated via scanning corresponding code segment in the syntax parser tree. The clone detection based on a characteristic metrics creates basic knowledge base by extracting partial instances of open vulnerability database, and precisely locates the vulnerability codes by performing cluster calculation on the same codes responding to multiple types of code clone. Comparing with the detection method based on single characteristic vector, the proposed method produces more precise description about vulnerability. This detection method also offers a remedy to the drawbacks of formal detection method on its vulnerability type covering ability. Nine vulnerabilities are detected in an android-kernel system test. Testing on software of different code sizes shows that the performance of this method is linear with the size of the code. Key words: vulnerabilitydetection; codeclone; syntax parser tree; metrics of characteristics 
},
  File                     = {:home/ccc/github/literature/article/??????????????????????.pdf:PDF},
  Review                   = {???? ISSN 1000-9825, CODEN RUXUEW E-mail: jos@iscas.ac.cn Journal of Software,2015,26(2):348?363 [doi: 10.13328/j.cnki.jos.004786] http://www.jos.org.cn �??????????????. Tel: +86-10-62562563 
 
??????????????????????? ??? 1, ??? 1, ??? 1, ??? 2 1(????????????????(???????????),?? ?? 214083) 2(???????????????(????),?? ?? 210023) ????: ???, E-mail: ganshuitao@gmail.com 
? ?: ??????????????????????.?????,???????????????.? ????????????????,???????????????????????????,? 4 ??? ????????????,????????????????????????,?????????.??? ???????????????????,????????????????????,????????? ???????????.????????????????,?????????????,??????,? ?????????????????????????.?? android-kernel????????? 9????.? ???????????????,????????????????????. ???: ?????;????;?????;???? ??????: TP311 
??????: ??? ,???,??? ,???.??????????????????????.????,2015,26(2): 348?363. http://www.jos.org.cn/1000-9825/4786.htm ??????: Gan ST, Qin XJ, Chen ZN, Wang LZ. Software vulnerability code clone detection method based on characteristic metrics. Ruan Jian Xue Bao/Journal of Software, 2015,26(2):348?363 (in Chinese). http://www.jos.org.cn/1000-9825/4786.htm 
Software Vulnerability Code Clone Detection Method Based on Characteristic Metrics 
GAN Shui-Tao1, QIN Xiao-Jun1, CHEN Zuo-Ning1, WANG Lin-Zhang2 
1(State Key Laboratory of Mathematical Engineering and Advanced Computing (Jiangnan Institute of Computing Technique), Wuxi 214083, China) 
2(State Key Laboratory for Novel Software Technology (Nanjing University), Nanjing 210023, China) 
Abstract: This article proposes a clone detection method based on a program characteristic metrics. Though analyzing the syntax and semantic characteristics of vulnerabilities, this detection method abstracts certain key nodes which describe different forms of vulnerability type from syntax parser tree, and expands four basic types of code clone to auxiliary classes. The characteristic metrics of the code then is finalized by obtaining the number of key nodes which are calculated via scanning corresponding code segment in the syntax parser tree. The clone detection based on a characteristic metrics creates basic knowledge base by extracting partial instances of open vulnerability database, and precisely locates the vulnerability codes by performing cluster calculation on the same codes responding to multiple types of code clone. Comparing with the detection method based on single characteristic vector, the proposed method produces more precise description about vulnerability. This detection method also offers a remedy to the drawbacks of formal detection method on its vulnerability type covering ability. Nine vulnerabilities are detected in an android-kernel system test. Testing on software of different code sizes shows that the performance of this method is linear with the size of the code. Key words: vulnerabilitydetection; codeclone; syntax parser tree; metrics of characteristics 
 ? ????: ????????(91318301, 61170066, 6147179) 
 ????: 2014-07-09; ????: 2014-10-31; ????: 2014-11-26 
 

}
}

@InProceedings{,
  Title                    = {??????????????????? ??},
  Author                   = {??????????????????????????????????????? and ???? Web and of Science and ????????????Refine???????????? and ??????????????????? },
  Year                     = {2007},

  File                     = {:home/ccc/github/literature/article/??????Web_of_Science?SCI??.pdf:PDF},
  Review                   = {??????????????????? ??? 
??????????????????????????????????????? ???? Web of Science ????????????Refine???????????? ??????????????????? 
1??? Web of Science ??????? ????www.isiknowledge.com, ??ISI Web of Knowledge??? ??Web of Science????(?? ???WOK4.0 ????)? ????????? 2007 ??????????????????-Giant Magnetoresistance?????? -MATERIALS SCIENCE?????? 
 
2?????-Refine ??????????????????-Refine ????????????????????????? ??????????? Subject Areas ?????????????? 

}
}

@InProceedings{,
  Title                    = {????ISS1N000?9825R?CUOXDEUNEW E�mail?jos@iseas?ae?cn JournalofSoftware?2012?23(2)?378?393?doi?10?3724?SP?J?1001?2012h?t0t3p?9?5?3w?ww?jos?org?cn o??????????????? Tel?Fax},
  Author                   = {????????????????????� },
  Year                     = {9825},

  Abstract                 = {?aTphepirsopoasesesmantic?abpapsreodtaocmhalwabreheavisoirganlateuxrteracatniodn
},
  Doi                      = {r?nEg�mail?wangrui@is?iscas????},
  File                     = {:home/ccc/github/literature/article/????????????????????.pdf:PDF},
  Review                   = {????ISS1N000?9825R?CUOXDEUNEW E�mail?jos@iseas?ae?cn JournalofSoftware?2012?23(2)?378?393?doi?10?3724?SP?J?1001?2012h?t0t3p?9?5?3w?ww?jos?org?cn o??????????????? Tel?Fax-+86?10?62562563
????????????????????�
??1,2+9???13???3????3
1(???????????? 100049)
2(???????????(????????????)'j1E0?0029)
3(?????????????100190)
SemanticMs-aBiawsBaerdheavSiogrnaEtxutrreacatniDdoentecMtieotnhod
WANRuGil?2+?DFeEnNgG?Guol??Y?iY3A?NPGSuU-Rui3
1(GradUunaitveersiCthy?iTnhAeecseadoefmSyciences1?0B0e0U4i9n?gChina)
2(StKateeLyaboraotfoIrnyforSmeactuirointy(IonfsItniftoutreEmnagtiinoeneriCnhgi?nTAehsceeadoefmScyiences)1?0B0e0i2j9?ing
China)
3(InstiotfSuotfetwarCeh?iTnhAecseadoefSmcyiences?1Be0i0j1i9n0g?China) +Correspaoutnhdoir?nEg�mail?wangrui@is?iscas????
WanRgFenDgG?YaYn?gSuPR?Semanticmsa?iBwasaberdheavsiiogrnateuxrteracatnidoentection method?JofuSronftawlare,2012,23(2)?378-393?http???wwwdos?org?cn?1000?9825?3953?htm
Abstractp?aTphepirsopoasesesmantic?abpapsreodtaocmhalwabreheavisoirganlateuxrteracatniodn
detectiona?pTphrioseaxcthracrtistimcallwabreheaviaoswreslalsdependeanmcioentshgesbehaviors?
integraitnisntgructiotani-anltneavleylasnibdsehaviors-elmeavnetalniaclsysis?Tahceqnu?iartnetsi�interference
malwaberheavsiiogrnatusriensagnti-obfuesncgatitinooeindentsifeymanitrircelevaanndscemantically
equivalence?pFruorttohtseyrps,etabeamsoendthissignaetxutrreacatnidoentectaipopnroisadcehvelaopnedd evaluabtymeudltimplaelwasarmeples?Experresiumlhetansvtdeaelmonsttrhatheemdalwsairgenatures extracstheodgwooadbilittoayntoibfuscatnidtohnedetectbiaosneodnthesseisgnatcuoruelsrdecognize malwvaarreiaenftfsectively?
Kewyords?malware?semantiscisg?nbaethxurtaervaicotriond?emtaelcwtairoen
??? ?????????????????????????????????????????????
???????????????????????????????????????????????????
???????????????????????????????????????????????????
??????????????????????????????????????????????????? ?????????????????????
???? ?????????????????????
???????TP309 ??????A
�?????????????(60703076?61073179)????????????(863)(2007AA012451?2009AA012435)
?????2010?04?12??????2010?09-10??????2010?10?II
????

}
}

@InProceedings{,
  Title                    = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jos@iiscas?ac?cn JournalofSoft?aye?2011,22(3)?495-508?doi?10?3724?SELl001?201ht1t?p0?3??7w5w1w??jos?org?cn @??????????????? Tel?Fax},
  Author                   = {??Markov???????????????? },
  Year                     = {9825},

  Doi                      = {r?nEg?mail?jz?ang@zmail?ustc?edu?en},
  File                     = {:home/ccc/github/literature/article/??Markov???????????????.pdf:PDF},
  Review                   = {????ISS1N000?9825R?CUOXDEUNEW E-mail?jos@iiscas?ac?cn JournalofSoft?aye?2011,22(3)?495-508?doi?10?3724?SELl001?201ht1t?p0?3??7w5w1w??jos?org?cn @??????????????? Tel?Fax?+86?10?62562563
??Markov????????????????
??+????????????
(?????????????????230027)
NetwSoerckurSityuaAtiwoanreAnpepsrsoBacsohendMarkGoavmMeodel
ZHAYNoGng+?XTiaAoN??BinX?iCaUoI--LHiong,-X?ISheng
(DeparotfmAauattomation?UonfSicvieeransnciTdeteychnoolfCohgiyna?H2e3f0e0i27?Chimf)
+Correspaoutnhdoir?nEg?mail?jz?ang@zmail?ustc?edu?en
ZhanY?gTaXnB?CXuLi?XHiS?NetsweocrukrsiittyuataiwoanreanepspsrobaacsheodnMarkgoavme
model?JooufSronfatlware,2011?22(3)?495-508?http???www?jos?org??cn?1000--9825?3751?htm
Abstracatn?aTlyothzienfluoenfpcreopagaotnainoentwsoyrsktaenmadccuraetveallyusaytsetsecmurity,
thipsapepropoasnaepsprotaoicmhprtohveaewareonfensestwsoerckurityo,nbthaeMseadrkGoavmMeodel
IIMGM)?aTphpirsogaacihnastanddaartodafassets?threvautlsn?earnadbivliiaftuiseiasnvgarieotfsyystem
securdiattycaollebctymeudlti??senseorvse?rtFhyorreat?a?intalytzheersuloefpropagatnibdouniMatshreat
propaganteitownork(TuPsNin)tgh?eGBaymTeheotroaynalythzbeehaviooftrhsreats?administrators?and
ordinuasreyrse?istabliatshreepselayMeGrM?oIrndteormaktehevaluaptrioocnearsesal-otpiemreation?it
optimithzreeslaatledgorithMm?GTchMaerdlynamiecvaallluysaytsetsemcursiittyuatainopdnrovithdbeest
reinforcsecmheenfmtoratheadministraetvoarl?uTahotefiaosnpecinfiectwionrdkicattheatsheapproias ch suitafbolarereanletweonrvkironmentth?e?vaanlduarteisouinlsptreciasnedfficienrte?iTnhfeorsccemhenmta Caenffectcivuertlbhyepropagoaftthiroenats?
Kewyords?nestecwuorsriitktyuataiwoanrenessp?rtohrpeaagtnateitownork?gMarmmkeoodevlI
??? ???????????????????????????????????????????????
??Markov???????????????????????????????????????????? ???????????????????????????????????????????????????? ??????????????Markov???????????????????????????????Markov
?????????????????????????????????????????????????
Markov?????????????????????????????????????????? ???? ?????????????????Markov????
???????TP393 ??????A
??????????????????????????????????????????????
????????????????????????????????????????????????
�????????????????(863)(2006?Olz449)?????????????[|(20070420738) ?????2009?06?-24??????2009?10-?10
????

}
}

@InProceedings{,
  Title                    = {????????????????????},
  Author                   = {?2012?? },
  Year                     = {2012},

  File                     = {:home/ccc/github/literature/article/?????????????????2012.pdf:PDF},
  Review                   = { 
 
 
 
 
 
???????????????????? 
?2012?? 
 
 
 
 
 
 
??????? 

}
}

@InProceedings{,
  Title                    = {????? VM,SE???? Handle},
  Author                   = {/************************************** and /* ??:???? and /* ??:http://cnblogs.com/bjblcracked and /* ?? and 19:01 and /************************************** },
  Year                     = {2014},

  File                     = {:home/ccc/github/literature/article/?????VM,SE????Handle.pdf:PDF},
  Review                   = {????? VM,SE???? Handle
/************************************** /* ??:???? /* ??:http://cnblogs.com/bjblcracked /* ??:2014-12-08 19:01 /**************************************
?????????????????????????!
??????????,?????????? Handle???. ????????????? :) ?????????????? ????????.????????,??????????,?? ????????,????? :(
VMProtect?Handle????, ???EP??, ????? ?????Handle. Vmprotect???????????, ?? ????? 200??????? Handle.
???????????? Vmprotect Handle. ???? OD?? Vmprotect.exe, ????? Vmprotect 2.13.5 ??????.

}
}

@Article{[Anonymous]2015,
  Title                    = {2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE). Proceedings},
  Author                   = {[Anonymous]},
  Journal                  = {2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE). Proceedings},
  Year                     = {2015},
  Pages                    = {558 pp--558},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The following topics are dealt with: vibration signal based monitoring; mechanical microdrilling; rule based inflectional urdu stemmer usal; rule based derivational urdu stemmer usal; fuzzy logic controller; heat exchanger temperature process; text dependent speaker recognition; MFCC; SBC; multikeyword based sorted querying; encrypted cloud data; communication understandability enhancement; GSD; parsing; input power quality; switched reluctance motor drive; externally powered upper limb prostheses; program test data generation; launch vehicle optimal trajectory generation; misalignment fault detection; induction motors; current signature analysis; vibration signature analysis; wind power plants; vortex induced vibration; mechanical structure modal analysis; machining parameter optimization; diesel engines; high speed nonvolatile NEMS memory devices; image fusion; RGB color space; LUV color space; offline English character recognition; human skin detection; tumor boundary extraction; MR images; OdiaBraille; text transcription; shadow detection; YIQ color models; color aerial images; moving object segmentation; image data deduplication; iris recognition; two-stage series connected thermoelectric generator; education information system; cyclone separator CFD simulation; imperfect debugging; vulnerability discovery model; stochastic differential equation; cloud data access; attribute based encryption; agile SCRUM framework; PID controller optimisation; hybrid watermarking technique; privacy preservation; vertical partitioned medical database; power amplifier; software reliability growth modeling; cochlear implantation; cellular towers; feedforward neural networks; MBSOM; agent based semantic ontology matching; phonetic word identification; test case selection; MANET security issues; online movie data classification; modified LEACH protocol; mobile ad hoc networks; virtual machine introspection; task scheduling; cluster computing; image compression; green cloud computing; critical health data transmission system; irreversible regenerative Brayton cycle; task set based adaptive round robin scheduling; database security; heterogeneous online social networks; aspect oriented systems; IP network; MPLS network; DBSCAN algorithm; VANET; self-organizing feature map; image segmentation; enzyme classification; wireless sensor networks; energy smart routing protocol; adaptive gateway discovery mechanism; heuristic job scheduling; AODV based congestion control protocol; expert system; home appliances; relay node based heer protocol; data storage; TORA security; data aggregation; low energy adaptive stable energy efficient protocol; fuzzy logic based clustering algorithm; hybrid evolutionary MPLS tunneling algorithm; English mobile teaching; eigenvector centrality; genetic algorithms; data mining; heart disease prediction; lossless data compression; reconfigurable ring resonator; triple band stacked patch antenna; energy based spectrum sensing; cognitive radio networks; FPGA; knowledge representation; multiband microstrip antenna; Web indexing; HTML priority system; Web cache recommender system; e-learning; IT skill learning for visual impaired; user review data analysis; software up-gradation model; software testing; Web crawlers; secret key watermarking; WAV audio file; SRM drive; ZETA converter; fractional PID tuning; medical image reconstruction; speech recognition system; video authentication; digital forensics; content based image retrieval; image classification; hybrid wavelet transform; facial feature extraction; RBSD adder; smart home environment; generalized discrete time model; We Chat marketing; foreign language learning; carbon dioxide emission mitigation; power generation; smartphone storage enhancement; and virtualization.},
  Bn                       = {978-1-4799-8433-6},
  Cl                       = {Noida, India},
  Ct                       = {2015 International Conference on Futuristic Trends on ComputationalEOLEOLAnalysis and Knowledge Management (ABLAZE)},
  Cy                       = {25-27 Feb. 2015},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15293145},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{,
  Title                    = {???????? ISS1N000?12319l??C1N7771TP},
  Author                   = {JournoafClompuRteesreaarncDdhevelopment 48(6)},
  Year                     = {1231},

  File                     = {:home/ccc/github/literature/article/?????????.pdf:PDF},
  Review                   = {???????? ISS1N000?12319l??C1N7771TP
JournoafClompuRteesreaarncDdhevelopment 48(6)?923?9133?201
?????????
???h3? ???1 ???1?3? ???2 1(??????????100084)
2(??????????????? 100084)
3(????????????????100084)
4(????????????(????)??100084)
(chaokun@tsinghua?edu?en)
SurvoefSyoftwTaarmepPerorofTiencghnique
WanCghaokunl?3J?u?nFnuin91J?iWanamnignl�3Y?u?Zahnidwei2
1(SchoofoSloftware?TUsninvgerhsuiaty?1B0e0i0j8i4n)g
2(DeparotfCmoemnptuStceireamnFTdechnology?UTnsiivnegrhsiutay?1B0e0i0j8i4n)g
3(TsinNgahtuiaoLnaabloraftoorIrnyformaStcieoancneTdechnology1?0B0e0i8j4i)ng
4(KLeayboraftoorhrfyormaStyisotSneemc?urity(UTnsivnegrhsuiaty)o?MfEidnuicsattriyon?1B0e0i0j8i4n)g
AbstraWcittthhewiduesoefcomputteecrhnologiesh?asbsoefctowimanredeispeninsoaubrdlaeily
lifeandthecorresposnedciunrigstsyuienssoftwsayrseteamrsemoraendmorperominent?
Especialtloyd?ehsoiawgpnractipcraoltecstcihoneismqeuitiemportaannhdtagsresaitgnifiicnance
thesoftwraerseeaarncdheveloipnmdeunsttrioenseo?ftAhsekeymethfoodrssoftwparroetection?
thesoftwtaarmepperrooftiencghniaqtutreamctuscahttentfiroonrmesearcbhoetrahtshomaend
abroianrdeceynetars?tSeuchniaqiumaestpreventthiecnrgitipcarlogrinafmormaftriotomhne
unauthomroidziefdicatniduosness?alnsdaotgenerathienrgespononsceteshetamperising
detected?Prientsheinsptaepdeisrarevioefwtheanalyosfissoftwtaarmepperroofinogu?rIn
discussion?dtiafmfpeperrreonotfmientghoadrecslassiifniettdowocategoriestsa?ttihcaemper
proofmientghobadseodnthecodoebfuscastwieolanlsthedynatmaimcpperroofmientghboadsed
ontheverification?reasdpvoannsteaa?ngTdehisesadvantagesa?nsdwtereankgnteohsfstsheesse
methaordpesresenintdedtailt?hIennd?thrtohuesguhrvoeftyhesteampperrooftiencghniques?
asummaisorbytaiwnheidicnhclundoetosnltyhecharacterisatlistcohse?bxuitstpirnogblaenmds
futuwroerokfthesoftwtaarmepperrooftiencghnique?
Kewyordtsampperroofing?sporfottweacrteiont?asmtappterirocoftiencghnique?tdaymnpaemric
prooftiencghnique?verificationb?fruesscpaotnisoen?code
???????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????
?????2010-01?26??????2010?08?11
???????fj???????(90718010?60803016)??????i?????????????(2007CB310802)??????i????
??????????(2008AA042301)????????????(2010ZX01042?002?002?01)??????????????
?(?)????????
????

}
}

@TechReport{AbalBrabrandWa˛sowski2014,
  Title                    = {40 Variability Bugs in the Linux Kernel A Qualitative Study},
  Author                   = {Iago Abal and Claus Brabrand and Andrzej Wa˛sowski},
  Year                     = {2014},
  Number                   = {TR-2014-180},

  File                     = {:article\\40 Variability Bugs in the Linux Kernel.pdf:PDF},
  Groups                   = {binarary vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {对50个Linux内核漏洞进行了归类分析。陌生的术语太多，分析内容基本没看懂，什么叫Variability？。}
}

@Article{AbstractHovemeyerPugh2004,
  Title                    = {Finding Bugs is Easy},
  Author                   = {(Extended Abstract) and David Hovemeyer and William Pugh},
  Year                     = {2004},

  Abstract                 = {// Eclipse 3.0, // org.eclipse.jdt.internal.ui.compare,
},
  File                     = {:article\\Finding bugs is easy.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Java}
}

@Article{AlhazmiMalaiyaRay2007,
  Title                    = {Measuring, analyzing and predicting security vulnerabilities in software systems},
  Author                   = {Alhazmi, O.H. and Malaiya, Y.K. and Ray, I.},
  Journal                  = {Computers \& Security},
  Year                     = {2007},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {219–228},
  Volume                   = {26},

  Doi                      = {10.1016/j.cose.2006.10.002},
  File                     = {:home/ccc/github/literature/article/Measuring, analyzing and predicting security vulnerabilities in software systems.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0167-4048},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.cose.2006.10.002}
}

@Article{AlseiariAung2015,
  Title                    = {Real-time anomaly-based distributed intrusion detection systems for advanced metering Infrastructure utilizing stream data mining},
  Author                   = {Alseiari, F. A. A. and Aung, Z.},
  Journal                  = {2015 International Conference on Smart Grid and Clean Energy Technologies (ICSGCE). Proceedings},
  Year                     = {2015},
  Pages                    = {148--53},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The advanced Metering Infrastructure (AMI) is one of the core components of smart grids' architecture. As AMI components are connected through mesh networks in a distributed mechanism, new vulnerabilities will be exploited by grid's attackers who intentionally interfere with network's communication system and steal customer data. As a result, identifying distributed security solutions to maintain the confidentiality, integrity, and availability of AMI devices' traffic is an essential requirement that needs to be taken into account. This paper proposes a real-time distributed intrusion detection system (DIDS) for the AMI infrastructure that utilizes stream data mining techniques and a multi-layer implementation approach. Using unsupervised online clustering techniques, the anomaly-based DIDS monitors the data flow in the AMI and distinguish if there are anomalous traffics. By comparing between online and offline clustering techniques, the experimental results showed that online clustering ldquoMini-Batch K-meansrdquo were successfully able to suit the architecture requirements by giving high detection rate and low false positive rates.},
  Bn                       = {978-1-4673-8734-7},
  Cl                       = {Offenburg, Germany},
  Ct                       = {2015 International Conference on Smart Grid and Clean EnergyEOLEOLTechnologies (ICSGCE)},
  Cy                       = {20-23 Oct. 2015},
  Doi                      = {10.1109/ICSGCE.2015.7454287},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15935991},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{AlshammariFidgeCorney2016,
  Title                    = {Developing Secure Systems: A Comparative Study of Existing Methodologies},
  Author                   = {Alshammari, Bandar M. and Fidge, Colin J. and Corney, Diane},
  Journal                  = {Lecture Notes on Software Engineering},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {139–146},
  Volume                   = {4},

  Doi                      = {10.7763/lnse.2016.v4.239},
  File                     = {:home/ccc/github/literature/article/Developing Secure Systems\: A Comparative Study of.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {2301-3559},
  Publisher                = {EJournal Publishing},
  Url                      = {http://dx.doi.org/10.7763/LNSE.2016.V4.239}
}

@InProceedings{AnalysisExploits2004,
  Title                    = {Dynamic Taint Analysis for Automatic Detection},
  Author                   = {Analysis and Signature Generation of Exploits},
  Year                     = {2004},

  File                     = {:article\\Dynamic Taint Analysis for Automatic Detection Analysis, and Signature Generation of Exploits of Commority software,.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software James Newsome Dawn Song May 2004 Last updated July 2005 CMU-CS-04-140 School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213

}
}

@TechReport{Analysis1508,
  Title                    = {Towards Vulnerability Discovery Using Extended},
  Author                   = {Compile-time Analysis},
  Year                     = {1508},
  Number                   = {increased},

  File                     = {:article\\Towards Vulnerability Discovery Using Extended Compile-time Analysis.pdf:PDF},
  Review                   = {Towards Vulnerability Discovery Using Extended Compile-time Analysis Bhargava Shastry bshastry@sec.t-labs.tu-berlin.de Security in Telecommunications Telekom Innovation Labs Technische Universita¨t Berlin August 20, 2015 Abstract Exploitable vulnerabilities, are often, an outcome of semantic bugs in a program’s software implementation. Since analyzing large codebases for detecting semantic bugs is hard, there is a reliance on software testing for uncovering defects. Compiler-driven program analyzers such as the Clang Static Analyzer are promising, but a local outlook limits their potential. In this paper, we propose an extended compile-time analysis frame- work for detecting potentially security-critical defects in object-oriented code, and evaluate the proposal against large codebases. Our framework, that we call Me´lange, non-intrusively retrofits a two-stage analysis pipeline into a codebase’s build system. Me´lange complements software testing: It empowers developers to fix defects during active software development. Our analyzer scales up to large codebases and has, thus far, reported known vulnerabilities in Chromium source code. 1 Introduction Finding semantic bugs in large and constantly evolving codebases is challenging. Software testing (unit-tests, fuzzing etc.) is effective at uncovering bugs, and remains to be used in production environments. However, for large programs, the state space of program inputs is too vast to have a reasonable assurance that testing uncovered a majority of program paths. Program analysis offers a complementary approach for bug discovery. Ever since Lint was written in the late 70s, technical advances together with increased availability of computational power have made semantic (deep) program analysis possible. In contrast to syntactic (shallow) analysis, deep program analysis attempts to capture the semantics of a program short of running the program with concrete inputs. 1 arXiv:1508.04627v1 [cs.CR] 19 Aug 2015

}
}

@InProceedings{AnalysisSecurity1540,
  Title                    = {Building Security In Editor: Gary McGraw, gem@cigital.com},
  Author                   = {Static Analysis and for Security},
  Booktitle                = {32 PUBLISHED BY THE IEEE COMPUTER SOCIETY ■},
  Year                     = {1540},
  Publisher                = {IEEE},

  File                     = {:home/ccc/github/literature/article/Static Analysis for Security.pdf:PDF},
  Review                   = {Building Security In Editor: Gary McGraw, gem@cigital.com Static Analysis for Security ll software projects are guaranteed to have one ar- ties is complicated by the fact that A they often exist in hard-to-reachtifact in common—source code. Together with states or crop up in unusual circum-stances. Static analysis tools can peerarchitectural risk analysis,1 code review for secu- into more of a program’s dark cor-ners with less fuss than dynamicrity ranks very high on the list of software security analysis, which requires actually run- ning the code. Static analysis also has best practices (see Figure 1).2 Here, we’ll look at how to automate the potential to be applied before a program reaches a level of comple- BRIAN CHESS source-code security analysis with analysis is to identify many common tion at which testing can be mean- Fortify static analysis tools. coding problems automatically be- ingfully performed. Software Since ITS4’s release in early 2000 fore a program is released. (www.cigital.com/its4/), the idea of Static analysis tools examine the Aim for good, GARY detecting security problems through text of a program statically, without not perfect MCGRAW source code has come of age. ITS4 is attempting to execute it. Theoreti- Static analysis can’t solve all your secu- Cigital extremely simple—the tool basically cally, they can examine either a pro- rity problems. For starters, static scans through a file looking for syntac- gram’s source code or a compiled analysis tools look for a fixed set of tic matches based on several simple form of the program to equal bene- patterns, or rules, in the code. Al- “rules” that might indicate possible se- fit, although the problem of decod- though more advanced tools allow curity vulnerabilities (for example, use ing the latter can be difficult. We’ll new rules to be added over time, if a of strcpy() should be avoided). focus on source code analysis here rule hasn’t been written yet to find a Much better approaches exist. because that’s where the most ma- particular problem, the tool will never ture technology exists. find that problem. When it comes to Catching Manual auditing, a form of static security, what you don’t know is implementation analysis, is very time-consuming, likely to hurt you, so beware of any bugs early and to do it effectively, human code tool that says something like, “zero Programmers make little mistakes all auditors must first know what secu- defects found, your program is now the time—a missing semicolon here, rity vulnerabilities look like before secure.” The appropriate output is, an extra parenthesis there. Most of they can rigorously examine the “sorry, couldn’t find any more bugs.” the time, these gaffes are inconse- code. Static analysis tools compare A static analysis tool’s output still quential; the compiler notes the favorably to manual audits because requires human evaluation. There’s error, the programmer fixes the they’re faster, which means they can no way for a tool to know exactly code, and the development process evaluate programs much more fre- which problems are more or less im- continues. This quick cycle of feed- quently, and they encapsulate secu- portant to you automatically, so back and response stands in sharp rity knowledge in a way that doesn’t there’s no way to avoid trawling contrast to what happens with most require the tool operator to have the through the output and making a security vulnerabilities, which can same level of security expertise as a judgment call about which issues lie dormant (sometimes for years) human auditor. Just as a programmer should be fixed and which ones rep- before discovery. The longer a vul- can rely on a compiler to consistently resent an acceptable level of risk. nerability lies dormant, the more ex- enforce the finer points of language Knowledgeable people still need to pensive it can be to fix, and adding syntax, the operator of a good static get a program’s design right to avoid insult to injury, the programming analysis tool can successfully apply any flaws—although static analysis community has a long history of re- that tool without being aware of the tools can find bugs in the nitty-gritty peating the same security-related finer points of security bugs. details, they can’t critique design. mistakes. The promise of static Testing for security vulnerabili- Don’t expect any tool to tell you, “I 32 PUBLISHED BY THE IEEE COMPUTER SOCIETY ■ 1540-7993/04/$20.00 © 2004 IEEE ■ IEEE SECURITY & PRIVACY 

}
}

@Book{AssessmentIdentifying2007,
  Title                    = {The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities},
  Author                   = {Assessment and Identifying and},
  Year                     = {2007},

  File                     = {:article\\the art of software security assessment.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = { The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 

}
}

@Article{AustinHolmgreenWilliams2013,
  Title                    = {A comparison of the efficiency and effectiveness of vulnerability discovery techniques},
  Author                   = {Austin, Andrew and Holmgreen, Casper and Williams, Laurie},
  Journal                  = {Information and Software Technology},
  Year                     = {2013},

  Month                    = {Jul},
  Number                   = {7},
  Pages                    = {1279–1288},
  Volume                   = {55},

  Doi                      = {10.1016/j.infsof.2012.11.007},
  File                     = {:home/ccc/github/literature/article/-A comparison of the efficiency and effectiveness of vulnerability discovery techniques.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2012.11.007}
}

@Article{AustinWilliams2011,
  Title                    = {One Technique is Not Enough: A Comparison of Vulnerability Discovery Techniques},
  Author                   = {Austin, Andrew and Williams, Laurie},
  Journal                  = {2011 International Symposium on Empirical Software Engineering and Measurement},
  Year                     = {2011},

  Month                    = {Sep},

  Doi                      = {10.1109/esem.2011.18},
  File                     = {:home/ccc/github/literature/article/One Technique is Not Enough\: A Comparison of Vulnerability Discovery Techniques.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-0-7695-4604-9},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/ESEM.2011.18}
}

@Article{Bahamdain2015,
  Title                    = {Open Source Software (OSS) Quality Assurance: A Survey Paper},
  Author                   = {Bahamdain, Salem S.},
  Journal                  = {Procedia Computer Science},
  Year                     = {2015},
  Pages                    = {459鈥�464},
  Volume                   = {56},

  Doi                      = {10.1016/j.procs.2015.07.236},
  File                     = {:article\\Open Source Software (OSS) Quality Assurance A Survey Paper.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1877-0509},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.procs.2015.07.236}
}

@Article{BajracharyaOssherLopes2014,
  Title                    = {Sourcerer: An infrastructure for large-scale collection and analysis of open-source code},
  Author                   = {Bajracharya, Sushil and Ossher, Joel and Lopes, Cristina},
  Journal                  = {Science of Computer Programming},
  Year                     = {2014},

  Month                    = {Jan},
  Pages                    = {241鈥�259},
  Volume                   = {79},

  Doi                      = {10.1016/j.scico.2012.04.008},
  File                     = {:article\\Sourcerer An infrastructure for large-scale collection and analysis of open-source code.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0167-6423},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.scico.2012.04.008}
}

@Article{BakarKasirunSalleh2015,
  Title                    = {Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review},
  Author                   = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2015},

  Month                    = {Aug},
  Pages                    = {132–149},
  Volume                   = {106},

  Doi                      = {10.1016/j.jss.2015.05.006},
  File                     = {:home/ccc/github/literature/article/Feature extraction approaches from natural language requirements for reuse in software product lines\: A systematic literature review.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0164-1212},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.jss.2015.05.006}
}

@TechReport{BalzarottiCovaFelmetsgerEtAl2007,
  Title                    = {Saner: Composing Static and Dynamic Analysis to Validate Sanitization in Web Applications},
  Author                   = {Davide Balzarotti and Marco Cova and Vika Felmetsger and Nenad Jovanovic and Engin Kirda and Christopher Kruegel and Giovanni Vigna},
  Year                     = {2007},
  Number                   = {France},

  Abstract                 = {full-fledged, complex applications. Such applications (e.g., email readers, web portals, or e-commerce front-ends) are
},
  File                     = {:home/ccc/github/literature/article/Saner\: Composing Static and Dynamic Analysis to Validate Sanitization in Web Applications.pdf:PDF},
  Review                   = {Saner: Composing Static and Dynamic Analysis to Validate Sanitization in Web Applications Davide Balzarotti§, Marco Cova§, Vika Felmetsger§, Nenad Jovanovic∗, Engin Kirda¶, Christopher Kruegel§, and Giovanni Vigna§ §University of California, Santa Barbara {balzarot,marco,rusvika,chris,vigna}@cs.ucsb.edu ∗Secure Systems Lab ¶Institute Eurecom Technical University Vienna France enji@seclab.tuwien.ac.at engin.kirda@eurecom.fr Abstract full-fledged, complex applications. Such applications (e.g., email readers, web portals, or e-commerce front-ends) are Web applications are ubiquitous, perform mission- developed using a number of different technologies and critical tasks, and handle sensitive user data. Unfortu- frameworks, such as ASP.NET [21] or PHP [32]. Even nately, web applications are often implemented by devel- though these technologies provide a number of mecha- opers with limited security skills, and, as a result, they nisms to protect an application from attacks, the security contain vulnerabilities. Most of these vulnerabilities stem of web applications ultimately rests in the hands of the from the lack of input validation. That is, web applications programmers. Unfortunately, these programmers are of- use malicious input as part of a sensitive operation, with- ten under time-to-market pressure and not always aware of out having properly checked or sanitized the input values the available protection mechanisms and their correct us- prior to their use. age. As a result, web applications are riddled with security Past research on vulnerability analysis has mostly fo- flaws that can be exploited to circumvent authentication, cused on identifying cases in which a web application di- bypass authorization checks, or access sensitive user infor- rectly uses external input in critical operations. However, mation. A report published by Symantec in March 2007 little research has been performed to analyze the correct- states that, out of the 2,526 vulnerabilities that were docu- ness of the sanitization process. Thus, whenever a web ap- mented in the second half of 2006, 66% affected web ap- plication applies some sanitization routine to potentially plications [42]. malicious input, the vulnerability analysis assumes that the result is innocuous. Unfortunately, this might not be the One of the most common sources of vulnerabilities case, as the sanitization process itself could be incorrect is the lack of proper validation of the parameters that or incomplete. are passed by the client to the web application. In fact, In this paper, we present a novel approach to the analy- OWASP’s Top Ten Project, which lists the top ten sources sis of the sanitization process. More precisely, we combine of vulnerabilities in web applications, puts unvalidated in- static and dynamic analysis techniques to identify faulty put as the number one cause of vulnerabilities in web ap- sanitization procedures that can be bypassed by an at- plications [30]. Input validation is a generic security proce- tacker. We implemented our approach in a tool, called dure, where an application ensures that the input received Saner, and we applied it to a number of real-world ap- from an external source (e.g., a user) is valid and meaning- plications. Our results demonstrate that we were able to ful. For example, an application might check that the num- identify several novel vulnerabilities that stem from erro- ber of items to purchase, sent as part of a form submission, neous sanitization procedures. is actually provided as an integer value and not as a non- numeric string or a float. As another example, an applica- tion might need to ensure that the message submitted to a 1 Introduction bulletin board does not exceed a certain length or does notcontain JavaScript code. Also, a program typically has to enforce that arguments to database queries do not contain Web applications have evolved from simple CGI-based elements that alter the intended meaning of these queries, gateways that provide access to back-end databases into leading to SQL injection attacks. 1

}
}

@InProceedings{BarakGoldreichImpagliazzoEtAl9404,
  Title                    = {On the (Im)possibility of Obfuscating Programs},
  Author                   = {Boaz Barak and Oded Goldreich and Russell Impagliazzo and Steven Rudich},
  Year                     = {9404},

  File                     = {:article\\On the (Im)possibility of Obfuscating Programs.pdf:PDF},
  Groups                   = {software protection},
  Keywords                 = {cryptography, complexity theory, software protection, homomorphic encryption, Rice’s The- orem, software watermarking, pseudorandom functions, statistical zero knowledge},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {On the (Im)possibility of Obfuscating Programs∗ Boaz Barak† Oded Goldreich‡ Russell Impagliazzo§ Steven Rudich¶ Amit Sahai‖ Salil Vadhan∗∗ Ke Yang†† July 29, 2010 Abstract Informally, an obfuscator O is an (efficient, probabilistic) “compiler” that takes as input a program (or circuit) P and produces a new program O(P ) that has the same functionality as P yet is “unintelligible” in some sense. Obfuscators, if they exist, would have a wide variety of cryptographic and complexity-theoretic applications, ranging from software protection to homomorphic encryption to complexity-theoretic analogues of Rice’s theorem. Most of these applications are based on an interpretation of the “unintelligibility” condition in obfuscation as meaning that O(P ) is a “virtual black box,” in the sense that anything one can efficiently compute given O(P ), one could also efficiently compute given oracle access to P . In this work, we initiate a theoretical investigation of obfuscation. Our main result is that, even under very weak formalizations of the above intuition, obfuscation is impossible. We prove this by constructing a family of efficient programs P that are unobfuscatable in the sense that (a) given any efficient program P ′ that computes the same function as a program P ∈ P , the “source code” P can be efficiently reconstructed, yet (b) given oracle access to a (randomly selected) program P ∈ P , no efficient algorithm can reconstruct P (or even distinguish a certain bit in the code from random) except with negligible probability. We extend our impossibility result in a number of ways, including even obfuscators that (a) are not necessarily computable in polynomial time, (b) only approximately preserve the functionality, and (c) only need to work for very restricted models of computation (TC0). We also rule out several potential applications of obfuscators, by constructing “unobfuscatable” signature schemes, encryption schemes, and pseudorandom function families. Keywords: cryptography, complexity theory, software protection, homomorphic encryption, Rice’s The- orem, software watermarking, pseudorandom functions, statistical zero knowledge ∗A preliminary version of this paper appeared in CRYPTO’01 [BGI+]. †Department of Computer Science, Princeton University, NJ 08540. E-mail: boaz@cs.princeton.edu ‡Department of Computer Science, Weizmann Institute of Science, Rehovot, ISRAEL. E-mail: oded.goldreich@weizmann.ac.il §Department of Computer Science and Engineering, University of California, San Diego, La Jolla, CA 92093-0114. E-mail: russell@cs.ucsd.edu ¶Computer Science Department, Carnegie Mellon University, 5000 Forbes Ave. Pittsburgh, PA 15213. E-mail: rudich@cs.cmu.edu ‖Department of Computer Science, UCLA, Los Angeles, CA 90095. Email: sahai@cs.ucla.edu ∗∗School of Engineering and Applied Sciences and Center for Research on Computation and Society, Harvard University, 33 Oxford Street, Cambridge, MA 02138. E-mail: salil@seas.harvard.edu ††Google Inc., Moutain View, CA 94043. E-mail: yangke@gmail.com 1

}
}

@Article{BashahMatAliYaseenIbrahimShakhatrehSyazwanAbdullahEtAl2011,
  Title                    = {SQL-injection vulnerability scanning tool for automatic creation of SQL-injection attacks},
  Author                   = {Bashah Mat Ali, Abdul and Yaseen Ibrahim Shakhatreh, Ala鈥� and Syazwan Abdullah, Mohd and Alostad, Jasem},
  Journal                  = {Procedia Computer Science},
  Year                     = {2011},
  Pages                    = {453鈥�458},
  Volume                   = {3},

  Doi                      = {10.1016/j.procs.2010.12.076},
  File                     = {:article\\SQL-injection vulnerability scanning tool for automatic creation of SQL-injection attacks.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1877-0509},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.procs.2010.12.076}
}

@Article{BingjiangYujing2014,
  Title                    = {Research On Security Vulnerabilities Mining Method For Software On Android Platform},
  Author                   = {Gong Bingjiang and Tang Yujing},
  Journal                  = {Computer Applications and Software},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {1000-386X(2014)31:1<311:APTXRJ>2.0.TX;2-E},
  Volume                   = {31},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In order to reduce the privacy data leak problems of the Android system users,we put forward a vulnerability mining method aiming at the source code of the Android applications.On the basis of Android vulnerability database and permission-method set,the method adopts static analysis to obtain the algebraic expression of special permission vulnerability matrix of Android and the test case of vulnerability points,mutates the test cases based on vulnerability knowledge to obtain semi-efficient data,and uses stain injection and data flow analysis to mine Fuzzing.Through example analyses on 400 Android applications source code,the results show that the method can mine the conventional vulnerability and has distinct effect in mining the special permission information vulnerability of Android.The number of the test cases derived from using constraint analysis is less,and the pertinency of semi-efficient data derived from vulnerability knowledge is high.This method has high code coverage and precision as well.},
  Groups                   = {Code Mining},
  Sn                       = {1000-386X},
  Tc                       = {1},
  Ut                       = {CSCD:5037716},
  Z1                       = {Androidå¹³å°ä¸è½¯ä»¶å®å¨æ¼æ´æææ¹æ³ç ç©¶},
  Z2                       = {é¾ç³æ±EOLEOLåå®æ¬},
  Z3                       = {è®¡ç®æºåºç¨ä¸è½¯ä»¶},
  Z4                       = {ä¸ºäºåå°Androidç³»ç»ç¨æ·çéç§æ°æ®æ³é²é®é¢,æåºä¸ç§éå¯¹Androidåºç¨ç¨åºæºç çæ¼æ´æææ¹æ³ãè¯¥æ¹æ³å¨Androidæ¼æ´åºåæéæ¹æ³éEOLEOLåçåºç¡ä¸,éç¨éæåæå¾å°Androidç¹æçæéæ¼æ´ç©éµä»£æ°å¼åæ¼æ´ç¹å¤æµè¯ç¨ä¾,åºäºæ¼æ´ç¥è¯å¯¹æµè¯ç¨ä¾åå¼å¾å°åæææ°æ®,å©ç¨æ±¡ç¹æ³¨å¥åæ°EOLEOLæ®æµåæè¿è¡Fuzzingææãç»è¿å¯¹400ä¸ªAndroidåºç¨ç¨åºæºç è¿è¡å®ä¾åæ,ç»æè¡¨æè¯¥æ¹æ³ä¸ä»è½ææå¸¸è§æ¼æ´,èä¸å¨Androidç¹æEOLEOLçæéä¿¡æ¯æ¼æ´æææ¹é¢ææææ¾ãå©ç¨çº¦æåæå¾å°çæµè¯ç¨ä¾æ°éå°,èéè¿æ¼æ´ç¥è¯å¾å°çåæææ°æ®çéå¯¹æ§å¼º,å¹¶ä¸ä»£ç è¦ççåç²¾ç¡®åº¦è¾é«ã},
  Z8                       = {1},
  Z9                       = {2},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{BishopEngleHowardEtAl2012,
  Title                    = {A Taxonomy of Buffer Overflow Characteristics},
  Author                   = {Matt Bishop and Sophie Engle and Damien Howard and Sean Whalen},
  Journal                  = {{IEEE} Trans. Dependable and Secure Comput.},
  Year                     = {2012},

  Month                    = {may},
  Number                   = {3},
  Pages                    = {305--317},
  Volume                   = {9},

  Doi                      = {10.1109/tdsc.2012.10},
  File                     = {:article\\A Taxonomy of Buffer Overflow Characteristics.pdf:PDF},
  Groups                   = {source code vulnerability},
  Publisher                = {Institute of Electrical {\&} Electronics Engineers ({IEEE})},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/TDSC.2012.10}
}

@InProceedings{BoomCanneytBohezEtAl1512,
  Title                    = {Learning Semantic Similarity for Very Short Texts},
  Author                   = {Cedric De Boom and Steven Van Canneyt and Steven Bohez and Thomas Demeester and Bart Dhoedt and Ghent University and – iMinds},
  Year                     = {1512},

  Abstract                 = {Levering data on social media, such as Twitter a single sentence representation that contains most of its and Facebook, requires information retrieval algorithms to semantic information. Many authors choose to average or become able to relate very short text fragments to each maximize across the embeddings in a text [8], [9], [10] or other. Traditional text similarity methods such as tf-idf cosine- similarity, based on word overlap, mostly fail to produce combine them through a multi-layer perceptron [6], [11], by good results in this case, since word overlap is little or non- clustering [12], or by trimming the text to a fixed length existent. Recently, distributed word representations, or word [11]. embeddings, have been shown to successfully allow words The Paragraph Vector algorithm by Le and Mikolov— to match on the semantic level. In order to pair short text also termed paragraph2vec—is a powerful method to find fragments—as a concatenation of separate words—an adequate distributed sentence representation is needed, in existing lit- suitable vector representations for sentences, paragraphs and erature often obtained by naively combining the individual documents of variable length [13]. The algorithm tries to word representations. We therefore investigated several text find embeddings for separate words and paragraphs at the representations as a combination of word embeddings in the same time through a procedure similar to word2vec. The context of semantic pair matching. This paper investigates the collection of paragraphs, however, is known beforehand. effectiveness of several such naive techniques, as well as tradi- tional tf-idf similarity, for fragments of different lengths. Our This implies that finding a vector representation for a new main contribution is a first step towards a hybrid method that and probably unseen paragraph—the theoretical number of combines the strength of dense distributed representations— different paragraphs is after all many times higher than as opposed to sparse term matching—with the strength of the number of different words—requires additional training. tf-idf based methods to automatically reduce the impact of Paragraph2vec is therefore not a fit candidate to be used in, less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the e.g., a stream of messages as is the case with social media. conclusion that the combination of word embeddings and tf-idf Further research is thus needed to derive optimal sentence information might lead to a better model for semantic content representations based on word embeddings. By investigating within very short text fragments. and comparing the performance of several word combination
},
  File                     = {:article\\Learning Semantic Similarity for Very Short Texts.pdf:PDF},
  Review                   = {Learning Semantic Similarity for Very Short Texts Cedric De Boom, Steven Van Canneyt, Steven Bohez, Thomas Demeester, Bart Dhoedt Ghent University – iMinds Gaston Crommenlaan 8-201, 9050 Ghent, Belgium {cedric.deboom, steven.vancanneyt, steven.bohez, thomas.demeester, bart.dhoedt}@ugent.be Abstract—Levering data on social media, such as Twitter a single sentence representation that contains most of its and Facebook, requires information retrieval algorithms to semantic information. Many authors choose to average or become able to relate very short text fragments to each maximize across the embeddings in a text [8], [9], [10] or other. Traditional text similarity methods such as tf-idf cosine- similarity, based on word overlap, mostly fail to produce combine them through a multi-layer perceptron [6], [11], by good results in this case, since word overlap is little or non- clustering [12], or by trimming the text to a fixed length existent. Recently, distributed word representations, or word [11]. embeddings, have been shown to successfully allow words The Paragraph Vector algorithm by Le and Mikolov— to match on the semantic level. In order to pair short text also termed paragraph2vec—is a powerful method to find fragments—as a concatenation of separate words—an adequate distributed sentence representation is needed, in existing lit- suitable vector representations for sentences, paragraphs and erature often obtained by naively combining the individual documents of variable length [13]. The algorithm tries to word representations. We therefore investigated several text find embeddings for separate words and paragraphs at the representations as a combination of word embeddings in the same time through a procedure similar to word2vec. The context of semantic pair matching. This paper investigates the collection of paragraphs, however, is known beforehand. effectiveness of several such naive techniques, as well as tradi- tional tf-idf similarity, for fragments of different lengths. Our This implies that finding a vector representation for a new main contribution is a first step towards a hybrid method that and probably unseen paragraph—the theoretical number of combines the strength of dense distributed representations— different paragraphs is after all many times higher than as opposed to sparse term matching—with the strength of the number of different words—requires additional training. tf-idf based methods to automatically reduce the impact of Paragraph2vec is therefore not a fit candidate to be used in, less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the e.g., a stream of messages as is the case with social media. conclusion that the combination of word embeddings and tf-idf Further research is thus needed to derive optimal sentence information might lead to a better model for semantic content representations based on word embeddings. By investigating within very short text fragments. and comparing the performance of several word combination approaches in a short-text matching task, we arrive at a I. INTRODUCTION novel technique in which we aggregate both tf-idf and word On social media billions of small text messages are made embedding signals. In this paper, we show how word em- public every day: own research indicates that almost every beddings can be combined into a new vector representation tweet is comprised of one up to approximately thirty words. for the entire considered fragment, in which the impact To tap into this stream of extremely short text fragments, we of frequent words—i.e. with a low idf-component, and need appropriate information retrieval algorithms. Tf-idf is therefore mostly non-informative—is reduced with respect to an example of a traditional and very popular representation more informative words. This leads to a significant increase to compare texts, such as news articles, with each other [1], in the effectiveness of detecting semantically similar short- [2]. It relies on word overlap to find similarities, but in very text fragments, compared to traditional tf-idf techniques short texts, in which word overlap is rare, tf-idf often fails. or simple heuristic methods to combine word embeddings. For this reason we need sentence representations that grasp Our approach is a first step towards a hybrid method that more than just word contents. unites word embedding and tf-idf information of a short text In 2013, Mikolov et al. published three papers on the fragment into a distributed representation that catches most topic of distributed word embeddings to catch semantic of that fragment’s semantic information. similarities between words [3], [4], [5], which resulted Very recently, Kusner et al. devised a simple method in Google’s word2vec software. Since then scientists have to measure the similarity between documents based on the extensively used such embeddings to improve state-of-the- minimal distance word embeddings have to travel from one art algorithms in natural language processing, such as part- document to another [14]. In this, the authors only consider of-speech tagging [6], sentence completion [7], hashtag non stop words, and evaluate their distance measure using prediction [8], etc. There is however a lack of research kNN classification. We, however, will learn vector repres- and insight on how to effectively combine embeddings into entations for documents, and we will evaluate our technique arXiv:1512.00765v1 [cs.CL] 2 Dec 2015

}
}

@Article{BreueraPickinb2013,
  Title                    = {Open Source Verification in an Anonymous Volunteer Network},
  Author                   = {Peter T. Breuera and Simon Pickinb},
  Year                     = {2013},

  File                     = {:article\\Open Source Verification in an Anonymous Volunteer Network.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Formal Methods, Software Verification, Static Analysis, Open Source, Cloud Computing, Distributed Computation,literature,article},
  Read                     = {未读},
  Review                   = {Open Source Veriﬁcation in an Anonymous Volunteer Network Peter T. Breuera,∗, Simon Pickinb a Dept. Comp. Sci., University of Birmingham, Birmingham, UK b Dpto. de Sist. Inf. y Comp., Universidad Complutense de Madrid, Madrid, SPAIN Abstract An ‘open’ certification process is characterised here that is not based on any central agency, but rather on the option for any party to confirm any part of the certification process at will. The model for this paradigm has been a distributed, piece-wise, semantic audit carried out on the Linux kernel source code using a lightweight formal method. Our goal is a technology that allows open source developers to receive for- mally backed certifications for their project, in quid pro quo exchanges of re- sources and expertise with other developers within an amorphous and anony- mous cloud of volunteers. To help ensure the integrity of the results, identifying details such as subroutine and variable names are not included in the data sent for analysis, each part of the computation is repeated many times at different sites, and checkpoint information is generated that enables independent checks to be carried out without starting from scratch each time. Keywords: Formal Methods, Software Verification, Static Analysis, Open Source, Cloud Computing, Distributed Computation 1. Introduction We have prototyped a distributed platform aimed at the formal verification and static analysis of large open source code bases and the software has been tested using the Linux kernel source code as target. Our approach is motivated by the vision of a future in which a formal verification problem can be sent out to an ‘open verification’ community of anonymously volunteered resources for resolution. The community ethos means that a problem submitter ought to contribute resources to solving others problems, and volunteers whose machines perform the computations do so partially as quid-pro-quo for their verification problems being solved too. Eric Raymond’s arguments in [Ray01] suggest the rationale may be ‘selfish altruism’ – it is worth a contributor’s while to help get ∗ Corresponding author Email addresses: ptb@ieee.org (Peter T. Breuer), spickin@fdi.ucm.es (Simon Pickin) Preprint submitted to Science of Computer Programming May 24, 2013

}
}

@Article{BrownNoetzliEngler2016,
  Title                    = {How to Build Static Checking Systems Using Orders of Magnitude Less Code},
  Author                   = {Brown, Fraser and Nötzli, Andres and Engler, Dawson},
  Journal                  = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS ’16},
  Year                     = {2016},

  Doi                      = {10.1145/2872362.2872364},
  File                     = {:home/ccc/github/literature/article/How to Build Static Checking Systems.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/9781450340915},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/2872362.2872364}
}

@InProceedings{BugFinding2005,
  Title                    = {Static Analysis Versus Model Checking},
  Author                   = {for Bug and Finding},
  Booktitle                = {CONCUR 2005},
  Year                     = {2005},
  Editor                   = {M. Abadi and L. de Alfaro},
  Publisher                = {Springer},
  Series                   = {LNCS},
  Volume                   = {3653},

  Abstract                 = {This talk tries to distill several years of experience using both model checking and static analysis to ﬁnd errors in large software systems. We initially thought that the tradeoﬀs between the two was clear: static analysis was easy but would mainly ﬁnd shallow bugs, while model checking would require more work but would be strictly better — it would ﬁnd more errors, the errors would be deeper and the approach would be more powerful. These expectations were often wrong. This talk will describe some of the sharper tradeoﬀs between the two, as well as a detailed discussion of one domain — ﬁnding errors in ﬁle systems code — where model checking seems to work very well.
},
  File                     = {:article\\Static Analysis Versus Model Checking for Bug Finding.pdf:PDF},
  Groups                   = {source code vulnerability},
  Review                   = {Static Analysis Versus Model Checking for Bug Finding Dawson Engler Computer Systems Laboratory, Stanford University, Stanford, CA 94305, U.S.A Abstract. This talk tries to distill several years of experience using both model checking and static analysis to ﬁnd errors in large software systems. We initially thought that the tradeoﬀs between the two was clear: static analysis was easy but would mainly ﬁnd shallow bugs, while model checking would require more work but would be strictly better — it would ﬁnd more errors, the errors would be deeper and the approach would be more powerful. These expectations were often wrong. This talk will describe some of the sharper tradeoﬀs between the two, as well as a detailed discussion of one domain — ﬁnding errors in ﬁle systems code — where model checking seems to work very well. M. Abadi and L. de Alfaro (Eds.): CONCUR 2005, LNCS 3653, p. 1, 2005. ©c Springer-Verlag Berlin Heidelberg 2005

}
}

@InProceedings{,
  Title                    = {? ? ? ? ? ?},
  Author                   = {???? Bug??????????? and Data Analysis and with Applications and of Software and Bug Repositories},
  Year                     = {1090},

  File                     = {:home/ccc/github/literature/article/????bug???????????.pdf:PDF},
  Review                   = { 
 
? ? ? ? ? ? 
???? Bug??????????? Data Analysis with Applications of Software Bug Repositories 
? ? ? ?? ??? 
??? ??? ???? 
? ?? 10901034 
? ? ? ?? ?? 
? ? ? ?? 
 
?????? 
Dalian University of Technology 
 

}
}

@InProceedings{By2009,
  Title                    = {DISCOVERING NEGLECTED CONDITIONS IN SOFTWARE BY MINING PROGRAM DEPENDENCE GRAPHS},
  Author                   = {By},
  Year                     = {2009},

  File                     = {:article\\Discovering neglected conditions in software by mining dependence graphs.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Review                   = {DISCOVERING NEGLECTED CONDITIONS IN SOFTWARE BY MINING PROGRAM DEPENDENCE GRAPHS By RAY-YAUNG CHANG Submitted in partial fulfillment of the requirements For the degree of Doctor of Philosophy Dissertation Advisor: Dr. H. Andy Podgurski Department of Electrical Engineering and Computer Science CASE WESTERN RESERVE UNIVERSITY January, 2009 

}
}

@TechReport{CaballeroJohnsonMcCamantEtAl2009,
  Title                    = {Binary Code Extraction and Interface Identification for Security Applications},
  Author                   = {Juan Caballero and Noah M. Johnson and Stephen McCamant and Dawn Song},
  Year                     = {2009},
  Number                   = {UCB/EECS-2009-133},

  Abstract                 = {二进制代码},
  File                     = {:article\\Binary code extraction and interface identification for security applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Binary Code Extraction and Interface Identification for Security Applications Juan Caballero Noah M. Johnson Stephen McCamant Dawn Song Electrical Engineering and Computer Sciences University of California at Berkeley Technical Report No. UCB/EECS-2009-133 http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-133.html October 2, 2009

}
}

@InProceedings{CadarDunbarEnglerEtAl2008,
  Title                    = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
  Author                   = {Cristian Cadar and Daniel Dunbar and Dawson Engler and Stanford University},
  Year                     = {2008},

  Abstract                 = {bolic values and replace corresponding concrete program
},
  File                     = {:article\\KLEE Unassisted and Automatic Generation of High-Coverag Tests for Complex Systems Programs.2008-12-OSDI-KLEE.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs Cristian Cadar, Daniel Dunbar, Dawson Engler ∗ Stanford University Abstract bolic values and replace corresponding concrete program W operations with ones that manipulate symbolic values.e present a new symbolic execution tool, KLEE, ca- pable of automatically g When program execution branches based on a symbolicenerating tests that achieve high value, the system (conceptually) follows both branches,coverage on a diverse set of complex and envi on each path maintaining a set of constraints called theronmentally-intensive programs. We used KLEE to thoroughly check all 89 path condition which must hold on execution of thatstand-alone programs in the GNU C path. When a path terminates or hits a bug, a test caseOREUTILS utility suite, which form the core user-level environment installed on milli can be generated by solving the current path conditionons of Unix sys- t for concrete values. Assuming deterministic code, feed-ems, and arguably are the single most heavily tested set of op ing this concrete input to a raw, unmodified version ofen-source programs in existence. KLEE-generated t the checked code will make it follow the same path andests achieve high line coverage — on average over 90% per tool (median: over 94%) — and significantly b hit the same bug.eat th Results are promising. However, while researcherse coverage of the developers’ own hand-written test have shown such tools can sometimes get good cover- suites. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, result age and find bugs on a small number of programs, its were even better, including 100% has been an open question whether the approach has anycoverage on 31 of them. W hope of consistently achieving high coverage on real ap-e also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code) plications. Two common concerns are (1) the exponen-, where it found 56 tial number of paths through code and (2) the challengesserious bugs, including three in COREUTILS th in handling code that interacts with its surrounding envi-at had been missed for over 15 years. Finally, we used KLEE to cross-check purportedly identical B ronment, such as the operating system, the network, orUSY- the user (colloquially: “the environment problem”). Nei- BOX and COREUTILS utilities, finding functional cor- rectness errors and a myriad of i ther concern has been much helped by the fact that mostnconsistencies. past work, including ours, has usually reported results on 1 Introduction a limited set of hand-picked benchmarks and typically has not included any coverage numbers. Many classes of errors, such as functional correctness This paper makes two contributions. First, we present bugs, are difficult to find without executing a piece of a new symbolic execution tool, KLEE, which we de- code. The importance of such testing — combined with signed for robust, deep checking of a broad range of ap- the difficulty and poor performance of random and man- plications, leveraging several years of lessons from our ual approaches — has led to much recent work in us- previous tool, EXE [16]. KLEE employs a variety of con- ing symbolic execution to automatically generate test in- straint solving optimizations, represents program states puts [11, 14–16, 20–22, 24, 26, 27, 36]. At a high-level, compactly, and uses search heuristics to get high code these tools use variations on the following idea: Instead coverage. Additionally, it uses a simple and straight- of running code on manually- or randomly-constructed forward approach to dealing with the external environ- input, they run it on symbolic input initially allowed to ment. These features improve KLEE’s performance by be “anything.” They substitute program inputs with sym- over an order of magnitude and let it check a broad range of system-intensive programs “out of the box.” ∗Author names are in alphabetical order. Daniel Dunbar is the main author of the KLEE system.

}
}

@Article{CadarEnglerSystemsEtAl2005,
  Title                    = {Execution Generated Test Cases: How to Make Systems Code Crash Itself},
  Author                   = {Cristian Cadar and Dawson Engler and Computer Systems and Laboratory},
  Year                     = {2005},

  Abstract                 = {the charm that it requires no manual work, other than This paper presents a technique that interfacing the generator to the tested code. However,uses code to auto-
},
  File                     = {:article\\Execution Generated Test Cases-How to Make Systems Code Crash Itself10.1.1.79.6663.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Execution Generated Test Cases: How to Make Systems Code Crash Itself Cristian Cadar and Dawson Engler∗ Computer Systems Laboratory Stanford University Stanford, CA 94305, U.S.A. Abstract the charm that it requires no manual work, other than This paper presents a technique that interfacing the generator to the tested code. However,uses code to auto- random test generation by itself has several severe draw- matically generate its own test cases at run-time by using a combination of symbolic and concrete (i.e., backs. First, blind generation of values means that itregular) misses errors triggered by narrow ranges of inputs. A execution. The input values to a program (or software component) provide the standard interf trivial example: if a function only has an error if its 32-ace of any test- ing framework with the program it is testing, and gen- bit integer argument is equal to “12345678” then random will most likely have to generate billions of test cases be- erating input values that will explore all the “interest- ing” behavior in the tested program remains an important fore it hits this specific case. Second, and similarly, ran-dom testing has difficulty hitting errors that depend on open problem in software testing research. Our approach works by turning the problem on its head: we lazily gen- several different inputs being within specific (even wide) ranges of values. Third, the ability of random testing to erate, from within the program itself, the input values to the program (and effectively generate random noise is also its curse. It isvalues derived from input values) as very poor at generating input that has structure, and as a needed. We applied the technique to real code and found result will miss errors that require some amount of cor- numerous corner-case errors ranging from simple mem- ory overflows and infinite loops to subtle issues in the rect structure in input before they can be hit. A clear interpretation of language standards. example would be using random test generation to findbugs in a language parser. It will find cases where the parser cannot handle garbage inputs. However, because 1 Introduction of the extreme improbability of random generation con- structing inputs that look anything like legal programs it Systems code is difficult to test comprehensively. Exter will miss almost all errors cases where the parser mis-- nally, systems interfaces tend towards the baroque, with handles them. many different possible behaviors based on tricky combi- Of course, random can be augmented with some nations of inputs. Internally, their implementations tend amount of guidance to more intelligently generate in- towards heavily entangling nests of conditionals that are puts, though this comes at the cost of manual interven- difficult to enumerate, much less exhaust with test cases. tion. A typical example would be writing a tool to take Both features conspire to make comprehensive, manual a manually-written language grammar and use it to ran- testing an enormous undertaking, so enormous that em- domly generate legal and illegal programs that are fed pirically, many systems code test suites consist only of to the tested program. Another would be having a spec- a handful of simple cases or, perhaps even more com- ification or model of what a function’s external behav- monly, none at all. ior is and generate test cases using this model to try to Random testing can augment manual testing to some hit “interesting” combinations. However, all such hybrid degree. A good example is the fuzz [22, 21] tool, which approaches require manual labor and, more importantly, automatically generates random inputs, which is enough a willingness of implementors to provide this labor at to find errors in many applications. Random testing has all. The reluctance of systems builders to write speci- fications, grammars, models of what their code does, or ∗CSTR 2005-04 3: 3/25/05 6/9/05: This exact text (with author names elided) was submitted to the 20th ACM Symposium on Operat- even assertions is well known. As a result, very few real ing Systems Principles, March 25, 2005. systems have used such approaches.

}
}

@Article{CadarGodefroidKhurshidEtAl2011,
  Title                    = {Symbolic Execution for Software Testing in Practice – Preliminary Assessment},
  Author                   = {Cristian Cadar and Patrice Godefroid and Sarfraz Khurshid and Corina S. Pa˘sa˘reanu and Imperial College and London Microsoft and Research U. Texas and at Austin and CMU/NASA Ames},
  Year                     = {2011},

  Abstract                 = {Symbolic execution is now the underlying technique of We present results for the “Impact Project Focus Area” on several popular testing tools, many of them open-source:
},
  File                     = {:article\\Symbolic execution for software testing in practice=preliminary assessment.symex-icse-impact-11.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Software testing is the most commonly used technique for validating the quality of software, but it is typically a mostly,literature,article},
  Read                     = {未读},
  Review                   = {Symbolic Execution for Software Testing in Practice – Preliminary Assessment Cristian Cadar Patrice Godefroid Sarfraz Khurshid Corina S. Pa˘sa˘reanu∗ Imperial College London Microsoft Research U. Texas at Austin CMU/NASA Ames c.cadar@imperial.ac.uk pg@microsoft.com khurshid@ece.utexas.edu corina.s.pasareanu@nasa.gov Koushik Sen Nikolai Tillmann Willem Visser U.C. Berkeley Microsoft Research Stellenbosch University ksen@eecs.berkeley.edu nikolait@microsoft.com visserw@sun.ac.za ABSTRACT Symbolic execution is now the underlying technique of We present results for the “Impact Project Focus Area” on several popular testing tools, many of them open-source: the topic of symbolic execution as used in software testing. NASA’s Symbolic (Java) PathFinder 1, UIUC’s CUTE and Symbolic execution is a program analysis technique intro- jCUTE 2, Stanford’s KLEE3, UC Berkeley’s CREST4 and duced in the 70s that has received renewed interest in recent BitBlaze 5, etc. Symbolic execution tools are now used in in- years, due to algorithmic advances and increased availability dustrial practice at Microsoft (Pex 6, SAGE [29], YOGI7 and of computational power and constraint solving technology. PREfix [10]), IBM (Apollo [2]), NASA and Fujitsu (Sym- We review classical symbolic execution and some modern bolic PathFinder), and also form a key part of the com- extensions such as generalized symbolic execution and dy- mercial testing tool suites from Parasoft and other compa- namic test generation. We also give a preliminary assess- nies [60]. ment of the use in academia, research labs, and industry. Although we acknowledge that the impact of symbolic ex- ecution in software practice is still limited, we believe that the explosion of work in this area over the past years makes Categories and Subject Descriptors for an interesting story about the increasing impact of sym- D.2.5 [Testing and Debugging]: Symbolic execution bolic execution since it was first introduced in the 1970s. Note that this paper is not meant to provide a comprehen- General Terms sive survey of symbolic execution techniques; such surveyscan be found elsewhere [19, 44, 49]. Instead, we focus here Reliability on a few modern symbolic execution techniques that have shown promise to impact software testing in practice. Keywords Software testing is the most commonly used technique for validating the quality of software, but it is typically a mostly Generalized symbolic execution, dynamic test generation manual process that accounts for a large fraction of software development and maintenance. Symbolic execution is one of the many techniques that can be used to automate software 1. INTRODUCTION testing by automatically generating test cases that achieve high coverage of program executions. The ACM-SIGSOFT Impact Project is documenting the Symbolic execution is a program analysis technique that impact that software engineering research has had on soft- executes programs with symbolic rather than concrete in- ware development practice. In this paper, we present pre- puts and maintains a path condition that is updated when- liminary results for documenting the impact of research in ever a branch instruction is executed, to encode the con- symbolic execution for automated software testing. Sym- straints on the inputs that reach that program point. Test bolic execution is a program analysis technique that was generation is performed by solving the collected constraints introduced in the 70s [8, 15, 31, 35, 46], and that has found using a constraint solver. Symbolic execution can also be renewed interest in recent years [9,12,13,28,29,32,33,40,42, used for bug finding, where it checks for run-time errors or 43,50–52,56,57]. assertion violations and it generates test inputs that trigger ∗ those errors.We thank Matt Dwyer for his advice The original approaches to symbolic execution [8,15,31,35, 1 http://babelfish.arc.nasa.gov/trac/jpf/wiki/projects/ Permission to make digital or hard copies of all or part of this work for jpf-symbc personal or classroom use is granted without fee provided that copies are 2http://osl.cs.uiuc.edu/~ksen/cute/ not made or distributed for profit or commercial advantage and that copies 3http://klee.llvm.org/ bear this notice and the full citation on the first page. To copy otherwise, to 4http://code.google.com/p/crest/ republish, to post on servers or to redistribute to lists, requires prior specific 5 http://bitblaze.cs.berkeley.edu/ permission and/or a fee. 6 ICSE ’11, May 21–28, 2011, Waikiki, Honolulu, HI, USA http://research.microsoft.com/en-us/projects/pex/7 Copyright 2011 ACM 978-1-4503-0445-0/11/05 ...$10.00. http://research.microsoft.com/en-us/projects/yogi/

}
}

@Article{CaiZouMaEtAl2016,
  Title                    = {SwordDTA: A dynamic taint analysis tool for software vulnerability detection},
  Author                   = {Cai, Jun and Zou, Peng and Ma, Jinxin and He, Jun},
  Journal                  = {Wuhan Univ. J. Nat. Sci.},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {10–20},
  Volume                   = {21},

  Doi                      = {10.1007/s11859-016-1133-1},
  File                     = {:home/ccc/github/literature/article/SwordDTA\: A dynamic taint analysis tool for software vulnerability detection.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1993-4998},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s11859-016-1133-1}
}

@PhdThesis{Caliskan-Islam2015,
  Title                    = {Stylometric Fingerprints and Privacy Behavior in Textual Data},
  Author                   = {Aylin Caliskan-Islam},
  School                   = {Drexel University},
  Year                     = {2015},

  File                     = {:article\\Stylometric Fingerprints and Privacy Behavior in Textual Data.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Stylometric Fingerprints and Privacy Behavior in Textual Data A Thesis Submitted to the Faculty of Drexel University by Aylin Caliskan-Islam in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science May 2015

}
}

@Article{CankayaNairCankaya2013,
  Title                    = {Applying error correction codes to achieve security and dependability},
  Author                   = {Cankaya, Ebru Celikel and Nair, Suku and Cankaya, Hakki C.},
  Journal                  = {Computer Standards \& Interfaces},
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {78鈥�86},
  Volume                   = {35},

  Doi                      = {10.1016/j.csi.2012.06.009},
  File                     = {:article\\Applying error correction codes to achieve security and dependability.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0920-5489},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.csi.2012.06.009}
}

@TechReport{CarliniCaliforniaBerkeleyEtAl9319,
  Title                    = {Control-Flow Bending: On the Effectiveness of Control-Flow Integrity},
  Author                   = {Nicolas Carlini and University of California and Berkeley; Antonio Barresi and ETH Zürich and Mathias Payer and Purdue University; David Wagner and University of California and Berkeley},
  Year                     = {9319},

  File                     = {:home/ccc/github/literature/article/sec15-paper-carlini.pdf:PDF},
  Review                   = {Control-Flow Bending: On the Effectiveness of Control-Flow Integrity Nicolas Carlini, University of California, Berkeley; Antonio Barresi, ETH Zürich; Mathias Payer, Purdue University; David Wagner, University of California, Berkeley; Thomas R. Gross, ETH Zürich https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/carlini This paper is included in the Proceedings of the 24th USENIX Security Symposium August 12–14, 2015 • Washington, D.C. ISBN 978-1-931971-232 Open access to the Proceedings of the 24th USENIX Security Symposium is sponsored by USENIX

}
}

@TechReport{,
  Title                    = {Control-Flow Bending: On the Effectiveness of Control-Flow Integrity},
  Author                   = {Nicolas Carlini and University of California and Berkeley; Antonio Barresi and ETH Z�rich and Mathias Payer and Purdue University; David Wagner and University of California and Berkeley},
  Year                     = {9319},

  File                     = {:home/ccc/github/literature/article/Control-Flow Bending\: On the Effectiveness of Control-Flow Integrity.pdf:PDF},
  Review                   = {Control-Flow Bending: On the Effectiveness of Control-Flow Integrity
Nicolas Carlini, University of California, Berkeley; Antonio Barresi, ETH Z�rich; Mathias Payer, Purdue University; David Wagner, University of California, Berkeley; 
Thomas R. Gross, ETH Z�rich
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/carlini
This paper is included in the Proceedings of the 24th USENIX Security Symposium
August 12?14, 2015 ? Washington, D.C.
ISBN 978-1-931971-232
Open access to the Proceedings of the 24th USENIX Security Symposium 
is sponsored by USENIX

}
}

@InProceedings{CarolinaUniversityDriveEtAl8206,
  Title                    = {A Systematic Literature Review of Actionable Alert Identification Techniques for Automated Static Code Analysis Sarah Heckman (corresponding author) and Laurie Williams},
  Author                   = {North Carolina and State University and 890 Oval Drive and Campus Box and Raleigh and NC and (phone) +1.919.515.2042 and (fax) +1.919.515.7896 and [heckman and williams]@csc.ncsu.edu},
  Year                     = {8206},

  File                     = {:home/ccc/github/literature/article/A systematic literature review of actionable alert identification techniques for automated static code analysis-.pdf:PDF},
  Keywords                 = {automated static analysis, systematic literature review, actionable alert identification, unactionable alert mitigation, warning prioritization, actionable alert prediction},
  Review                   = {A Systematic Literature Review of Actionable Alert Identification Techniques for Automated Static Code Analysis Sarah Heckman (corresponding author) and Laurie Williams North Carolina State University 890 Oval Drive, Campus Box 8206, Raleigh, NC 27695-8206 (phone) +1.919.515.2042 (fax) +1.919.515.7896 [heckman, williams]@csc.ncsu.edu Abstract Context: Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice. Objective: The goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT). Method: Relevant studies about AAITs were gathered via a systematic literature review. Results: We selected eighteen peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or test case failures to classify and prioritize actionable alerts. All of the AAITs are evaluated via an experiment or case study with a variety of evaluation metrics. Conclusion: The selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the eighteen selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program’s history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and analysis language in the selected studies preclude meta-analysis and prevent the current results from informing evidenced-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics. Keywords: automated static analysis, systematic literature review, actionable alert identification, unactionable alert mitigation, warning prioritization, actionable alert prediction 1. Introduction Static analysis is “the process of evaluating a system or component based on its form, structure, content, or documentation” [20]. Automated static analysis (ASA) can identify common coding problems 

}
}

@Article{Catal2011,
  Title                    = {Software fault prediction: A literature review and current trends},
  Author                   = {Catal, Cagatay},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2011},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {4626–4636},
  Volume                   = {38},

  Doi                      = {10.1016/j.eswa.2010.10.024},
  File                     = {:home/ccc/github/literature/article/Software fault prediction\: A literature review and current trends.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0957-4174},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.eswa.2010.10.024}
}

@Article{CatalDiri2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Catal, Cagatay and Diri, Banu},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},

  Month                    = {May},
  Number                   = {4},
  Pages                    = {7346–7354},
  Volume                   = {36},

  Doi                      = {10.1016/j.eswa.2008.10.027},
  File                     = {:home/ccc/github/literature/article/A systematic review of software fault prediction studies.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0957-4174},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.eswa.2008.10.027}
}

@InProceedings{,
  Title                    = {Research of the Key Technologies in Binary Code Obfuscation Xu Wang1, Wen Qing Fan2, Wei Huang},
  Author                   = {1. Information Security Center and Beijing University of Posts and Telecommunications and Beijing and China and 2. Communication University of China and Beijing and China},
  Year                     = {1008},

  Abstract                 = {Binary code obfuscation protects software from cracking and alteration by means of code reorganization and deformation. This paper analyses several cutting-edge code obfuscation algorithms, including the code out of order, insert opaque predicate and control flow flattening. Moreover, this paper shows the code obfuscation technology research at present through comparing these algorithms� advantages and disadvantages. At last, this paper will make a summary pointing out that code obfuscation technology in the theoretical studies still lack a complete argument, but some of which like control flow flattening has already used well in software anti-reverse. Keywords: Code Obfuscation; Program Control Flow Obfuscation; Reverse analysis.
},
  File                     = {:home/ccc/github/literature/article/?????????????.pdf:PDF},
  Review                   = {Research of the Key Technologies in Binary Code Obfuscation Xu Wang1, Wen Qing Fan2, Wei Huang2
1. Information Security Center, Beijing University of Posts and Telecommunications, Beijing 100876, China 2. Communication University of China, Beijing 100024, China
Abstract: Binary code obfuscation protects software from cracking and alteration by means of code reorganization and deformation. This paper analyses several cutting-edge code obfuscation algorithms, including the code out of order, insert opaque predicate and control flow flattening. Moreover, this paper shows the code obfuscation technology research at present through comparing these algorithms� advantages and disadvantages. At last, this paper will make a summary pointing out that code obfuscation technology in the theoretical studies still lack a complete argument, but some of which like control flow flattening has already used well in software anti-reverse. Keywords: Code Obfuscation; Program Control Flow Obfuscation; Reverse analysis.
?????????????
? ? 1???? 2??? 2
1. ???????????????????100876 2. ?????????????100024
? ??????????????????????????????????????????????? ????????????????????????????????????????????????? ????????????????????????????????????????????????? ??????????????????? ?????????????????????????????? ?????????
????????????????????
1 ?? ??????????????????????
?????surreptitious software???????? ????????????????????????
???????????????????????? ????????????????????????
???????????????????????? ?????????????????
???????????????????????? 2.1 ????? ???????????????????????? ?????????????????????? ???????????????????????? ????????????????????[8]??? ?????????????????[4]? ????????????????? ???????? ?????????????????????? ????????????????????????
???????????????????????? ???????????????????????? ???????????????????????? ???????????????????????? ???????????????????????? ???????????????????????? ???????????????????????? ????????????????????? ???????????????????????? ?????????????????????? ??????[7]? ???????????????????????? ?????????????????????? ?????CFG??????????????? G=
???????????????????????? ?N? A? s???? N??????A?????? ???????????????????????? ??????? s????????????[1]??? ??????????? ????????????????????????
2 ??????? ??????????????????????


}
}

@Article{CepedaColomeCastrillon2011,
  Title                    = {Dynamic Vulnerability Assessment due to Transient Instability based on Data Mining Analysis for Smart Grid Applications},
  Author                   = {Cepeda, J. C. and Colome, D. G. and Castrillon, N. J.},
  Journal                  = {2011 IEEE PES Conference on Innovative Smart Grid Technologies},
  Year                     = {2011},
  Pages                    = {7 pp.--7},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In recent years, some Smart Grid applications have been designed in order to perform timely Self-Healing and adaptive reconfiguration actions based on system-wide analysis, with the objective of reducing the risk of power system blackouts. Real time dynamic vulnerability assessment (DVA) has to be done in order to decide and coordinate the appropriate corrective control actions, depending on the event evolution. This paper presents a novel approach for carrying out real time DVA, focused on Transient Stability Assessment (TSA), based on some time series data mining techniques (Multichannel Singular Spectrum Analysis MSSA, and Principal Component Analysis PCA), and a machine learning tool (Support Vector Machine Classifier SVM-C). In addition, a general overview of the state of the art of the methods to perform vulnerability assessment, with emphasis in the potential use of PMUs for post-contingency DVA, is described. The developed methodology is tested in the IEEE 39 bus New England test system, where the simulated cause of vulnerability is transient instability. The results show that time series data mining tools are useful to find hidden patterns in electric signals, and SVM-C can use those patterns for effectively classifying the system vulnerability status.},
  Be                       = {Gualteros, M.V.EOLEOLRodriguez, Y.A.EOLEOLAldana, A.},
  Bn                       = {978-1-4577-1802-1},
  Cl                       = {Medellin, Colombia},
  Ct                       = {2011 IEEE PES Conference on Innovative Smart Grid Technologies},
  Cy                       = {19-21 Oct. 2011},
  Doi                      = {10.1109/ISGT-LA.2011.6083211},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:12377680},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{CepedaRuedaErlichEtAl2012,
  Title                    = {Recognition of post-contingency dynamic vulnerability regions: Towards smart grids},
  Author                   = {Cepeda, J. C. and Rueda, J. L. and Erlich, I. and Colome, D. G.},
  Journal                  = {2012 IEEE Power \& Energy Society General Meeting. New Energy Horizons - Opportunities and Challenges},
  Year                     = {2012},
  Pages                    = {8 pp.--8},

  __markedentry            = {[ccc:6]},
  Abstract                 = {This paper presents a novel approach for determining post-contingency dynamic vulnerability regions (DVRs), oriented to assess vulnerability in real time as part of Smart Grid applications. Based on the probabilistic models of input parameters, such as load variation and the occurrence of contingencies, Monte Carlo-type simulation is performed to iteratively evaluate the system time domain responses. The dynamic probabilistic attributes are then analyzed using time series data mining techniques, namely Multichannel Singular Spectrum Analysis (MSSA), and Principal Component Analysis (PCA), in order to recognize the system DVRs based on the patterns associated to three different short-term stability phenomena. The vulnerability criterion consists in the possibility of some N-1 contingencies driving the system to further undesirable events (i.e. N-2 contingencies), which could be considered as the beginning of a cascading event. The proposal is tested on the IEEE New England 39-bus test system. Results show the feasibility of the methodology in finding hidden patterns in dynamic electric signals as well as in numerically mapping power system DVRs due to its ability to consider relevant operating statistics, including the most probably severe events that could lead the system to potential insecure conditions and subsequent blackouts.},
  Bn                       = {978-1-4673-2727-5},
  Cl                       = {San Diego, CA, USA},
  Ct                       = {2012 IEEE Power & Energy Society General Meeting. New Energy Horizons -EOLEOLOpportunities and Challenges},
  Cy                       = {22-26 July 2012},
  Doi                      = {10.1109/PESGM.2012.6345032},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:13170366},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{ChaAvgerinosRebertEtAl2012,
  Title                    = {Unleashing MAYHEM on Binary Code},
  Author                   = {Sang Kil Cha and Thanassis Avgerinos and Alexandre Rebert and David Brumley and Carnegie Mellon and University},
  Year                     = {2012},

  Abstract                 = {In this paper we present MAYHEM, a new sys- In order to tackle this problem, MAYHEM’s design is based tem for automatically finding exploitable bugs in binary (i.e., on four main principles: 1) the system should be able to executable) programs. Every bug reported by MAYHEM is make forward progress for arbitrarily long times—ideally run accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is security- “forever”—without exceeding the given resources (especially critical and actionable. MAYHEM works on raw binary code memory), 2) in order to maximize performance, the system without debugging information. To make exploit generation should not repeat work, 3) the system should not throw away possible at the binary-level, MAYHEM addresses two major any work—previous analysis results of the system should technical challenges: actively managing execution paths without be reusable on subsequent runs, and 4) the system should exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user be able to reason about symbolic memory where a load input. To this end, we propose two novel techniques: 1) hybrid or store address depends on user input. Handling memory symbolic execution for combining online and offline (concolic) addresses is essential to exploit real-world bugs. Principle #1 execution to maximize the benefits of both techniques, and is necessary for running complex applications, since most 2) index-based memory modeling, a technique that allows non-trivial programs will contain a potentially infinite number MAYHEM to efficiently reason about symbolic memory at the binary level. We used MAYHEM to find and demonstrate of paths to explore. 29 exploitable vulnerabilities in both Linux and Windows Current approaches to symbolic execution, e.g., CUTE [26], programs, 2 of which were previously undocumented. BitBlaze [5], KLEE [9], SAGE [13], McVeto [27], AEG [2],
},
  File                     = {:article\\Unleashing MAYHEM on Binary Code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {hybrid execution, symbolic memory, index-based S2E [28], and others [3], [21], do not satisfy all the memory modeling, exploit generation above design points. Conceptually, current executors can be},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Unleashing MAYHEM on Binary Code Sang Kil Cha, Thanassis Avgerinos, Alexandre Rebert and David Brumley Carnegie Mellon University Pittsburgh, PA {sangkilc, thanassis, alexandre.rebert, dbrumley}@cmu.edu Abstract—In this paper we present MAYHEM, a new sys- In order to tackle this problem, MAYHEM’s design is based tem for automatically finding exploitable bugs in binary (i.e., on four main principles: 1) the system should be able to executable) programs. Every bug reported by MAYHEM is make forward progress for arbitrarily long times—ideally run accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is security- “forever”—without exceeding the given resources (especially critical and actionable. MAYHEM works on raw binary code memory), 2) in order to maximize performance, the system without debugging information. To make exploit generation should not repeat work, 3) the system should not throw away possible at the binary-level, MAYHEM addresses two major any work—previous analysis results of the system should technical challenges: actively managing execution paths without be reusable on subsequent runs, and 4) the system should exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user be able to reason about symbolic memory where a load input. To this end, we propose two novel techniques: 1) hybrid or store address depends on user input. Handling memory symbolic execution for combining online and offline (concolic) addresses is essential to exploit real-world bugs. Principle #1 execution to maximize the benefits of both techniques, and is necessary for running complex applications, since most 2) index-based memory modeling, a technique that allows non-trivial programs will contain a potentially infinite number MAYHEM to efficiently reason about symbolic memory at the binary level. We used MAYHEM to find and demonstrate of paths to explore. 29 exploitable vulnerabilities in both Linux and Windows Current approaches to symbolic execution, e.g., CUTE [26], programs, 2 of which were previously undocumented. BitBlaze [5], KLEE [9], SAGE [13], McVeto [27], AEG [2], Keywords-hybrid execution, symbolic memory, index-based S2E [28], and others [3], [21], do not satisfy all the memory modeling, exploit generation above design points. Conceptually, current executors can be divided into two main categories: offline executors — which I. INTRODUCTION concretely run a single execution path and then symbolically Bugs are plentiful. For example, the Ubuntu Linux bug execute it (also known as trace-based or concolic executors, management database currently lists over 90,000 open e.g., SAGE), and online executors — which try to execute bugs [17]. However, bugs that can be exploited by attackers all possible paths in a single run of the system (e.g., S2E). are typically the most serious, and should be patched first. Neither online nor offline executors satisfy principles #1-#3. Thus, a central question is not whether a program has bugs, In addition, most symbolic execution engines do not reason but which bugs are exploitable. about symbolic memory, thus do not meet principle #4. In this paper we present MAYHEM, a sound system Offline symbolic executors [5], [13] reason about a single for automatically finding exploitable bugs in binary (i.e., execution path at a time. Principle #1 is satisfied by iteratively executable) programs. MAYHEM produces a working control- picking new paths to explore. Further, every run of the hijack exploit for each bug it reports, thus guaranteeing each system is independent from the others and thus results of bug report is actionable and security-critical. By working previous runs can be immediately reused, satisfying principle with binary code MAYHEM enables even those without source #3. However, offline does not satisfy principle #2. Every code access to check the (in)security of their software. run of the system needs to restart execution of the program MAYHEM detects and generates exploits based on the from the very beginning. Conceptually, the same instructions basic principles introduced in our previous work on AEG [2]. need to be executed repeatedly for every execution trace. Our At a high-level, MAYHEM finds exploitable paths by aug- experimental results show that this re-execution can be very menting symbolic execution [16] with additional constraints expensive (see §VIII). at potentially vulnerable program points. The constraints Online symbolic execution [9], [28] forks at each branch include details such as whether an instruction pointer can be point. Previous instructions are never re-executed, but the redirected, whether we can position attack code in memory, continued forking puts a strain on memory, slowing down and ultimately, whether we can execute attacker’s code. If the the execution engine as the number of branches increase. resulting formula is satisfiable, then an exploit is possible. The result is no forward progress and thus principles #1 A main challenge in exploit generation is exploring enough and #3 are not met. Some online executors such as KLEE of the state space of an application to find exploitable paths. stop forking to avoid being slowed down by their memory

}
}

@InProceedings{ChabbiPerianayagamAndrewsEtAl2012,
  Title                    = {Efficient Dynamic Taint Analysis Using Multicore Machines},
  Author                   = {Milind Chabbi and Somu Perianayagam and Gregory Andrews and Saumya Debray and Department of Computer Science and University of Arizona and Tucson and AZ and USA},
  Year                     = {2012},

  Abstract                 = {propagating and checking taint values, and by the time spent saving Dynamic taint analysis is an effective method for detecting mem- and restoring registers and flags in order to run the taint-checking
},
  File                     = {:home/ccc/github/literature/article/Efficient Dynamic Taint Analysis Using Multicore Machines.pdf:PDF},
  Review                   = {Efficient Dynamic Taint Analysis Using Multicore Machines Milind Chabbi, Somu Perianayagam, Gregory Andrews, and Saumya Debray Department of Computer Science, University of Arizona, Tucson, AZ 85721, USA. Abstract propagating and checking taint values, and by the time spent saving Dynamic taint analysis is an effective method for detecting mem- and restoring registers and flags in order to run the taint-checking ory overwrite attacks before they can cause harm. However, initial code (this is needed in order not to interfere with the original pro- implementations of the method employed interpretive execution, gram). which causes an execution slowdown of 20 to 40 times. Recent This paper describes a novel approach to the implementation work has reduced execution overhead by means of special hardware of taint checking that uses multithreading to offload the work of or by using dynamic binary instrumentation coupled with heavily the taint checker. Our approach is motivated by the observations optimized implementation of the taint checking code. This paper that modern machines now have two or more execution cores, describes a new approach that uses two threads executing on dif- the number of cores is likely to double at least every 18 months ferent processor cores. In particular, one thread executes the orig- for many years, and there will almost always be idle cores that inal computation and a second thread “shadows” execution of the have nothing to do. Like TaintCheck and LIFT, we use binary original to propagate taint values and to check for exploits. The rewriting, so that we do not require access to source code (which two threads synchronize just enough for the shadow to follow the is often unavailable for commercial software), and we provide a control flow of the program and to ensure that exploits are caught software-only solution that executes on conventional processors. before they can cause damage. The approach is implemented by The distinguishing characteristics of our approach are as follows: means of static binary rewriting. A prototype implementation cor-  Given the execution binary for a program, we statically rewrite rectly catches security exploits, with a performance overhead that the binary to contain two threads. The first thread executes the ranges from a few percent on most programs to a factor of about original program. The second thread implements taint check- two on some short-running programs. ing and vulnerability monitoring. It shadows execution of the original thread, following just behind it. 1. Introduction  The shadow thread is simpler than the original thread because it does less work. Moreover, it has its own stack and registers. A program contains a vulnerability if there is a way in which an attacker can gain control of the program (and possibly the system  The original thread communicates with the shadow thread by on which it executes). Despite the best efforts of software designers, dumping call and branch targets into a circular buffer whenever the number of known vulnerabilities continues to increase at a rapid it makes a conditional or indirect control transfer. This allows rate [1]. Over 90% of known vulnerabilities result from failure to the shadow thread to follow the equivalent execution path as check array bounds or input arguments (e.g., buffer overflow and the original thread without having to repeat all of that thread’s format string attacks). computations. Taint checking is a very promising method for protecting pro-  The original thread synchronizes with the shadow thread at po- grams from memory corruption attacks [2, 3, 5, 6, 7, 8, 9, 12, 13]. tential vulnerabilities, e.g., before executing a function return. The basic idea is as follows: Data values that originate from out- In particular, the original thread waits for the shadow thread to side a program—e.g., from command line arguments, user input, indicate that it is safe to proceed. or network packets—are considered to be tainted. A program is  instrumented or monitored to follow the flow of these data val- The shadow thread prints an error message and aborts program ues. At points at which an attacker could take control of program execution if it detects unsafe usage of tainted memory. execution—i.e., at indirect control transfers such as function re- The net effects are high accuracy due to dynamic taint analysis, low turns or at sensitive system calls—potentially vulnerable memory execution overhead on multicore machines, and a one-time cost for locations are checked. If they are tainted, the program is halted be- constructing a self-monitoring version of a program (as opposed to cause it most likely is under attack. dynamic instrumentation employed by others). By using static bi- Taint checking is very effective at catching exploits based on nary rewriting, we are also able to optimize the program in ways memory corruption errors, but it can add significant overhead to that are difficult or impossible with dynamic rewriting (see Section program execution because in general it requires monitoring execu- 4). Finally, our approach is also generalizable to multithreaded pro- tion of every instruction. For example, TaintCheck [7] implements grams, including operating system kernels, although our prototype taint checking by means of program emulation; consequently, pro- implementation at present only handles single-threaded application grams typically execute from 20–40 times slower. Special hardware programs. improves performance significantly [2, 3, 11], but it does not help The remainder of the paper is organized as follows. Section 2 the vast majority of programs that run on conventional platforms. summarizes dynamic taint analysis. Section 3 describes our sys- Recently, LIFT has significantly lowered taint checking overhead tem architecture and implementation. Section 4 explains how we by minimizing the code that is monitored and by heavily optimiz- optimize thread interaction to minimize communication and syn- ing taint-checking code [9]. However, adding taint-checking code chronization overhead. Section 5 shows that our prototype imple- to a program necessarily slows it down both by the time that is spent mentation is able to detect real security attacks. Section 6 gives 1 2012/12/20

}
}

@InProceedings{ChenEgeleWooEtAl2016,
  Title                    = {Towards Automated Dynamic Analysis for Linux-based Embedded Firmware},
  Author                   = {Daming D. Chen and Manuel Egele and Maverick Woo and David Brumley and ∗ Carnegie and Mellon University},
  Year                     = {2016},

  Abstract                 = {Commercial-off-the-shelf (COTS) network-enabled I. INTRODUCTION embedded devices are usually controlled by vendor firmware to perform integral functions in our daily lives. For example, With the proliferation of the so-called “Internet of Things”, wireless home routers are often the first and only line of defense an increasing number of embedded devices are being connected that separates a home user’s personal computing and information to the Internet at an alarming rate. Commodity networking devices from the Internet. Such a vital and privileged position in equipment such as routers and network-attached storage boxes the user’s network requires that these devices operate securely. are joined by IP cameras, thermostats, or even remotely- Unfortunately, recent research and anecdotal evidence suggest controllable power outlets. These devices frequently share that such security assumptions are not at all upheld by the devices certain technical characteristics, such as embedded system deployed around the world. on a chip (SOC) designs based on ARM or MIPS CPUs,
},
  Doi                      = {.org/10.14722/ndss.2016.23415},
  File                     = {:home/ccc/github/literature/article/Towards Automated Dynamic Analysis for.pdf:PDF},
  Review                   = {Towards Automated Dynamic Analysis for Linux-based Embedded Firmware Daming D. Chen∗, Manuel Egele†, Maverick Woo∗, and David Brumley∗ ∗ Carnegie Mellon University {ddchen, pooh, dbrumley}@cmu.edu † Boston University {megele}@bu.edu Abstract—Commercial-off-the-shelf (COTS) network-enabled I. INTRODUCTION embedded devices are usually controlled by vendor firmware to perform integral functions in our daily lives. For example, With the proliferation of the so-called “Internet of Things”, wireless home routers are often the first and only line of defense an increasing number of embedded devices are being connected that separates a home user’s personal computing and information to the Internet at an alarming rate. Commodity networking devices from the Internet. Such a vital and privileged position in equipment such as routers and network-attached storage boxes the user’s network requires that these devices operate securely. are joined by IP cameras, thermostats, or even remotely- Unfortunately, recent research and anecdotal evidence suggest controllable power outlets. These devices frequently share that such security assumptions are not at all upheld by the devices certain technical characteristics, such as embedded system deployed around the world. on a chip (SOC) designs based on ARM or MIPS CPUs, A first step to assess the security of such embedded device network connectivity via Ethernet or WiFi, and a wide variety firmware is the accurate identification of vulnerabilities. However, of communication interfaces such as GPIO, I2C, or SPI. the market offers a large variety of these embedded devices, Nevertheless, many of these devices are controlled by vendor which severely impacts the scalability of existing approaches in and chipset-specific firmware that is rarely, if ever, updated to this area. In this paper, we present FIRMADYNE, the first address security vulnerabilities affecting these devices. automated dynamic analysis system that specifically targets Linux- based firmware on network-connected COTS devices in a scalable Unfortunately, the poor security practices of these device manner. We identify a series of challenges inherent to the dynamic vendors are only further exacerbated by the privileged network analysis of COTS firmware, and discuss how our design decisions position that many of these devices occupy. For example, a address them. At its core, FIRMADYNE relies on software-based wireless router is frequently the first and only line of defense full system emulation with an instrumented kernel to achieve the between a user’s computing equipment (e.g., laptops, mobile scalability necessary to analyze thousands of firmware binaries phones, and tablets) and the Internet. An attacker that succeeds automatically. in compromising such a networking device is able to gain access to the user’s network, and can further reconfigure the We evaluate FIRMADYNE on a real-world dataset of 23,035 device to tamper with arbitrary network traffic. Since most firmware images across 42 device vendors gathered by our system. vendors have not taken any initiative to improve the security of Using a sample of 74 exploits on the 9,486 firmware images that our system can successfully extract, we discover that 887 firmware their devices, millions of home and small business networks are images spanning at least 89 distinct products are vulnerable to one left vulnerable to both known and unknown threats. As a first or more of the sampled exploit(s). This includes 14 previously- step towards improving the security of commodity computer unknown vulnerabilities that were discovered with the aid of equipment, we propose to address the challenge of accurately our framework, which affect 69 firmware images spanning at identifying vulnerabilities in embedded firmware head-on. least 12 distinct products. Furthermore, our results show that 11 of our tested attacks affect firmware images from more than Previous research on the security of embedded firmware one vendor, suggesting that code-sharing and common upstream can be categorized based on various analysis approaches. For manufacturers (OEMs) are quite prevalent. example, Zaddach et al. [19] perform dynamic analysis by partially offloading execution of firmware to actual hardware. While such an approach is precise, it incurs significant hurdles Note: This version has been corrected to account for 41 exploited firmware for large-scale analysis. First, the requirement that the analyst images that were not ping reachable, fix a typo in Fig. 1, and clarify Table VI. must obtain the physical hardware for the device under test poses a significant financial burden. Second, and more Permission to freely reproduce all or part of this paper for noncommercial importantly, the manual effort needed to identify and interface purposes is granted provided that copies bear this notice and the full citation with the debugging port on the device places strict limits on the on the first page. Reproduction for commercial purposes is strictly prohibited scalability of this technique, especially for consumer equipment without the prior written consent of the Internet Society, the first-named author that may not support hardware debugging functionality. (for reproduction of an entire paper only), and the author’s employer if the paper was prepared within the scope of employment. In contrast, Costin et al. [8] utilize static analysis techniques NDSS ’16, 21-24 February 2016, San Diego, CA, USA Copyright 2016 Internet Society, ISBN 1-891562-41-X to unpack the firmware of embedded devices and identify http://dx.doi.org/10.14722/ndss.2016.23415 potentially vulnerable code or binaries inside. While this

}
}

@InProceedings{ChenWagnerCaliforniaEtAl2002,
  Title                    = {MOPS: an Infrastructure for Examining Security Properties of Software},
  Author                   = {Hao Chen and David Wagner and University of California and at Berkeley and University of California and at Berkeley},
  Booktitle                = {CCS2002},
  Year                     = {2002},

  Abstract                 = {Software vulnerabilities are an enormous cause of security We describe a formal approach for finding bugs in security- incidents in computer systems. A system is only as secure as
},
  File                     = {:article\\MOPS an Infrastructure for Examining Security Properties of Software.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {• Property 1. Suppose a process uses the chroot system call to confine its access to a sub filesystem. In this,literature,article},
  Read                     = {未读},
  Review                   = {MOPS∗: an Infrastructure for Examining Security Properties of Software† Hao Chen David Wagner University of California at Berkeley University of California at Berkeley hchen@cs.berkeley.edu daw@cs.berkeley.edu ABSTRACT Software vulnerabilities are an enormous cause of security We describe a formal approach for finding bugs in security- incidents in computer systems. A system is only as secure as relevant software and verifying their absence. The idea is as its weakest link, and often the software is the weakest link. follows: we identify rules of safe programming practice, en- We can attribute software vulnerabilities to several causes. code them as safety properties, and verify whether these Some bugs, like buffer overruns in C, reflect poorly designed properties are obeyed. Because manual verification is too language features and can be avoided by switching to a safer expensive, we have built a program analysis tool to auto- language, like Java. However, safer programming languages mate this process. Our program analysis models the pro- alone cannot prevent many other security bugs, especially gram to be verified as a pushdown automaton, represents those involving higher level semantics. As a typical example, the security property as a finite state automaton, and uses OS system calls have implicit constraints on how they should model checking techniques to identify whether any state vi- be called; if coding errors cause a program to violate such olating the desired security goal is reachable in the program. constraints when interacting with the OS kernel, this may The major advantages of this approach are that it is sound introduce vulnerabilities. in verifying the absence of certain classes of vulnerabilities, In this paper, we focus on detecting violations of order- that it is fully interprocedural, and that it is efficient and ing constraints, also known as temporal safety properties. A scalable. Experience suggests that this approach will be use- temporal safety property dictates the order of a sequence ful in finding a wide range of security vulnerabilities in large of security-relevant operations. Our experience shows that programs efficiently. many rules of good programming practice for security pro- grams can be described by temporal safety properties. Al- though violating such properties may merely indicate risky Categories and Subject Descriptors features in a program in some cases, it often renders the D.4.6 [Operating Systems]: Security and Protection— program vulnerable to attack, depending on the nature of verification; D.2.4 [Software Engineering]: Software/ Pro- the violation. In either case, the ability to detect violations gram Verification—formal methods, model checking of the properties or to verify the satisfaction of them would be a significant help in reducing the frequency of software vulnerabilities. General Terms To illustrate the relevance of such temporal safety prop- erties, we give next a few examples that reflect prudent pro- Security, Languages, Verification gramming practice for Unix applications. Keywords • Property 1. Suppose a process uses the chroot system call to confine its access to a sub filesystem. In this security, model checking, verification, static analysis case, the process should immediately call chdir(“/”) to change its working directory to the root of the sub 1. INTRODUCTION filesystem. This rule can be described by the temporal safety prop- ∗MOPS: MOdel Checking Programs for Security properties erty that any call to chroot should be immediately fol- †This research was supported in part by DARPA contract lowed by a call to chdir(“/”). The program in Fig- N66001-01-C-8040 and by an equipment grant from Intel. ure 1(b) violates this property: it fails to call chdir(“/”) after chroot(“/var/ftp/pub”), so its current directory remains /var/ftp. As a result, a malicious user may ask the program to open the file ../../etc/passwd Permission to make digital or hard copies of all or part of this work for successfully even though this is outside the chroot jail personal or classroom use is granted without fee provided that copies are and the programmer probably intended to make it in- not made or distributed for profit or commercial advantage and that copies accessible. Here, the malicious user takes advantage bear this notice and the full citation on the first page. To copy otherwise, to of the method by which the operating system enforces republish, to post on servers or to redistribute to lists, requires prior specific permission chroot(new root). When a process requests access to aand/or a fee. CCS’02, November 18–22, 2002, Washington, DC, USA. file, the operating system follows every directory com- Copyright 2002 ACM 1-58113-612-9/02/0011 ...$5.00. ponent in the path of the file sequentially to locate the}
}

@Article{ChenZhangLiu2016,
  Title                    = {Dynamically Discovering Likely Memory Layout to Perform Accurate Fuzzing},
  Author                   = {Chen, Kai and Zhang, Yingjun and Liu, Peng},
  Journal                  = {IEEE Trans. Rel.},
  Year                     = {2016},
  Pages                    = {1–15},

  Doi                      = {10.1109/tr.2015.2512220},
  File                     = {:home/ccc/github/literature/article/Dynamically Discovering Likely Memory.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1558-1721},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TR.2015.2512220}
}

@Article{ChenXuKalbarczykEtAl2006,
  Title                    = {Security Vulnerabilities: From Analysis to Detection and Masking Techniques},
  Author                   = {Chen, S. and Xu, J. and Kalbarczyk, Z. and Iyer, K.},
  Journal                  = {Proc. IEEE},
  Year                     = {2006},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {407–418},
  Volume                   = {94},

  Doi                      = {10.1109/jproc.2005.862473},
  File                     = {:home/ccc/github/literature/article/Security Vulnerabilities\: From Analysis to Dectection and Masking Techniques.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1558-2256},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/JPROC.2005.862473}
}

@Article{ChowdhuryZulkernine2011,
  Title                    = {Using complexity, coupling, and cohesion metrics as early indicators of vulnerabilities},
  Author                   = {Chowdhury, Istehad and Zulkernine, Mohammad},
  Journal                  = {Journal of Systems Architecture},
  Year                     = {2011},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {294--313},
  Volume                   = {57},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Software security failures are common and the problem is growing. A vulnerability is a weakness in the software that, when exploited, causes a security failure. It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of software, because security concerns are often not addressed or known sufficiently early during the software development life cycle. Numerous studies have shown that complexity, coupling, and cohesion (CCC) related structural metrics are important indicators of the quality of software architecture, and software architecture is one of the most important and early design decisions that influences the final quality of the software system. Although these metrics have been successfully employed to indicate software faults in general, there are no systematic guidelines on how to use these metrics to predict vulnerabilities in software. If CCC metrics can be used to indicate vulnerabilities, these metrics could aid in the conception of more secured architecture, leading to more secured design and code and eventually better software. In this paper, we present a framework to automatically predict vulnerabilities based on CCC metrics. To empirically validate the framework and prediction accuracy, we conduct a large empirical study on fifty-two releases of Mozilla Firefox developed over a period of four years. To build vulnerability predictors, we consider four alternative data mining and statistical techniques - C4.5 Decision Tree, Random Forests, Logistic Regression, and Naive-Bayes - and compare their prediction performances. We are able to correctly predict majority of the vulnerability-prone files in Mozilla Firefox, with tolerable false positive rates. Moreover, the predictors built from the past releases can reliably predict the likelihood of having vulnerabilities in the future releases. The experimental results indicate that structural information from the non-security realm such as complexity, coupling, and cohesion are useful in vulnerability prediction. (C) 2010 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.sysarc.2010.06.003},
  Groups                   = {Code Mining},
  Si                       = {SI},
  Sn                       = {1383-7621},
  Tc                       = {11},
  Ut                       = {WOS:000289705000007},
  Z8                       = {1},
  Z9                       = {12},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{,
  Title                    = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication},
  Author                   = {Chucky:ExposingMissingChecksinSource and CodeforVulnerabilityDiscovery and CONFERENCEPAPER�OCTOBER2013 and DOI:10.1145/2508859.2516665 },
  Year                     = {2589},

  Doi                      = {10.1145/2508859.2516665},
  File                     = {:home/ccc/github/literature/article/Chucky- Exposing missing checks in source code for Vulnerability Discovery?CR 12?.pdf:PDF},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/258903333
Chucky:ExposingMissingChecksinSource CodeforVulnerabilityDiscovery CONFERENCEPAPER�OCTOBER2013 DOI:10.1145/2508859.2516665
CITATIONS READS 3 58
4AUTHORS:
FabianYamaguchi ChristianWressnegger Georg-August-Universit�tG�ttingen Georg-August-Universit�tG�ttingen 6PUBLICATIONS24CITATIONS 7PUBLICATIONS9CITATIONS
SEEPROFILE SEEPROFILE
HugoGascon KonradRieck Georg-August-Universit�tG�ttingen Georg-August-Universit�tG�ttingen 10PUBLICATIONS34CITATIONS 59PUBLICATIONS883CITATIONS
SEEPROFILE SEEPROFILE
Availablefrom:HugoGascon Retrievedon:19October2015

}
}

@InProceedings{CodeAnalysisLouridas1970,
  Title                    = {open source E d i t o r : C h r i s t o f E b e r t ■ A l c a t e l ■ c h r i s t o f . e b e r t @ a l c a t e l . c o m},
  Author                   = {Static Code and Analysis and Panagiotis Louridas},
  Year                     = {1970},

  File                     = {:home/ccc/github/literature/article/Static Code Analysis.pdf:PDF},
  Review                   = {open source E d i t o r : C h r i s t o f E b e r t ■ A l c a t e l ■ c h r i s t o f . e b e r t @ a l c a t e l . c o m Static Code Analysis Panagiotis Louridas ool has a tendency to collect static (perhaps even training) to do the work right. W electricity and thus to attract dust It’s therefore impossible to use them on a pro-and lint. Developers know that pro- ject’s complete code base.grams have a similar tendency to Moreover, the earlier we find bugs, the eas-attract defects and, furthermore, ier it is to fix them. Ideally, we would like tothat many of them aren’t visible to catch errors when we make them, or as close compilers. In the 1970s, Stephen Johnson, then afterward as possible, and not ipso facto with at the Bell Laboratories, wrote Lint, a tool to reviewing or testing. examine C source programs Most errors fall into known categories, as that had compiled without er- people tend to fall into the same traps repeat- rors and to find bugs that had edly. It’s exactly the predictability of people’s escaped detection.1 fallibility that gives tools such as Lint a chance. There are many ways to re- Lint worked by looking for known error pat- duce the number of bugs in a terns. It didn’t try to execute the program and program. Writing tests is one, compare actual with expected behavior, which and tools such as JUnit help is the dynamic analysis that we do when we programmers do this.2 Re- test. Instead, Lint trawled through the program search tells us that code re- source trying to match patterns. Such tools are views are probably the best called static checkers. They check our programs way to eliminate bugs. Unfortunately, getting for errors without executing them, in a process the right people together to study programs called static code analysis. and identify problem areas in them takes a lot Programmers usually employ static checkers of time. Code review teams also need practice after compilation and before testing. In this way, they work with a program that has an ini- tial indication of correctness (because it com- Editor’s introduction piles) and try to avoid well-known traps and pitfalls before measuring it against its specifica- Bugs are disturbing. This holds in summer with the insect types and tions (when it’s tested). Static checking is rela- year-round with the software types. While developers have long de- tively painless, although it can be humbling— ployed reviews, inspections, and different testing strategies to find especially the first time. Lint’s success has given software bugs, they don’t yet widely use the available semiautomatic today’s programmers many descendant tools, defect-detection techniques. Panagiotis Louridas explains here how static both open source and proprietary, that target code-analysis tools are used and what defects they can detect. As usual, different languages and operating systems. the column compares several open source tools together with a commer- cially available tool. They can help you reduce the number of bugs of all Static code checking in Java the software types—security, memory, data typing, and so on—before To see static code checking in action, let’s you deploy the more expensive verification techniques. start with the coding horror in figure 1. Al- —Christof Ebert though this program compiles, it fails to do what the programmer wants. The programmer intends to read a string from the user, substi- 5 8 I E E E S O F T W A R E P u b l i s h e d b y t h e I E E E C o m p u t e r S o c i e t y 0 7 4 0 - 7 4 5 9 / 0 6 / $ 2 0 . 0 0 © 2 0 0 6 I E E E

}
}

@TechReport{CodeControversiesJustEtAl2004,
  Title                    = {in the news F e a t u r e s E d i t o r : R e b e c c a L . D e u e l  r d e u e l @ c o m p u t e r . o r g},
  Author                   = {Source Code and Controversies and Not Just and about Security and Greg Goth},
  Year                     = {2004},
  Number                   = {done.},

  File                     = {:home/ccc/github/literature/article/Source Code Cotroversies Not Just about Security.pdf:PDF},
  Review                   = {in the news F e a t u r e s E d i t o r : R e b e c c a L . D e u e l  r d e u e l @ c o m p u t e r . o r g Source Code Controversies Not Just about Security Greg Goth n February 2004, Microsoft confirmed and in a big company there’s lots of turnover,” I that several million lines of Windows NT says Avi Rubin, technical director of the Johnsand Windows 2000 source code had been Hopkins Information Security Institute. “Ri-leaked and had made its way to the pub- vals try to entice talented programmers away,lic on Internet peer-to-peer networks and or people leave because they’re disgruntled.relay chats. Security analysts following And people keep copies of what they’ve done. the leak disagreed on whether it would lead to When I’ve worked on programming projects I any serious security lapses in the operating sys- have copies everywhere. I want to make sure tem’s affected versions. Three months later, the that, no matter what, I don’t lose this stuff I’ve Internet still operated as usual. worked so hard on. So while companies may In mid May 2004, a Russian Web site re- have strict rules about not making copies or ported someone had stolen some 800 Mbytes taking work off company premises, program- of source code for Cisco’s IOS 12.3 and 12.3t mers are very independent thinkers and proba- operating system. Once again, security ana- bly break those rules, so in some sense it’s al- lysts were widely quoted as wondering how most surprising we don’t have more of this. and if the stolen source code would be used for Maybe we do and just don’t know it.” malicious intent and how widespread any at- Although companies can try methods such tacks on the network might be. as keeping all source code on corporate virtual Yet, while the immediate discussions about private networks and severely penalizing de- the incidents might have focused on the most velopers who break security rules, Rubin says obvious implications of network vulnerabili- senior managers should consider every pro- ties—those more easily discovered in readable gram to be accessible to the public. This posi- source code than in compiled operating system tion would mandate a more secure design and or application code—the events responsible for implementation of code. those discussions could also be fairly narrowly “You may think you’re not opening your defined. If someone stole proprietary source code, but it may get opened for you, like what code, or otherwise let proprietary source code happened to Cisco and to others,” Rubin says. be leaked for malicious purposes, he or she was “That’s why it’s a lot safer if you’ve planned clearly in the wrong. But stemming such leaks, on it being opened. You don’t necessarily have especially in larger companies that design and to open it, but you should build it as though write some of the world’s most used and criti- it’s going to be opened.” cal software, can be incredibly difficult. The code’s rightful owners might be asked, and might ask themselves, how to better se- Secure code woes cure their code. However, the larger questions “In a big project, think about the sheer of the societal responsibilities software engi- number of people who can have access to code, neers must assume in writing, safeguarding, 9 6 I E E E S O F T W A R E P u b l i s h e d b y t h e I E E E C o m p u t e r S o c i e t y 0 7 4 0 - 7 4 5 9 / 0 4 / $ 2 0 . 0 0 © 2 0 0 4 I E E E

}
}

@Article{CollingbourneCadarKelly2014,
  Title                    = {Symbolic Crosschecking of Data-Parallel Floating-Point Code},
  Author                   = {Collingbourne, Peter and Cadar, Cristian and Kelly, Paul H. J.},
  Journal                  = {IEEE Trans. Software Eng.},
  Year                     = {2014},

  Month                    = {Jul},
  Number                   = {7},
  Pages                    = {710–737},
  Volume                   = {40},

  Doi                      = {10.1109/tse.2013.2297120},
  File                     = {:home/ccc/github/literature/article/Symbolic Crosschecking of Data-Parallel Floating-Point Code.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2013.2297120}
}

@InProceedings{,
  Title                    = {772 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 37, NO. 6, NOVEMBER/DECEMBER},
  Author                   = {Evaluating Complexity and Code Churn and Developer Activity and Metrics as Indicators},
  Year                     = {2011},
  Publisher                = {IEEE},

  Abstract                 = {Security inspection and testing require experts in security who think like an attacker. Security experts need to know code
},
  File                     = {:home/ccc/github/literature/article/Evaluating complexity, code churn, developer activity metrics as indicators of software vulnerabilities-??.pdf:PDF},
  Review                   = {772 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 37, NO. 6, NOVEMBER/DECEMBER 2011
Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators
of Software Vulnerabilities Yonghee Shin, Andrew Meneely, Laurie Williams, Member, IEEE, and Jason A. Osborne
Abstract?Security inspection and testing require experts in security who think like an attacker. Security experts need to know code
locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code
locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are
discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection
and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We
performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat
Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects.
The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent
false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced
the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.
Index Terms?Fault prediction, software metrics, software security, vulnerability prediction.
�
1 INTRODUCTION
A single exploited software vulnerability 1 can cause vulnerable modules is to build a statistical model using
severe damage to an organization. Annual world-wide software metrics that measure the attributes of the software losses caused from cyber attacks have been reported to be as products and development process related to software high as $226 billion [2]. Loss in stock market value in the vulnerabilities. Historically, prediction models trained days after an attack is estimated from $50 to $200 million using software metrics to find faults have been known to per organization [2]. The importance of detecting and be effective [4], [5], [6], [7], [8], [9], [10]. mitigating software vulnerabilities before software release However, prediction models must be trained on what is paramount. they are intended to look for. Rather than arming the
Experience indicates that the detection and mitigation of security expert with all the modules likely to contain faults, a vulnerabilities are best done by engineers specifically security prediction model can point toward the set of trained in software security and who ?think like an modules likely to contain what a security expert is looking
attacker? in their daily work [3]. Therefore, security testers for: security vulnerabilities. Establishing predictive power in
need to have specialized knowledge in and a mindset for a security prediction model is challenging because security
what attackers will try. If we could predict which parts of vulnerabilities and non-security-related faults have similar
the code are likely to be vulnerable, security experts can symptoms. Differentiating a vulnerability from a fault can
focus on these areas of highest risk. One way of predicting be nebulous even to a human, much less a statistical model. Additionally, the number of reported security vulnerabil-
1. An instance of a [fault] in the specification, development, or ities with which to train a model are few compared to configuration of software such that its execution can violate an [implicit nonsecurity-related faults. Colloquially, security prediction or explicit] security policy [1].
models are ?searching for a needle in a haystack.? In this paper, we investigate the applicability of three
. Y. Shin is with the College of Computing and Digital Media, DePaul types of software metrics to build vulnerability prediction University, 243 S. Wabash Ave., Chicago, IL 60614. models: complexity, code churn, and developer activity E-mail: yshin2@ncsu.edu.
. (CCD) metrics. Complexity can make code difficult toA. Meneely, and L. Williams are with the Department of Computer Science, North Carolina State University, 3231 EB II, 890 Oval Drive, understand and to test for security. Frequent or large Campus Box 8206, Raleigh, NC 27695-8206. amount of code change can introduce vulnerabilities. Poor E-mail: apmeneel@ncsu.edu, williams@csc.ncsu.edu. developer collaboration can diminish project-wide secure
. J.A. Osborne is with the Department of Statistics, North Carolina State University, 5238 SAS Hall, Campus Box 8203, Raleigh, NC 27695-8203. coding practices. E-mail: jaosborn@stat.ncsu.edu. The goal of this study is to guide security inspection and
Manuscript received 18 Mar. 2009; revised 10 Nov. 2009; accepted 2 July testing by analyzing if complexity, code churn, and developer 2010; published online 24 Aug. 2010. activity metrics can be used to 1) discriminate between Recommended for acceptance by K. Inoue. vulnerable and neutral files and 2) predict vulnerabilities. For For information on obtaining reprints of this article, please send e-mail to: tse@computer.org, and reference IEEECS Log Number TSE-2009-03-0060. this purpose, we performed empirical case studies on two Digital Object Identifier no. 10.1109/TSE.2010.81. widely used, large-scale open-source projects: the Mozilla
0098-5589/11/$26.00  2011 IEEE Published by the IEEE Computer Society

}
}

@InProceedings{CooperativeSearch-BasedSoftwareEtAl2014,
  Title                    = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 40, NO. 9, SEPTEMBER},
  Author                   = {A Cooperative and Parallel Search-Based and Software and Engineering Approach and for Code-Smells},
  Year                     = {2014},
  Publisher                = {IEEE},

  Abstract                 = {We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different
},
  File                     = {:home/ccc/github/literature/article/A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection..pdf:PDF},
  Review                   = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 40, NO. 9, SEPTEMBER 2014 841 A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection Wael Kessentini, Student Member, IEEE, Marouane Kessentini,Member, IEEE, Houari Sahraoui,Member, IEEE, Slim Bechikh,Member, IEEE, and Ali Ouni, Student Member, IEEE Abstract—We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells. Index Terms—Search-based software engineering, code-smells, software quality, distributed evolutionary algorithms Ç 1 INTRODUCTION SOURCE code of large systems is iteratively refined, [19], [20]. In these settings, rules are manually defined torestructured and evolved due to many reasons such as identify the key symptoms that characterize a code-smell correcting errors in design, modifying a design to accommo- using combinations of mainly quantitative (metrics), struc- date changes in requirements, and modifying a design to tural, and/or lexical information. However, in an exhaus- enhance existing features. Many studies reported that these tive scenario, the number of possible code-smells to software maintenance activities consume up to 90 percent manually characterize with rules can be large. For each of the total cost of a typical software project [12]. code-smell, rules that are expressed in terms of metric com- This high cost could potentially be greatly reduced by binations need substantial calibration efforts to find the providing automatic or semi-automatic solutions to increase right threshold value for each metric. Another important their understandability, adaptability and extensibility to issue is that translating symptoms into rules is not obvious avoid bad-practices. As a result, there has been much because there is no consensual symptom-based definition of research focusing on the study of bad design practices, also code-smells [13]. When consensus exists, the same symptom called code-smells, defects, anti-patterns or anomalies [13], could be associated to many code-smells types, which may [14], [15], [25] in the literature. Although these bad practices compromise the precise identification of code-smell types. are sometimes unavoidable, they should be in general pre- To address these issues, we proposed an approach based vented by the development teams and removed from their on the use of genetic programming (GP) to generate detec- code base as early as possible. In fact, detecting and remov- tion rules from examples of code-smells using structural ing these code-smells help developers to easily understand metrics [21]. However, the quality of the generated rules source code [13]. In this work, we focus on the detection of depends on the coverage of the different suspicious behav- code-smells. iors of code-smells, and it is difficult to ensure such cover- The vast majority of existing work in code-smells detec- age. Thus, there are still some uncertainties regarding the tion relies on declarative rule specification [15], [17], [18], detected code-smells due to the difficulty to evaluate the coverage of the base of code-smell examples. In another recent work, we proposed an approach based on an artificial  W. Kessentini, H. Sahraoui, and A. Ouni are with the Department of immune system metaphor to detect code-smells by devia- Computer Science, University of Montreal, Montreal, Quebec, Canada. tion with examples of good code-practices and well- E-mail: {kessentw, sahraouh, ouniali}@iro.umontreal.ca.  designed systems [22]. Some of the detected code fragmentsM. Kessentini and S. Bechikh are with the Department of Computer Science, University of Michigan, Dearborn, MI. that are different from well-designed code were not code- E-mail: {marouane, slim}@umich.edu. smells but just new good-practice behavior. Thus, we Manuscript received 23 Apr. 2013; revised 3 Apr. 2014; accepted 11 June believe that an efficient approach will be to combine both 2014. Date of publication 15 June 2014; date of current version 18 Sept. 2014. detection algorithms to find better consensus when detect- Recommended for acceptance by H.C. Gall. ing code-smells. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. We propose in this paper to consider code-smells detec- Digital Object Identifier no. 10.1109/TSE.2014.2331057 tion as a distributed optimization problem. The idea is that 0098-5589 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

}
}

@Article{CowanWaglePu2000,
  Title                    = {Buffer Overflows Attacks and Defenses for the Vulnerability of the Decade},
  Author                   = {Crispin Cowan and Perry Wagle and Calton Pu},
  Year                     = {2000},

  Abstract                 = {bility presents the attacker with exactly what they need:},
  File                     = {:article\\Buffer Overflows  Attacks and Defenses for the Vulnerability of the Decade.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {未读},
  Review                   = {综述了缓冲区溢出漏洞的类型和缓解方法，以及自己提出的StackGuard方法}
}

@InProceedings{,
  Title                    = {270 IEEE TRANSACTIONS ON RELIABILITY, VOL. 63, NO. 1, MARCH},
  Author                   = {Software Crash and Analysis for Automatic and Exploit and Generation on Binary and Programs },
  Year                     = {2014},
  Publisher                = {IEEE},

  Abstract                 = {This paper presents a new method, capable of auto- EBP Extended Base Register matically generating attacks on binary programs from software crashes.We analyze software crashes with a symbolic failuremodel Concolic Concrete and Symbolic Execution by performing concolic executions following the failure directed CVE Common Vulnerabilities and Exposures paths, using a whole system environment model and concrete ad- dress mapped symbolic memory in . We propose a new selec- DoS Denial of Service tive symbolic inputmethod and lazy evaluation on pseudo symbolic variables to handle symbolic pointers and speed up the process. EIP Extended Instruction Pointer This is an end-to-end approach able to create exploits from crash ELF Executable and Linking Format inputs or existing exploits for various applications, including most of the existing benchmark programs, and several large scale appli- GOT Global Offset Table cations, such as a word processor (Microsoft office word), a media IDS Intrusion Detection System player (mpalyer), an archiver (unrar), or a pdf reader (foxit).We can deal with vulnerability types including stack and heap overflows, IR Intermediate Representation format string, and the use of uninitialized variables. Notably, these applications have become software fuzz testing targets, but still re- LLVM Low Level Virtual Machine quire a manual process with security knowledge to produce mit- LOC Lines of Code igation-hardened exploits. Using this method to generate exploits is an automated process for software failures without source code. NOP No Operation The proposed method is simpler, more general, faster, and can be ROP Return-Oriented Programming scaled to larger programs than existing systems. We produce the exploits within one minute for most of the benchmark programs, SMT Satisfiability Modulo Theories including mplayer. We also transform existing exploits ofMicrosoft office word into new exploits within four minutes. The best speedup SQL Structured Query Language is 7,211 times faster than the initial attempt. For heap overflow vul- TCG Tiny Code Generator nerability, we can automatically exploit the unlink()macro of glibc, which formerly requires sophisticated hacking efforts. OS Operating System
},
  File                     = {:home/ccc/github/literature/article/(2014 J)Software Crash Analysis for Automatic Exploit Generation on Binary Programs.pdf:PDF},
  Review                   = {270 IEEE TRANSACTIONS ON RELIABILITY, VOL. 63, NO. 1, MARCH 2014
Software Crash Analysis for Automatic Exploit Generation on Binary Programs
Shih-Kun Huang, Member, IEEE, Min-Hsiang Huang, Po-Yen Huang, Han-Lin Lu, and Chung-Wei Lai
Abstract?This paper presents a new method, capable of auto- EBP Extended Base Register matically generating attacks on binary programs from software crashes.We analyze software crashes with a symbolic failuremodel Concolic Concrete and Symbolic Execution by performing concolic executions following the failure directed CVE Common Vulnerabilities and Exposures paths, using a whole system environment model and concrete ad- dress mapped symbolic memory in . We propose a new selec- DoS Denial of Service tive symbolic inputmethod and lazy evaluation on pseudo symbolic variables to handle symbolic pointers and speed up the process. EIP Extended Instruction Pointer This is an end-to-end approach able to create exploits from crash ELF Executable and Linking Format inputs or existing exploits for various applications, including most of the existing benchmark programs, and several large scale appli- GOT Global Offset Table cations, such as a word processor (Microsoft office word), a media IDS Intrusion Detection System player (mpalyer), an archiver (unrar), or a pdf reader (foxit).We can deal with vulnerability types including stack and heap overflows, IR Intermediate Representation format string, and the use of uninitialized variables. Notably, these applications have become software fuzz testing targets, but still re- LLVM Low Level Virtual Machine quire a manual process with security knowledge to produce mit- LOC Lines of Code igation-hardened exploits. Using this method to generate exploits is an automated process for software failures without source code. NOP No Operation The proposed method is simpler, more general, faster, and can be ROP Return-Oriented Programming scaled to larger programs than existing systems. We produce the exploits within one minute for most of the benchmark programs, SMT Satisfiability Modulo Theories including mplayer. We also transform existing exploits ofMicrosoft office word into new exploits within four minutes. The best speedup SQL Structured Query Language is 7,211 times faster than the initial attempt. For heap overflow vul- TCG Tiny Code Generator nerability, we can automatically exploit the unlink()macro of glibc, which formerly requires sophisticated hacking efforts. OS Operating System
Index Terms?Automatic exploit generation, bug forensics, soft- PC Path Conditions ware crash analysis, symbolic execution, taint analysis. POSIX Portable Operating System Interface
XSS Cross Site Scripting
ACRONYMS AND ABBREVIATIONS NOTATIONS AEG Automatic Exploit Generation The symbolic read data object. API Application Programming Interface The symbolic address expression. ASLR Address Space Layout Randomization The pseudo symbolic variable. EAX Extended Accumulator Register The memory snapshot.
All Dereference objects during the execution Manuscript received September 02, 2012; revised April 18, 2013; accepted of the state.
May 20, 2013. Date of publication January 20, 2014; date of current version Feb- ruary 27, 2014. This work was supported in part by NCP, TWISC, the National The Boolean expression. Science Council(NSC-101-2221-E-009-037-MY2, and NSC 100-2219-E-009- The mapping that maps each element of 005), and the Industrial Technology Research Institute of Taiwan (ITRI FY101 B3522Q1100). to a concrete address . S.-K. Huang is with the Information Technology Service Center, National The value of under the variable assignment
Chiao Tung University, Hsinchu 30010, Taiwan (e-mail: skhuang@cs.nctu.edu. tw). Associate Editor: S. Shieh. . M.-H. Huang, P.-Y. Huang, H.-L. Lu, and C.-W. Lai are with the Depart-
ment of Computer Science, National Chiao Tung University, Hsinchu 30010, Taiwan (e-mail: mhhuang@cs.nctu.edu.tw; luhl@cs.nctu.edu.tw; huangpy@cs. I. INTRODUCTION nctu.edu.tw; laicw@cs.nctu.edu.tw). C RAFTING exploits for control flow hijacking, SQL in-Color versions of one or more of the figures in this paper are available onlineat http://ieeexplore.ieee.org. jection, and cross-site scripting (XSS) attacks is typically Digital Object Identifier 10.1109/TR.2014.2299198 a manual process requiring security knowledge [1]. However,
0018-9529 � 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

}
}

@Article{CuiWangHaoEtAl2016,
  Title                    = {WhirlingFuzzwork: a taint-analysis-based {API} in-memory fuzzing framework},
  Author                   = {Cui, Baojiang and Wang, Fuwei and Hao, Yongle and Chen, Xiaofeng},
  Journal                  = {Soft Computing},
  Year                     = {2016},

  Month                    = {Jan},

  Doi                      = {10.1007/s00500-015-2017-6},
  File                     = {:home/ccc/github/literature/article/WhirlingFuzzwork\: a taint-analysis-based API in-memory fuzzing framework.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1433-7479},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s00500-015-2017-6}
}

@InProceedings{CuiPeinadoResearchEtAl,
  Title                    = {Tracking Rootkit Footprints with a Practical Memory Analysis System},
  Author                   = {Weidong Cui and Marcus Peinado and Microsoft Research and Microsoft Research},

  Abstract                 = {An important task in detecting and analyzing kernel rootkits is to identify all the changes a rootkit makes to
},
  File                     = {:article\\Tracking Rootkit Footprints with a Practical Memory Analysis System.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Tracking Rootkit Footprints with a Practical Memory Analysis System Weidong Cui Marcus Peinado Microsoft Research Microsoft Research wdcui@microsoft.com marcuspe@microsoft.com Zhilei Xu Ellick Chan Massachusetts Institute of Technology University of Illinois at Urbana-Champaign timxu@mit.edu emchan@illinois.edu Abstract An important task in detecting and analyzing kernel rootkits is to identify all the changes a rootkit makes to In this paper, we present MAS, a practical memory anal- an infected OS kernel for hijacking code execution or ysis system for identifying a kernel rootkit’s memory hiding its activities. We call these changes a rootkit’s footprint in an infected system. We also present two memory footprint. We perform this task in two common large-scale studies of applying MAS to 848 real-world scenarios: We detect if real-world computer systems are Windows kernel crash dumps and 154,768 potential mal- infected by kernel rootkits. We also analyze suspicious ware samples. software in a controlled environment. One can use either Error propagation and invalid pointers are two key execution tracing or memory analysis in a controlled en- challenges that stop previous pointer-based memory vironment, but is usually limited to memory analysis for traversal solutions from effectively and efficiently ana- real-world systems. In this paper we focus on the mem- lyzing real-world systems. MAS uses a new memory ory analysis approach since it can be applied in both sce- traversal algorithm to support error correction and stop narios. error propagation. Our enhanced static analysis allows After many years of research on kernel rootkits, we the MAS memory traversal to avoid error-prone opera- still lack a practical memory analysis system that is ac- tions and provides it with a reliable partial type assign- curate, robust, and performant. In other words, we ex- ment. pect such a practical system to correctly and quickly Our experiments show that MAS was able to analyze identify all memory changes made by a rootkit to arbi- all memory snapshots quickly with typical running times trary systems that may have a variety of kernel modules between 30 and 160 seconds per snapshot and with near loaded. Furthermore, we lack a large-scale study of ker- perfect accuracy. Our kernel malware study observes nel rootkit behaviors, partly because there is no practi- that the malware samples we tested hooked 191 differ- cal system that can analyze memory infected by kernel ent function pointers in 31 different data structures. With rootkits in an accurate, robust and performant manner. MAS, we were able to determine quickly that 95 out of In this paper, we present MAS, a practical memory the 848 crash dumps contained kernel rootkits. analysis system for identifying a rootkit’s memory foot- print. We also present the results of two large-scale ex- 1 Introduction periments in which we use MAS to analyze 837 kernel crash dumps of real-world systems running Windows 7, Kernel rootkits represent a significant threat to computer and 154,768 potential malware samples from the reposi- security because, once a rootkit compromises the OS ker- tory of a major commercial anti-malware vendor. These nel, it owns the entire software stack which allows it to are the two major contributions of this paper. evade detections and launch many kinds of attacks. For Previous work [2, 3, 19] has established that, to iden- instance, the Alureon rootkit [1] was infamous for steal- tify a rootkit’s memory footprint, we need to check not ing passwords and credit card data, running botnets, and only the integrity of kernel code and static data but also causing a large number of Windows systems to crash. the integrity of dynamic data, and the real challenge lies Kernel rootkits also present a serious challenge for mal- in the latter task. ware analysis because, to hide its existence, a rootkit at- In order to locate dynamic data, these systems first lo- tempts to manipulate the kernel code and data of an in- cate static data objects in each loaded module, then re- fected system. cursively follow the pointers in these objects and in all

}
}

@Article{DaoxinYijunZhi2016,
  Title                    = {Remote Control Trojan Vulnerability Mining Based on Reverse Analysis of Network Protocol},
  Author                   = {Pan Daoxin and Wang Yijun and Xue Zhi},
  Journal                  = {Computer Engineering},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {1000-3428(2016)42:2<146:JYWLXY>2.0.TX;2-F},
  Volume                   = {42},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In view of Trojan' s control of the popular Advanced Persistent Threat (APT) attack' s method, this paper presents an active defense idea,namely for fuzzing and paralysis attack of closed source code Trojan. It uses generalized suffix tree and hierarchical clustering algorithm to learn characteristics of Trojan' s network traffic to construct protocol format. Then it combines Peach with this Fuzz framework, automatically generating configuration file of Fuzz test through the protocol format, so as to largely enhance efficiency of Fuzz test. After a series of tests through remote control Trojan, it successfully discovers several Trojan' s vulnerabilities, which illustrates that the remote control Trojan vulnerability mining method is a kind of innovative and effective solution.},
  Groups                   = {Code Mining},
  Sn                       = {1000-3428},
  Tc                       = {0},
  Ut                       = {CSCD:5625835},
  Z1                       = {åºäºç½ç»åè®®éååæçè¿ç¨æ§å¶æ¨é©¬æ¼æ´ææ},
  Z2                       = {æ½éæ¬£EOLEOLçè½¶éªEOLEOLèè´¨},
  Z3                       = {è®¡ç®æºå·¥ç¨},
  Z4                       = {ä¸ºé²èæç»­æ§å¨èæ»å»ä¸­çè¿ç¨æ§å¶æ¨é©¬,æåºä¸ç§ä¸»å¨é²å¾¡æè·¯,å³éå¯¹ä¸å¬å¼æºä»£ç åç½ç»åè®®çæ¨é©¬ç¨åºè¿è¡æ¼æ´ææåç«çªæ»å»ãä½¿ç¨å¹¿ä¹åç¼æ ååå±æ¬¡EOLEOLèç±»ç­æ°æ®ææç®æ³éååææ¨é©¬ç½ç»åè®®çç¹å¾,èªå¨æé å¶åè®®æ ¼å¼ãå°å¶ä¸Fuzzæµè¯æ¡æ¶ç¸ç»å,éè¿å¯¼å¥ä¹åéååæå¾åºçåè®®æ ¼å¼èªå¨çæFuzEOLEOLzçéç½®æä»¶,ä»èè¾å¤§ç¨åº¦å°æé«æ¨¡ç³æµè¯åæ¼æ´æææçãç»è¿ä¸ç³»åéå¯¹å®éè¿ç¨æ§å¶æ¨é©¬ç¨åºçæµè¯,åç°è¥å¹²æ¨é©¬æ§å¶ç«¯çæ¼æ´,ä»èè¯´æè¯¥è¿ç¨æ§å¶æ¨EOLEOLé©¬æ¼æ´æææ¹æ³æ¯å¯è¡ãææç,å¹¶å·æä¸å®åæ°æ§ã},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{,
  Title                    = {FIE on Firmware: Finding Vulnerabilities in Embedded Systems using Symbolic Execution},
  Author                   = {Drew Davidson and Benjamin Moench and Somesh Jha and Thomas Ristenpart and University of Wisconsin?Madison},
  Year                     = {9319},

  File                     = {:home/ccc/github/literature/article/FIE on Firmware\: Finding Vulnerabilities in Embedded Systems using Symbolic Execution.pdf:PDF},
  Review                   = {FIE on Firmware: Finding Vulnerabilities in Embedded Systems using Symbolic Execution
Drew Davidson, Benjamin Moench, Somesh Jha, and Thomas Ristenpart, University of Wisconsin?Madison
This paper is included in the Proceedings of the 22nd USENIX Security Symposium. August 14?16, 2013 ? Washington, D.C., USA
ISBN 978-1-931971-03-4
Open access to the Proceedings of the 22nd USENIX Security Symposium 
is sponsored by USENIX

}
}

@InProceedings{Demand”otherwiseunacceptableInsteadtheywaitEtAl2003,
  Title                    = {S E C U R I T Y},
  Author                   = {“Patch on Demand” otherwise unacceptable and to patch orupgrade their systems when a vulnera-bility is announced. Instead and they wait and for news of an actual exploitation. In},
  Year                     = {2003},

  File                     = {:home/ccc/github/literature/article/"Patch on Demand" Saves Enven More Time.pdf:PDF},
  Review                   = {S E C U R I T Y “Patch on Demand” otherwise unacceptable, to patch orupgrade their systems when a vulnera-bility is announced. Instead, they wait for news of an actual exploitation. In Saves Even More many cases, this is simply too late. PATCH ON DEMAND What to do then? One new idea is to Time? integrate the vulnerability discovery,patch generation, and patch applica-tion cycles into a system that would automatically detect a new attack, ana- Angelos D. Keromytis, Columbia University lyze its modus operandi, determine the best software patch, and apply it at the desired level of granularity—LAN, enterprise, or Internet-wide. I n the June 2004 Security column(“A Patch in Nine SavesTime?”pp. 82-83), Bill Arbaugh A vaccination system couldmakes two interesting observa- automatically generate tions: ﬁrst, whoever has the tight- est observe-orient-decide-act (OODA) patches to protect an loop will prevail in a confrontation; application’s source code. second, the infection rates of recent worms suggest that the good guys are losing the battle. Arbaugh offers some sensible sug- attacks are those for which users Although the system would still gestions to vendors and security pro- receive no prior warning and thus have function by reacting to attacks, its fessionals on improving patch man- no preventive measures in place. response time would be significantly agement. However, the best indication To date, a combination of aggressive shorter. Perhaps most importantly, it that we are losing the battle is not the packet ﬁltering and proactive applica- could operate autonomously. infection rates of worms such as tion patching could—at least in prin- Furthermore, by retaining a focus on Slammer and Blaster, but the shrink- ciple—defeat all the worms we have software patching, this approach could ing interval between discovering and encountered. Although we could, in avoid at least some pitfalls of network- announcing a new vulnerability and theory, deploy patches and network ﬁl- based defense techniques. These tech- the appearance of a worm or attack ters automatically, the practicality of niques, which include packet and that exploits it. employing such measures and their content ﬁltering, can fall prey to worms The most recent example is the effect on regular system operation are that exploit opportunistic encryption, Witty worm, which effectively ex- an entirely different story. polymorphism, or metamorphism. ploited a vulnerability present in a Witty came close to being a zero-day Traditionial viruses use polymorphism small population of hosts—approxi- worm; for most organizations, it was. to make detection more difﬁcult: A mately 12,000 computers. Although it Few system administrators had even small decoder, which changes periodi- was ﬁrst discovered on 8 March 2004, seen the announcement before the cally, decrypts the virus’s main body the vendor didn’t announce the vul- attack, much less downloaded and prior to execution. nerability until 18 March, after it had installed the necessary software patch. Metamorphic viruses, on the other made a patch available. A little over Furthermore, as Arbaugh (“Win- hand, completely translate the virus a day after the announcement, Witty dows of Vulnerability: A Case Study of code to use different but equivalent made its ﬁrst appearance. Analysis,” Computer, Dec. 2000, pp. instructions every time it infects a new 52-59) and others such as Eric Rescorla target. Classic signature-matching tech- ZERO-DAY ATTACKS (“Security Holes … Who Cares?” Proc. niques are, for the most part, incapable Given such a short turnaround time, 12th Usenix Security Symp., Usenix, of detecting such viruses. we can reasonably expect to soon 2003, pp. 75-90) have noted, many Metamorphism, in particular, makes experience a zero-day worm. Zero-day administrators ﬁnd it impractical, if not detecting new viruses extremely difﬁ- 94 Computer

}
}

@Article{DeMillo1991,
  Title                    = {Constraint-Based Automatic Test Data Generation},
  Author                   = {Richard A. DeMillo and A. Je erson and O utt},
  Year                     = {1991},

  File                     = {:article\\Constraint-based automatic test data generation.10.1.1.91.1702.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {未读},
  Review                   = {Constraint-Based Automatic Test Data Generation Richard A. DeMillo A. Jeerson Outt Software Engineering Research Center Department of Computer Science Department of Computer Sciences Clemson University Purdue University Clemson, SC 29634 West Lafayette, IN 47907 February 21, 1997 Abstract This paper presents a new technique for automatically generating test data. The technique is based on mutation analysis and creates test data that approximates relative-adequacy. The technique is a fault-based technique that uses algebraic constraints to describe test cases designed to nd particular types of faults. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraints and solves them to create test cases for unit and module testing. Godzilla has been integrated with the Mothra testing system and has been used as an eective way to generate test data that kills program mutants. The paper includes an initial list of constraints and discusses some of the problems that have been solved to develop the complete implementation of the technique. Index Terms| Mothra, constraints, fault-based testing, mutation analysis, software testing, test data gen- eration. IEEE Transactions on Software Engineering, 17(9):900--910, September 1991.  Research supported in part by Contract F30602-85-C-0255 through Rome Air Development Center. 1

}
}

@Article{DeweyGiffin2011,
  Title                    = {Static detection of C++ vtable escape vulnerabilities in binary code},
  Author                   = {David Dewey and Jonathon Giffin},
  Year                     = {2011},

  Abstract                 = {escape vulnerabilities in widely deployed software, includ- ing Microsoft Excel [24], Adobe Reader [1], and Microsoft
},
  File                     = {:article\\Static detection of C++ vtable escape vulnerabilities in binary code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Static detection of C++ vtable escape vulnerabilities in binary code David Dewey Jonathon Giffin School of Computer Science, Georgia Institute of Technology {ddewey, giffin}@gatech.edu Abstract escape vulnerabilities in widely deployed software, includ- ing Microsoft Excel [24], Adobe Reader [1], and Microsoft Static binary code analysis is a longstanding technique Windows subsystem DLLs [23]. used to find security defects in deployed proprietary soft- Static detection of possible memory safety violations ware. The complexities of binary code compiled from employs offline analysis of source or binary code to find un- object-oriented source languages (e.g. C++) has limited the safe execution paths in the software. Many common com- utility of binary analysis to basic applications using simpler puting environments use proprietary software, so analysis coding constructs, so vulnerabilities in object-oriented code requires reverse engineering of binary code. For example, remain undetected. In this paper, we present vtable escape an intrusion prevention system (IPS) provider needs to be bugs—a class of type confusion errors specific to C++ code able to develop a signature that detects attacks against a present in real, deployed software including Adobe Reader, known vulnerability. An anti-virus vendor may need to in- Microsoft Office, and the Windows subsystem DLLs. We de- tegrate with an operating system in ways that are unpub- veloped automated binary code analyses able to statically lished. The complexity of binary code analysis is driven by detect vtable escape bugs by reconstructing high-level ob- the architectural decisions, choice of source language, and jects and analyzing the safety of their use. We implemented compiler options selected by the software’s developers. In our analysis in our own general object code decompilation this paper, we statically detect vtable escape vulnerabilities framework to demonstrate that classes of object-oriented by addressing the challenges faced by static binary analysis vulnerabilities can be uncovered from compiled binaries. of executable code that was originally developed in C++. We successfully found vtable escape bugs in a collection of Object code compiled from C++ includes complexities test samples that mimic publicly disclosed vulnerabilities in resulting from the object-oriented nature of the language. Adobe Reader and Microsoft Excel. With these new analy- When straight C code is compiled to its binary equivalent, ses, security analysts gain the ability to find common flaws the programmatic structure of the source code is largely left introduced by applications compiled from C++. intact. Most C-level constructs translate directly into assem- bly code. In contrast, C++ constructs are lost as the com- piler translates object-oriented source code into untyped as- 1. Introduction sembly, producing binary code with widespread program- matic flow through dynamically-computed indirect calls. Many security vulnerabilities in software arise due to The result is compiled code that obscures vulnerabilities violations of memory safety. Safety violations in object- from static analyzers. oriented programs include type confusion [21], a vulnera- We address the complexity of reverse engineering pro- bility that occurs when a pointer points to an object of an prietary software written for Microsoft Windows. Widely- incompatible type. By design, C++ permits the introduc- used applications are written in C++ so that they can take tion of type confusion errors via its static cast opera- advantage of the interoperability provided by the Windows tor: pointers can be unsafely downcast to incompatible child API. Reverse engineers working with Windows software types or cast through void. When vulnerable software sub- will regularly encounter C++ code, often more frequently sequently accesses virtual object methods through incom- than pure C. We create analysis passes in the IDA Pro dis- patible pointers, vtable escapes may occur: the process in- assembler and LLVM compiler framework that identify ob- terprets arbitrary memory beyond the bounds of a C++ ob- ject instantiation and construction, compute vtable bounds, ject’s vtable as a code pointer and unsafely transfers exe- track flows of object pointers to vtable dispatch locations, cution. Professional security analysts have identified vtable and verify the safety of the vtable accesses. Our analyzer re-

}
}

@Article{DeweyReavesTraynor2015,
  Title                    = {Uncovering Use-After-Free Conditions in Compiled Code},
  Author                   = {Dewey, David and Reaves, Bradley and Traynor, Patrick},
  Journal                  = {2015 10th International Conference on Availability, Reliability and Security},
  Year                     = {2015},

  Month                    = {Aug},

  Doi                      = {10.1109/ares.2015.61},
  File                     = {:home/ccc/github/literature/article/Uncovering Use-After-Free Conditions.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-1-4673-6590-1},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/ARES.2015.61}
}

@InProceedings{,
  Title                    = {A Gentle Introduction to Program Analysis},
  Author                   = {Is�?l Dillig and University of Texas and Austin},
  Year                     = {2014},

  File                     = {:home/ccc/github/literature/article/Static_Program_Analysis-plmw14.pdf:PDF},
  Review                   = {A Gentle Introduction to Program Analysis
Is�?l Dillig University of Texas, Austin
January 21, 2014 Programming Languages Mentoring Workshop
1 / 24

}
}

@Article{DingTanShar2015,
  Title                    = {Mining Patterns of Unsatisfiable Constraints to Detect Infeasible Paths},
  Author                   = {Sun Ding and Hee Beng Kuan Tan and Lwin Khin Shar},
  Journal                  = {2015 IEEE/ACM 10th International Workshop on Automation of Software Test (AST)},
  Year                     = {2015},
  Pages                    = {65--9},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Detection of infeasible paths is required in many areas including test coverage analysis, test case generation, security vulnerability analysis, etc. Existing approaches typically use static analysis coupled with symbolic evaluation, heuristics, or path-pattern analysis. This paper is related to these approaches but with a different objective. It is to analyze code of real systems to build patterns of unsatisfiable constraints in infeasible paths. The resulting patterns can be used to detect infeasible paths without the use of constraint solver and evaluation of function calls involved, thus improving scalability. The patterns can be built gradually. Evaluation of the proposed approach shows promising results.},
  Bn                       = {978-1-4673-7022-6},
  Cl                       = {Florence, Italy},
  Ct                       = {2015 IEEE/ACM 10th International Workshop on Automation of Software TestEOLEOL(AST)},
  Cy                       = {23-24 May 2015},
  Doi                      = {10.1109/AST.2015.21},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15341665},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{Vulnerabilitywatermarkingofnon-iidhostsignalstoattacksutilisingthestatisticsofEtAl2006,
  Title                    = {STEGANOGRAPHY AND DIGITAL WATERMARKING},
  Author                   = {Vulnerability of DM and watermarking of non-iid host and signals to attacks utilising the statistics of and independent components},
  Year                     = {2006},

  Abstract                 = {Security is one of the crucial requirements of a watermarking scheme, because hidden messages such as copyright information are likely to face hostile attacks. In this paper, we question the security of an important class of watermarking schemes based on dither modulation (DM). DM embedding schemes rely on the quantisation of a secret component according to an embedded message, and the strategies used to improve the security of these schemes are the use of a dither vector and the use of a secret carrier. In this paper we show that contrary to related works that deal with the security of spread spectrum and quantisation schemes, for non-iid host signals such as images, principal component analysis is not an appropriate technique to estimate the secret carrier. We propose the use of a blind source separation technique called independent component analysis (ICA) to estimate and remove the watermark. In the case of DM embedding, the watermark signal corresponds to a quantisation noise independent of the host signal. An attacking methodology using ICA is presented for digital images; this attack consists ﬁrst in estimating the secret carrier by an examination of the high-order statistics of the independent components and second in removing the embedded message by erasing the component related to the watermark. The ICA-based attack scheme is compared with a classical attack that has been proposed for attacking DM schemes. The results reported in this paper demonstrate how changes in natural image statistics can be used to detect watermarks and devise attacks. Different implementations of DM watermarking schemes such as pixel, DCT and spread transform- DM embedding can be attacked successfully. Our attack provides an accurate estimate of the secret key and an average improvement of 2 dB in comparison with optimal additive attacks. Such natural image statistics-based attacks may pose a serious threat against watermarking schemes which are based on quantisation techniques.
},
  Doi                      = {10.1049/ip-ifs:20055144},
  File                     = {:home/ccc/github/literature/article/Vulnerability of DM watermarking of non-iid hos signals to attacks utilising the statistics of independent components.pdf:PDF},
  Review                   = {STEGANOGRAPHY AND DIGITAL WATERMARKING Vulnerability of DM watermarking of non-iid host signals to attacks utilising the statistics of independent components P. Bas and J. Hurri Abstract: Security is one of the crucial requirements of a watermarking scheme, because hidden messages such as copyright information are likely to face hostile attacks. In this paper, we question the security of an important class of watermarking schemes based on dither modulation (DM). DM embedding schemes rely on the quantisation of a secret component according to an embedded message, and the strategies used to improve the security of these schemes are the use of a dither vector and the use of a secret carrier. In this paper we show that contrary to related works that deal with the security of spread spectrum and quantisation schemes, for non-iid host signals such as images, principal component analysis is not an appropriate technique to estimate the secret carrier. We propose the use of a blind source separation technique called independent component analysis (ICA) to estimate and remove the watermark. In the case of DM embedding, the watermark signal corresponds to a quantisation noise independent of the host signal. An attacking methodology using ICA is presented for digital images; this attack consists ﬁrst in estimating the secret carrier by an examination of the high-order statistics of the independent components and second in removing the embedded message by erasing the component related to the watermark. The ICA-based attack scheme is compared with a classical attack that has been proposed for attacking DM schemes. The results reported in this paper demonstrate how changes in natural image statistics can be used to detect watermarks and devise attacks. Different implementations of DM watermarking schemes such as pixel, DCT and spread transform- DM embedding can be attacked successfully. Our attack provides an accurate estimate of the secret key and an average improvement of 2 dB in comparison with optimal additive attacks. Such natural image statistics-based attacks may pose a serious threat against watermarking schemes which are based on quantisation techniques. 1 Introduction 1.1 Security of watermarking schemes According to [1, 2], a watermarking scheme is con- After more than 10 years of active development by the sidered secure if it is not possible to access the hidden watermarking scientiﬁc community, many of the pro- message channel. For example if an attacker is able to posed watermarking techniques are considered to be replace a hidden message by another one, then the mature because they are robust while preserving the watermarking scheme does not satisfy the security quality of the host data. However, if robustness and constraint. Many watermarking schemes claim to be ﬁdelity are mandatory requirements for a usable water- secure because they use a secret key during the embe- marking scheme, security is also a very important issue dding and detection process. However, this hypothesis is that is unfortunately rarely addressed. While robustness often too weak in real application scenarios and several denotes the ability to decode the watermark after security attacks have already been proposed. They can various operations (compression, ﬁltering, noise addi- be based on a full access to the detection process [3], the tion or geometric transforms), and ﬁdelity denotes the use of a symmetric detection scheme [4] or information property that the watermark is imperceptible, security is leakage when a database of hosts is watermarked using a more complex notion. the same secret key [2, 5, 6]. This paper focuses on the security of an important ª The Institution of Engineering and Technology 2006 class of watermarking schemes based on quantisation IEE Proceedings online no. 20055144 called dither-modulation (DM). The contribution of this doi:10.1049/ip-ifs:20055144 paper is to show how changes in the statistics of the host Paper received 14th December 2005 and in revised form 20th April 2006 signal can be used to detect watermarks and devise P. Bas is with Laboratoire des Images et des Signaux de Grenoble, 961 rue attacks against watermarking schemes. In general, de la Houille Blanche Domaine universitaire, B.P. 46 38402, Saint Martin watermarking may change the statistical properties of d’He`res cedex, France and also with Laboratory of Computer and the host signal, and this can be used to devise detection Information Science, Helsinki University of Technology, PO Box 5400, FI-02015 HUT, Finland and attack schemes. Here we show how a method used J. Hurri is with Helsinki Institute for Information Technology, Basic in blind source separation (BSS) called independent Research Unit, PO Box 68, FIN-00014, University of Helsinki, Finland component analysis (ICA; see Section 4.2.2) can be used E-mail: Patrick.Bas@inpg.fr to separate the watermark component from the host IEE Proc.-Inf. Secur., Vol. 153, No. 3, September 2006 127

}
}

@Article{DoganBetin-CanGarousi2014,
  Title                    = {Web application testing: A systematic literature review},
  Author                   = {Doğan, Serdar and Betin-Can, Aysu and Garousi, Vahid},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2014},

  Month                    = {May},
  Pages                    = {174–201},
  Volume                   = {91},

  Doi                      = {10.1016/j.jss.2014.01.010},
  File                     = {:home/ccc/github/literature/article/Web application testing\: A systematic literature review.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0164-1212},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.jss.2014.01.010}
}

@Article{DoudalisClauseVenkataramaniEtAl2012,
  Title                    = {Effective and Efficient Memory Protection Using Dynamic Tainting},
  Author                   = {Doudalis, Ioannis and Clause, James and Venkataramani, Guru and Prvulovic, Milos and Orso, Alessandro},
  Journal                  = {IEEE Trans. Comput.},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {87–100},
  Volume                   = {61},

  Doi                      = {10.1109/tc.2010.215},
  File                     = {:home/ccc/github/literature/article/Effective and Effcient Memory Protection Using Dynamic Tainting.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0018-9340},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TC.2010.215}
}

@InProceedings{DrS.CHITRA2Asst.Prof./CSEEtAl,
  Title                    = {Software Defect Prediction Using Software Metrics - A survey},
  Author                   = {K.PUNITHA1 Dr and . S.CHITRA2 and 1. Asst.Prof./CSE and Bhajarang Engineering College and Tiruvallur and Chennai and TamilNadu},

  Abstract                 = {Traditionally software metrics have been used to produce quality software. On the other hand, the define the complexity of the program, to estimate programming software development companies cannot risk their time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the business by shipping poor quality software [4] as it metric values are to be used in mathematical equations results in customer dissatisfaction. Bugs in software designed to represent a model of the software process, metrics product cause much loss of time and money. However, associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully learning from past experience, it would be possible to apply. Work on the mechanics of implementing metrics predict bugs in advance for new software products. To programs. The goal of this research is to help developers achieve this, we must first know which programs are identify defects based on existing software metrics using data more failure-prone than others. With this knowledge, we mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in can search for properties of the program or its the development and maintenance phase. This research focuses development process that commonly correlate with in identifying defective modules and hence the scope of causes of bugs. Previous studies have shown that, of the software that needs to be examined for defects can be overall development process 27% man hour is consumed prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology by testing [5]. To ameliorate the testing process we can helps in identifying modules that require immediate attention use the defect prediction models. These models can be and hence the reliability of the software can be improved faster used in defect prediction, risk analysis, effort estimation, as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the software testability and maintainability, and reliability Data mining algorithm. To initiate this process we initially analysis during early phases of software development. It propose to evaluate the existing classification algorithms and can also be used in business risk minimization by based on its weakness we propose a novel Neural network predicting the quality of the software in the early stages algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. of the software development lifecycle (SDLC). This Index Terms: Software defect prediction, software defect- would not only help in increasing client’s satisfaction but proneness prediction, machine learning, scheme evaluation. also trim down the cost of correction of defects. It has been reported in [4] that the cost of defect correction is 1. INTRODUCTION significantly high after software testing. An additional 
},
  File                     = {:home/ccc/github/literature/article/06508369.pdfSoftware Defect Prediction Using Software metrics - A survey.pdf:PDF},
  Review                   = {Software Defect Prediction Using Software Metrics - A survey K.PUNITHA1 Dr . S.CHITRA2 1. Asst.Prof./CSE, Bhajarang Engineering College, Tiruvallur, Chennai, TamilNadu. Email:dr.punithak@gmail.com 2. Professor/CSE, Er. Perumal Manimekalai College of Engineering, Hosur, Krishnagiri, Tamil Nadu. Abstract: Traditionally software metrics have been used to produce quality software. On the other hand, the define the complexity of the program, to estimate programming software development companies cannot risk their time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the business by shipping poor quality software [4] as it metric values are to be used in mathematical equations results in customer dissatisfaction. Bugs in software designed to represent a model of the software process, metrics product cause much loss of time and money. However, associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully learning from past experience, it would be possible to apply. Work on the mechanics of implementing metrics predict bugs in advance for new software products. To programs. The goal of this research is to help developers achieve this, we must first know which programs are identify defects based on existing software metrics using data more failure-prone than others. With this knowledge, we mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in can search for properties of the program or its the development and maintenance phase. This research focuses development process that commonly correlate with in identifying defective modules and hence the scope of causes of bugs. Previous studies have shown that, of the software that needs to be examined for defects can be overall development process 27% man hour is consumed prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology by testing [5]. To ameliorate the testing process we can helps in identifying modules that require immediate attention use the defect prediction models. These models can be and hence the reliability of the software can be improved faster used in defect prediction, risk analysis, effort estimation, as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the software testability and maintainability, and reliability Data mining algorithm. To initiate this process we initially analysis during early phases of software development. It propose to evaluate the existing classification algorithms and can also be used in business risk minimization by based on its weakness we propose a novel Neural network predicting the quality of the software in the early stages algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. of the software development lifecycle (SDLC). This Index Terms: Software defect prediction, software defect- would not only help in increasing client’s satisfaction but proneness prediction, machine learning, scheme evaluation. also trim down the cost of correction of defects. It has been reported in [4] that the cost of defect correction is 1. INTRODUCTION significantly high after software testing. An additional As our dependency on software is increasing, advantage of early defect prediction is better resource software quality is becoming gradually more and more planning [7] and test planning [6], [7]. Therefore, the key important in present era. Software used almost of developing reliable quality software within time and everywhere and in every tread of life. Software budget is to identify defect prone modules at an early consequences such as fault and failures may diminish the SDLC stage by using defect prediction models. The quality of software which leads to customer importance of defect prediction is evident from the dissatisfaction [1]. A software failure is the departure of research work conducted in this regard. the system from its required behavior; error is the incongruity between the required and actual 2. PREVIOUS STUDIES functionality; whereas adjudged or hypothesized cause As our dependency on software is increasing, of an error is a fault [2], which is also known as a defect software quality is becoming gradually more and more (or as a bug) among software professionals [3]. Due to important in present era. Software used almost the increasing of complexity and the constraints under everywhere and in every tread of life. Software which the software is developed, it is too difficult to consequences such as fault and failures may diminish the quality of software which leads to customer 

}
}

@InProceedings{,
  Title                    = {Software Defect Prediction Using Software Metrics - A survey},
  Author                   = {K.PUNITHA1 Dr and . S.CHITRA2 and 1. Asst.Prof./CSE and Bhajarang Engineering College and Tiruvallur and Chennai and TamilNadu},

  Abstract                 = {Traditionally software metrics have been used to produce quality software. On the other hand, the define the complexity of the program, to estimate programming software development companies cannot risk their time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the business by shipping poor quality software [4] as it metric values are to be used in mathematical equations results in customer dissatisfaction. Bugs in software designed to represent a model of the software process, metrics product cause much loss of time and money. However, associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully learning from past experience, it would be possible to apply. Work on the mechanics of implementing metrics predict bugs in advance for new software products. To programs. The goal of this research is to help developers achieve this, we must first know which programs are identify defects based on existing software metrics using data more failure-prone than others. With this knowledge, we mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in can search for properties of the program or its the development and maintenance phase. This research focuses development process that commonly correlate with in identifying defective modules and hence the scope of causes of bugs. Previous studies have shown that, of the software that needs to be examined for defects can be overall development process 27% man hour is consumed prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology by testing [5]. To ameliorate the testing process we can helps in identifying modules that require immediate attention use the defect prediction models. These models can be and hence the reliability of the software can be improved faster used in defect prediction, risk analysis, effort estimation, as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the software testability and maintainability, and reliability Data mining algorithm. To initiate this process we initially analysis during early phases of software development. It propose to evaluate the existing classification algorithms and can also be used in business risk minimization by based on its weakness we propose a novel Neural network predicting the quality of the software in the early stages algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. of the software development lifecycle (SDLC). This Index Terms: Software defect prediction, software defect- would not only help in increasing client?s satisfaction but proneness prediction, machine learning, scheme evaluation. also trim down the cost of correction of defects. It has been reported in [4] that the cost of defect correction is 1. INTRODUCTION significantly high after software testing. An additional 
},
  File                     = {:home/ccc/github/literature/article/Software Defect Prediction Using Software metrics - A survey.pdf:PDF},
  Review                   = {Software Defect Prediction Using Software Metrics - A survey 
K.PUNITHA1 Dr . S.CHITRA2 1. Asst.Prof./CSE, Bhajarang Engineering College, Tiruvallur, Chennai, TamilNadu. 
Email:dr.punithak@gmail.com 2. Professor/CSE, Er. Perumal Manimekalai College of Engineering, Hosur, Krishnagiri, Tamil Nadu. 
 
Abstract: Traditionally software metrics have been used to produce quality software. On the other hand, the define the complexity of the program, to estimate programming software development companies cannot risk their time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the business by shipping poor quality software [4] as it metric values are to be used in mathematical equations results in customer dissatisfaction. Bugs in software designed to represent a model of the software process, metrics product cause much loss of time and money. However, associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully learning from past experience, it would be possible to apply. Work on the mechanics of implementing metrics predict bugs in advance for new software products. To programs. The goal of this research is to help developers achieve this, we must first know which programs are identify defects based on existing software metrics using data more failure-prone than others. With this knowledge, we mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in can search for properties of the program or its the development and maintenance phase. This research focuses development process that commonly correlate with in identifying defective modules and hence the scope of causes of bugs. Previous studies have shown that, of the software that needs to be examined for defects can be overall development process 27% man hour is consumed prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology by testing [5]. To ameliorate the testing process we can helps in identifying modules that require immediate attention use the defect prediction models. These models can be and hence the reliability of the software can be improved faster used in defect prediction, risk analysis, effort estimation, as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the software testability and maintainability, and reliability Data mining algorithm. To initiate this process we initially analysis during early phases of software development. It propose to evaluate the existing classification algorithms and can also be used in business risk minimization by based on its weakness we propose a novel Neural network predicting the quality of the software in the early stages algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. of the software development lifecycle (SDLC). This Index Terms: Software defect prediction, software defect- would not only help in increasing client?s satisfaction but proneness prediction, machine learning, scheme evaluation. also trim down the cost of correction of defects. It has been reported in [4] that the cost of defect correction is 1. INTRODUCTION significantly high after software testing. An additional 
As our dependency on software is increasing, advantage of early defect prediction is better resource 
software quality is becoming gradually more and more planning [7] and test planning [6], [7]. Therefore, the key 
important in present era. Software used almost of developing reliable quality software within time and 
everywhere and in every tread of life. Software budget is to identify defect prone modules at an early 
consequences such as fault and failures may diminish the SDLC stage by using defect prediction models. The 
quality of software which leads to customer importance of defect prediction is evident from the 
dissatisfaction [1]. A software failure is the departure of research work conducted in this regard. 
the system from its required behavior; error is the 
incongruity between the required and actual 2. PREVIOUS STUDIES functionality; whereas adjudged or hypothesized cause As our dependency on software is increasing, 
of an error is a fault [2], which is also known as a defect software quality is becoming gradually more and more 
(or as a bug) among software professionals [3]. Due to important in present era. Software used almost 
the increasing of complexity and the constraints under everywhere and in every tread of life. Software 
which the software is developed, it is too difficult to consequences such as fault and failures may diminish the quality of software which leads to customer 

}
}

@Article{D铆azBermejo2013,
  Title                    = {Static analysis of source code security: Assessment of tools against SAMATE tests},
  Author                   = {D铆az, Gabriel and Bermejo, Juan Ram贸n},
  Journal                  = {Information and Software Technology},
  Year                     = {2013},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1462鈥�1476},
  Volume                   = {55},

  Doi                      = {10.1016/j.infsof.2013.02.005},
  File                     = {:article\\Static analysis of source code security_Assessment of tools against SAMATE tests.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2013.02.005}
}

@Article{EdvardssonComputerInformationScience1999,
  Title                    = {A Survey on Automatic Test Data Generation},
  Author                   = {Jon Edvardsson and and Dept. of ComputerInformation Science},
  Year                     = {1999},

  Abstract                 = {data generation. This is the most appropriate classi- fication in terms of test data generation, although the
},
  File                     = {:article\\A survey on automatic test data generation.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {对现有文献中的测试数据自动生成技术进行了综述。}
}

@InProceedings{EnckGilbertChunEtAl2010,
  Title                    = {TaintDroid: An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones},
  Author                   = {William Enck and Peter Gilbert and Byung-Gon Chun and The Pennsylvania and State University and Duke University and Intel Labs},
  Year                     = {2010},

  Abstract                 = {Resolving the tension between the fun and utility of running third-party mobile applications and the privacy
},
  Doi                      = {ng,},
  File                     = {:article\\TaintDroid - An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones..pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {TaintDroid: An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones William Enck Peter Gilbert Byung-Gon Chun The Pennsylvania State University Duke University Intel Labs Landon P. Cox Jaeyeon Jung Patrick McDaniel Anmol N. Sheth Duke University Intel Labs The Pennsylvania State University Intel Labs Abstract Resolving the tension between the fun and utility of running third-party mobile applications and the privacy Today’s smartphone operating systems frequently fail risks they pose is a critical challenge for smartphone plat- to provide users with adequate control over and visibility forms. Mobile-phone operating systems currently pro- into how third-party applications use their private data. vide only coarse-grained controls for regulating whether We address these shortcomings with TaintDroid, an ef- an application can access private information, but pro- ficient, system-wide dynamic taint tracking and analy- vide little insight into how private information is actu- sis system capable of simultaneously tracking multiple ally used. For example, if a user allows an application sources of sensitive data. TaintDroid provides realtime to access her location information, she has no way of analysis by leveraging Android’s virtualized execution knowing if the application will send her location to a environment. TaintDroid incurs only 14% performance location-based service, to advertisers, to the application overhead on a CPU-bound micro-benchmark and im- developer, or to any other entity. As a result, users must poses negligible overhead on interactive third-party ap- blindly trust that applications will properly handle their plications. Using TaintDroid to monitor the behavior of private data. 30 popular third-party Android applications, we found 68 instances of potential misuse of users’ private infor- This paper describes TaintDroid, an extension to the mation across 20 applications. Monitoring sensitive data Android mobile-phone platform that tracks the flow of with TaintDroid provides informed use of third-party ap- privacy sensitive data through third-party applications. plications for phone users and valuable input for smart- TaintDroid assumes that downloaded, third-party appli- phone security service firms seeking to identify misbe- cations are not trusted, and monitors–in realtime–how having applications. these applications access and manipulate users’ personaldata. Our primary goals are to detect when sensitive data 1 Introduction leaves the system via untrusted applications and to facil-itate analysis of applications by phone users or external A key feature of modern smartphone platforms is a security services [33, 55]. centralized service for downloading third-party applica- Analysis of applications’ behavior requires sufficient tions. The convenience to users and developers of such contextual information about what data leaves a device “app stores” has made mobile devices more fun and use- and where it is sent. Thus, TaintDroid automatically ful, and has led to an explosion of development. Apple’s labels (taints) data from privacy-sensitive sources and App Store alone served nearly 3 billion applications af- transitively applies labels as sensitive data propagates ter only 18 months [4]. Many of these applications com- through program variables, files, and interprocess mes- bine data from remote cloud services with information sages. When tainted data are transmitted over the net- from local sensors such as a GPS receiver, camera, mi- work, or otherwise leave the system, TaintDroid logs the crophone, and accelerometer. Applications often have le- data’s labels, the application responsible for transmitting gitimate reasons for accessing this privacy sensitive data, the data, and the data’s destination. Such realtime feed- but users would also like assurances that their data is used back gives users and security services greater insight into properly. Incidents of developers relaying private infor- what mobile applications are doing, and can potentially mation back to the cloud [35, 12] and the privacy risks identify misbehaving applications. posed by seemingly innocent sensors like accelerome- To be practical, the performance overhead of the Taint- ters [19] illustrate the danger. Droid runtime must be minimal. Unlike existing so-

}
}

@Article{Engler2002,
  Title                    = {Bugs as Deviant Behavior: A General Approach to Inferring Errors in Systems Code},
  Author                   = {Dawson Engler},
  Year                     = {2002},

  Abstract                 = {the biggest obstacle to finding many bugs is simply knowing what rules to check. Manually discovering any significant
},
  File                     = {:article\\Bugs as Deviant Behavior  A General Approach.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Bugs as Deviant Behavior: A General Approach to Inferring Errors in Systems Code Dawson Engler, David Yu Chen, Seth Hallem, Andy Chou, and Benjamin Chelf Computer Systems Laboratory Stanford University Stanford, CA 94305, U.S.A. Abstract the biggest obstacle to finding many bugs is simply knowing what rules to check. Manually discovering any significant A major obstacle to finding program errors in a real system number of rules a system must obey is a dispiriting adven- is knowing what correctness rules the system must obey. ture, especially when it must be repeated for each new re- These rules are often undocumented or specified in an ad lease of the system. In a large open source project such as hoc manner. This paper demonstrates techniques that auto- Linux, most rules evolve from the uncoordinated effort of matically extract such checking information from the source hundreds or thousands of developers. The end result is an code itself, rather than the programmer, thereby avoiding ad hoc collection of conventions encoded in millions of lines the need for a priori knowledge of system rules. of code with almost no documentation. The cornerstone of our approach is inferring programmer Since manually finding rules is difficult, we instead fo- “beliefs” that we then cross-check for contradictions. Beliefs cus on techniques to automatically extract rules from source are facts implied by code: a dereference of a pointer, p, im- code without a priori knowledge of the system. We want to plies a belief that p is non-null, a call to “unlock(l)” implies find what is incorrect without knowing what is correct. This that l was locked, etc. For beliefs we know the programmer problem has two well-known solutions: contradictions and must hold, such as the pointer dereference above, we im- common behavior. How can we detect a lie? We can cross- mediately flag contradictions as errors. For beliefs that the check statements from many witnesses. If two contradict, we programmer may hold, we can assume these beliefs hold and know at least one is wrong without knowing the truth. Sim- use a statistical analysis to rank the resulting errors from ilarly, how can we divine accepted behavior? We can look most to least likely. For example, a call to “spin lock” at examples. If one person acts in a given way, it may be followed once by a call to “spin unlock” implies that the correct behavior or it may be a coincidence. If thousands of programmer may have paired these calls by coincidence. If people all do the same action, we know the majority is prob- the pairing happens 999 out of 1000 times, though, then it ably right, and any contradictory action is probably wrong is probably a valid belief and the sole deviation a probable without knowing the correct behavior. error. The key feature of this approach is that it requires Our approach collects sets of programmer beliefs, which no a priori knowledge of truth: if two beliefs contradict, we are then checked for contradictions. Beliefs are facts about know that one is an error without knowing what the correct the system implied by the code. We examine two types of belief is. beliefs: MUST beliefs and MAY beliefs. MUST beliefs are Conceptually, our checkers extract beliefs by tailoring directly implied by the code, and there is no doubt that the rule “templates” to a system – for example, finding all func- programmer has that belief. A pointer dereference implies tions that fit the rule template “<a> must be paired with that a programmer must believe the pointer is non-null (as- <b>.” We have developed six checkers that follow this con- suming they want safe code). MAY beliefs are cases where ceptual framework. They find hundreds of bugs in real sys- we observe code features that suggest a belief but may in- tems such as Linux and OpenBSD. From our experience, stead be a coincidence. A call to “a” followed by a call to “b” they give a dramatic reduction in the manual effort needed implies the programmer may believe they must be paired, to check a large system. Compared to our previous work [9], but it could be a coincidence. these template checkers find ten to one hundred times more Once we have a set of beliefs, we do two things. For a rule instances and derive properties we found impractical to set of MUST beliefs, we look for contradictions. Any con- specify manually. tradiction implies the existence of an error in the code. For a set including MAY beliefs, we must separate valid beliefs 1 Introduction from coincidences. We start by assuming all MAY beliefs are MUST beliefs and look for violations (errors) of these We want to find as many serious bugs as possible. In our ex- beliefs. We then use a statistical analysis to rank each er- perience, the biggest obstacle to finding bugs is not the need ror by the probability of its beliefs. If a particular belief for sophisticated techniques nor the lack of either bugs or is observed in 999 out of 1000 cases, then it is probably a correctness constraints. Simple techniques find many bugs valid belief. If the belief happens only once, it is probably a and systems are filled with both rules and errors. Instead, coincidence. We apply the above approach by combining it with our prior work [9]. That work used system-specific static analy- ses to find errors with a fixed set of manually found and spec- ified rules (e.g.,“spin lock(l)must be paired with spin unlock(l)”). It leveraged the fact that abstract rules commonly map to fairly simple source code sequences. For example, one can check the rule above by inspecting each path after a call

}
}

@InProceedings{EnglerDunbarSystemsEtAl2007,
  Title                    = {Under-constrained Execution: Making Automatic Code Destruction Easy and Scalable},
  Author                   = {Dawson Engler and Daniel Dunbar and Computer Systems and Laboratory},
  Year                     = {2007},

  Doi                      = {ng},
  File                     = {:article\\Under-constrained Execution_ Making Automatic Code Destruction Easy and Scalable.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {ample: they do not work when run at user-space, yet almost all checking tools only work on user-space programs},
  Read                     = {未读},
  Review                   = {Under-constrained Execution: Making Automatic Code Destruction Easy and Scalable Dawson Engler and Daniel Dunbar Computer Systems Laboratory Stanford University Categories and Subject Descriptors large program makes it difficult to reach all code, even with D.2.5 [Software Engineering]: Testing and Debugging— new symbolic tools. Additionally, code guarded by com- Testing tools, Symbolic execution plex dependencies, for example global configuration options, thread scheduling, or intricate code paths can be tricky to G hit in practice. Finally, some programs just cannot run ineneral Terms the test environment and require laborious construction of Reliability, Languages a fake environment. Operating systems are an obvious ex- Keywords ample: they do not work when run at user-space, yet almost all checking tools only work on user-space programs. Bug finding, symbolic execution, dynamic analysis. An alternative is to cut the code to check out of its con- taining system and test it in isolation. This approach alle- 1. INTRODUCTION viates the burden of executing the entire system and allows Software testing is well-recognized as a crucial part of the testing modules that may not be able to natively run on the modern software development process. However, manual testing system. On the other hand, cutting subsystems out testing is labor intensive and often fails to produce impres- of large real systems has proven quite difficult because of sive coverage results. Random testing is easily applied but their deep entanglement with the surrounding system, col- gets poor coverage on complex code. Recent work has at- loquially referred to as “environment problem.” tacked these problems using Recent work has tried to minimize this effort. JPF in-symbolic execution to automat- ically generate high-coverage test inputs [3, 6, 4, 8, 5, 2]. troduced lazy initialization, which automatically generates At a high-level these tools use variations on the follow- complex data structures, potentially guided by user specifi- ing idea. Instead of running code on manually or randomly cation of invariants that must hold on them [7]. CUTE [8] constructed input, they run it on symbolic input initially al- does a similar thing using built-in consistency checks. Un- lowed to be “anything.” They substitute program variables fortunately, for large pre-existing systems it is not practical with symbolic values and replaces concrete program oper- to expect the system to provide built-in consistency checks ations with ones that manipulate symbolic values. When or to require the user to manually specify input precondi- program execution branches based on a symbolic value the tions. DART [6] makes up simple data structure elements ex system (conceptually) follows both branches at once, main- nihilo without user specifications. However, given its blind- taining a set of constraints called the path condition which ness to program invariants, what it makes up can easily must hold on execution of that path. When a path termi- cause explosions of false positives. nates or hits a bug, a test case can be generated by solving This paper’s contribution is the idea of under-constrained the current path condition to find concrete values. Assuming execution, a simple but powerful twist on symbolic execution deterministic code, feeding this concrete input to an unin- that makes it possible to take an arbitrary function and run strumented version of the checked code will cause it to follow it without initializing any of it data structures or doing en- the same path and hit the same bug. vironmental modelling, yet still find quality errors with few However, these tools (and all dynamic tools) assume you false positives. Under-constrained execution lets symbolic can run the code you want to check in the first place. In the values be explicitly marked as being under-constrained, indi- easiest case, testing just runs an entire application. This cating that their symbolic values may violate preconditions. requires no special work: just compile the program and exe- (I.e., that constraints on their symbolic values are missing, cute it. However, the exponential number of code paths in a such as that a pointer is not null.) It then works almost identically to symbolic execution with lazy initialization, but with one change: if an error involves an under-constrained operand u, it only flags the error if it can prove the error Permission to make digital or hard copies of all or part of this work for must occur for all values of u. Otherwise it asserts that the personal or classroom use is granted without fee provided that copies are constraint needed to prevent the error holds and continues not made or distributed for profit or commercial advantage and that copies execution. For example, given the expression x/u if u is bear this notice and the full citation on the first page. To copy otherwise, to under-constrained but it can prove u = 0 (perhaps because republish, to post on servers or to redistribute to lists, requires prior specific p of a prior comparison) then it will emit a “divide-by-zero”ermission and/or a fee. ISSTA’07, July 9–12, 2007, London, England, United Kingdom error. Otherwise it adds the constraint u 6= 0 and continues.. Copyright 2007 ACM 978-1-59593-734-6/07/0007 ...$5.00.

}
}

@InProceedings{EnglerMusuvathi2004,
  Title                    = {Static Analysis versus Software Model Checking for Bug Finding},
  Author                   = {Dawson Engler and Madanlal Musuvathi},
  Booktitle                = {VMCAI 2004},
  Year                     = {2004},
  Editor                   = {B. Steffen and G. Levi},
  Pages                    = {191−210},
  Publisher                = {Springer},
  Series                   = {LNCS},
  Volume                   = {2937},

  File                     = {:article\\Static analysis versus software model checking for bug finding.pdf:PDF},
  Groups                   = {source code vulnerability},
  Review                   = {Static Analysis versus Software Model Checking for Bug Finding Dawson Engler and Madanlal Musuvathi Computer Systems Laboratory Stanford University Stanford, CA 94305, U.S.A. {engler,madan}@cs.stanford.edu 1 Introduction This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better — it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong. This paper documents some of the lessons learned over the course of using software model checking for three years and three projects. The first two projects used both static analysis and model checking, while the third used only model checking, but sharply re-enforced the trade-offs we had previously observed. The first project, described in Section 3, checked FLASH cache coherence protocol implementation code [1]. We first used static analysis to find violations of FLASH-specific rules (e.g., that messages are sent in such a way as to prevent deadlock) [2] and then, in follow-on work, applied model checking [3]. A startling result (for us) was that despite model checking’s power, it found far fewer errors than relatively shallow static analysis: eight bugs versus 34. The second project, described in Section 4, checked three AODV network protocol [4] implementations. Here we first checked them with CMC [5], a model checker that directly checks C implementations. We then statically analyzed them. Model checking worked well, finding 42 errors (roughly 1 per 300 lines of code), about half of which involve protocol properties difficult to check statically. However, in the class of properties both methods could handle, static analysis found more errors than model checking. Also, it took much less effort: a couple of hours, while our model checking effort took approximately three weeks. The final project, described in Section 5, used CMC on the Linux TCP net- work stack implementation. The most startling result here was just how difficult it is to model check real code that was not designed for it. It turned out to be easier to run the entire Linux Kernel along with the TCP implementation in CMC rather than cut TCP out of Linux and make a working test harness. We found 4 bugs in the Linux TCP implementation. The main goal of this paper is to compare the merits of the two approaches for finding bugs in system software. In the properties that could be checked B. Steffen and G. Levi (Eds.): VMCAI 2004, LNCS 2937, pp. 191−210, 2004.  Springer-Verlag Berlin Heidelberg 2004

}
}

@InProceedings{,
  Title                    = {Static Analysis versus Software Model Checking for Bug Finding},
  Author                   = {Dawson Engler and Madanlal Musuvathi},
  Booktitle                = {VMCAI 2004},
  Year                     = {2004},
  Editor                   = {B. Steffen and G. Levi},
  Pages                    = {191?210},
  Publisher                = {Springer},
  Series                   = {LNCS},
  Volume                   = {2937},

  File                     = {:home/ccc/github/literature/article/Static Analysis versus Software Model Checking for Bug Finding.pdf:PDF},
  Review                   = {Static Analysis versus Software Model Checking for Bug Finding
Dawson Engler and Madanlal Musuvathi
Computer Systems Laboratory Stanford University
Stanford, CA 94305, U.S.A. {engler,madan}@cs.stanford.edu
1 Introduction
This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better ? it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong.
This paper documents some of the lessons learned over the course of using software model checking for three years and three projects. The first two projects used both static analysis and model checking, while the third used only model checking, but sharply re-enforced the trade-offs we had previously observed.
The first project, described in Section 3, checked FLASH cache coherence protocol implementation code [1]. We first used static analysis to find violations of FLASH-specific rules (e.g., that messages are sent in such a way as to prevent deadlock) [2] and then, in follow-on work, applied model checking [3]. A startling result (for us) was that despite model checking?s power, it found far fewer errors than relatively shallow static analysis: eight bugs versus 34.
The second project, described in Section 4, checked three AODV network protocol [4] implementations. Here we first checked them with CMC [5], a model checker that directly checks C implementations. We then statically analyzed them. Model checking worked well, finding 42 errors (roughly 1 per 300 lines of code), about half of which involve protocol properties difficult to check statically. However, in the class of properties both methods could handle, static analysis found more errors than model checking. Also, it took much less effort: a couple of hours, while our model checking effort took approximately three weeks.
The final project, described in Section 5, used CMC on the Linux TCP net- work stack implementation. The most startling result here was just how difficult it is to model check real code that was not designed for it. It turned out to be easier to run the entire Linux Kernel along with the TCP implementation in CMC rather than cut TCP out of Linux and make a working test harness. We found 4 bugs in the Linux TCP implementation.
The main goal of this paper is to compare the merits of the two approaches for finding bugs in system software. In the properties that could be checked
B. Steffen and G. Levi (Eds.): VMCAI 2004, LNCS 2937, pp. 191?210, 2004. ? Springer-Verlag Berlin Heidelberg 2004

}
}

@InProceedings{EschweilerYakdanGerhards-PadillaEtAl2016,
  Title                    = {discovRE: Efficient Cross-Architecture Identification of Bugs in Binary Code},
  Author                   = {Sebastian Eschweiler and Khaled Yakdan and Elmar Gerhards-Padilla and ∗University of Bonn and Germany †Fraunhofer FKIE and Germany},
  Year                     = {2016},

  Abstract                 = {The identification of security-critical vulnerabilities programs are used by millions of people on a daily basis. is a key for protecting computer systems. Being able to perform Prominent recent examples of these cases include the Heart- this process at the binary level is very important given that bleed vulnerability in the cryptographic library OpenSSL [5], many software projects are closed-source. Even if the source the “Shellshock” vulnerability in GNU Bash [8], and the code is available, compilation may create a mismatch between POODLE vulnerability in the SSLv3 protocol [7]. Given the the source code and the binary code that is executed by the processor, causing analyses that are performed on source code to evolving nature of programs and the increasing complexity, fail at detecting certain bugs and thus potential vulnerabilities. efficient and accurate approaches to identify vulnerabilities in Existing approaches to find bugs in binary code 1) use dynamic large code bases are needed. analysis, which is difficult for firmware; 2) handle only a single architecture; or 3) use semantic similarity, which is very slow There has been an extensive research on identifying vul- when analyzing large code bases. nerabilities at the source code level. Security research has
},
  Doi                      = {.org/10.14722/ndss.2016.23185},
  File                     = {:home/ccc/github/literature/article/discovre-efficient-cross-architecture-identification-bugs-binary-code.pdf:PDF},
  Review                   = {discovRE: Efficient Cross-Architecture Identification of Bugs in Binary Code Sebastian Eschweiler∗†, Khaled Yakdan∗†, Elmar Gerhards-Padilla†, ∗University of Bonn, Germany †Fraunhofer FKIE, Germany {yakdan, eschweil}@cs.uni-bonn.de elmar.gerhards-padilla@fkie.fraunhofer.de Abstract—The identification of security-critical vulnerabilities programs are used by millions of people on a daily basis. is a key for protecting computer systems. Being able to perform Prominent recent examples of these cases include the Heart- this process at the binary level is very important given that bleed vulnerability in the cryptographic library OpenSSL [5], many software projects are closed-source. Even if the source the “Shellshock” vulnerability in GNU Bash [8], and the code is available, compilation may create a mismatch between POODLE vulnerability in the SSLv3 protocol [7]. Given the the source code and the binary code that is executed by the processor, causing analyses that are performed on source code to evolving nature of programs and the increasing complexity, fail at detecting certain bugs and thus potential vulnerabilities. efficient and accurate approaches to identify vulnerabilities in Existing approaches to find bugs in binary code 1) use dynamic large code bases are needed. analysis, which is difficult for firmware; 2) handle only a single architecture; or 3) use semantic similarity, which is very slow There has been an extensive research on identifying vul- when analyzing large code bases. nerabilities at the source code level. Security research has focused on finding specific types of vulnerabilities, such as In this paper, we present a new approach to efficiently buffer overflows [66], integer-based vulnerabilities [17, 59], search for similar functions in binary code. We use this method insufficient validation of input data [37], or type-confusion to identify known bugs in binaries as follows: starting with a vulnerable binary function, we identify similar functions in vulnerabilities [42]. A similar line of research to our work other binaries across different compilers, optimization levels, is identifying bugs in source code by searching for code operating systems, and CPU architectures. The main idea is to fragments similar to an already known venerability [27, 34, compute similarity between functions based on the structure of 35, 38, 63]. Unfortunately, performing bug search at the the corresponding control flow graphs. To minimize this costly source level only is not sufficient for two reasons: first, many computation, we employ an efficient pre-filter based on numeric prominent and popular software projects such as MS Office, features to quickly identify a small set of candidate functions. Skype, and Adobe Flash Player are closed-source, and are This allows us to efficiently search for similar functions in large thus only available in the binary form. Second, compilation code bases. optimizations may change some code properties, creating a We have designed and implemented a prototype of our mismatch between the source code of a program and the approach, called discovRE, that supports four instruction set compiled binary code that is executed on the processor [15]. architectures (x86, x64, ARM, MIPS). We show that discovRE is For example, zeroing a buffer containing sensitive information four orders of magnitude faster than the state-of-the-art academic before freeing it may be marked as useless code and thus approach for cross-architecture bug search in binaries. We also removed by the compiler since the written values are never show that we can identify Heartbleed and POODLE vulnerabilities used at later stages. As a result, performing analysis on the in an Android system image that contains over 130,000 native source code may fail to detect certain vulnerabilities. This ARM functions in about 80 milliseconds. illustrates the importance of being able to perform bug search at the binary level. I. INTRODUCTION Bug search at the binary level is very challenging. Due to One key problem in computer security is the identification the NP-complete nature of the compiler optimization problem of bugs and security-critical vulnerabilities in software. Despite [12], even recompiling the same source code with the same intensive efforts by the research community and the industry, compiler and optimization options can potentially alter the vulnerabilities continue to emerge regularly in popular soft- resulting binary. In many cases the same source code is com- ware projects. Unfortunately, a single flaw in program code, piled using different toolchains, i.e., compilers or optimization such as the failure to check buffer boundaries or sanitize levels. This can heavily alter the generated binary code, making input data, can render the whole program vulnerable, which it extremely difficult to identify the binary code fragments that can have a huge security impact given that popular software stemmed from the same source code fragment. This is even further complicated by the fact the same source code can be Permission to freely reproduce all or part of this paper for noncommercial cross-compiled for different CPU architectures, which results purposes is granted provided that copies bear this notice and the full citation in binaries that differ, among others, in instruction sets, register on the first page. Reproduction for commercial purposes is strictly prohibited names, and function calling conventions. without the prior written consent of the Internet Society, the first-named author (for reproduction of an entire paper only), and the author’s employer if the Previous work to identify bugs in binary code either relied paper was prepared within the scope of employment. on dynamic analysis [e.g., 23], supported a single architec- NDSS ’16, 21-24 February 2016, San Diego, CA, USA Copyright 2016 Internet Society, ISBN 1-891562-41-X ture [e.g., 26], or used semantic similarity [e.g., 53, 44]. Dy- http://dx.doi.org/10.14722/ndss.2016.23185 namic analysis relies on architecture-specific tools to execute

}
}

@Article{Evans2002,
  Title                    = {Improving Security Using Extensible Lightweight Static Analysis},
  Author                   = {David Evans},
  Journal                  = {IEEE SOFTWARE},
  Year                     = {2002},

  File                     = {:article\\Improving Security Using  Extensible Lightweight  Static Analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis David Evans and David Larochelle, University of Virginia uilding secure systems involves numerous complex and challeng- B ing problems, ranging from building strong cryptosystems anddesigning authentication protocols to producing a trust modeland security policy. Despite these challenges, most security at- tacks exploit either human weaknesses—such as poorly chosen passwords and careless configuration—or software implementation flaws. Although it’s hard to do much about human frailties, some help is available through education, better interface design, and secu- cryptographic problems. Analyses of other Security attacks rity-conscious defaults. With software im- vulnerability and incident reports reveal sim- that exploit plementation flaws, however, the problems ilar repetition. For example, David Wagner well-known are typically both preventable and well and his colleagues found that buffer over- implementation understood. flow vulnerabilities account for approxi- flaws occur with Analyzing reports of security attacks mately 50 percent of the Software Engineer- disturbing frequency quickly reveals that most attacks do not re- ing Institute’s CERT advisories. 2 sult from clever attackers discovering new So why do developers keep making the because the software kinds of flaws, but rather stem from re- same mistakes? Some errors are caused by development process peated exploits of well-known problems. legacy code, others by programmers’ care- does not include Figure 1 summarizes Mitre’s Common Vul- lessness or lack of awareness about security techniques for nerabilities and Exposures list of 190 entries concerns. However, the root problem is that preventing those from 1 January 2001 through 18 September while security vulnerabilities, such as buffer flaws. The authors 2001.1 Thirty-seven of these entries are stan- overflows, are well understood, the tech- have developed an dard buffer overflow vulnerabilities (includ- niques for avoiding them are not codified extensible tool that ing three related memory-access vulnerabili- into the development process. Even conscien- detects common ties), and 11 involve format bugs. Most of tious programmers can overlook security is- flaws using the rest also reveal common flaws detectable sues, especially those that rely on undocu- lightweight by static analysis, including resource leaks mented assumptions about procedures and static analysis. (11), file name problems (19), and symbolic data types. Instead of relying on program-links (20). Only four of the entries involve mers’ memories, we should strive to produce 4 2 I E E E S O F T W A R E J a n u a r y / F e b r u a r y 2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0 © 2 0 0 2 I E E E

}
}

@InProceedings{EvansBenameurElderEtAl2014,
  Title                    = {Large-Scale Evaluation of a Vulnerability Analysis Framework},
  Author                   = {Nathan S. Evans and Azzedine Benameur and Matthew C. Elder and Symantec Research and Labs Symantec and Research Labs and Symantec Research and Labs},
  Booktitle                = {CSET'14},
  Year                     = {2014},

  Abstract                 = {Ensuring that exploitable vulnerabilities do Previous experimentation using an independent test and not exist in a piece of software written using type-unsafe evaluation suite demonstrated MINESTRONE’s effec- languages (e.g., C/C++) is still a challenging, largely un- tiveness at detecting two types of vulnerabilities: mem- solved problem. Current commercial security tools are improving but still have shortcomings, including limited de- ory corruption and null pointer errors [7]. Research on tection rates for certain vulnerability classes and high false- MINESTRONE and experimentation with an improved positive rates (which require a security expert’s knowledge test suite has continued and is described in this paper. to analyze). To address this there is a great deal of ongoing In particular, the MINESTRONE prototype addresses research in software vulnerability detection and mitiga- and is tested against four vulnerability types now (the tion as well as in experimentation and evaluation of the associated software security tools. We present the second- first two are new): number handling, resource drain, generation prototype of the MINESTRONE architecture memory corruption, and null pointer errors. The test suite along with a large-scale evaluation conducted under the improvements include the usage of real-world programs IARPA STONESOUP program. This second evaluation for testing, which enables testing against much larger, includes improvements in the scale and realism of the more realistic test programs: from 1KLOC in [7] to test suite with real-world test programs up to 200+KLOC. This paper presents three main contributions. First, we 200+KLOC in the current paper. show that the MINESTRONE framework remains a useful The remainder of the paper is organized as follows. tool for evaluating real-world software for security vulner- First, we present related work. Next, we detail our abilities. Second, we enhance the existing tools to provide methodology and the test suite used for our experiment detection of previously omitted vulnerabilities. Finally, we provide an analysis of the test corpus and give lessons and present the experimental setup used for evaluation of
},
  File                     = {:article\\Large-scale evalution of a vulnerability analysis framework-cset14.pdf:PDF},
  Groups                   = {source code vulnerability},
  Journal                  = {CSET'14},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Large-Scale Evaluation of a Vulnerability Analysis Framework Nathan S. Evans Azzedine Benameur Matthew C. Elder Symantec Research Labs Symantec Research Labs Symantec Research Labs Herndon, VA Herndon, VA Herndon, VA nathan evans@symantec.com azzedine benameur@symantec.com matthew elder@symantec.com Abstract—Ensuring that exploitable vulnerabilities do Previous experimentation using an independent test and not exist in a piece of software written using type-unsafe evaluation suite demonstrated MINESTRONE’s effec- languages (e.g., C/C++) is still a challenging, largely un- tiveness at detecting two types of vulnerabilities: mem- solved problem. Current commercial security tools are improving but still have shortcomings, including limited de- ory corruption and null pointer errors [7]. Research on tection rates for certain vulnerability classes and high false- MINESTRONE and experimentation with an improved positive rates (which require a security expert’s knowledge test suite has continued and is described in this paper. to analyze). To address this there is a great deal of ongoing In particular, the MINESTRONE prototype addresses research in software vulnerability detection and mitiga- and is tested against four vulnerability types now (the tion as well as in experimentation and evaluation of the associated software security tools. We present the second- first two are new): number handling, resource drain, generation prototype of the MINESTRONE architecture memory corruption, and null pointer errors. The test suite along with a large-scale evaluation conducted under the improvements include the usage of real-world programs IARPA STONESOUP program. This second evaluation for testing, which enables testing against much larger, includes improvements in the scale and realism of the more realistic test programs: from 1KLOC in [7] to test suite with real-world test programs up to 200+KLOC. This paper presents three main contributions. First, we 200+KLOC in the current paper. show that the MINESTRONE framework remains a useful The remainder of the paper is organized as follows. tool for evaluating real-world software for security vulner- First, we present related work. Next, we detail our abilities. Second, we enhance the existing tools to provide methodology and the test suite used for our experiment detection of previously omitted vulnerabilities. Finally, we provide an analysis of the test corpus and give lessons and present the experimental setup used for evaluation of learned from the test and evaluation. our framework. We then present and discuss the results and improvements we have made to the current tech- I. INTRODUCTION nologies to increase detection of vulnerabilities. Finally, Software security is, or should be, one of the most we share our lessons learned and conclude. important concerns for development teams. As such, many secure development methodologies have arisen [1], II. RELATED WORK [2]. While these methods prevent a number of naı¨ve mis- takes they can never prevent all vulnerabilities. Software There are a number of existing commercial tools for companies rely extensively on vulnerability detection finding vulnerabilities in source code. Previous studies tools [3], [4] to help test software for security vulner- found that these tools have significant gaps in detection abilities. These tools significantly reduce the number of capabilities across the tools and there is little overlap bugs and are fine grained. However, they are plagued by in the detection capabilities of the tools [5]. The end a large number of false positives, which requires expert result is that in practice an organization needs to run as domain knowledge to identify real vulnerabilities. The many different vulnerability detection tools as possible effectiveness of existing commercial tools is limited: a in order to get the best detection coverage. Another com- study by the NSA [5] demonstrated that the combination mon drawback of vulnerability detection tools, especially of five vulnerability detection tools against a C and C++ those based on static analysis, is the high rate of false test suite left 40% of the vulnerabilities unidentified. positive detections. MINESTRONE [6] is a software vulnerability testing Symbolic execution tools [8] can be used to identify framework for C and C++ languages. It is a hybrid software bugs or vulnerabilities. While their results are framework combining multiple tailored detection tools fine grained they have proven to be hard to use with that works on both source code and binaries and provides real-world programs [7] and suffer from a huge state an architecture for replicated execution and confinement. exploration problem.

}
}

@InProceedings{,
  Title                    = {To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Function},
  Author                   = {with an Existing and Exploit and 1Awad Younis and 1Yashwant K. Malaiya and 1Charles Anderson and 1Indrajit Ray and 1Computer Science Department and Colorado State University and Fort Collins and CO and USA},
  Year                     = {4959},

  Abstract                 = {combining all of the products they have studied only 15% of Not all vulnerabilities are equal. Some recent studies have shown disclosed vulnerabilities are ever exploited. Thus, identifying that only a small fraction of vulnerabilities that have been what characterizes a vulnerability having an exploit is needed; it reported has actually been exploited. Since finding and addressing can identify code that are more likely than others to have exploits potential vulnerabilities in a program can take considerable time and help security testers focus on areas of highest risk, thus saving and effort, recently effort has been made to identify code that is limited resources and time. It should be noted that having a more likely to be vulnerable. This paper tries to identify the reported exploit does not necessarily mean some company or attributes of the code containing a vulnerability that makes the individuals have suffered a real attack. It means that a proof for code more likely to be exploited. We examine 183 vulnerabilities exploiting a vulnerability exists. Obtaining data on real attacks is from the National Vulnerability Database for Linux Kernel and challenging because such data is generally kept confidential. Apache HTTP server. These include eighty-two vulnerabilities Therefore, we will use the presence of an exploit as the ground that have been found to have an exploit according to the Exploit truth for characterizing exploited vulnerabilities. Database. We characterize the vulnerable functions that have no Discriminating between a vulnerability that has no exploit from exploit and the ones that have an exploit using eight metrics. The the one that has an exploit is challenging because both of them results show that the difference between a vulnerability that has have similar characteristics. Besides, the number of vulnerabilities no exploit and the one that has an exploit can potentially be with a reported exploit are few compared to the vulnerabilities characterized using the chosen software metrics. However, without a reported exploit. Although vulnerability exploitability predicting exploitation of vulnerabilities is more complex than can be characterized by external factors such as attacker profile, predicting just the presence of vulnerabilities and further research software market share, etc., the focus of this study is on predicting is needed using metrics that consider security domain knowledge vulnerability exploitability using internal attributes. This can help for enhancing the predictability of vulnerability exploits. software developers predict vulnerabilities exploitability on the 
},
  File                     = {:home/ccc/github/literature/article/To Fear or Not to Fear That is the Question\: Code Characteristics of a Vulnerable Functionwith an Existing Exploit.pdf:PDF},
  Keywords                 = {development side rather than the deployment side. Vulnerabilities Severity; Exploitability; Software security; The objective of this research is to investigate what could Exploits; Data mining and machine learning; Feature selection; characterize a code containing a vulnerability with an exploit. To Prediction; Software metrics. address this objective, we have studied 183 vulnerabilities from},
  Review                   = {To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Function 
with an Existing Exploit 1Awad Younis, 1Yashwant K. Malaiya, 1Charles Anderson, and 1Indrajit Ray 1Computer Science Department, Colorado State University, Fort Collins, CO 80523, USA 
{younis,malaiya,anderson,indrajit}@cs.colostate.edu 
ABSTRACT combining all of the products they have studied only 15% of Not all vulnerabilities are equal. Some recent studies have shown disclosed vulnerabilities are ever exploited. Thus, identifying that only a small fraction of vulnerabilities that have been what characterizes a vulnerability having an exploit is needed; it reported has actually been exploited. Since finding and addressing can identify code that are more likely than others to have exploits potential vulnerabilities in a program can take considerable time and help security testers focus on areas of highest risk, thus saving and effort, recently effort has been made to identify code that is limited resources and time. It should be noted that having a more likely to be vulnerable. This paper tries to identify the reported exploit does not necessarily mean some company or attributes of the code containing a vulnerability that makes the individuals have suffered a real attack. It means that a proof for code more likely to be exploited. We examine 183 vulnerabilities exploiting a vulnerability exists. Obtaining data on real attacks is from the National Vulnerability Database for Linux Kernel and challenging because such data is generally kept confidential. Apache HTTP server. These include eighty-two vulnerabilities Therefore, we will use the presence of an exploit as the ground that have been found to have an exploit according to the Exploit truth for characterizing exploited vulnerabilities. Database. We characterize the vulnerable functions that have no Discriminating between a vulnerability that has no exploit from exploit and the ones that have an exploit using eight metrics. The the one that has an exploit is challenging because both of them results show that the difference between a vulnerability that has have similar characteristics. Besides, the number of vulnerabilities no exploit and the one that has an exploit can potentially be with a reported exploit are few compared to the vulnerabilities characterized using the chosen software metrics. However, without a reported exploit. Although vulnerability exploitability predicting exploitation of vulnerabilities is more complex than can be characterized by external factors such as attacker profile, predicting just the presence of vulnerabilities and further research software market share, etc., the focus of this study is on predicting is needed using metrics that consider security domain knowledge vulnerability exploitability using internal attributes. This can help for enhancing the predictability of vulnerability exploits. software developers predict vulnerabilities exploitability on the 
Keywords development side rather than the deployment side. Vulnerabilities Severity; Exploitability; Software security; The objective of this research is to investigate what could Exploits; Data mining and machine learning; Feature selection; characterize a code containing a vulnerability with an exploit. To Prediction; Software metrics. address this objective, we have studied 183 vulnerabilities from 
the National Vulnerability Database [8] for the Linux Kernel and 1. INTRODUCTION Apache HTTP server. The two software systems have been Identifying and addressing software vulnerabilities is important selected because of their rich history of publicly available before software release because a single software vulnerability vulnerabilities, availability of reported exploits, the existence of can lead to a breach with a high impact to an organization. an integrated repository, availability of the source code, and their However, identifying and addressing potential vulnerabilities can diversity in size, functionalities, and domain. For every selected take considerable expertise and effort. Recently, researchers [1], vulnerability, we verify whether it has an exploit reported in the [2], [3], [4] have started investigating ways to predict code areas Exploit Database or not [9]. Eighty-two vulnerabilities have been which are more likely to be vulnerable so security testers can found to have an exploit. Ten of them are for Apache HTTP focus on them. server and 72 for Linux Kernel. We then mapped these Software vulnerabilities, pose different levels of potential risk. A vulnerabilities to their locations at the function granularity level. vulnerability with an exploit written for it presents more risk than After that, we characterize the vulnerable functions with and the one without an exploit because the existence of an exploit without an exploit using the selected eight software metrics: allows an attacker to take advantage of a vulnerability and Source Line of Code, Cyclomatic complexity, CountPath, Nesting potentially compromise the affected systems. Allodi and Massacci Degree, Information Flow, Calling functions, Called by functions, in [5] have shown that out of the 49599 vulnerabilities reported by and Number of Invocations. The reasons why these metrics have the National Vulnerability Database, only 2.10% are in fact been selected are discussed in section 3.1. Based on the metrics exploited. Younis and Malaiya in [6] have also found that only values of the vulnerable functions with and without an exploit, we 6.8% out of 486 vulnerabilities of Microsoft Internet Explorer first test the individual selected metrics discriminative power have reported exploits. K. Nayak et al. [7] have reported that using Welch t-test [10]. Next, we select a combination of these metric using three feature selection methods: correlation-based, Permission to make digital or hard copies of all or part of this work for personal or wrapper, and principal component analysis. Then, we test the classroom use is granted without fee provided that copies are not made or distributed predictive power of the selected subset of metrics using four for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others classifiers: Logistic Regression, Na�ve Base, Random Forest, and than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, Support Vector machine. or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. CODASPY'16, March 09-11, 2016, New Orleans, LA, USA � 2016 ACM. ISBN 978-1-4503-3935-3/16/03?$15.00 DOI: http://dx.doi.org/10.1145/2857705.2857750 
97

}
}

@InProceedings{FangHafizUniversityEtAl2011,
  Title                    = {Discovering Buffer Overflow Vulnerabilities In The Wild: An Empirical Study},
  Author                   = {Ming Fang and Munawar Hafiz and Auburn University and Auburn University and Auburn and AL and USA Auburn and AL and USA},
  Year                     = {2011},

  Abstract                 = {ask her peers, read books and research papers on detecting We performed an empirical study on reporters of buffer over- overflows, or even train to become an ethical hacker. She
},
  File                     = {:home/ccc/github/literature/article/Discovering buffer overflow vulnerabilities in the wild\: an empirical study.pdf:PDF},
  Keywords                 = {reported hundreds of vulnerabilities, reporters who have a few months experience as well as reporters with over ten},
  Review                   = {Discovering Buffer Overflow Vulnerabilities In The Wild: An Empirical Study Ming Fang Munawar Hafiz Auburn University Auburn University Auburn, AL, USA Auburn, AL, USA mzf0018@auburn.edu munawar@auburn.edu ABSTRACT ask her peers, read books and research papers on detecting We performed an empirical study on reporters of buffer over- overflows, or even train to become an ethical hacker. She flow vulnerabilities to understand the methods and tools will be easily overwhelmed by the number of “best practices” used during the discovery. The participants were reporters suggested and the number of suggesters claiming that their featured in the SecurityFocus repository during two six- methods work best without backing the claims with empir- month periods; we collected 58 responses. We found that ical data. A researcher who wants to improve the security in spite of many apparent choices, reporters follow simi- engineering process to fix the reported vulnerabilities more lar approaches. Most reporters typically use fuzzing, but efficiently will also need empirical data to understand which their fuzzing tools are created ad hoc; they use a few debug- problems to target. A similar information gap confuses tool ging tools to analyze the crash introduced by a fuzzer; and builders who want to build popular, widely-adopted tools. static analysis tools are rarely used. We also found a seri- Researchers who have studied security engineering process ous problem in the vulnerability reporting process. Most re- focused on finding statistics of vulnerability trends, lifecy- porters, especially the experienced ones, favor full-disclosure cle of vulnerabilities, time to fix a vulnerability, etc [14–16, and do not collaborate with the vendors of vulnerable soft- 26, 27]. These studies have not focused on the approaches ware. They think that the public disclosure, sometimes sup- of the people who report buffer overflow vulnerabilities, i.e., ported by a detailed exploit, will put pressure on vendors to the tools they use and methodologies they follow. Also, pre- fix the vulnerabilities. But, in practice, the vulnerabilities vious studies explore the tip of an iceberg; they miss a lot not reported to vendors are less likely to be fixed. Ours is of activities going on behind vulnerability detection and re- the first study on vulnerability repositories that attempts to porting. The people involved in these activities carry first- collect information from the people involved in the process; hand information about corresponding security approaches. previous works have overlooked this rich information source. The knowledge that they possess is not stored in vulnerabil- The results are valuable for beginners exploring how to de- ity repositories or any other repositories. No attempts have tect and report buffer overflows and for tool vendors and been made to tap into this rich information source. researchers exploring how to automate and fix the process. This paper describes a study performed on reporters of buffer overflow vulnerabilities at SecurityFocus repository [29]. Categories and Subject Descriptors These participants reported buffer overflow vulnerabilities that were featured in the repository during two six-month D.2.8 [Software Engineering]: Metrics; A.1 [Introductory periods (Dec 2011 - May 2012, Dec 2012 - May 2013). We and Survey]; K.6.5 [Management of Computing and ran the study in the form of an email questionnaire. Our Information Systems]: Security and Protection analysis is based on the responses of 58 reporters (out of General Terms 229 with contact information, 25.33% response rate). The group of responders included reporters who have discovered Experimentation, Measurement, Security. a vulnerability for the first time as well as reporters who have Keywords reported hundreds of vulnerabilities, reporters who have a few months experience as well as reporters with over ten Secure Software Engineering, Vulnerability, Empirical Study. years of experience, reporters working on source code as well as reporters working on binaries, and reporters work- 1. INTRODUCTION ing on a Windows platform as well as reporters working on When Beth, a beginner, wants to find information about a Unix platform. We asked them about the methods they how to detect buffer overflow vulnerabilities, she can pursue followed to detect and report buffer overflow vulnerabilities many sources of information. She can search the Internet, and the tools they used. The questions were open-ended. We analyzed the responses by applying a structural coding approach [25] using Text Analysis Markup System Analyzer Permission to make digital or hard copies of all or part of this work for (TAMS Analyzer) [32], an open source tool for coding. personal or classroom use is granted without fee provided that copies are We analyzed the responses in the two rounds of the study not made or distributed for profit or commercial advantage and that copies separately and aggregated the results for common trends. bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific Our study focused on the approach taken by the developers permission and/or a fee. to explore for buffer overflows in software, the tools that they Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

}
}

@Article{FangLiuZhangEtAl2015,
  Title                    = {IVDroid: Static Detection for Input Validation Vulnerability in Android Inter-component Communication},
  Author                   = {Fang, Zhejun and Liu, Qixu and Zhang, Yuqing and Wang, Kai and Wang, Zhiqiang},
  Journal                  = {Information Security Practice and Experience, Ispec 2015},
  Year                     = {2015},
  Pages                    = {Huawei Int},
  Volume                   = {9065},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Input validation vulnerability in Android inter-component communication is a kind of severe vulnerabilities in Android apps. Malicious attacks can exploit the vulnerability to bypass Android security mechanism and compromise the integrity, confidentiality and availability of Android devices. However, so far there is not a sound approach at source code level designed for app developers to detect such vulnerabilities. In this paper we propose a novel approach aiming at detecting input validation flaws in Android apps and implement a prototype named IVDroid, which provides practical static analysis of Java source code. IVDroid leverages backward program slicing to abstract application logic from Java source code. On slice level, IVDroid detects flaws of known pattern by security rule matching and detects flaws of unknown pattern by duplicate validation behavior mining. Then IVDroid semi-automatically confirms the suspicious rule violations and report the confirmed ones as vulnerabilities. We evaluate IVDroid on 3 versions of Android spanning from version 2.2 to 4.4.2 and it detects 37 vulnerabilities including confused deputy and denial of service attack. Our results prove that IVDroid can provide a practical defence solution for app developers.},
  Be                       = {Lopez, JEOLEOLWu, Y},
  Bn                       = {978-3-319-17533-1; 978-3-319-17532-4},
  Cl                       = {Beihang Univ, Beijing, PEOPLES R CHINA},
  Ct                       = {11th International Conference on Information Security Practice andEOLEOLExperience (ISPEC)},
  Cy                       = {MAY 05-08, 2015},
  Doi                      = {10.1007/978-3-319-17533-1_26},
  Groups                   = {Code Mining},
  Ho                       = {Beihang Univ},
  Se                       = {Lecture Notes in Computer Science},
  Sn                       = {0302-9743},
  Tc                       = {0},
  Ut                       = {WOS:000363247500026},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{FangLiuZhangEtAl2015a,
  Title                    = {IVDroid: static detection for input validation vulnerability in Android inter-component communication},
  Author                   = {Zhejun Fang and Qixu Liu and Yuqing Zhang and Kai Wang and Zhiqiang Wang},
  Journal                  = {Information Security Practice and Experience. 11th International Conference, ISPEC 2015. Proceedings: LNCS 9065},
  Year                     = {2015},
  Pages                    = {378--92},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Input validation vulnerability in Android inter-component communication is a kind of severe vulnerabilities in Android apps. Malicious attacks can exploit the vulnerability to bypass Android security mechanism and compromise the integrity, confidentiality and availability of Android devices. However, so far there is not a sound approach at source code level designed for app developers to detect such vulnerabilities. In this paper we propose a novel approach aiming at detecting input validation flaws in Android apps and implement a prototype named IVDroid, which provides practical static analysis of Java source code. IVDroid leverages backward program slicing to abstract application logic from Java source code. On slice level, IVDroid detects flaws of known pattern by security rule matching and detects flaws of unknown pattern by duplicate validation behavior mining. Then IVDroid semi-automatically confirms the suspicious rule violations and report the confirmed ones as vulnerabilities. We evaluate IVDroid on 3 versions of Android spanning from version 2.2 to 4.4.2 and it detects 37 vulnerabilities including confused deputy and denial of service attack. Our results prove that IVDroid can provide a practical defence solution for app developers.},
  Be                       = {Lopez, J.EOLEOLYongdong Wu},
  Bn                       = {978-3-319-17532-4},
  Cl                       = {Beijing, China},
  Ct                       = {Information Security Practice and Experience. 11th InternationalEOLEOLConference, ISPEC 2015},
  Cy                       = {5-8 May 2015},
  Doi                      = {10.1007/978-3-319-17533-1_26},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15043883},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{FangZhangKongEtAl2014,
  Title                    = {Static detection of logic vulnerabilities in Java web applications},
  Author                   = {Zhejun Fang and Yuqing Zhang and Ying Kong and Qixu Liu},
  Journal                  = {Security and Communication Networks},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {519--31},
  Volume                   = {7},

  __markedentry            = {[ccc:6]},
  Abstract                 = {This paper concerns about logic vulnerabilities that result from faulty logic of a web application. Logic vulnerabilities typically accompany with the exposure of unexpected functionalities and lead to the bypass of the intended constraints. From a semantic perspective, logic vulnerabilities occur when mistakes arise in the control flows guarding the processes of invoking critical functionalities. In this paper, we propose the first lightweight static analysis approach to automatically detect logic vulnerabilities in Java web applications. Logic errors in our approach are characterized as erroneous invocations of functionalities. Program-slicing technique has been leveraged to capture the processes of invoking critical functionalities. A back-tracing algorithm is originally designed to extract control flows guarding functionality-invocation processes. Finally, logic vulnerability detection is transformed into mining abnormal functionality-invocation processes in a cluster of similar ones by comparing these processes' control flows. We implemented our approach in a prototype tool named logic vulnerability detector and evaluated it on seven real-world applications scaled from thousands to million lines of code. The evaluation results show that our approach achieves bigger coverage with acceptable cost and better scalability than previous approaches. Copyright copy 2013 John Wiley & Sons, Ltd.},
  Doi                      = {10.1002/sec.747},
  Groups                   = {Code Mining},
  Sn                       = {1939-0114},
  Tc                       = {0},
  Ut                       = {INSPEC:14738087},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@TechReport{FrisbyMoenchRechtEtAl2012,
  Title                    = {Security Analysis of Smartphone Point-of-Sale Systems},
  Author                   = {WesLee Frisby and Benjamin Moench and Benjamin Recht and Thomas Ristenpart},
  Year                     = {2012},
  Number                   = {one},

  Abstract                 = {ceipt printer, and network connection. (Tradition- We experimentally investigate the security of sev- ally the telephone system, though increasingly via
},
  File                     = {:article\\Security Analysis of Smartphone Point-of-Sale Systems.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Security Analysis of Smartphone Point-of-Sale Systems WesLee Frisby Benjamin Moench Benjamin Recht Thomas Ristenpart University of Wisconsin–Madison {wfrisby,bsmoench}@wisc.edu, {recht,rist}@cs.wisc.edu Abstract ceipt printer, and network connection. (Tradition- We experimentally investigate the security of sev- ally the telephone system, though increasingly via eral smartphone point-of-sale (POS) systems that the Internet.) These are reasonably expensive (e.g., consist of a software application combined with an hundreds of US dollars), and have significant secu- audio-jack magnetic stripe reader (AMSR). The lat- rity features. ter is a small hardware dongle that reads magnetic stripes on payment cards, (sometimes) encrypts the Recent years have seen growth in alternate POS sensitive card data, and transmits the result to the form factors. One such uses an inexpensive hardware application. Our main technical result is a complete dongle MSR plugged into the audio jack of a smart- break of a feature-rich AMSR with encryption sup- phone; we refer to this component as an audio-jack port. We show how an arbitrary application running magnetic stripe reader (AMSR). A payment applica- on the phone can permanently disable the AMSR, tion, colloquially called an “app”, is installed on the extract the cryptographic keys it uses to protect phone via an app store associated with the phone, cardholder data, or gain the privileged access needed e.g. the Apple App Store [6] or the Google Play to upload new firmware to it. Android market [8]. Collecting payment credentials proceeds by swiping a card through the AMSR, hav- 1 Introduction ing the app receive data from the AMSR via the au- dio jack, and, finally, communicating this data over The ubiquity of commodity smartphones has the Internet to a payment processing service. prompted companies to leverage them as a platform for replacing dedicated hardware computing devices. In this work, we consider the security implications In this paper, we relay our experience performing of this trend for the case of smartphone-based point a security audit of a collection of smartphone POS of sale (POS) devices. systems. This collection includes four AMSRs and a A POS device is responsible for collecting, trans- larger set of apps (some AMSRs work with multiple mitting, and (sometimes) storing payment creden- apps). The four AMSR devices range in complexity tials in order to facilitate the sale of some good or and security features, from an analogue-only device service. While newer mechanisms for POS systems up to microcontrollers that encrypt payment card exist (e.g., chip-and-pin [7], near-field communica- data before transmission to the smartphone app. tion [9], and radio-frequency identification [10]), the Our main technical result is a complete break of one predominant mechanism in North America [28] re- of the latter AMSR systems, which implements an mains plastic cards with credentials encoded onto a extensive firmware API accessible from the phone. magnetic-stripe. POS devices use a magnetic-stripe We show how an arbitrary app running on the phone reader (MSR) to conveniently read the encoded data can: (1) disable the AMSR (brick it); (2) turn off en- — typically a credit or debit card account number cryption of card data; (3) extract secret keys used and some supporting account details. to encrypt cardholder data; and (4) gain the neces- While there are a plethora of POS solutions, the sary credentials needed to upload new firmware to canonical contemporary system is an all-in-one stan- the AMSR. These all stem from a handful of basic dalone device with an MSR, number pad, small re- software vulnerabilities in the firmware. 1

}
}

@Article{Garc铆a-FerreiraLaordenSantosEtAl2014,
  Title                    = {A Survey on Static Analysis and Model Checking},
  Author                   = {Garc铆a-Ferreira, Iv谩n and Laorden, Carlos and Santos, Igor and Bringas, Pablo Garc铆a},
  Journal                  = {International Joint Conference SOCO鈥�14-CISIS鈥�14-ICEUTE鈥�14},
  Year                     = {2014},
  Pages                    = {443鈥�452},

  Doi                      = {10.1007/978-3-319-07995-0_44},
  File                     = {:article\\A Survey on Static Analysis and Model Checking.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-3-319-07995-0},
  ISSN                     = {2194-5365},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-07995-0_44}
}

@Article{GlodekHarang2013,
  Title                    = {Rapid Permissions-Based Detection and Analysis of Mobile Malware Using Random Decision Forests},
  Author                   = {Glodek, William and Harang, Richard},
  Journal                  = {MILCOM 2013 - 2013 IEEE Military Communications Conference},
  Year                     = {2013},

  Month                    = {Nov},

  Doi                      = {10.1109/milcom.2013.170},
  File                     = {:home/ccc/github/literature/article/-Rapid Permissions-based Detection and Analysis of Mobile Malware Using Random Decision.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-0-7695-5124-1},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/MILCOM.2013.170}
}

@Article{GosainSharma2015,
  Title                    = {Static Analysis: A Survey of Techniques and Tools},
  Author                   = {Gosain, Anjana and Sharma, Ganga},
  Journal                  = {Advances in Intelligent Systems and Computing},
  Year                     = {2015},
  Pages                    = {581–591},

  Doi                      = {10.1007/978-81-322-2268-2_59},
  File                     = {:home/ccc/github/literature/article/Static Analysis\: A Survey of Techniques and Tools.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-81-322-2268-2},
  ISSN                     = {2194-5365},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/978-81-322-2268-2_59}
}

@Article{GosainSharma2015a,
  Title                    = {A Survey of Dynamic Program Analysis Techniques and Tools},
  Author                   = {Gosain, Anjana and Sharma, Ganga},
  Journal                  = {Proceedings of the 3rd International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2014},
  Year                     = {2015},
  Pages                    = {113–122},

  Doi                      = {10.1007/978-3-319-11933-5_13},
  File                     = {:home/ccc/github/literature/article/A Survey of Dynamic Program Analysis Techniques and Tools.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-3-319-11933-5},
  ISSN                     = {2194-5365},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-11933-5_13}
}

@Article{Goseva-PopstojanovaPerhinschi2015,
  Title                    = {On the capability of static code analysis to detect security vulnerabilities},
  Author                   = {Goseva-Popstojanova, Katerina and Perhinschi, Andrei},
  Journal                  = {Information and Software Technology},
  Year                     = {2015},

  Month                    = {Dec},
  Pages                    = {18鈥�33},
  Volume                   = {68},

  Doi                      = {10.1016/j.infsof.2015.08.002},
  File                     = {:article\\On the capability of static code analysis to detect security vulnerabilities.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Review                   = {评估现有的静态分析工具（三款商业）对安全漏洞的检测效果。总体效果很一般。},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2015.08.002}
}

@Article{GriecoLuisLucasEtAl2015,
  Title                    = {Toward large-scale vulnerability discovery using Machine Learning},
  Author                   = {Gustavo Grieco and Guillermo Luis and Grinblat§ Lucas and Uzal§},
  Year                     = {2015},

  Abstract                 = {Nevertheless, vulnerability detection is not a simple opera- With sustained growth of software complexity, finding se- tion. As has been pointed out in [1], curity vulnerabilities in operating systems has become an “The defect caused an infection, which caused important necessity. Nowadays, OS are shipped with thou- a failure and when we saw the failure we tracked sands of binary executables. Unfortunately, methodologies the infection, and finally found and fixed the de- and tools for an OS scale program testing within a limited fect.” time budget are still missing.},
  File                     = {:article\\Toward large-scale vulnerability discovery using Machine Learning.pdf:PDF},
  Groups                   = {binarary vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {纯粹的静态分析技术不足以检测漏洞，应该辅以能够提供一些预处理信息的机器学习，concolic execution等技术。文章主要表明对于大规模的二进制程序，即使没有源码，通过提取动态和静态的特征，使用机器学习技术能够区分开程序是否易受攻击的，即说明了机器学习可以有效用于或辅助漏洞挖掘。文中也包括一些机器学习的经典算法和一些结论性的观点。该文未在学术会议、期刊上发表，是VDiscovery（一个新的漏洞挖掘工具）的理论文献，该工具是开源的并且包括用于分类学习的一个原始公开数据库（内含debain的bug数据）}
}

@InProceedings{Guide2012,
  Title                    = {Juliet Test Suite v1.2 for C/C},
  Author                   = {User Guide},
  Year                     = {2012},

  File                     = {:article\\Juliet Test Suite v1.2 for C Cpp - User Guide.pdf:PDF},
  Review                   = { Juliet Test Suite v1.2 for C/C++ User Guide Center for Assured Software National Security Agency 9800 Savage Road Fort George G. Meade, MD 20755-6738 cas@nsa.gov December 2012 

}
}

@InProceedings{,
  Title                    = {Juliet Test Suite v1.2 for Java},
  Author                   = {User Guide},
  Year                     = {2012},

  File                     = {:home/ccc/github/literature/article/Juliet Test Suite v1.2 for_Java - User_Guide.pdf:PDF},
  Review                   = { 
 
 
 
 
 Juliet Test Suite v1.2 for Java 
User Guide 
 
 
 
Center for Assured Software 
National Security Agency 
9800 Savage Road 
Fort George G. Meade, MD 20755-6738 cas@nsa.gov 
 
 
 
 
 
December 2012 
 
 

}
}

@Article{GuptaGovilSingh2015,
  Title                    = {Text-mining based predictive model to detect XSS vulnerable files in web applications},
  Author                   = {Gupta, M. K. and Govil, M. C. and Singh, G.},
  Journal                  = {2015 Annual IEEE India Conference (INDICON)},
  Year                     = {2015},
  Pages                    = {6 pp.--6},

  __markedentry            = {[ccc:6]},
  Abstract                 = {This paper presents a text-mining based approach to detect cross-site scripting (XSS) vulnerable code files in the web applications. It uses a tailored tokenizing process to extract text-features from the source code of web applications. In this process, each code file is transformed into a set of unique text-features with their associated frequencies. These features are used to build vulnerability prediction models. The efficiency of proposed approach based model is evaluated on a publicly available dataset having 9408 labelled source code files. Experimental results show that proposed features based best predictive model achieves a true average rate of 87.8% with low false rate of 12.3% in the detection of XSS vulnerable files. It is significantly better than the performance of existing text-mining approach based model that achieves a true average rate of 71.6% with false rate of 33.1% on the same data set.},
  Bn                       = {978-1-4673-6540-6},
  Cl                       = {New Delhi, India},
  Ct                       = {2015 Annual IEEE India Conference (INDICON)},
  Cy                       = {17-20 Dec. 2015},
  Doi                      = {10.1109/INDICON.2015.7443332},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15888095},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{GuptaGovilSinghEtAl4799,
  Title                    = {Predicting Cross-Site Scripting (XSS) Security Vulnerabilities in Web Applications},
  Author                   = {Mukesh Kumar Gupta and Mahesh Chandra Govil and Girdhari Singh and Department of Computer and Science and Engineering},
  Year                     = {4799},
  Publisher                = {IEEE},

  Abstract                 = {Recently, machine-learning based vulnerability pre- metrics and static code attributes are important features in diction models are gaining popularity in web security space, as building of machine learning model for predicting security these models provide a simple and efficient way to handle web vulnerabilities. XSS vulnerability has different characteristics application security issues. Existing state-of-art Cross-Site Script- than general vulnerabilities. The existing XSS prediction ap- ing (XSS) vulnerability prediction approaches do not consider proaches [5] [6] [7] do not consider the contexts of user input, the context of the user-input in output-statement, which is very important to identify context-sensitive security vulnerabilities. In which is very important for prediction of XSS vulnerabilities. this paper, we propose a novel feature extraction algorithm to In this paper, we propose a novel approach to extract extract basic and context features from the source code of web basic and context features from source code to build machine- applications. Our approach uses these features to build various machine-learning models for predicting context-sensitive Cross- learning based vulnerability prediction model. To the best
},
  File                     = {:home/ccc/github/literature/article/-Predicting Cross-Site Scripting (XSS) security vulnerabilities in web applications.pdf:PDF},
  Keywords                 = {web application security, cross-site scripting vulner- The rest of the paper is organized as follows. Section 2 ability, machine learning, context-sensitive, input validation presents the background and motivation for this work. Section},
  Review                   = {Predicting Cross-Site Scripting (XSS) Security Vulnerabilities in Web Applications Mukesh Kumar Gupta ∗, Mahesh Chandra Govil †, Girdhari Singh ‡ Department of Computer Science & Engineering Malviya National Institute of Technology, Jaipur-302017, Rajasthan, INDIA Email: ∗mukeshgupta@skit.ac.in, †govilmc@yahoo.com, ‡girdharisingh@rediffmail.com Abstract—Recently, machine-learning based vulnerability pre- metrics and static code attributes are important features in diction models are gaining popularity in web security space, as building of machine learning model for predicting security these models provide a simple and efficient way to handle web vulnerabilities. XSS vulnerability has different characteristics application security issues. Existing state-of-art Cross-Site Script- than general vulnerabilities. The existing XSS prediction ap- ing (XSS) vulnerability prediction approaches do not consider proaches [5] [6] [7] do not consider the contexts of user input, the context of the user-input in output-statement, which is very important to identify context-sensitive security vulnerabilities. In which is very important for prediction of XSS vulnerabilities. this paper, we propose a novel feature extraction algorithm to In this paper, we propose a novel approach to extract extract basic and context features from the source code of web basic and context features from source code to build machine- applications. Our approach uses these features to build various machine-learning models for predicting context-sensitive Cross- learning based vulnerability prediction model. To the best Site Scripting (XSS) security vulnerabilities. Experimental results of our knowledge, ours is the first approach to use context show that the proposed features based prediction models can information for predicting XSS vulnerabilities. The proposed discriminate vulnerable code from non-vulnerable code at a very approach has implemented in a prototype tool for automatic low false rate. extraction of these features from PHP source code. Keywords—web application security, cross-site scripting vulner- The rest of the paper is organized as follows. Section 2 ability, machine learning, context-sensitive, input validation presents the background and motivation for this work. Section 3 discusses prior works related to the cross-site scripting vulnerability prediction. Section 4 describes a novel feature extraction approach for building prediction models. Section I. INTRODUCTION 5 provides the details of the data set, experimental setting, Nowadays, a large number of people are depending on web and performance measures, which are utilized to evaluate the applications for social communications, health services, finan- performance of the proposed approach. Section 6 discusses the cial transactions and other purposes. However, the presence experimental results and compares the proposed approach with of security vulnerabilities limits the use of these applications existing approaches. Finally, Section 7 concludes the paper as malicious user can steal sensitive information (e.g. cookie, noting and mentions future research directions. session), send illegal HTTP requests, redirect benign user to malicious websites, install malware, and perform various other II. BACKGROUND AND MOTIVATION malicious operations. The recent security statistical reports re- veal that approximately 55% of assessed web applications have In this section, we describe cross-site scripting vulnerabili- security vulnerability [1]. In 2013, Open Web Application ties, and discuss the limitations of existing related approaches, Security Project (OWASP) and Common Vulnerabilities and which motivates us for this work. Exposures (CWE) reported Cross-Site Scripting (XSS) as one of the most serious vulnerability in web applications. The main A. Cross-Site Scripting Vulnerabilities reason of XSS vulnerabilities is weakness in the source code Cross-site scripting (XSS) is an application-level code- which permits the use of user-input in web server’ output- injection type security vulnerability. It occurs whenever a statement without any validation. server program (i.e. dynamic web page) uses unrestricted Researchers have proposed various static and dynamic input via HTTP request, database, or files in it’s response analysis based approaches [2] to detect XSS vulnerabilities without any validation. It allows a malicious user to steal in source code of web applications. Static analysis based sensitive information (i.e. cookie, session) and performs other detection techniques use a set of predefined rules to detect malevolent operations. The figure 1 illustrates the sequence vulnerabilities in source code without executing it. These of given below steps to perform stored XSS attack. Initially, techniques are easy to implement, but produce too many the malicious-user uses a blog site comment-form to inserts false positive results. Dynamic analysis based techniques use and stores the malicious scripts into site’ database. Then, the complex analysis to produce more accurate results. However, legitimate-user sends an HTTP request to site for viewing the they require large test cases to ensure any false negative results. latest comments. The site returns the stored comments along Alternatively, researchers [3] [4] have revealed that software with the scripts in it’s response. Finally, the legitimate user’ browser executes the scripts and sends legitimate-user sensitive information to an attacker’ server. 978-1-4799-1966-6/15/$31.00 ©2015 IEEE 162

}
}

@Article{HafizFang2015,
  Title                    = {Game of detections: how are security vulnerabilities discovered in the wild?},
  Author                   = {Hafiz, Munawar and Fang, Ming},
  Journal                  = {Empir Software Eng},
  Year                     = {2015},

  Month                    = {Sep},

  Doi                      = {10.1007/s10664-015-9403-7},
  File                     = {:article\\Game of detections  how are security vulnerabilities discovered in the wild.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1573-7616},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10664-015-9403-7}
}

@Article{HallBeechamBowesEtAl2012,
  Title                    = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
  Author                   = {Hall, T. and Beecham, S. and Bowes, D. and Gray, D. and Counsell, S.},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1276–1304},
  Volume                   = {38},

  Doi                      = {10.1109/tse.2011.103},
  File                     = {:home/ccc/github/literature/article/A Systematic Literature Review on Fault Prediction Performance in Software Engineering.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2011.103}
}

@InProceedings{HamiltonJrUniversityEtAl8500,
  Title                    = {Finding Bugs in Source Code Using Commonly Available Development Metadata},
  Author                   = {Devin Cook Yung Ryn Choe John A. Hamilton and Jr and Auburn University and Sandia National and Laboratories∗ Mississippi and State University},
  Year                     = {8500},

  Abstract                 = {miss. Through two case studies using test data from cpython
},
  File                     = {:home/ccc/github/literature/article/Finding Bugs in Source Code Using Commonly Available Development.pdf:PDF},
  Review                   = {Finding Bugs in Source Code Using Commonly Available Development Metadata Devin Cook Yung Ryn Choe John A. Hamilton, Jr. Auburn University Sandia National Laboratories∗ Mississippi State University Abstract miss. Through two case studies using test data from cpython Developers and security analysts have been using static and Roundup, we demonstrate that it is possible to iden- analysis for a long time to analyze programs for defects tify new vulnerabilities that are similar to previously re- and vulnerabilities. Generally a static analysis tool is run ported vulnerabilities within that application, that it is on the source code for a given program, flagging areas possible to then use those learned signatures and models of code that need to be further inspected by a human an- to identify new vulnerabilities in an unrelated applica- alyst. These tools tend to work fairly well – every year tion, and that it is possible to calculate some interesting they find many important bugs. These tools are more and useful development metrics that may assist future de- impressive considering the fact that they only examine velopment of secure code. the source code, which may be very complex. Now con- sider the amount of data available that these tools do not analyze. There are many additional pieces of informa- 2 Related Work tion available that would prove useful for finding bugs in code, such as a history of bug reports, a history of There are already some existing methods of finding vul- all changes to the code, information about committers, nerabilities in software. There is also past research on etc. By leveraging all this additional data, it is possible the use of metadata from various inputs and some inter- to find more bugs with less user interaction, as well as esting research related to identifying similar code using track useful metrics such as number and type of defects abstract syntax tree comparison. All of these approaches injected by committer. This paper provides a method for are related to our methods. leveraging development metadata to find bugs that would Static analysis has been practical since the 1970s with otherwise be difficult to find using standard static analy- pioneers such as Cousot [16] leading the way. Today sis tools. We showcase two case studies that demonstrate there are a number of existing static analysis tools in use the ability to find new vulnerabilities in large and small including some that are free software such as rosecheck- software projects by finding new vulnerabilities in the ers [2], Clang Static Analyzer [3], splint [10], and Find- cpython and Roundup open source projects. Bugs [13]. In addition to many of these more “academic” and open source tools, there are also several proprietary tools available like Coverity [4], MAYHEM [15], and 1 Introduction Fortify [5]. These tools are already used in the real world on a daily basis to find vulnerabilities. This paper showcases some of the benefits of utilizing additional metadata to augment traditional static analy- 2.1 Leveraging Development Metadata sis. By leveraging the wealth of information provided by a project’s bug tracker and source code repository, we There has been other research related to mining devel- can find bugs that traditional static analysis tools may opment metadata. However, it generally focuses too nar- rowly on only input, and makes little attempt to associate ∗Sandia National Laboratories is a multi-program laboratory man- multiple sources of information. That means it exam- aged and operated by Sandia Corporation, a wholly owned subsidiary ines only the data in a software repository, or only the of Lockheed Martin Corporation, for the U.S. Department of Energys National Nuclear Security Administration under contract DE-AC04- data in a bugtracker. Livshits and Zimmermann created 94AL85000. a tool called DynaMine that mines the revision history of

}
}

@InProceedings{HanYanGaoEtAl2008,
  Title                    = {Comparing Mobile Privacy Protection through Cross-Platform Applications},
  Author                   = {Jin Han and Qiang Yan and Debin Gao and Jianying Zhou and Robert Deng},
  Year                     = {2008},

  Abstract                 = {With the rapid growth of the mobile market, secu- in comparing the abstract and general practices towards rity of mobile platforms is receiving increasing attention from security [1], [2], [3], [4], we make the first attempt to both research community as well as the public. In this paper, establish such a baseline by analyzing the security-sensitive we make the first attempt to establish a baseline for security comparison between the two most popular mobile platforms. API usage on cross-platform applications. We investigate applications that run on both Android and A cross-platform application is an application that runs on iOS and examine the difference in the usage of their security multiple mobile platforms, e.g., the Facebook application has sensitive APIs (SS-APIs). Our analysis over 2,600 applications both an Android and an iOS version with almost identical shows that iOS applications consistently access more SS-APIs functionality. We first try to identify these cross-platform than their counterparts on Android. The additional privileges gained on iOS are often associated with accessing private applications by crawling information on both Google Play resources such as device ID, camera, and users’ contacts. and iTunes App Store. Our web crawler collects information
},
  File                     = {:article\\Comparing Mobile Privacy Protection through Cross-Platform Applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Comparing Mobile Privacy Protection through Cross-Platform Applications Jin Han∗, Qiang Yan†, Debin Gao†, Jianying Zhou∗, Robert Deng† ∗Cryptography and Security Department †School of Information Systems Institute for Infocomm Research Singapore Management University {hanj, jyzhou}@i2r.a-star.edu.sg {qiang.yan.2008, robertdeng, dbgao}@smu.edu.sg Abstract—With the rapid growth of the mobile market, secu- in comparing the abstract and general practices towards rity of mobile platforms is receiving increasing attention from security [1], [2], [3], [4], we make the first attempt to both research community as well as the public. In this paper, establish such a baseline by analyzing the security-sensitive we make the first attempt to establish a baseline for security comparison between the two most popular mobile platforms. API usage on cross-platform applications. We investigate applications that run on both Android and A cross-platform application is an application that runs on iOS and examine the difference in the usage of their security multiple mobile platforms, e.g., the Facebook application has sensitive APIs (SS-APIs). Our analysis over 2,600 applications both an Android and an iOS version with almost identical shows that iOS applications consistently access more SS-APIs functionality. We first try to identify these cross-platform than their counterparts on Android. The additional privileges gained on iOS are often associated with accessing private applications by crawling information on both Google Play resources such as device ID, camera, and users’ contacts. and iTunes App Store. Our web crawler collects information A possible explanation for this difference in SS-API usage of more than 300,000 Android applications and 400,000 iOS is that privileges obtained by an application on the current applications. Several data mining techniques are adopted to iOS platform are invisible to end users. Our analysis shows match the applications released for the two platforms. We that: 1) third-party libraries (specifically advertising and an- alytic libraries) on iOS invoke more SS-APIs than those on find that 12.2% of the applications on Google Play have a Android; 2) Android application developers avoid requesting replica on iTunes Store. Among them, we select the most unnecessary privileges which will be shown in the permission popular 1,300 pairs to further analyze their security-sensitive list during application installation. Considering the fact that API usage. an Android application may gain additional privileges with A security-sensitive API (SS-API) is a public API pro- privilege-escalation attacks and iOS provides a more restricted privilege set accessible by third-party applications, our results vided for third-party applications that may have access to do not necessarily imply that Android provides better privacy private user data or control over certain device components protection than iOS. However, our evidence suggests that (e.g., Bluetooth and camera). In order to analyze the similar- Apple’s application vetting process may not be as effective ities and differences of the SS-API usage, the first challenge as Android’s privilege notification mechanism, particularly in is to develop an SS-API mapping between Android an iOS. protecting sensitive resources from third-party applications. Based on the permission concept on Android and the existing Android API-to-permission mapping provided by Felt et I. INTRODUCTION al. [5], we group the SS-APIs on iOS into 20 different The current intensive competition among mobile plat- API types and map them to the corresponding Android SS- forms has sparked a heated debate on which platform has APIs. Our analysis produces a list of SS-API types that a better architecture for security and privacy protection. are both supported by Android and iOS. With such API Discussions usually focus on Google’s Android and Apple’s mappings available, we statically analyze the cross-platform iOS, which are the top two players in terms of user base [1], applications (Android Dalvik binaries and iOS Objective-C [2]. Some claim that Android is better since it makes the executables). complete permission list visible to users and it takes an By analyzing the 1,300 pairs of cross-platform applica- open-source approach [2]. Some argue that iOS is better tions, which are sampled from the most popular applications, because 1) Apple screens all applications before releasing we show that 73% of them on iOS access additional SS- them to the iTunes App Store (aka. Apple’s vetting process); APIs, compared to their replicas on Android. The addi- 2) Apple has complete control of its hardware so that OS tional SS-APIs invoked are mostly for accessing sensitive patches and security fixes are more smoothly applied on all resources such as device ID, camera, user contacts, and devices; and 3) the open-source nature of Android makes calendar, which may cause privacy breaches or security risks it an easier target of attacks than iOS [1]. Others [3], [4] without being noticed. We further investigate the underlying suggest that the two platforms achieve comparable security reasons by separately analyzing third-party libraries and but in different ways. These different voices clearly raise applications’ own code. Our results show that the commonly the need for establishing a baseline for security comparison used third-party libraries on iOS, especially the advertising among different mobile platforms. Unlike most prior efforts and analytic libraries access more SS-APIs compared to

}
}

@Article{HeRastogiCaoEtAl2015,
  Title                    = {Vetting SSL Usage in Applications with SSLINT},
  Author                   = {Boyuan He and Vaibhav Rastogi and Yinzhi Cao and Yan Chen},
  Year                     = {2015},

  Abstract                 = {Secure Sockets Layer (SSL) and Transport Layer In particular, we ask the following research question: Is it Security (TLS) protocols have become the security backbone of possible to design scalable techniques that detect incorrect use the Web and Internet today. Many systems including mobile and desktop applications are protected by SSL/TLS protocols of APIs in applications using SSL/TLS libraries? This question
},
  Doi                      = {10.1109/SP.2015.38},
  File                     = {:article\\Vetting SSL Usage in Applications with SSLINT.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {未读},
  Review                   = {2015 IEEE Symposium on Security and Privacy Vetting SSL Usage in Applications with SSLINT Boyuan He1, Vaibhav Rastogi2, Yinzhi Cao3, Yan Chen2, V.N. Venkatakrishnan4, Runqing Yang1, and Zhenrui Zhang1 1Zhejiang University 2Northwestern University 3Columbia University 4University of Illinois, Chicago heboyuan@zju.edu.cn vrastogi@u.northwestern.edu yzcao@cs.columbia.edu ychen@northwestern.edu venkat@uic.edu rainkin1993@gmail.com jerryzh@zju.edu.cn Abstract—Secure Sockets Layer (SSL) and Transport Layer In particular, we ask the following research question: Is it Security (TLS) protocols have become the security backbone of possible to design scalable techniques that detect incorrect use the Web and Internet today. Many systems including mobile and desktop applications are protected by SSL/TLS protocols of APIs in applications using SSL/TLS libraries? This question against network attacks. However, many vulnerabilities caused poses the following challenges: by incorrect use of SSL/TLS APIs have been uncovered in recent • Deﬁning and representing correct use. Given an SSL years. Such vulnerabilities, many of which are caused due to poor API design and inexperience of application developers, often lead library, how do we model correct use of the API to to conﬁdential data leakage or man-in-the-middle attacks. In this facilitate detection? paper, to guarantee code quality and logic correctness of SSL/TLS • Analysis techniques for incorrect usage in software. applications, we design and implement SSLINT, a scalable, Given a representation of correct usage, how do we de- automated, static analysis system for detecting incorrect use sign techniques for analyzing programs to detect incorrect of SSL/TLS APIs. SSLINT is capable of performing automatic logic veriﬁcation with high efﬁciency and good accuracy. To use? demonstrate it, we apply SSLINT to one of the most popular • Identifying candidate programs in a distribution. From Linux distributions – Ubuntu. We ﬁnd 27 previously unknown an OS distribution, how do we identify and select candi- SSL/TLS vulnerabilities in Ubuntu applications, most of which date programs using SSL/TLS libraries? are also distributed with other Linux distributions. • Precision, Accuracy and Efﬁciency. How do we design our techniques so that they offer acceptable results in I. INTRODUCTION terms of precision, accuracy and efﬁciency? Secure Socket Layer (SSL) and its successor Transport Lay- We address these questions in this paper proposing an er Security (TLS) provide end-to-end communication security approach and tool called SSLINT– a scalable, automated, static over the Internet. Based on the model of Public Key Infras- analysis tool – that is aimed towards automatically identifying tructure (PKI) and X509 certiﬁcates, SSL/TLS is designed incorrect use of SSL/TLS APIs in client-side applications. to guarantee conﬁdentiality, authenticity, and integrity for The main enabling technology behind SSLINT is the use communications against Man-In-The-Middle (MITM) attacks. of graph mining for automated analysis. By representing both The details of SSL/TLS protocol are complex, involving the correct API use and SSL/TLS applications as program six major steps during the handshaking protocol [1]. To ease dependence graphs (PDGs), SSLINT converts the problem of the burden of developers, these details are encapsulated inside checking correct API use into a graph query problem. These open source SSL/TLS libraries such as OpenSSL, GnuTL- representations allow for the correct use patterns to precisely S, and NSS (Network Security Services). However, recent capture temporal sequencing of API calls, data ﬂows between work [2] has shown that incorrect use of such libraries could arguments and returns of a procedure, data ﬂows between lead to certiﬁcate validation problems, making applications various program objects, and path constraints. Using these vulnerable to MITM attacks. Their work sheds light on a representations we develop rich models of correct API usage very important issue for Internet applications, and since then patterns, which are subsequently used by a graph matching SSL implementations have received considerable scrutiny and procedure for vulnerability detection. follow-up research [3]–[8]. To evaluate SSLINT in practice, we applied it to the In this backdrop, we focus on the problem of large-scale source code of 381 software packages from Ubuntu. The detection of SSL certiﬁcate validation vulnerabilities in client result shows that SSLINT discovers 27 previously unknown software. By large-scale, we refer to techniques that could SSL/TLS vulnerabilities. Then, we reported our ﬁndings to check, say, an entire OS distribution for the presence of such all the developers of software with such vulnerabilities and vulnerabilities. Previous research, including [2], on ﬁnding received 14 conﬁrmations – out of which, four have already SSL vulnerabilities in client-server applications, mostly relied ﬁxed the vulnerability based on our reports. For those we on a black-box testing approach. Such an approach is not have not received conﬁrmations from, we validated them by suitable for large-scale vulnerability detection, as it involves performing MITM attacks, and the result shows that they are activities such as installation, conﬁguration and testing, some all vulnerable. of which involve a human-in-the-loop. To summarize, this paper makes the following contributions: © 2015, Boyuan He. Under license to IEEE. 519 DOI 10.1109/SP.2015.38

}
}

@InProceedings{HeabLiaEtAl1402,
  Title                    = {An Empirical Study on Software Defect Prediction with a Simplified Metric Set},
  Author                   = {Peng Hea and b and Bing Lia and b and c and Xiao Liua and b and Jun Chenb and d and Yutao Maa and b and c},
  Year                     = {1402},

  File                     = {:home/ccc/github/literature/article/An empirical study on software defect prediction with a simplified metric set.pdf:PDF},
  Keywords                 = {defect prediction, software metrics, metric set simplification, software quality},
  Review                   = {An Empirical Study on Software Defect Prediction with a Simplified Metric Set Peng Hea,b, Bing Lia,b,c, Xiao Liua,b, Jun Chenb,d, Yutao Maa,b,c,∗ aState Key Lab of Software Engineering, Wuhan University, Wuhan 430072, China bSchool of Computer, Wuhan University, Wuhan 430072, China cResearch Center for Complex Network, Wuhan University, Wuhan 430072, China dNational Engineering Research Center for Multimedia Software, Wuhan University, Wuhan 430072, China Abstract Context: Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. Objective: The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. Method: First, based on six typical classifiers, we constructed three types of predictors using the size of software metric set in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests. Results: The study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 10. Conclusion: The experimental results indicate that (1) the choice of training data should depend on the specific requirement of prediction accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naı¨ve Bayes) also tend to perform well when using a simplified metric set for defect prediction; (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice. Keywords: defect prediction, software metrics, metric set simplification, software quality 1. Introduction data of historical releases in the same project and predicted de- fects in the upcoming releases, or reported the results of cross- In software engineering, defect prediction can precisely es- validation on the same data set [16], which is referred to as timate the most defect-prone software components, and help Within-Project Defect Prediction (WPDP). Zimmermann et al. software engineers allocate limited resources to those bits of [3] stated that defect prediction performs well within projects as the systems that are most likely to contain defects in testing and long as there is a sufficient amount of data available to train any maintenance phases. Understanding and building defect pre- models. However, it is not practical for new projects to collect dictors (also known as defect prediction models) for one soft- such sufficient historical data. Thus, achieving high-accuracy ware project is useful for a variety of software development or defect prediction based on within-project data is impossible in maintenance activities, such as assessing software quality and some cases. monitoring quality assurance (QA). Conversely, there are many public on-line defect data sets The importance of defect prediction has motivated numerous available, such as PROMISE1, NASA2 and Eclipse3. Some researchers to define different types of models or predictors that researchers have been inspired to overcome this challenge by characterize various aspects of software quality. Most studies applying the predictors built for one project to a different one usually formulate such a problem as a supervised learning prob- [3, 17, 65]. Utilizing data across projects to build defect pre- lem, and the outcomes of those defect prediction models de- diction models is commonly referred to as Cross-Project De- pend on historical data. That is, they trained predictors from the ∗Corresponding author. Tel: +86 27 68776081 1http://promisedata.org E-mail: {penghe (P. He), bingli (B. Li), lxiao (X. Liu), chenj (J. Chen), ytma 2http://mpd.ivv.nasa.gov/repository.html (Y.T. Ma)}@whu.edu.cn 3http://eclipse.org Preprint submitted to arXiv March 3, 2014 arXiv:1402.3873v2 [cs.SE] 28 Feb 2014

}
}

@Article{HeckmanWilliams2011,
  Title                    = {A systematic literature review of actionable alert identification techniques for automated static code analysis},
  Author                   = {Heckman, Sarah and Williams, Laurie},
  Journal                  = {Information and Software Technology},
  Year                     = {2011},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {363–387},
  Volume                   = {53},

  Doi                      = {10.1016/j.infsof.2010.12.007},
  File                     = {:home/ccc/github/literature/article/A systematic literature review of actionable alert identification techniques for automated static code analysis.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2010.12.007}
}

@InProceedings{HollerHerzigZellerEtAl2008,
  Title                    = {Fuzzing with Code Fragments},
  Author                   = {Christian Holler and Kim Herzig and Andreas Zeller and Mozilla Corporation and Saarland University and Saarland University and choller@mozilla.com herzig@cs.uni-saarland.de zeller@cs.uni-saarland.de},
  Year                     = {2008},

  Abstract                 = {JavaScript interpreter must follow the syntactic rules of JavaScript. Otherwise, the JavaScript interpreter will re-
},
  File                     = {:article\\Fuzzing with Code Fragments.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Fuzzing with Code Fragments Christian Holler Kim Herzig Andreas Zeller Mozilla Corporation∗ Saarland University Saarland University choller@mozilla.com herzig@cs.uni-saarland.de zeller@cs.uni-saarland.de Abstract JavaScript interpreter must follow the syntactic rules of JavaScript. Otherwise, the JavaScript interpreter will re- Fuzz testing is an automated technique providing random ject the input as invalid, and effectively restrict the test- data as input to a software system in the hope to expose ing to its lexical and syntactic analysis, never reaching a vulnerability. In order to be effective, the fuzzed input areas like code transformation, in-time compilation, or must be common enough to pass elementary consistency actual execution. To address this issue, fuzzing frame- checks; a JavaScript interpreter, for instance, would only works include strategies to model the structure of the de- accept a semantically valid program. On the other hand, sired input data; for fuzz testing a JavaScript interpreter, the fuzzed input must be uncommon enough to trigger this would require a built-in JavaScript grammar. exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using Surprisingly, the number of fuzzing frameworks that a grammar to randomly generate valid programs; the generate test inputs on grammar basis is very limited [7, code fragments, however, partially stem from programs 17, 22]. For JavaScript, jsfunfuzz [17] is amongst the known to have caused invalid behavior before. LangFuzz most popular fuzzing tools, having discovered more that is an effective tool for security testing: Applied on the 1,000 defects in the Mozilla JavaScript engine. jsfunfuzz Mozilla JavaScript interpreter, it discovered a total of is effective because it is hardcoded to target a specific 105 new severe vulnerabilities within three months of interpreter making use of specific knowledge about past operation (and thus became one of the top security bug and common vulnerabilities. The question is: Can we bounty collectors within this period); applied on the PHP devise a generic fuzz testing approach that nonetheless interpreter, it discovered 18 new defects causing crashes. can exploit project-specific knowledge? In this paper, we introduce a framework called LangFuzz that allows black-box fuzz testing of engines 1 Introduction based on a context-free grammar. LangFuzz is not bound against a specific test target in the sense that it takes the Software security issues are risky and expensive. grammar as its input: given a JavaScript grammar, it will In 2008, the annual CSI Computer Crime & Security sur- generate JavaScript programs; given a PHP grammar, it vey reported an average loss of 289,000 US$ for a single will generate PHP programs. To adapt to specific targets, security incident. Security testing employs a mix of tech- LangFuzz can use its grammar to learn code fragments niques to find vulnerabilities in software. One of these from a given code base. Given a suite of previously fail- techniques is fuzz testing—a process that automatically ing programs, for instance, LangFuzz will use and re- generates random data input. Crashes or unexpected be- combine fragments of the provided test suite to generate havior point to potential software vulnerabilities. new programs—assuming that a recombination of pre- In web browsers, the JavaScript interpreter is partic- viously problematic inputs has a higher chance to cause ularly prone to security issues; in Mozilla Firefox, for new problems than random input. instance, it encompasses the majority of vulnerability The combination of fuzz testing based on a language fixes [13]. Hence, one could assume the JavaScript in- grammar and reusing project-specific issue-related code terpreter would make a rewarding target for fuzz test- fragments makes LangFuzz an effective tool for secu- ing. The problem, however, is that fuzzed input to a rity testing. Applied on the Mozilla JavaScript engine, ∗At the time of this study, Christan Holler was writing his master it discovered a total of 105 new vulnerabilities within thesis at Saarland University. He is now employed at Mozilla. three months of operation. These bugs are serious and 1

}
}

@TechReport{HolzBochum9319,
  Title                    = {Static Detection of Second-Order Vulnerabilities in Web Applications},
  Author                   = {and Johannes DahseThorsten Holz and Ruhr-University Bochum},
  Year                     = {9319},

  File                     = {:article\\Static detection of second-order vulnerabilities in web applications.pdf:PDF},
  Review                   = {Static Detection of Second-Order Vulnerabilities in Web Applications Johannes Dahse and Thorsten Holz, Ruhr-University Bochum https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/dahse This paper is included in the Proceedings of the 23rd USENIX Security Symposium. August 20–22, 2014 • San Diego, CA ISBN 978-1-931971-15-7 Open access to the Proceedings of the 23rd USENIX Security Symposium is sponsored by USENIX

}
}

@Article{HuangHuangHuangEtAl2014,
  Title                    = {Software Crash Analysis for Automatic Exploit Generation on Binary Programs},
  Author                   = {Huang, Shih-Kun and Huang, Min-Hsiang and Huang, Po-Yen and Lu, Han-Lin and Lai, Chung-Wei},
  Journal                  = {IEEE Trans. Rel.},
  Year                     = {2014},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {270–289},
  Volume                   = {63},

  Doi                      = {10.1109/tr.2014.2299198},
  File                     = {:home/ccc/github/literature/article/Software crash analysis for automatic exploit generation on binary programs.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1558-1721},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/TR.2014.2299198}
}

@Article{HuangYuHangEtAl2004,
  Title                    = {Securing Web Application Code by Static Analysis and Runtime Protection},
  Author                   = {Yao-Wen Huang and Fang Yu and Christian Hang and Chung-Hung Tsai and D. T. Lee and Sy-Yen Kuo},
  Year                     = {2004},

  Abstract                 = {Security remains a major roadblock to universal acceptance of the 1. INTRODUCTION Web for many kinds of transactions, especially since the recent As more and more services are provided via the World Wide Web,},
  File                     = {:article\\Securing Web Application Code by Static Analysis and Runtime Protection.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {level traffic. According to a recent Gartner report [67], those that don't offer application-level protection will eventually “face Web application security, security vulnerabilities, program extinction.” security, verification, type systems, information flow, noninterference. Although application-level firewalls offer immediate assurance of,literature,article},
  Read                     = {未读},
  Review                   = {WEB类}
}

@InProceedings{HungLeSangEtAl,
  Title                    = {Predicting Vulnerable Software Components with Dependency Graphs},
  Author                   = {Viet Hung and Nguyen Le and Minh Sang and Tran and University of Trento and Italy University of Trento and Italy and vhnguyen@disi.unitn.it tran@disi.unitn.it},

  Abstract                 = {A trade-off solution widely adopted is to focus only on Security metrics and vulnerability prediction for software the most vulnerable parts of software. Concentration on the
},
  File                     = {:home/ccc/github/literature/article/-Predicting Vulnerable Software Components with Dependency Graphs.pdf:PDF},
  Review                   = {Predicting Vulnerable Software Components with Dependency Graphs ∗ Viet Hung Nguyen Le Minh Sang Tran University of Trento, Italy University of Trento, Italy vhnguyen@disi.unitn.it tran@disi.unitn.it ABSTRACT A trade-off solution widely adopted is to focus only on Security metrics and vulnerability prediction for software the most vulnerable parts of software. Concentration on the have gained a lot of interests from the community. Many rest will come later in the maintenance phase, in which dis- software security metrics have been proposed e.g., complex- covered vulnerabilities are fixed by perioding patches. For ity metrics, cohesion and coupling metrics. In this paper, we this purpose, software developers can employ security met- propose a novel code metric based on dependency graphs to rics to prioritize testing effort. The product manager, on the predict vulnerable components. To validate the efficiency of other hand, also needs security metrics to determine whether the proposed metric, we conduct a prediction model which company’s products have reached to an acceptable level of targets the JavaScript Engine of Firefox. In this experiment, security so that they are ready to the market. our prediction model has obtained a very good result in term So far many software security metrics have been intro- of accuracy and recall rates. This empirical result is a good duced, but they are inadequate. For example, estimating evidence showing dependency graphs are also a good option the complexity of a software system and its correlation with for early indicating vulnerability. vulnerability could give an indicator to the security level of software’s components. However, the software complexity Categories and Subject Descriptors itself is very difficult to measure. The software complexitycan be seen in two perspectives: structural and semantic. H.4 [Information Systems Applications]: Miscellaneous; The structural complexity is represented in the code struc- D.2.8 [Software Engineering]: Metrics—complexity mea- ture. Most existing complexity metrics such as total line sures, performance measures of code, nesting levels fall into this category. The semantic complexity is the semantic of the code, which is the com- General Terms plexity of the problems, algorithms. Measuring semantic complexity then is more difficult than structural one. As a Security, Vulnerability, Prediction consequence, current complexity metrics could not perfectly reflect the real nature complexity of software system. 1. INTRODUCTION Contribution. We are interested in studying security It is the fact that most of software have security prob- metrics, in particular metrics for vulnerabilities prediction. lems during lifetime. This insecure phenomenon eventually In this work, we introduce a new prediction model using comes from two reasons: complexity and motivations [15]. dependency graphs of software system. These dependency As requirements are continuously evolving and growing, soft- graphs are based on the relationship among software ele- ware is going to include more and more functions while still ments (i.e., components, classes, functions, variables) which guaranteeing the backward compatibility. This increases the can be obtained from a static code analyzers (e.g., Doxy- complexity of the software. Meanwhile, the software market gen) or from a detail design specification. Therefore, the is the market of “lemons” [1, 15], as first comers take better proposed model can be applied in both design phase and advantages, and have a higher probability to success. There- developing phase. The proposed model is supported by an fore, software vendors lack motivations for putting more experiment on JavaScript Engine of Firefox. testing effort on their products, instead they rush themselves The rest of this paper is organized as follows. We briefly to reduce the time to market. present the vulnerability prediction model in the next sec- ∗ tion (§2), we discuss issues that a prediction model needs toThis research is partly supported by the project EU-FET- IP-SECURE CHANGE (www.securechange.eu). concern, and how to evaluate a prediction model. Then we describe the definition of dependency graph (§3) and our ex- periment (§4). Next we discuss the validity (§5) of our work, we discuss both internal and external factors that might im- pact the experiment result. Finally we concisely describe Permission to make digital or hard copies of all or part of this work for studies in this area for the last ten years (§6) and conclude personal or classroom use is granted without fee provided that copies are the paper with future work (§7). not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific 2. VULNERABILITY PREDICTIONMODEL permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00. This section discusses common research issues, objectives 1

}
}

@Article{HuynhMiller2010,
  Title                    = {An empirical investigation into open source web applications implementation vulnerabilities},
  Author                   = {Huynh, Toan and Miller, James},
  Journal                  = {Empir Software Eng},
  Year                     = {2010},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {556鈥�576},
  Volume                   = {15},

  Doi                      = {10.1007/s10664-010-9131-y},
  File                     = {:article\\An empirical investigation into open source web applications’ implementation vulnerabilities.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1573-7616},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10664-010-9131-y}
}

@Article{HydaraSultanZulzalilEtAl2015,
  Title                    = {Current state of research on cross-site scripting (XSS) – A systematic literature review},
  Author                   = {Hydara, Isatou and Sultan, Abu Bakar Md. and Zulzalil, Hazura and Admodisastro, Novia},
  Journal                  = {Information and Software Technology},
  Year                     = {2015},

  Month                    = {Feb},
  Pages                    = {170–186},
  Volume                   = {58},

  Doi                      = {10.1016/j.infsof.2014.07.010},
  File                     = {:home/ccc/github/literature/article/Current state of research on cross-site scripting (XSS) - A systematic literature review.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2014.07.010}
}

@InProceedings{DETECTIONSOURCE2010,
  Title                    = {TOWARDS THE AUTOMATION OF VULNERABILITY},
  Author                   = {DETECTION IN and SOURCE CODE},
  Year                     = {2010}
}

@TechReport{IndexTransactionsSoftwareEtAl2015,
  Title                    = {104 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 41, NO. 1, JANUARY},
  Author                   = {2014 Index and IEEE Transactions and on Software and Engineering},
  Year                     = {2015},
  Number                   = {and},

  File                     = {:home/ccc/github/literature/article/IEEE Transactions on Software Engineering Vol 40 Index.pdf:PDF},
  Publisher                = {IEEE},
  Review                   = {104 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 41, NO. 1, JANUARY 2015 2014 Index IEEE Transactions on Software Engineering Vol. 40 This index covers all technical items— papers, correspondence, reviews, etc. Casagrande, E., Woldeamlak, S., Woon, W.L., Zeineldin, H.H., and — that appeared in this periodical during 2014, and items from previous years Svetinovic, D., NLP-KAOS for Systems Goal Elicitation: Smart Metering that were commented upon or corrected in 2014. Departments and other items System Case Study; TSE Oct. 2014 941-956 may also be covered if they have been judged to have archival value. Cerrada, C., see Fernandez-Amoros, D., TSE Sept. 2014 895-910 The Author Index contains the primary entry for each item, listed under the Cerrada, J.A., see Fernandez-Amoros, D., TSE Sept. 2014 895-910 ﬁrst author's name. The primary entry includes the coauthors’ names, the title Chan, W. K., see Cai, Y., TSE March 2014 266-281 of the paper or other item, and its location, speciﬁed by the publication abbrevi- Chen, T.Y., see Liu, H., TSE Jan. 2014 4-22 ation, year, month, and inclusive pagination. The Subject Index contains entries Cheung, S., see Liu, Y., TSE Sept. 2014 911-940 describing the item under all appropriate subject headings, plus the ﬁrst author’s Chu, E.T., see Lin, Y., TSE Oct. 2014 957-970 name, the publication abbreviation, month, and year, and inclusive pages. Note Clavel, M., see Basin, D., TSE April 2014 324-337 that the item title is found only under the primary entry in the Author Index. Cohen, M. B., see Yilmaz, C., TSE Jan. 2014 43-66 Collingbourne, P., Cadar, C., and Kelly, P.H.J., Symbolic Crosschecking of Data-Parallel Floating-Point Code; TSE July 2014 710-737 AUTHOR INDEX Coppa, E., Demetrescu, C., and Finocchi, I., Input-Sensitive Proﬁling; TSE Dec. 2014 1185-1205 Cuadrado, J.S., Guerra, E., and de Lara, J., A Component Model for Model A Transformations; TSE Nov. 2014 1042-1060 Andre, E., see Lin, S.-W., TSE Feb. 2014 137-153 Antoniol, G., see Arnaoudova, V., TSE May 2014 502-532 D Araujo, W., Briand, L.C., and Labiche, Y., On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Functional Faults in Con- current Object-Oriented Software; Dagenais, B., and Robillard, M.P., Using Traceability Links to RecommendTSE Oct. 2014 971-992 Arnaoudova, V., Eshkevari, L. M., Penta, M. D., Oliveto, R., Antoniol, G., Adaptive Changes for Documentation Evolution; TSE Nov. 2014 1126- and Gueheneuc, Y.-G., REPENT: Analyzing the Nature of Identiﬁer Re- 1146 namings; Damas, C., Lambeau, B., and van Lamsweerde, A., Analyzing Critical Deci-TSE May 2014 502-532 Artho, C., see Leungwattanakit, W., sion-Based Processes; TSE April 2014 338-365TSE May 2014 483-501 Athanasiou, D., Nugroho, A., Visser, J., and Zaidman, A., Test Code Quality Dang, Y., see Wang, X., TSE Aug. 2014 773-794 and Its Relation to Issue Handling Performance; Dania, C., see Basin, D., TSE April 2014 324-337TSE Nov. 2014 1100-1125 Avgeriou, P., Das, S., see Groce, A., TSE March 2014 307-323see Galster, M., TSE March 2014 282-306 de Dios, M.A.G., see Basin, D., TSE April 2014 324-337 de Lara, J., see Cuadrado, J.S., TSE Nov. 2014 1042-1060 B De Lucia, A., see Bavota, G., TSE July 2014 671-694Demetrescu, C., see Coppa, E., TSE Dec. 2014 1185-1205 Dias-Neto, A., and Travassos, G. H., Supporting the Combined Selection of Bartel, A., Klein, J., Monperrus, M., and Le Traon, Y., Static Analysis for Ex- Model-Based Testing Techniques; TSE Oct. 2014 1025-1041 tracting Permission Checks of a Large Scale Framework: The Challenges Diaz, G., Cambronero, M. E., Martinez, E., and Schneider, G., Speciﬁcation and Solutions for Analyzing Android; TSE June 2014 617-632 and Veriﬁcation of Normative Texts Using C-O Diagrams; TSE Aug. 2014 Basin, D., Clavel, M., Egea, M., de Dios, M.A.G., and Dania, C., A Model- 795-817 Driven Methodology for Developing Secure Data-Management Applica- Dong, J. S., see Lin, S.-W., TSE Feb. 2014 137-153 tions; TSE April 2014 324-337 Draxler, S., Stevens, G., and Boden, A., Keeping the Development Environ- Bavota, G., Oliveto, R., Gethers, M., Poshyvanyk, D., and De Lucia, A., ment Up to Date—A Study of the Situated Practices of Appropriating the Methodbook: Recommending Move Method Refactorings via Relational Eclipse IDE; TSE Nov. 2014 1061-1074 Topic Models; TSE July 2014 671-694 Dreiling, A., see Kastner, C., TSE Jan. 2014 67-82 Bechikh, S., see Kessentini, W., TSE Sept. 2014 841-861 Drusinsky, D., see Schumann, M.A., TSE Feb. 2014 154-166 Bener, A.B., see Misirli, A.T., TSE June 2014 533-554 Duan, Z., see Tian, C., TSE Dec. 2014 1206-1223 Bergersen, G.R., Sjoberg, D.I., and Dyba, T., Construction and Validation of an Duan, Z., see Tian, C., TSE Dec. 2014 1206-1223 Instrument for Measuring Programming Skill; TSE Dec. 2014 1163-1184 Dumlu, E., see Yilmaz, C., TSE Jan. 2014 43-66 Beydoun, G., see Miller, T., TSE Oct. 2014 1007-1024 Dyba, T., see Bergersen, G.R., TSE Dec. 2014 1163-1184 Bice, F., see Groce, A., TSE March 2014 307-323 Binder, W., see Trummer, I., TSE Feb. 2014 167-191 Blair, G. S., see Taiani, F., TSE Feb. 2014 123-136 E Boden, A., see Draxler, S., TSE Nov. 2014 1061-1074 Bollati, V.A., see Vara, J.M., TSE June 2014 555-583 Bowes, D., see Shepperd, M., TSE June 2014 603-616 Egea, M., see Basin, D., TSE April 2014 324-337 Briand, L.C., see Araujo, W., TSE Oct. 2014 971-992 Eshkevari, L. M., see Arnaoudova, V., TSE May 2014 502-532 Burnett, M., see Groce, A., TSE March 2014 307-323 F C Faltings, B., see Trummer, I., TSE Feb. 2014 167-191 Cadar, C., see Collingbourne, P., TSE July 2014 710-737 Fernandez-Amoros, D., Heradio, R., Cerrada, J.A., and Cerrada, C., A Scal- Cadar, C., see Song, J., TSE July 2014 695-709 able Approach to Exact Model and Commonality Counting for Extended Cai, K.-Y., see Lv, J., TSE April 2014 396-412 Feature Models; TSE Sept. 2014 895-910 Cai, Y., and Chan,W.K.,Magiclock: Scalable Detection of Potential Deadlocks Finocchi, I., see Coppa, E., TSE Dec. 2014 1185-1205 in Large-Scale Multithreaded Programs; TSE March 2014 266-281 Foster, J. S., see Song, C., TSE March 2014 251-265 Cambronero, M. E., see Diaz, G., TSE Aug. 2014 795-817 Furia, C. A., see Pei, Y., TSE May 2014 427-449

}
}

@InProceedings{InformationEngElectronicsTelecommunicationsEtAl1512,
  Title                    = {Uncertainty Principle and Sampling of Signals Defined on Graphs Mikhail Tsitsvero1, Sergio Barbarossa1, and Paolo Di Lorenzo},
  Author                   = {1Department of Information Eng and Electronics and Telecommunications and Sapienza University of Rome and 2Department of Engineering and University of Perugia and Via G. Duranti and Perugia and Italy},
  Year                     = {1512},

  Abstract                 = {In many applications of current interest, the obser- requires a careful reformulation of spread in vertex and its vations are represented as a signal defined over a graph. The transformed domain, which should not make any assumption analysis of such signals requires the extension of standard signal about ordering and metrics over the graph domain. processing tools. Building on the recently introduced Graph Fourier Transform, the first contribution of this paper is to A further fundamental tool in signal processing is sam- provide an uncertainty principle for signals on graph. As a pling theory. An initial basic contribution to the extension by-product of this theory, we show how to build a dictionary of sampling theory to graph signals was given in [10]. The of maximally concentrated signals on vertex/frequency domains. theory developed in [10] aimed to show that, given a subset Then, we establish a direct relation between uncertainty principle of samples, there exists a cutoff frequency ω such that, if the and sampling, which forms the basis for a sampling theorem of signals defined on graph. Based on this theory, we show that, spectral support of the signal lies in [0, ω], the overall signal besides sampling rate, the samples’ location plays a key role in can be reconstructed with no errors. Later, [11] extended the the performance of signal recovery algorithms. Hence, we suggest results of [10] providing a method to identify uniqueness sets, a few alternative sampling strategies and compare them with compute the cut-off frequency and to interpolate signals which recently proposed methods. are not exactly band-limited. Further very recent works pro-
},
  File                     = {:article\\Uncertainty Principle and Sampling of Signals Defined on Graphs.pdf:PDF},
  Review                   = {Uncertainty Principle and Sampling of Signals Defined on Graphs Mikhail Tsitsvero1, Sergio Barbarossa1, and Paolo Di Lorenzo2 1Department of Information Eng., Electronics and Telecommunications, Sapienza University of Rome, 2Department of Engineering, University of Perugia, Via G. Duranti 93, 06125, Perugia, Italy, E-mail: tsitsvero@gmail.com, sergio.barbarossa@uniroma1.it, paolo.dilorenzo@unipg.it Abstract—In many applications of current interest, the obser- requires a careful reformulation of spread in vertex and its vations are represented as a signal defined over a graph. The transformed domain, which should not make any assumption analysis of such signals requires the extension of standard signal about ordering and metrics over the graph domain. processing tools. Building on the recently introduced Graph Fourier Transform, the first contribution of this paper is to A further fundamental tool in signal processing is sam- provide an uncertainty principle for signals on graph. As a pling theory. An initial basic contribution to the extension by-product of this theory, we show how to build a dictionary of sampling theory to graph signals was given in [10]. The of maximally concentrated signals on vertex/frequency domains. theory developed in [10] aimed to show that, given a subset Then, we establish a direct relation between uncertainty principle of samples, there exists a cutoff frequency ω such that, if the and sampling, which forms the basis for a sampling theorem of signals defined on graph. Based on this theory, we show that, spectral support of the signal lies in [0, ω], the overall signal besides sampling rate, the samples’ location plays a key role in can be reconstructed with no errors. Later, [11] extended the the performance of signal recovery algorithms. Hence, we suggest results of [10] providing a method to identify uniqueness sets, a few alternative sampling strategies and compare them with compute the cut-off frequency and to interpolate signals which recently proposed methods. are not exactly band-limited. Further very recent works pro- Index Terms—Signals on graphs, Graph Fourier Transform, uncertainty principle, sampling theory. vided the conditions for perfect recovery of band-limited graph signals: [12], [5], based on the adjacency matrix formulation I. I of the GFT; [13], based on the identification of an orthonormalNTRODUCTION basis maximally concentrated over the joint vertex/frequency In many applications, from sensor to social networks, gene domain; [14], based on local-set graph signal reconstructions; regulatory networks or big data, observations can be repre- [15], illustrating the conditions for perfect recovery, based on sented as a signal defined over the vertices of a graph [1], successive local aggregations. [2]. Over the last few years, a series of papers produced The contribution of this paper is threefold: a) we derive an a significant advancement in the development of processing uncertainty principle for graph signals, based on the gener- tools for the analysis of signals defined over a graph, or graph alization of classical Slepian-Landau-Pollack seminal works signals for short [1], [3]. A central role is of course played [16], [17], including the conditions for perfect localization of by spectral analysis of graph signals, which passes through a graph signal in both vertex and frequency domains; b) we the introduction of the so called Graph Fourier Transform establish a link between uncertainty principle and sampling (GFT). Alternative definitions of GFT exist, depending on the theory, thus deriving the necessary and sufficient conditions for different perspectives used to extend classical tools. Two basic the recovery of band-limited graph signals from its samples; c) approaches are available, proposing the projection of the graph we provide alternative sampling strategies aimed at improving signal onto the eigenvectors of either the graph Laplacian, see, the performance of the recovery algorithms in the presence e.g., [1], [4] or of the adjacency matrix, see, e.g. [3], [5]. of noisy observations and compare their performance with Typically, even though a Laplacian matrix can be defined for recently proposed methods. both directed and undirected graphs, the methods in the first class assume undirected graphs, whereas the methods in the II. BASIC DEFINITIONS second class consider the more general directed case. Given We consider a graph G = (V, E) consisting of a set of N the GFT definition, in [6] and very recently in [7], [8], [9], it nodes V = {1, 2, ..., N}, along with a set of weighted edges was derived a graph uncertainty principle aimed at expressing E = {aij}i,j∈V , such that aij > 0, if there is a link from node the fundamental relation between the spread of a signal over j to node i, or aij = 0, otherwise. A signal x over a graph G is the vertex and spectral domains. The approach used in [6] defined as a mapping from the vertex set to complex vectors of is based on the transposition of classical Heisenberg’s method size N , i.e. x : V → C|V|. The adjacency matrix A of a graph to graph signals. However, although the results are interesting, is the collection of all the∑weights aij , i, j = 1, . . . , N . The this transposition gives rise to a series of questions, essentially degree of node i is Nki := j=1 aij . The degree matrix is a related to the fact that while time and frequency domains diagonal matrix having the node degrees on its diagonal: K = are inherently metric spaces, the vertex domain is not. This diag {k1, k2, ..., kN}. The combinatorial Laplacian matrix is arXiv:1512.00775v1 [cs.IT] 2 Dec 2015

}
}

@InProceedings{IsCausation2011,
  Title                    = {For Good Measure},
  Author                   = {Correlation Is and Not Causation},
  Booktitle                = {MARCH/APRIL 2011},
  Year                     = {2011},
  Publisher                = {IEEE},

  File                     = {:home/ccc/github/literature/article/Correlation is not causation.pdf:PDF},
  Review                   = {For Good Measure Correlation Is Not Causation You’re trying too hard to find a correlation here. You don’t know Black Hat 2002. Like all volun- these people, you don’t know what they intended. You try to compile teer efforts, curatorial consisten- statistics and correlate them to a result that amounts to nothing more cy is not its strong suit, but we’re than speculation. —Marc Racicot not here to quibble. Its data store is retrospective, but taking the he IEEE S&P audience certainly knows that cor- arbitrary start point of January T 1990, the frequency of vulner-relation is not the same as causation—that “cor- ability disclosure by date looks Daniel e. like this: Geer Jr.relation is not causation” is taught to more people In-Q-Tel1,5001,250with less effect than almost any other concept, June 20061,000 750 so, of course, there is an XKCD tion. This “complexity is to inse- 500 535 for that (http://xkcd.com/552), curity as simplicity is to security” 250 although nothing comes close idea goes back at least as far as Jan 1990 Jan 1995 Jan 2000 Jan 2005 Jan 2010 to the correlation fixation over Fred Brooks, whose 1975 classic, at http://blog.okcupid.com. But, The Mythical Man Month, made where you will note that the at the same time, security people clear that repairing a system adds rate of new disclosure peaked in are constantly presented with complexity, thus after enough re- June 2006. circumstances in which causa- pair, it is the wise manager who A different source of vulner- tion is rather unclear, and our refuses to patch any more bugs ability information is the US desire to discover it is irresistible. because the cost of introducing National Vulnerability Database If we had a total surveillance so- an unknown new one comes to (NVD) and its Common Vul- ciety, then I suppose that causa- exceed the benefit of removing nerability Enumeration (CVE) tion for acts could almost always a known old one. Brooks called work. That, too, began in 2002 be ascertained. this the “irreducible number of and is likewise retrospective, but Take the idea that complex- errors,” which he presented as a for consistency, let’s again look at ity is the enemy of security. I be- constant for any given project. January 1990, onward: lieve Bruce Schneier said it first: That said, let’s ask the most 1,500 not only do complex systems central question of correlation May 2005 1,250 fail complexly, they must. But versus causality that we wrestle 1,000 is that correlation or causation? with every day: Is software flaw 750 I’m pretty sure it’s causation, but correlated with, or causal of, data 500 the only evidence I have is thin. loss? This isn’t a new question; in 250 240 Code-complexity measures do fact, it turns out to be a complex Jan 1990 Jan 1995 Jan 2000 Jan 2005 Jan 2010 seem to be highest in applica- problem itself so, of course, try- tions that have rather a lot to ing to answer it leads to the re- Here, we see a peak in May 2005. do. Having massive numbers of searcher’s failure to come to a Because whatever method- code paths does make security repeatable result. ological differences exist between hard because massive numbers Putting aside the question of the two are noise for the purpos- of code paths means a large at- whose definition of vulnerabil- es of this discussion, let’s convert tack surface, but massive numbers ity we use, let’s first look at the each graph to a quarterly vulner- of code paths have an even more Open Source Vulnerability Da- ability index and make the start pronounced effect on modifica- tabase (OSVDB), which traces to point January 2005: MARCH/APRIL 2011 1540-7993/11/$26.00 © 2011 IEEE COPUBLISHED BY THE IEEE COMPUTER AND RELIABILITY SOCIETIES 93 

}
}

@InProceedings{ISSNE-ISSN2016,
  Title                    = {Asia Pacific Journal of Research Vol: I. Issue XXXV, January},
  Author                   = {ISSN and E-ISSN},
  Year                     = {2016},

  File                     = {:home/ccc/github/literature/article/A SURVEY ON EXPOSED VULNERABILITIES IN WEB PPLICATIONS.pdf:PDF},
  Keywords                 = {vulnerabilities, web applications, confidentiality},
  Review                   = {Asia Pacific Journal of Research Vol: I. Issue XXXV, January 2016 ISSN: 2320-5504, E-ISSN-2347-4793 A SURVEY ON EXPOSED VULNERABILITIES IN WEB PPLICATIONS A.Saravanan Assistant Professor, Department of MCA, Sri Krishna College of Technology, Coimbatore, Tamil Nadu, INDIA M.S.Irfan Ahmed Director, Department of MCA, Nehru Institute of Engineering and Technology,Coimbatore, Tamil Nadu, INDIA S.SathyaBama Assistant Professor, Department of MCA, Sri Krishna College of Technology, Coimbatore, Tamil Nadu, INDIA ABSTRACT Internet becomes more and more integrated in our society and our offline time continually decreases. However, the number of reported web application vulnerabilities is increasing dramatically. Security vulnerabilities in web applications may result in stealing of confidential data, breaking of data integrity or affect web application availability. So, it is clear that these vulnerabilities are complex and widespread. Thus, the task of securing web applications is not only important but also needs immediate attention, since for most people, Internet and the web are utilities that have become as common as food and water. In this paper, we explore some security breaches in web applications which needs immediate attention. We describe some of the attacks that enable an attacker to impersonate a victim. Keywords: vulnerabilities, web applications, confidentiality 1. Introduction Over the past few years, a clear inclination has emerged that the web applications are under attack. Millions of users connect every day to different web-based applications to search for information, exchange messages, interact with each other, conduct business, perform financial operations and many more [1]. Web security www.apjor.com Page 84 

}
}

@InProceedings{,
  Title                    = {COMPUSOFT, An international journal of advanced computer technology, 5 (3), March - 2016 (Volume-V, Issue-III)},
  Author                   = {ISSN:2320-0790 },
  Year                     = {2016},

  Abstract                 = {? As critical and sensitive systems increasingly rely on complex software systems, identifying software vulnerabilities is becoming increasingly important. It has been suggested in previous work that some bugs are only identified 
},
  File                     = {:home/ccc/github/literature/article/A Survey-Vulnerability Classification of Bug Reports using Multiple machine learning technology.pdf:PDF},
  Keywords                 = {Na�ve Bayes, classification, bug database mining, text mining},
  Review                   = {COMPUSOFT, An international journal of advanced computer technology, 5 (3), March - 2016 (Volume-V, Issue-III) 
 
ISSN:2320-0790 
 A Survey-Vulnerability Classification of Bug Reports using Multiple 
Machine Learning Approach 
 Krishna A Patel, Prof. Rohan C Prajapati 
Ipcowala Institute of Engineering & Technology Dharmaj, Anand, Gujarat, India-388430 
Abstract ? As critical and sensitive systems increasingly rely on complex software systems, identifying software vulnerabilities is becoming increasingly important. It has been suggested in previous work that some bugs are only identified 
as vulnerabilities long after the bug has been made public. These bugs are known as Hidden Impact Bugs (HIBs). This paper 
presents a hidden impact bug identification methodology by means of text mining bug databases. The presented methodology 
utilizes the textual description of the bug report for extracting textual information. The text mining process extracts 
syntactical information of the bug reports and compresses the information for easier manipulation and divided into frequency 
base and context base bug then give bug ranking. 
Keywords: Na�ve Bayes, classification, bug database mining, text mining 
I. INTRODUCTION databases also keep track of the fixes being released for different bugs and what stage of the resolution process a bug 
In data mining, high quality of data are a valuable asset. is in. Because different entities with different levels of 
This also applies to empirical software engineering as well. expertise and requirements report bugs to these databases, 
Since now, mining data from changes and bug databases had the information contained in bug reports is highly noisy and 
become common. As bug database is built from bug reports, not in standard form [17][18]. However, this information 
quality of bug reports are crucial to data quality. Correctly has been successfully used for various classification purpose 
classified bug reports will greatly help in both research [17][18][19]. 
validity and modeling performance. More detail bug report 
will also contain more information which could help in 
understanding data. 
In that paper presents a software vulnerability identification 
methodology using HIBs, that utilizes the textual description 
of the bugs that were reported to publically available bug 
databases. The presented methodology utilizes text mining 
techniques to 1) extract syntactical information of bug 
reports, 2) compares the information for easier 
manipulation, and 3) use this information to generate a 
feature vector which is used for classification. Thus, the 
presented system is intended to classify bugs as potential 
vulnerabilities as they are being reported to bug databases, 
thereby reducing the time software is exposed to attack 
through the vulnerability. 
II. BECKGROUND 
A. Bug Classification Methodology 
Bug databases are used by software developers to identify 
and keep track of information about software bugs that were 
not identified at the time of software release. Developers 
will utilize these bug reports for different purposes such as 
improving reliability and improving future requirements 
[14][15]. Publically available bug databases enable users to Fig.1. Identification methodology of vulnerabilities using report bugs as they encounter it and search the bug database [1]bug report for bugs they might encounter in the future [16]. Bug 
2071 

}
}

@Article{JianKeginSunlu2012,
  Title                    = {Vulnerability analysis of the Android operating system code based on control flow mining},
  Author                   = {Liu Jian and Sun Kegin and Wang Sunlu},
  Journal                  = {Journal of Tsinghua University (Science and Technology)},
  Year                     = {2012},
  Number                   = {10},
  Pages                    = {1335--9},
  Volume                   = {52},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The Android operating system is widely used in smart phones, tablet PCs and other portable mobile devices. Therefore, the security and reliability of the Android operating system code is very important. Systematic checking is applied here to the Android code using control flow mining and manual checking scripts for typical kernel errors. A horizontal analysis and comparison among multiple versions of the Android operating system codes is given. This is the first analysis using control flow mining methods on the Android system code, which includes many new modules such as additional drivers and the Yaffs2 file system. The analysis reveals many vulnerabilities.},
  Groups                   = {Code Mining},
  Sn                       = {1000-0054},
  Tc                       = {0},
  Ut                       = {INSPEC:13625266},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{JimMorrisettGrossmanEtAl2002,
  Title                    = {Cyclone: A safe dialect of C},
  Author                   = {Trevor Jim and Greg Morrisett and Dan Grossman and Michael Hicks},
  Year                     = {2002},

  Abstract                 = {programs reach deeper than just poor training and effort: they have their roots in the design of C itself.
},
  File                     = {:article\\Cyclone A safe dialect of C.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Cyclone: A safe dialect of C Trevor Jim∗ Greg Morrisett† Dan Grossman† Michael Hicks† James Cheney† Yanling Wang† Abstract programs reach deeper than just poor training and effort: they have their roots in the design of C itself. Cyclone is a safe dialect of C. It has been designed Take buffer overflows, for example. Every introduc- from the ground up to prevent the buffer overflows, tory C programming course warns against them and format string attacks, and memory management er- teaches techniques to avoid them, yet they continue rors that are common in C programs, while retain- to be announced in security bulletins every week. ing C’s syntax and semantics. This paper examines There are reasons for this that are more fundamen- safety violations enabled by C’s design, and shows tal than poor training: how Cyclone avoids them, without giving up C’s hallmark control over low-level details such as data representation and memory management. • One cause of buffer overflows in C is bad pointer arithmetic, and arithmetic is tricky. To put it plainly, an off-by-one error can cause a buffer overflow, and we will never be able to train pro- grammers to the point where off-by-one errors 1 Introduction are completely eliminated. • C uses NUL-terminated strings. This is crucial It is a commonly held belief in the security commu- for efficiency (a buffer can be allocated once and nity that safety violations such as buffer overflows used to hold many different strings of different are unprofessional and even downright sloppy. This lengths before deallocation), but there is always recent quote [33] is typical: a danger of overwriting the NUL terminator, usually leading to a buffer overflow in a library function. Some library functions (strcat) have Common errors that cause vulnerabilities alternate versions (strncat) that help, by let- — buffer overflows, poor handling of unex- ting the programmer give a bound on the length pected types and amounts of data — are of a string argument, but there are many dozens well understood. Unfortunately, features of functions in POSIX with no such alternative. still seem to be valued more highly among manufacturers than reliability. • Out-of-bounds pointers are commonplace in C. The standard way to iterate over the elements of an array is to start with a pointer to the first The implication is that safety violations can be pre- element and increment it until it is just past vented just by changing priorities. the end of the array. This is blessed by the C standard, which states that the address just It’s true that highly trained and motivated program- past the end of any array must be valid. When mers can produce extremely robust systems when out-of-bounds pointers are common, you have security is a top priority (witness OpenBSD). It’s to expect that occasionally one will be derefer- also true that most programmers can and should do enced or assigned, causing a buffer overflow. more to ensure the safety and security of the pro- grams that they write. However, we believe that the reasons that safety violations show up so often in C In short, the design of the C programming language ∗ encourages programming at the edge of safety. ThisAT&T Labs Research, trevor@research.att.com †Cornell University, http://www.cs.cornell.edu/ makes programs efficient but also vulnerable, and projects/cyclone leads us to conclude that safety violations are likely

}
}

@Article{JingLiehuiTiemingEtAl2016,
  Title                    = {Multidimensional Graphs Extraction Method In Software Reverse Analysis Process},
  Author                   = {Jing Jing and Jiang Liehui and Liu Tieming and Si Binbin and Zeng Yun and Zhu Xiaoqing},
  Journal                  = {Computer Applications and Software},
  Year                     = {2016},
  Number                   = {4},
  Pages                    = {1000-386X(2016)33:4<1:RJNXFX>2.0.TX;2-N},
  Volume                   = {33},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The results of software reverse analysis process are usually the formal symbols,which are complicated and unintelligibility, therefore the efficiency of the works of code behaviour understanding or code vulnerabilities mining based on software reverse analysis is very slow. In view of this,we design the multidimensional graphs extraction framework firstly,and based on this framework we define the multidimensional graphs description constraints (schema), which makes the multidimensional graphs extraction and specific reverse analysis process be independent from each other. And we present the construction approach of reverse analysis algorithm library and design the call interfaces of the algorithm library,realise the multidimensional graphs description-based abstract graphs generation. Whats more,we design the abstract graphs convert interface which is based on DOT (a graph description language), and achieves the fast visualisation of abstract graphs. At last we present an algorithm of multidimensional graphs extraction. Experimental results show that to use this algorithm can effectively improve the readability of the generated results in reverse analysis process,and greatly increase the work efficiency of analysers in code behaviour understanding and code vulnerabilities mining.},
  Groups                   = {Code Mining},
  Sn                       = {1000-386X},
  Tc                       = {0},
  Ut                       = {CSCD:5683894},
  Z1                       = {è½¯ä»¶éååæè¿ç¨ä¸­çå¤ç»´å¾è°±æ½åæ¹æ³},
  Z2                       = {äºéEOLEOLèçè¾EOLEOLåéé­EOLEOLå¸å½¬å½¬EOLEOLæ¾éµEOLEOLæ±ææ¸},
  Z3                       = {è®¡ç®æºåºç¨ä¸è½¯ä»¶},
  Z4                       = {ç±äºè½¯ä»¶éååæè¿ç¨ä¸­äº§ççç»æéå¸¸æ¯å½¢å¼åçç¬¦å·,ä¸å¤æé¾æ,å æ­¤åºäºè½¯ä»¶éååæè¿è¡ä»£ç éè¯»ææ¼æ´ææç­å·¥ä½çæçéå¸¸ä½ä¸ãéå¯¹è¿ç§æåµ,é¦EOLEOLåè®¾è®¡å¤ç»´å¾è°±æ½åæ¡æ¶,åºäºè¯¥æ¡æ¶å®ä¹å¤ç»´å¾è°±æè¿°çº¦æ(schema), ä½¿å¤ç»´å¾è°±æ½åä¸å·ä½çéååæè¿ç¨ä¹é´ç¸äºç¬ç«;EOLEOLç»åºéååæç®æ³åºçæé æ¹å¼åç®æ³åºè°ç¨æ¥å£,å®ç°åºäºå¤ç»´å¾è°±æè¿°çæ½è±¡å¾è°±çæ;EOLEOLè®¾è®¡åºäºå¾å½¢æè¿°è¯­è¨DOTçæ½è±¡å¾è°±è½¬æ¢æ¥å£,å®ç°æ½è±¡å¾è°±çå¿«éå¯è§å;EOLEOLæåç»åºå¤ç»´å¾è°±æ½åç®æ³ãå®éªç»æè¡¨æ,éç¨è¯¥æ¹æ³è½æææé«éååæè¿ç¨ä¸­çæç»æçå¯è¯»æ§,å¤§å¹æååæäººåä»£ç éè¯»åæ¼æ´ææçå·¥ä½æçã},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@PhdThesis{Joh2011,
  Title                    = {QUANTITATIVE ANALYSES and OF SOFTWARE and VULNERABILITIES},
  Author                   = {HyunChul Joh},
  School                   = {Colorado State University},
  Year                     = {2011},

  File                     = {:article\\QUANTITATIVE ANALYSES OF SOFTWARE VULNERABILITIES.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {web}
}

@Article{JohnsonWayeMooreEtAl2015,
  Title                    = {Exploring and Enforcing Security Guarantees via Program Dependence Graphs},
  Author                   = {Andrew Johnson and Lucas Waye and Scott Moore and and MIT Lincoln LaboratoryHarvard University and USA Stephen Chong},
  Year                     = {2015},

  Abstract                 = {their application provides, such as how public outputs may reveal We present PIDGIN, a program analysis and understanding tool that confidential information and how potentially dangerous operations
},
  Doi                      = {.org/10.1145/2737924.2737957},
  File                     = {:article\\Exploring and Enforcing Security Guarantees via Program Dependence Graphs.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {tion specific, since they are queries in a query language de-Application-specific security, program dependence signed specifically for finding and describing information flows graph, graph query language in a program. Queries can succinctly express global security,literature,article},
  Read                     = {未读},
  Review                   = {Exploring and Enforcing Security Guarantees via Program Dependence Graphs Andrew Johnson Lucas Waye Scott Moore MIT Lincoln Laboratory and Harvard University, USA Stephen Chong ajohnson@seas.harvard.edu Harvard University, USA lwaye, sdmoore, chong@seas.harvard.edu Abstract their application provides, such as how public outputs may reveal We present PIDGIN, a program analysis and understanding tool that confidential information and how potentially dangerous operations enables the specification and enforcement of precise application- may be influenced by untrusted data. These guarantees will nec- specific information security guarantees. PIDGIN also allows de- essarily be application specific, since different applications han- velopers to interactively explore the information flows in their ap- dle different kinds of information, with different requirements for plications to develop policies and investigate counter-examples. the correct handling of information. Moreover, these guarantees are PIDGIN combines program dependence graphs (PDGs), which properties of the entire application, rather than properties that arise precisely capture the information flows in a whole application, with from the correctness of a single component. a custom PDG query language. Queries express properties about Current tools and techniques fall short in helping develop- the paths in the PDG; because paths in the PDG correspond to ers address information security. Testing cannot easily verify information flows in the application, queries can be used to specify information-flow requirements such as “no information about the global security policies. password is revealed except via the encryption function.” Exist- PIDGIN is scalable. Generating a PDG for a 330k line Java ap- ing tools for information-flow security are inadequate for a variety plication takes 90 seconds, and checking a policy on that PDG takes of reasons, since they either unsoundly ignore important informa- under 14 seconds. The query language is expressive, supporting a tion flows, require widespread local annotations, prevent functional large class of precise, application-specific security guarantees. Poli- testing and deployment, or fail to support the specification and en- cies are separate from the code and do not interfere with testing or forcement of application-specific policies. development, and can be used for security regression testing. We present PIDGIN, a system that uses program dependence We describe the design and implementation of PIDGIN and re- graphs (PDGs) [16] to precisely and intuitively capture the infor- port on using it: (1) to explore information security guarantees in mation flows within an entire program 1 and a custom PDG query legacy programs; (2) to develop and modify security policies con- language to allow the exploration, specification, and enforcement currently with application development; and (3) to develop policies of information security guarantees. PDGs express the control and based on known vulnerabilities. data dependencies in a program and abstract away unimportant de- tails such the sequential order of non-interacting statements. They Categories and Subject Descriptors D.4.6 [Operating Systems]: are a great fit for reasoning about information security guarantees, Security and Protection—Information flow controls; F.3.2 [Pro- since paths in the PDG correspond to information flows in the ap- gramming Languages]: Semantics of Programming Languages— plication. Our queries express properties of PDGs which thus cor- Program analysis; F.3.1 [Programming Languages]: Specifying respond to information-flow guarantees about the application. Our and Verifying and Reasoning about Programs—Specification tech- approach has several benefits: niques • PIDGIN security policies are expressive, precise, and applica- Keywords tion specific, since they are queries in a query language de-Application-specific security, program dependence signed specifically for finding and describing information flows graph, graph query language in a program. Queries can succinctly express global security guarantees such as noninterference [18], absence of explicit in- 1. Introduction formation flows, trusted declassification [24], and mediation of Many applications store and compute with sensitive information, information-flow by access control checks. including confidential and untrusted data. Thus, application devel- • Developers can interactively explore an application’s informa- opers must be concerned with the information security guarantees tion security guarantees. If there is no predefined security spec- ification then PIDGIN can be used to quickly explore security- relevant information flows and discover and specify the precise Permission to make digital or hard copies of all or part of this work for personal or security policies that an application satisfies. If a policy is spec- classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation ified but not satisfied, then PIDGIN can help a developer under- on the first page. Copyrights for components of this work owned by others than the stand why by finding information flows that violate the policy. author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or • PIDGIN security policies are not embedded in the code. PIDGIN republish, to post on servers or to redistribute to lists, requires prior specific permission policies are specified separate from the code. The code doesn’t and/or a fee. Request permissions from permissions@acm.org. require program annotations nor does it mention or depend on PLDI’15, June 13–17, 2015, Portland, OR, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. PIDGIN policies. This enables the use of PIDGIN to specify secu- ACM 978-1-4503-3468-6/15/06. . . $15.00. http://dx.doi.org/10.1145/2737924.2737957 1 PDGs for a whole program are also called system-dependence graphs [26].

}
}

@Article{JosephLaskovRoliEtAl2013,
  Title                    = {Machine Learning Methods for Computer Security},
  Author                   = {Anthony D. Joseph and Pavel Laskov and Fabio Roli and J. Doug Tygar and Blaine Nelson},
  Journal                  = {Dagstuhl Manifestos},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {1--30},
  Volume                   = {3},

  Address                  = {Dagstuhl, Germany},
  Annote                   = {Keywords: Adversarial Learning, Computer Security, Robust Statistical Learning, Online Learning with Experts, Game Theory, Learning Theory},
  Doi                      = {http://dx.doi.org/10.4230/DagMan.3.1.1},
  Editor                   = {Anthony D. Joseph and Pavel Laskov and Fabio Roli and J. Doug Tygar and Blaine Nelson},
  File                     = {:article\\Machine Learning Methods for Computer Security.pdf:PDF},
  ISSN                     = {2193-2433},
  Publisher                = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  Url                      = {http://drops.dagstuhl.de/opus/volltexte/2013/4356},
  Urn                      = {urn:nbn:de:0030-drops-43569}
}

@InProceedings{,
  Title                    = {Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)},
  Author                   = {Nenad Jovanovic and Christopher Kruegel and Engin Kirda and Technical University and of Vienna},

  Abstract                 = {resulting damages are increasing. The main reasons for this phenomenon are time and nancial constraints, limited pro-
},
  File                     = {:home/ccc/github/literature/article/Pixy A static analysis tool for detecting web application vulnerabilities?CR 120?.pdf:PDF},
  Review                   = {Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)
Nenad Jovanovic, Christopher Kruegel, and Engin Kirda Technical University of Vienna
Secure Systems Lab fenji,chris,ekg@seclab.tuwien.ac.at
Abstract resulting damages are increasing. The main reasons for this phenomenon are time and nancial constraints, limited pro-
The number and the importance of Web applications gramming skills, or lack of security awareness on part of have increased rapidly over the last years. At the same time, the developers. the quantity and impact of security vulnerabilities in such The existing approaches for mitigating threats to Web applications have grown as well. Since manual code re- applications can be divided into client-side and server-side views are time-consuming, error-prone and costly, the need solutions. The only client-side tool known to the authors for automated solutions has become evident. is Noxes [14], an application-level rewall offering protec-
In this paper, we address the problem of vulnerable tion in case of suspected cross-site scripting (XSS) attacks Web applications by means of static source code analysis. that attempt to steal a user's credentials. Server-side solu- More precisely, we use ow-sensitive, interprocedural and tions have the advantage of being able to discover a larger context-sensitive data ow analysis to discover vulnerable range of vulnerabilities, and the benet of a security aw points in a program. In addition, alias and literal analysis xed by the service provider is instantly propagated to all are employed to improve the correctness and precision of its clients. These server-side techniques can be further clas- the results. The presented concepts are targeted at the gen- sied into dynamic and static approaches. Dynamic tools eral class of taint-style vulnerabilities and can be applied to (e.g., [9, 18, 21], and Perl's taint mode try to detect attacks the detection of vulnerability types such as SQL injection, while executing the audited program, whereas static analyz- cross-site scripting, or command injection. ers ([10, 11, 15, 16]) scan the Web application's source code
Pixy, the open source prototype implementation of our for vulnerabilities. concepts, is targeted at detecting cross-site scripting vul- In this paper, we present Pixy, the rst open source tool nerabilities in PHP scripts. Using our tool, we discovered for statically detecting XSS vulnerabilities in PHP 4 [20] and reported 15 previously unknown vulnerabilities in three code by means of data ow analysis. We chose PHP as web applications, and reconstructed 36 known vulnerabil- target language since it is widely used for designing Web ities in three other web applications. The observed false applications [23], and a substantial number of security ad- positive rate is at around 50% (i.e., one false positive for visories refer to PHP programs [3]. Although our prototype each vulnerability) and therefore, low enough to permit ef- is aimed at the detection of XSS aws, it can be equally fective security audits. applied to other taint-style vulnerabilities such as SQL in-
jection or command injection (see Section 2). The main contributions of this paper are as follows:
1. Introduction  A ow-sensitive, interprocedural, and context-
sensitive data ow analysis for PHP, targeted at Web applications have become one of the most impor- detecting taint-style vulnerabilities. This analysis
tant communication channels between various kinds of ser- process had to overcome signicant conceptual vice providers and clients. Along with the increased impor- challenges due to the untyped nature of PHP. tance of Web applications, the negative impact of security aws in such applications has grown as well. Vulnerabili-  Additional literal analysis and alias analysis steps that ties that may lead to the compromise of sensitive informa- lead to more comprehensive and precise results than tion are being reported continuously, and the costs of the those provided by previous approaches.

}
}

@InProceedings{JovanovicKruegelKirdaEtAl2006,
  Title                    = {Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)},
  Author                   = {Nenad Jovanovic and Christopher Kruegel and Engin Kirda and Technical University and of Vienna},
  Year                     = {2006},

  Abstract                 = {resulting damages are increasing. The main reasons for this phenomenon are time and financial constraints, limited pro-
},
  File                     = {:article\\Pixy A static analysis tool for detecting web application vulnerabilities（CR 120）.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper) Nenad Jovanovic, Christopher Kruegel, and Engin Kirda Technical University of Vienna Secure Systems Lab {enji,chris,ek}@seclab.tuwien.ac.at Abstract resulting damages are increasing. The main reasons for this phenomenon are time and financial constraints, limited pro- The number and the importance of Web applications gramming skills, or lack of security awareness on part of have increased rapidly over the last years. At the same time, the developers. the quantity and impact of security vulnerabilities in such The existing approaches for mitigating threats to Web applications have grown as well. Since manual code re- applications can be divided into client-side and server-side views are time-consuming, error-prone and costly, the need solutions. The only client-side tool known to the authors for automated solutions has become evident. is Noxes [14], an application-level firewall offering protec- In this paper, we address the problem of vulnerable tion in case of suspected cross-site scripting (XSS) attacks Web applications by means of static source code analysis. that attempt to steal a user’s credentials. Server-side solu- More precisely, we use ow-sensitive, interprocedural and tions have the advantage of being able to discover a larger context-sensitive data ow analysis to discover vulnerable range of vulnerabilities, and the benefit of a security flaw points in a program. In addition, alias and literal analysis fixed by the service provider is instantly propagated to all are employed to improve the correctness and precision of its clients. These server-side techniques can be further clas- the results. The presented concepts are targeted at the gen- sified into dynamic and static approaches. Dynamic tools eral class of taint-style vulnerabilities and can be applied to (e.g., [9, 18, 21], and Perl’s taint mode try to detect attacks the detection of vulnerability types such as SQL injection, while executing the audited program, whereas static analyz- cross-site scripting, or command injection. ers ([10, 11, 15, 16]) scan the Web application’s source code Pixy, the open source prototype implementation of our for vulnerabilities. concepts, is targeted at detecting cross-site scripting vul- In this paper, we present Pixy, the first open source tool nerabilities in PHP scripts. Using our tool, we discovered for statically detecting XSS vulnerabilities in PHP 4 [20] and reported 15 previously unknown vulnerabilities in three code by means of data flow analysis. We chose PHP as web applications, and reconstructed 36 known vulnerabil- target language since it is widely used for designing Web ities in three other web applications. The observed false applications [23], and a substantial number of security ad- positive rate is at around 50% (i.e., one false positive for visories refer to PHP programs [3]. Although our prototype each vulnerability) and therefore, low enough to permit ef- is aimed at the detection of XSS flaws, it can be equally fective security audits. applied to other taint-style vulnerabilities such as SQL in- jection or command injection (see Section 2). The main contributions of this paper are as follows: 1. Introduction • A flow-sensitive, interprocedural, and context- sensitive data flow analysis for PHP, targeted at Web applications have become one of the most impor- detecting taint-style vulnerabilities. This analysis tant communication channels between various kinds of ser- process had to overcome significant conceptual vice providers and clients. Along with the increased impor- challenges due to the untyped nature of PHP. tance of Web applications, the negative impact of security flaws in such applications has grown as well. Vulnerabili- • Additional literal analysis and alias analysis steps that ties that may lead to the compromise of sensitive informa- lead to more comprehensive and precise results than tion are being reported continuously, and the costs of the those provided by previous approaches.

}
}

@Article{KazmanGoldensonMonarchEtAl2016,
  Title                    = {Evaluating the Effects of Architectural Documentation: A Case Study of a Large Scale Open Source Project},
  Author                   = {Kazman, Rick and Goldenson, Dennis and Monarch, Ira and Nichols, William and Valetto, Giuseppe},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2016},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {220–260},
  Volume                   = {42},

  Doi                      = {10.1109/tse.2015.2465387},
  File                     = {:home/ccc/github/literature/article/Evaluating the Effects of Architectural.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2015.2465387}
}

@Article{KessentiniKessentiniSahraouiEtAl2014,
  Title                    = {A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection},
  Author                   = {Kessentini, Wael and Kessentini, Marouane and Sahraoui, Houari and Bechikh, Slim and Ouni, Ali},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2014},

  Month                    = {Sep},
  Number                   = {9},
  Pages                    = {841–861},
  Volume                   = {40},

  Doi                      = {10.1109/tse.2014.2331057},
  File                     = {:home/ccc/github/literature/article/A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection..pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2014.2331057}
}

@Article{KhalidZimmermannCorneyEtAl2010,
  Title                    = {Automatic Generation of Assertions to Detect Potential Security Vulnerabilities in C Programs That Use Union and Pointer Types},
  Author                   = {Khalid, Shamsul Kamal Ahmad and Zimmermann, Jacob and Corney, Diane and Fidge, Colin},
  Journal                  = {2010 Fourth International Conference on Network and System Security},
  Year                     = {2010},

  Month                    = {Sep},

  Doi                      = {10.1109/nss.2010.63},
  File                     = {:home/ccc/github/literature/article/Automatic Generation of Assertions to Detect Potential Security Vulnerabilities in C.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-1-4244-8484-3},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/NSS.2010.63}
}

@Article{KhaliliSamiAzimiEtAl2014,
  Title                    = {Employing secure coding practices into industrial applications: a case study},
  Author                   = {Khalili, Abdullah and Sami, Ashkan and Azimi, Mahdi and Moshtari, Sara and Salehi, Zahra and Ghiasi, Mahboobe and Safavi, Ali Akbar},
  Journal                  = {Empir Software Eng},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {1},
  Pages                    = {4–16},
  Volume                   = {21},

  Doi                      = {10.1007/s10664-014-9341-9},
  File                     = {:home/ccc/github/literature/article/Employing secure coding practices into industrial applications\: a case study.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1573-7616},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10664-014-9341-9}
}

@Article{Khedker2014,
  Title                    = {Buffer Overflow Analysis for C},
  Author                   = {Uday P. Khedker},
  Journal                  = {Department of Computer and Science and Engineering and Indian Institute and of Technology and Bombay},
  Year                     = {2014},

  File                     = {:article\\Buffer Overflow Analysis for C.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Buffer Overflow Analysis for C Uday P. Khedker Department of Computer Science and Engineering Indian Institute of Technology Bombay Email: uday@cse.iitb.ac.in Abstract Buffer overflow detection and mitigation for C programs has been an important concern for a long time. This paper defines a string buffer overflow analysis for C. The key ideas of our formulation are (a) separating buffers from the pointers that point to them, (b) modelling buffers in terms of sizes and sets of positions of null characters, and (c) defining stateless functions to compute the sets of null positions and mappings between buffers and pointers. This exercise has been carried out to test the feasibility of describing such an analysis in terms of lattice valued functions and relations to facilitate automatic construction of an analyser without the user having to write C/C++/Java code. This is facilitated by avoiding stateful formulations because they combine effects through side effects in states raising a natural requirement of C/C++/Java code to be written to describe them. Given the above motivation, the focus of this paper is not to build good static approximations for buffer overflow analysis but to show how given static approximations could be formalized in terms of stateless formulations so that they become amenable to automatic construction of analysers. 1 Introduction Low level modelling of strings in C and associated unchecked operations potentially lead to the possibility of buffer overflows. Given the possibility of a potentially fraudulent use of these loop holes in C programs, detection and mitigation of buffer overflows is critical [1, 4, 10, 14, 16]. This paper proposes an analysis to discover buffer overflows. The key ideas of our formulation are (a) separating buffers from the pointers that point to them, (b) modelling buffers in terms of sizes and sets of positions of null characters, and (c) defining stateless functions to compute the sets of null positions and mappings between buffers and pointer. The first idea is not new; the novelty of our work lies in modelling the computations of null position sets and an insistence on stateless formulations. As is customary, we present our formulation in an intraprocedural setting. It should be easy to lift it to an interprocedural setting using standard techniques of interprocedural data flow analysis such as the method of value contexts [7, 8, 12]. Our goal is not to device the best possible static approximations for buffer overflow analysis but to show how given static approximations could be formalized to devise a mathematical formulation which can be transcribed into a declarative specification of the analysis so that it becomes amenable to automatic construction of analysers. The rest of the paper is organized as follows: Section 2 describes the requirements of buffer overflow analysis by defining the core statements for analysis, the soundness criterion, and our assumptions. Sec- tion 3 describes our modelling and defines the analysis in terms of lattices and lattice valued functions and relations. For simplicity of exposition, it assumes that a pointer points to a single buffer at a program point. Section 4 shows the analysis of our running example. Section 5 shows how the model can be extended to allow a pointer to point to multiple buffers at a program point. Section 6 briefly describes the related work arXiv:1412.5400v4 [cs.PL] 28 Dec 2014

}
}

@InProceedings{KhinSharHeeEtAl2012,
  Title                    = {Predicting Common Web Application Vulnerabilities from Input Validation and Sanitization Code Patterns},
  Author                   = {Lwin Khin and Shar and Hee and Beng Kuan and Tan and School of Electrical and Electronic Engineering},
  Year                     = {2012},

  Abstract                 = {However, web applications also adopt input validation as Software defect prediction studies have shown that defect complementary or alternative to input sanitization. Conditional predictors built from static code attributes are useful and branching is a common input validation method for preventing effective. On the other hand, to mitigate the threats posed by web security vulnerabilities and thus, any technique that ignores common web application vulnerabilities, many vulnerability control flow is prone to errors [14]. Data dependence graphs were detection approaches have been proposed. However, finding used for data collection in our previous work. As data dependence alternative solutions to address these risks remains an important graphs do not include predicates, we could not consider input research problem. As web applications generally adopt input validation code patterns previously. As such, our initial work is validation and sanitization routines to prevent web security risks, incomplete. in this paper, we propose a set of static code attributes that Hence, in this paper, we propose additional static code attributes represent the characteristics of these routines for predicting the that characterize input validation code patterns. Static code two most common web application vulnerabilities—SQL attributes are collected from backward static program slices of injection and cross site scripting. In our experiments, vulnerability sensitive program points to mine both input sanitization code predictors built from the proposed attributes detected more than patterns and input validation code patterns. From such static code 80% of the vulnerabilities in the test subjects at low false alarm attributes and vulnerability information of existing web rates. applications, we then train vulnerability prediction models for 
},
  File                     = {:home/ccc/github/literature/article/Predicting Common Web Application Vulnerabilities from.pdf:PDF},
  Keywords                 = {occurs when an HTML output statement references unrestricted user inputs because an attacker could insert malicious scripts into Defect prediction; static code attributes; web application the inputs to change the intended HTML response output. As vulnerabilities; input validation and sanitization; empirical study such, both SQLI and XSS vulnerabilities are caused by absence},
  Review                   = {Predicting Common Web Application Vulnerabilities from Input Validation and Sanitization Code Patterns Lwin Khin Shar and Hee Beng Kuan Tan School of Electrical and Electronic Engineering Nanyang Technological University Singapore 639798 +65 97423763 {shar0035, ibktan}@ntu.edu.sg ABSTRACT However, web applications also adopt input validation as Software defect prediction studies have shown that defect complementary or alternative to input sanitization. Conditional predictors built from static code attributes are useful and branching is a common input validation method for preventing effective. On the other hand, to mitigate the threats posed by web security vulnerabilities and thus, any technique that ignores common web application vulnerabilities, many vulnerability control flow is prone to errors [14]. Data dependence graphs were detection approaches have been proposed. However, finding used for data collection in our previous work. As data dependence alternative solutions to address these risks remains an important graphs do not include predicates, we could not consider input research problem. As web applications generally adopt input validation code patterns previously. As such, our initial work is validation and sanitization routines to prevent web security risks, incomplete. in this paper, we propose a set of static code attributes that Hence, in this paper, we propose additional static code attributes represent the characteristics of these routines for predicting the that characterize input validation code patterns. Static code two most common web application vulnerabilities—SQL attributes are collected from backward static program slices of injection and cross site scripting. In our experiments, vulnerability sensitive program points to mine both input sanitization code predictors built from the proposed attributes detected more than patterns and input validation code patterns. From such static code 80% of the vulnerabilities in the test subjects at low false alarm attributes and vulnerability information of existing web rates. applications, we then train vulnerability prediction models for predicting SQL injection (SQLI) and cross site scripting (XSS) Categories and Subject Descriptors vulnerabilities, the two most common and critical security issues D.2.4 [Software Engineering]: Software/Program Verification – found in web applications [9]. Statistical methods. D.2.8 [Software Engineering]: Metrics – performance measures, product metrics. 2. CLASSIFICATION SQLI vulnerability occurs when an SQL statement access General Terms database via a query built with unrestricted user inputs because an Measurement, Experimentation, Security. attack could include SQL characters or keywords in the inputs to alter the intended query syntax. Similarly, XSS vulnerability Keywords occurs when an HTML output statement references unrestricted user inputs because an attacker could insert malicious scripts into Defect prediction; static code attributes; web application the inputs to change the intended HTML response output. As vulnerabilities; input validation and sanitization; empirical study such, both SQLI and XSS vulnerabilities are caused by absence, inadequate, or insufficient implementation of input validation and 1. INTRODUCTION sanitization methods. Since inputs accessed by web application Recent research in software defect prediction shows that static programs are typically strings, input validation checks and code attributes such as cyclomatic complexity can be used to sanitization operations if performed in programs are mainly based effectively predict defective software modules. Following the data on string operations. mining techniques used in this defect prediction study, in our Therefore, the concept of our approach is to classify the string earlier work [10], we showed that vulnerability predictors, built operations applied on the inputs according to their potential from static code attributes that characterize input sanitization code effects on the vulnerability of sensitive program statement k patterns, provide an alternative and cheaper way of addressing which reference those inputs. common web application vulnerabilities. Intuitively, such validation checks and sanitization operations can be found in backward static slice of the given web program with respect to k and set of variables referenced in k. As given by Weiser [12], backward static slice Sk with respect to slicing Pe rmission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are criterion <k, V> contains all nodes (including predicate nodes) in not made or distributed for profit or commercial advantage and that copies the CFG which may affect the values of V at k where V is the set bear this notice and the full citation on the first page. To copy otherwise, to of variables used in k. republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Following our initial work [10], we classify a node k in the CFG ASE’12, September 3–7, 2012, Essen, Germany Copyright 2012 ACM 978-1-4503-1204-2/12/09 ...$15.00 of a web program as SQL sink if the execution of k may lead to 310

}
}

@InProceedings{KhurshidPa˘sa˘reanuVisser9403,
  Title                    = {Generalized Symbolic Execution for Model Checking and Testing},
  Author                   = {Sarfraz Khurshid and Corina S. Pa˘sa˘reanu and Willem Visser},
  Year                     = {9403},

  Abstract                 = {Modern software systems, which often are concurrent and manipulate complex data structures must be extremely reliable. We present a novel framework based on symbolic execution, for automated checking of such systems. We provide a two-fold generalization of tra- ditional symbolic execution based approaches. First, we define a source to source translation to instrument a program, which enables standard model checkers to perform symbolic execution of the program. Second, we give a novel symbolic execution algorithm that handles dynamically allocated structures (e.g., lists and trees), method preconditions (e.g., acyclicity), data (e.g., integers and strings) and concurrency. The pro- gram instrumentation enables a model checker to automatically explore different program heap configurations and manipulate logical formulae on program data (using a decision procedure). We illustrate two applica- tions of our framework: checking correctness of multi-threaded programs that take inputs from unbounded domains with complex structure and generation of non-isomorphic test inputs that satisfy a testing criterion. Our implementation for Java uses the Java PathFinder model checker.
},
  File                     = {:article\\Generalized Symbolic Execution for Model Checking and Testing.pdf:PDF},
  Groups                   = {source code vulnerability},
  Review                   = {Generalized Symbolic Execution for Model Checking and Testing Sarfraz Khurshid1, Corina S. Pa˘sa˘reanu2, and Willem Visser3 1 MIT Laboratory for Computer Science, Cambridge, MA 02139, USA khurshid@lcs.mit.edu 2 Kestrel Technology LLC and 3 RIACS/USRA, NASA Ames Research Center, Moffett Field, CA 94035, USA {pcorina,wvisser}@email.arc.nasa.gov Abstract. Modern software systems, which often are concurrent and manipulate complex data structures must be extremely reliable. We present a novel framework based on symbolic execution, for automated checking of such systems. We provide a two-fold generalization of tra- ditional symbolic execution based approaches. First, we define a source to source translation to instrument a program, which enables standard model checkers to perform symbolic execution of the program. Second, we give a novel symbolic execution algorithm that handles dynamically allocated structures (e.g., lists and trees), method preconditions (e.g., acyclicity), data (e.g., integers and strings) and concurrency. The pro- gram instrumentation enables a model checker to automatically explore different program heap configurations and manipulate logical formulae on program data (using a decision procedure). We illustrate two applica- tions of our framework: checking correctness of multi-threaded programs that take inputs from unbounded domains with complex structure and generation of non-isomorphic test inputs that satisfy a testing criterion. Our implementation for Java uses the Java PathFinder model checker. 1 Introduction Modern software systems, which often are concurrent and manipulate complex dynamically allocated data structures (e.g., linked lists or binary trees), must be extremely reliable and correct. Two commonly used techniques for checking correctness of such systems are testing and model checking. Testing is widely used but usually involves manual test input generation. Furthermore, testing is not good at finding errors related to concurrent behavior. Model checking, on the other hand, is automatic and particularly good at analyzing (concurrent) reactive systems. A drawback of model checking is that it suffers from the state- space explosion problem and typically requires a closed system, i.e., a system together with its environment, and a bound on input sizes [4, 6, 9, 19]. We present a novel framework based on symbolic execution [14], which au- tomates test case generation, allows model checking concurrent programs that take inputs from unbounded domains with complex structure, and helps com- bat state-space explosion. Symbolic execution is a well known program analysis

}
}

@Article{KimMaPark2016,
  Title                    = {An analysis on secure coding using symbolic execution engine},
  Author                   = {Kim, Joon-Ho and Ma, Myung-Chul and Park, Jae-Pyo},
  Journal                  = {J Comput Virol Hack Tech},
  Year                     = {2016},

  Month                    = {Jan},

  Doi                      = {10.1007/s11416-016-0263-5},
  File                     = {:home/ccc/github/literature/article/An analysis on secure coding using symbolic execution engine.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {2263-8733},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s11416-016-0263-5}
}

@Article{KumarKavitha2014,
  Title                    = {Analysis of Mobile Agents Applications in Distributed System},
  Author                   = {Kumar, K. Senthil and Kavitha, S.},
  Journal                  = {Proceedings of International Conference On Internet Computing and Information Communications (icicic Global 2012)},
  Year                     = {2014},
  Pages                    = {ICICIC Global; SCBIT},
  Volume                   = {216},

  __markedentry            = {[ccc:6]},
  Abstract                 = {With the increasing demand to extend data mining technology to datasets inherently distributed among a large number of autonomous and heterogeneous sources over a network, there comes a new innovative technology that leads the system based on the mobility code especially mobile agents. Primary focus in distributed systems is that mobile agents have a number of key features that allow them to reduce the network load and overcome network latency. Mobile agents offer a more uniform approach to handling code and data in a distributed system. They can encapsulate protocols, and they can work remotely, even asynchronously and disconnected from a network. It improves the latency and bandwidth of client-server applications and reducing vulnerability to network disconnection. This paper presents an analysis of the various applications of mobile agents and some of the benefits and challenges of this new technology.},
  Be                       = {Sathiakumar, SEOLEOLAwasthi, LKEOLEOLMasillamani, MREOLEOLSridhar, SS},
  Bn                       = {978-81-322-1298-0; 978-81-322-1299-7},
  Cl                       = {Chennai, INDIA},
  Ct                       = {1st International Conference on Internet Computing and InformationEOLEOLCommunications (ICICIC)},
  Cy                       = {FEB 12-14, 2012},
  Doi                      = {10.1007/978-81-322-1299-7_46},
  Groups                   = {Code Mining},
  Se                       = {Advances in Intelligent Systems and Computing},
  Sn                       = {2194-5357},
  Tc                       = {1},
  Ut                       = {WOS:000328352200046},
  Z8                       = {0},
  Z9                       = {1},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{KumarDishtiAakritiEtAl3805,
  Title                    = {A Systematic Review on Software Defect Prediction},
  Author                   = {Pradeep Kumar and Singh Dishti and Agarwal Aakriti and Gupta and Department of CSE and Department of CSE and Department of CSE},
  Year                     = {3805},
  Publisher                = {IEEE},

  Abstract                 = {- This paper explains how to find the defects in the • Each paper is thoroughly analyzed and then findings have software using various techniques. We have analyzed been reported. different data sets which have been used in finding faults in • Finally, tables were made to categorize the major various research papers. The main aim of this paper is to contribution of the authors, data sets used in various study various methods that can be used to predict the defects studies. in software. We have decided following research goals to do the analysis on software defect prediction: Keywords - Software Defect, Faults, Prediction, Software 
},
  File                     = {:home/ccc/github/literature/article/A Systematic Review on Software Defect Predication.pdf:PDF},
  Review                   = { A Systematic Review on Software Defect Prediction Pradeep Kumar Singh Dishti Agarwal Aakriti Gupta Department of CSE, Department of CSE, Department of CSE, ASET, Amity University, ASET, Amity University, ASET, Amity University, Uttar Pradesh, Noida, INDIA Uttar Pradesh, Noida, INDIA Uttar Pradesh, Noida, INDIA Email Id: pksingh16@amity.edu Email Id: dishti.agarwal@gmail.com Email Id: superaakriti1@gmail.com Abstract - This paper explains how to find the defects in the • Each paper is thoroughly analyzed and then findings have software using various techniques. We have analyzed been reported. different data sets which have been used in finding faults in • Finally, tables were made to categorize the major various research papers. The main aim of this paper is to contribution of the authors, data sets used in various study various methods that can be used to predict the defects studies. in software. We have decided following research goals to do the analysis on software defect prediction: Keywords - Software Defect, Faults, Prediction, Software Faults, Defect Density Research Questions: RQ1: To identify the techniques used in the defect I. INTRODUCTION prediction? The defect is a flaw in the component or system which RQ2: To identify the factors affecting the defect or can cause it to fail to perform its desired function i.e., an incorrect software metrics which can be used to predict the bugs? statement or data definition. A defect, if encountered during RQ3: To identify the data used for the bugs? execution may cause a failure of the system or a component. Defect Prediction helps in identifying the vulnerabilities in the project plan in t III. RELATED WORK erms of lack of resources, improperly defined timelines, predictable defects, etc. It can help organizations to In [1], Jureczko discussed about the importance of different fetch huge profits without getting delayed on schedules planned product metrics and processes in the prediction model. In their or overrun on estimates of budget. It helps in modifying the paper, they have focused upon the correlations, metric parameters in order to meet the schedule variations. occurrences in the defect prediction model by using different The methods to estimate the software defects are regression, techniques and the number of defects was calculated for each genetic programming, clustering, neural network, statistical of the metric respectively. technique of discriminate analysis, dictionary learning In [2], Rathore et al. aimed to propose object-oriented metrics approach, hybrid attribute selection approach, classification, in order to evaluate the quality of an object-oriented software attribute selection and instance filtering, Bayesian Belief system. They have focused upon combining the design Networks, K-means clustering, Association Rule Mining. attributes in order to achieve higher correctness and higher level of Accuracy. The result evaluated found that the design II. REVIEW PROCEDURE attributes significant to fault proneness are complexity, To analyze th coupling and size. Hence it was found that models which are e software defect prediction technique we have read nearly 150 research papers. Similar to the Singh et al. [21, built on complexity and coupling are more accurate and 22], we have identified the most relevant papers based on th precise than the models which are built on other metrics. e following In [3], Mohd et al. discussed about the Automated steps: • Downloaded the papers based on the search string Fault Proneness Prediction tool which finds out probability of : Soft faults in a software, and uses computer aided approach. In ware Defect Prediction or Bug prediction in software. • We read the Abstract, Title and Conclusio their paper, Genetic algorithm is used to classify the software n. as faulty or non-faulty and takes the input values as metrics • Based on the content of papers we have selected the counted from the Open source software. It is then finally research papers relevant to our analysis. represented using Unified Modeling Language. • Out of the 150 papers 20 most relevant papers were found. 978-9-3805-4416-8/15/$31.00©c 2015 IEEE 1793

}
}

@InProceedings{Landi1992,
  Title                    = {Undecidability of Static Analysis},
  Author                   = {William Landi},
  Year                     = {1992},

  File                     = {:article\\Undecidability of static analysis.10.1.1.35.9722.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读}
}

@InProceedings{Larochellelarochelle@cs.virginia.edu,
  Title                    = {Statically Detecting Likely Buffer Overflow Vulnerabilities},
  Author                   = {David Larochelle and larochelle@cs.virginia.edu},

  File                     = {:home/ccc/github/literature/article/Statically Detecting Likely Buffer Overflow Vulnerabilities (1).pdf:PDF},
  Review                   = { Statically Detecting Likely Buffer Overflow Vulnerabilities David Larochelle larochelle@cs.virginia.edu University of Virginia, Department of Computer Science David Evans evans@cs.virginia.edu University of Virginia, Department of Computer Science Abstract Buffer overflow attacks may be today’s single most important security threat. This paper presents a new approach to mitigating buffer overflow vulnerabilities by detecting likely vulnerabilities through an analysis of the program source code. Our approach exploits information provided in semantic comments and uses lightweight and efficient static analyses. This paper describes an implementation of our approach that extends the LCLint annotation-assisted static checking tool. Our tool is as fast as a compiler and nearly as easy to use. We present experience using our approach to detect buffer overflow vulnerabilities in two security-sensitive programs. 1. Introduction ed a prototype tool that does this by extending LCLint [Evans96]. Our work differs from other work on static detection of buffer overflows in three key ways: (1) we Buffer overflow attacks are an important and persistent exploit semantic comments added to source code to security problem. Buffer overflows account for enable local checking of interprocedural properties; (2) approximately half of all security vulnerabilities we focus on lightweight static checking techniques that [CWPBW00, WFBA00]. Richard Pethia of CERT have good performance and scalability characteristics, identified buffer overflow attacks as the single most im- but sacrifice soundness and completeness; and (3) we portant security problem at a recent software introduce loop heuristics, a simple approach for engineering conference [Pethia00]; Brian Snow of the efficiently analyzing many loops found in typical NSA predicted that buffer overflow attacks would still programs. be a problem in twenty years [Snow99]. The next section of this paper provides some Programs written in C are particularly susceptible to background on buffer overflow attacks and previous buffer overflow attacks. Space and performance were attempts to mitigate the problem. Section 3 gives an more important design considerations for C than safety. overview of our approach. In Section 4, we report on Hence, C allows direct pointer manipulations without our experience using our tool on wu-ftpd and BIND, two any bounds checking. The standard C library includes security-sensitive programs. The following two sec- many functions that are unsafe if they are not used tions provide some details on how our analysis is done. carefully. Nevertheless, many security-critical pro- Section 7 compares our work to related work on buffer grams are written in C. overflow detection and static analysis. Several run-time approaches to mitigating the risks associated with buffer overflows have been proposed. 2. Buffer Overflow Attacks and Defenses Despite their availability, these techniques are not used widely enough to substantially mitigate the The simplest buffer overflow attack, stack smashing effectiveness of buffer overflow attacks. The next [AlephOne96], overwrites a buffer on the stack to section describes representative run-time approaches replace the return address. When the function returns, and speculates on why they are not more widely used. instead of jumping to the return address, control will We propose, instead, to tackle the problem by detecting jump to the address that was placed on the stack by the likely buffer overflow vulnerabilities through a static attacker. This gives the attacker the ability to execute analysis of program source code. We have implement- arbitrary code. Programs written in C are particularly 

}
}

@Article{Larochellelarochelle@cs.virginia.edu2001,
  Title                    = {Statically Detecting Likely Buffer Overflow Vulnerabilities},
  Author                   = {David Larochelle and larochelle@cs.virginia.edu},
  Year                     = {2001},

  File                     = {:article\\Statically Detecting Likely Buffer Overflow Vulnerabilities.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = { Statically Detecting Likely Buffer Overflow Vulnerabilities David Larochelle larochelle@cs.virginia.edu University of Virginia, Department of Computer Science David Evans evans@cs.virginia.edu University of Virginia, Department of Computer Science Abstract Buffer overflow attacks may be today’s single most important security threat. This paper presents a new approach to mitigating buffer overflow vulnerabilities by detecting likely vulnerabilities through an analysis of the program source code. Our approach exploits information provided in semantic comments and uses lightweight and efficient static analyses. This paper describes an implementation of our approach that extends the LCLint annotation-assisted static checking tool. Our tool is as fast as a compiler and nearly as easy to use. We present experience using our approach to detect buffer overflow vulnerabilities in two security-sensitive programs. 1. Introduction ed a prototype tool that does this by extending LCLint [Evans96]. Our work differs from other work on static detection of buffer overflows in three key ways: (1) we Buffer overflow attacks are an important and persistent exploit semantic comments added to source code to security problem. Buffer overflows account for enable local checking of interprocedural properties; (2) approximately half of all security vulnerabilities we focus on lightweight static checking techniques that [CWPBW00, WFBA00]. Richard Pethia of CERT have good performance and scalability characteristics, identified buffer overflow attacks as the single most im- but sacrifice soundness and completeness; and (3) we portant security problem at a recent software introduce loop heuristics, a simple approach for engineering conference [Pethia00]; Brian Snow of the efficiently analyzing many loops found in typical NSA predicted that buffer overflow attacks would still programs. be a problem in twenty years [Snow99]. The next section of this paper provides some Programs written in C are particularly susceptible to background on buffer overflow attacks and previous buffer overflow attacks. Space and performance were attempts to mitigate the problem. Section 3 gives an more important design considerations for C than safety. overview of our approach. In Section 4, we report on Hence, C allows direct pointer manipulations without our experience using our tool on wu-ftpd and BIND, two any bounds checking. The standard C library includes security-sensitive programs. The following two sec- many functions that are unsafe if they are not used tions provide some details on how our analysis is done. carefully. Nevertheless, many security-critical pro- Section 7 compares our work to related work on buffer grams are written in C. overflow detection and static analysis. Several run-time approaches to mitigating the risks associated with buffer overflows have been proposed. 2. Buffer Overflow Attacks and Defenses Despite their availability, these techniques are not used widely enough to substantially mitigate the The simplest buffer overflow attack, stack smashing effectiveness of buffer overflow attacks. The next [AlephOne96], overwrites a buffer on the stack to section describes representative run-time approaches replace the return address. When the function returns, and speculates on why they are not more widely used. instead of jumping to the return address, control will We propose, instead, to tackle the problem by detecting jump to the address that was placed on the stack by the likely buffer overflow vulnerabilities through a static attacker. This gives the attacker the ability to execute analysis of program source code. We have implement- arbitrary code. Programs written in C are particularly 

}
}

@Article{Larochelle2001,
  Title                    = {Improving security using extensible lightweight static analysis （CR 477）},
  Author                   = {David EvansDavid Larochelle},
  Journal                  = {IEEE SOFTWARE},
  Year                     = {2001},

  File                     = {:article\\Improving security using extensible lightweight static analysis （CR 281）.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis David Evans and David Larochelle, University of Virginia uilding secure systems involves numerous complex and challeng- B ing problems, ranging from building strong cryptosystems anddesigning authentication protocols to producing a trust modeland security policy. Despite these challenges, most security at- tacks exploit either human weaknesses—such as poorly chosen passwords and careless configuration—or software implementation flaws. Although it’s hard to do much about human frailties, some help is available through education, better interface design, and secu- cryptographic problems. Analyses of other Security attacks rity-conscious defaults. With software im- vulnerability and incident reports reveal sim- that exploit plementation flaws, however, the problems ilar repetition. For example, David Wagner well-known are typically both preventable and well and his colleagues found that buffer over- implementation understood. flow vulnerabilities account for approxi- flaws occur with Analyzing reports of security attacks mately 50 percent of the Software Engineer- disturbing frequency quickly reveals that most attacks do not re- ing Institute’s CERT advisories. 2 sult from clever attackers discovering new So why do developers keep making the because the software kinds of flaws, but rather stem from re- same mistakes? Some errors are caused by development process peated exploits of well-known problems. legacy code, others by programmers’ care- does not include Figure 1 summarizes Mitre’s Common Vul- lessness or lack of awareness about security techniques for nerabilities and Exposures list of 190 entries concerns. However, the root problem is that preventing those from 1 January 2001 through 18 September while security vulnerabilities, such as buffer flaws. The authors 2001.1 Thirty-seven of these entries are stan- overflows, are well understood, the tech- have developed an dard buffer overflow vulnerabilities (includ- niques for avoiding them are not codified extensible tool that ing three related memory-access vulnerabili- into the development process. Even conscien- detects common ties), and 11 involve format bugs. Most of tious programmers can overlook security is- flaws using the rest also reveal common flaws detectable sues, especially those that rely on undocu- lightweight by static analysis, including resource leaks mented assumptions about procedures and static analysis. (11), file name problems (19), and symbolic data types. Instead of relying on program-links (20). Only four of the entries involve mers’ memories, we should strive to produce 4 2 I E E E S O F T W A R E J a n u a r y / F e b r u a r y 2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0 © 2 0 0 2 I E E E}
}

@InProceedings{LarochelleVirginia2001,
  Title                    = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis},
  Author                   = {and David EvansDavid Larochelle and University of Virginia},
  Year                     = {2001},

  File                     = {:home/ccc/github/literature/article/Improving Security Using Extensible Lightweight Static Analysis.pdf:PDF},
  Review                   = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis David Evans and David Larochelle, University of Virginia uilding secure systems involves numerous complex and challeng- B ing problems, ranging from building strong cryptosystems anddesigning authentication protocols to producing a trust modeland security policy. Despite these challenges, most security at- tacks exploit either human weaknesses—such as poorly chosen passwords and careless configuration—or software implementation flaws. Although it’s hard to do much about human frailties, some help is available through education, better interface design, and secu- cryptographic problems. Analyses of other Security attacks rity-conscious defaults. With software im- vulnerability and incident reports reveal sim- that exploit plementation flaws, however, the problems ilar repetition. For example, David Wagner well-known are typically both preventable and well and his colleagues found that buffer over- implementation understood. flow vulnerabilities account for approxi- flaws occur with Analyzing reports of security attacks mately 50 percent of the Software Engineer- disturbing frequency quickly reveals that most attacks do not re- ing Institute’s CERT advisories. 2 sult from clever attackers discovering new So why do developers keep making the because the software kinds of flaws, but rather stem from re- same mistakes? Some errors are caused by development process peated exploits of well-known problems. legacy code, others by programmers’ care- does not include Figure 1 summarizes Mitre’s Common Vul- lessness or lack of awareness about security techniques for nerabilities and Exposures list of 190 entries concerns. However, the root problem is that preventing those from 1 January 2001 through 18 September while security vulnerabilities, such as buffer flaws. The authors 2001.1 Thirty-seven of these entries are stan- overflows, are well understood, the tech- have developed an dard buffer overflow vulnerabilities (includ- niques for avoiding them are not codified extensible tool that ing three related memory-access vulnerabili- into the development process. Even conscien- detects common ties), and 11 involve format bugs. Most of tious programmers can overlook security is- flaws using the rest also reveal common flaws detectable sues, especially those that rely on undocu- lightweight by static analysis, including resource leaks mented assumptions about procedures and static analysis. (11), file name problems (19), and symbolic data types. Instead of relying on program-links (20). Only four of the entries involve mers’ memories, we should strive to produce 4 2 I E E E S O F T W A R E J a n u a r y / F e b r u a r y 2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0 © 2 0 0 2 I E E E

}
}

@InProceedings{LarochelleVirginia2001a,
  Title                    = {Improving Security Using Extensible Lightweight Static Analysis},
  Author                   = {and David EvansDavid Larochelle and University of Virginia},
  Year                     = {2001},

  File                     = {:home/ccc/github/literature/article/Improving security using extensible.pdf:PDF},
  Review                   = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis David Evans and David Larochelle, University of Virginia uilding secure systems involves numerous complex and challeng- B ing problems, ranging from building strong cryptosystems anddesigning authentication protocols to producing a trust modeland security policy. Despite these challenges, most security at- tacks exploit either human weaknesses—such as poorly chosen passwords and careless configuration—or software implementation flaws. Although it’s hard to do much about human frailties, some help is available through education, better interface design, and secu- cryptographic problems. Analyses of other Security attacks rity-conscious defaults. With software im- vulnerability and incident reports reveal sim- that exploit plementation flaws, however, the problems ilar repetition. For example, David Wagner well-known are typically both preventable and well and his colleagues found that buffer over- implementation understood. flow vulnerabilities account for approxi- flaws occur with Analyzing reports of security attacks mately 50 percent of the Software Engineer- disturbing frequency quickly reveals that most attacks do not re- ing Institute’s CERT advisories. 2 sult from clever attackers discovering new So why do developers keep making the because the software kinds of flaws, but rather stem from re- same mistakes? Some errors are caused by development process peated exploits of well-known problems. legacy code, others by programmers’ care- does not include Figure 1 summarizes Mitre’s Common Vul- lessness or lack of awareness about security techniques for nerabilities and Exposures list of 190 entries concerns. However, the root problem is that preventing those from 1 January 2001 through 18 September while security vulnerabilities, such as buffer flaws. The authors 2001.1 Thirty-seven of these entries are stan- overflows, are well understood, the tech- have developed an dard buffer overflow vulnerabilities (includ- niques for avoiding them are not codified extensible tool that ing three related memory-access vulnerabili- into the development process. Even conscien- detects common ties), and 11 involve format bugs. Most of tious programmers can overlook security is- flaws using the rest also reveal common flaws detectable sues, especially those that rely on undocu- lightweight by static analysis, including resource leaks mented assumptions about procedures and static analysis. (11), file name problems (19), and symbolic data types. Instead of relying on program-links (20). Only four of the entries involve mers’ memories, we should strive to produce 4 2 I E E E S O F T W A R E J a n u a r y / F e b r u a r y 2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0 © 2 0 0 2 I E E E

}
}

@InProceedings{,
  Title                    = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis},
  Author                   = { and David EvansDavid Larochelle and University of Virginia},
  Year                     = {2001},

  File                     = {:home/ccc/github/literature/article/Improving security using extensible lightweight static analysis ?CR 281?.pdf:PDF},
  Review                   = {fbouildcinug sosftware securely Improving Security Using Extensible Lightweight Static Analysis
David Evans and David Larochelle, University of Virginia
uilding secure systems involves numerous complex and challeng-
B ing problems, ranging from building strong cryptosystems anddesigning authentication protocols to producing a trust modeland security policy. Despite these challenges, most security at- tacks exploit either human weaknesses?such as poorly chosen passwords and careless configuration?or software implementation flaws. Although it?s hard to do much about human frailties, some help is available through 
education, better interface design, and secu- cryptographic problems. Analyses of other Security attacks rity-conscious defaults. With software im- vulnerability and incident reports reveal sim- that exploit plementation flaws, however, the problems ilar repetition. For example, David Wagner well-known are typically both preventable and well and his colleagues found that buffer over- implementation understood. flow vulnerabilities account for approxi-
flaws occur with Analyzing reports of security attacks mately 50 percent of the Software Engineer-
disturbing frequency quickly reveals that most attacks do not re- ing Institute?s CERT advisories. 2
sult from clever attackers discovering new So why do developers keep making the because the software kinds of flaws, but rather stem from re- same mistakes? Some errors are caused by development process peated exploits of well-known problems. legacy code, others by programmers? care- does not include Figure 1 summarizes Mitre?s Common Vul- lessness or lack of awareness about security techniques for nerabilities and Exposures list of 190 entries concerns. However, the root problem is that preventing those from 1 January 2001 through 18 September while security vulnerabilities, such as buffer flaws. The authors 2001.1 Thirty-seven of these entries are stan- overflows, are well understood, the tech- have developed an dard buffer overflow vulnerabilities (includ- niques for avoiding them are not codified extensible tool that ing three related memory-access vulnerabili- into the development process. Even conscien- detects common ties), and 11 involve format bugs. Most of tious programmers can overlook security is- flaws using the rest also reveal common flaws detectable sues, especially those that rely on undocu-
lightweight by static analysis, including resource leaks mented assumptions about procedures and
static analysis. (11), file name problems (19), and symbolic data types. Instead of relying on program-links (20). Only four of the entries involve mers? memories, we should strive to produce
4 2 I E E E S O F T W A R E J a n u a r y / F e b r u a r y 2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0 � 2 0 0 2 I E E E

}
}

@InProceedings{LarusBallDasEtAl2004,
  Title                    = {Righting Software},
  Author                   = {James R. Larus and Thomas Ball and Manuvir Das and Robert DeLine and Manuel Fähndrich and Jon Pincus and Sriram K. Rajamani and Ramanathan},
  Year                     = {2004},

  File                     = {:article\\Righting Software.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {fceorraecttneussr tooels Righting Software James R. Larus, Thomas Ball, Manuvir Das, Robert DeLine, Manuel Fähndrich, Jon Pincus, Sriram K. Rajamani, and Ramanathan Venkatapathy, Microsoft Research hat tools do you use to develop and debug software? Most of W us rely on a full-screen editor to write code, a compiler totranslate it, a source-level debugger to correct it, and asource-code control system to archive and share it. These tools originated in the 1970s, when the change from batch to interactive programming stimulated the development of innovative languages, tools, environments, and other utilities we take for granted. Three decades later, these are still the pri- These correctness tools help close the gap that mary tools developers use to write software. separates a programmer’s intent—which can of- Although they’ve been refined, the tools have ten be concisely stated—from the vast amount neither progressed to meet the challenge of of code required to realize that goal. A devel- complex software nor evolved to exploit faster oper’s job is to bridge this chasm; the job of cor- computers. Today, tools from an era of com- rectness tools is to ensure that the resulting span putational scarcity run on machines four or- is straight, level, and connects the right points. ders of magnitude faster. Developers are strug- We don’t believe that better programming gling to write, understand, and manipulate tools—or faster computers—will turn software large, complex software, while vast computa- development into a routine job. Programming is tional resources sit idle beneath their desks. a difficult intellectual task that requires talented Microsoft Research has developed two gen- people to apply sustained and focused effort. erations of tools, some of which Microsoft de- However, just as mechanical devices can unleash velopers already use to find and correct bugs. creative potential by amplifying physical effort, programming tools can improve software devel- opment by helping developers manage details, Correctness tools can improve software development by find inconsistencies, and ensure uniform quality. systematically detecting programming errors. Microsoft Research Correctness tools has developed two generations of these tools that help programmers Program errors detectable by tools fall into find and fix errors early in the development process. two broad categories: 9 2 I E E E S O F T W A R E P u b l i s h e d b y t h e I E E E C o m p u t e r S o c i e t y 0 7 4 0 - 7 4 5 9 / 0 4 / $ 2 0 . 0 0 © 2 0 0 4 I E E E

}
}

@InProceedings{LattnerAdveIllinoisEtAl,
  Title                    = {LLVM: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  Author                   = {Chris Lattner and Vikram Adve and University of Illinois and at Urbana-Champaign},

  Abstract                 = {at link-time (to preserve the benefits of separate compila- This paper describes LLVM (Low Level Virtual Machine), tion), machine-dependent optimizations at install time on
},
  File                     = {:article\\LLVM_ A compilation framework for lifelong program analysis & transformation.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Review                   = {LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation Chris Lattner Vikram Adve University of Illinois at Urbana-Champaign {lattner,vadve}@cs.uiuc.edu http://llvm.cs.uiuc.edu/ ABSTRACT at link-time (to preserve the benefits of separate compila- This paper describes LLVM (Low Level Virtual Machine), tion), machine-dependent optimizations at install time on a compiler framework designed to support transparent, life- each system, dynamic optimization at runtime, and profile- long program analysis and transformation for arbitrary pro- guided optimization between runs (“idle-time”) using profile grams, by providing high-level information to compiler information collected from the end-user. transformations at compile-time, link-time, run-time, and Program optimization is not the only use for lifelong anal- oﬄine. LLVM defines a common, low-level code representa- ysis and transformation. Other emerging applications of tion in Static Single Assignment (SSA) form, with several static analysis are fundamentally interprocedural, and are novel features: a simple, language-independent type-system therefore most convenient to perform at link-time (exam- that exposes the primitives commonly used to implement ples include static debugging, static leak detection [22], and high-level language features; an instruction for typed ad- memory management transformations [26]). Sophisticated dress arithmetic; and a simple mechanism that can be used analyses and transformations are being developed to enforce to implement the exception handling features of high-level program safety, but must be done at software installation languages (and setjmp/longjmp in C) uniformly and effi- time or load-time [17]. Allowing lifelong reoptimization of ciently. The LLVM compiler framework and code repre- the program gives architects the power to evolve processors sentation together provide a combination of key capabili- and exposed interfaces in more flexible ways [10, 18], while ties that are important for practical, lifelong analysis and allowing legacy applications to run well on new systems. transformation of programs. To our knowledge, no existing This paper presents LLVM — Low-Level Virtual Ma- compilation approach provides all these capabilities. We de- chine — a compiler framework that aims to make lifelong scribe the design of the LLVM representation and compiler program analysis and transformation available for arbitrary framework, and evaluate the design in three ways: (a) the software, and in a manner that is transparent to program- size and effectiveness of the representation, including the mers. LLVM achieves this through two parts: (a) a code rep- type information it provides; (b) compiler performance for resentation with several novel features that serves as a com- several interprocedural problems; and (c) illustrative exam- mon representation for analysis, transformation, and code ples of the benefits LLVM provides for several challenging distribution; and (b) a compiler design that exploits this compiler problems. representation to provide a combination of capabilities that is not available in any previous compilation approach we know of. 1. INTRODUCTION The LLVM code representation describes a program using Modern applications are increasing in size, becoming more an abstract RISC-like instruction set but with key higher- dynamic, change behavior throughout their execution, and level information for effective analysis, including type infor- often have components written in multiple different lan- mation, explicit control flow graphs, and an explicit dataflow guages. While some applications have small hot spots, oth- representation (using an infinite, typed register set in Static ers spread their execution time evenly throughout the ap- Single Assignment form [14]). There are several novel fea- plication [11]. In order to provide maximum efficiency for tures in the LLVM code representation: (1) A low-level, all of these programs, we believe that program analysis and language-independent type system that can be used to imple- transformation must be performed throughout the lifetime ment data types and operations from high-level languages, of the program. Such “lifelong code optimization” tech- exposing their implementation behavior to all stages of op- niques encompass interprocedural optimizations performed timization. (2) Instructions for performing type conversions and low-level address arithmetic while preserving type in- formation. (3) Two low-level exception-handling instruc- tions for implementing language-specific exception seman- tics, while exposing exceptional control flow to the compiler explicitly. (4) A simple, low-level memory model distin- guishing heap, stack, global data, and code memory regions, accessed through typed pointers. The LLVM representation is source-language-independent, for two reasons. First, it uses a low-level instruction set and

}
}

@InProceedings{LAVOIE2015,
  Title                    = {LEVERAGING SOFTWARE and CLONES FOR and SOFTWARE COMPREHENSION},
  Author                   = {THIERRY M. LAVOIE},
  Year                     = {2015},

  File                     = {:article\\LEVERAGING SOFTWARE CLONES FOR SOFTWARE COMPREHENSION -TECHNIQUES and PRACTICE.pdf:PDF},
  Review                   = {UNIVERSITE´ DE MONTRE´AL LEVERAGING SOFTWARE CLONES FOR SOFTWARE COMPREHENSION: TECHNIQUES AND PRACTICE THIERRY M. LAVOIE DE´PARTEMENT DE GE´NIE INFORMATIQUE ET GE´NIE LOGICIEL E´COLE POLYTECHNIQUE DE MONTRE´AL THE`SE PRE´SENTE´E EN VUE DE L’OBTENTION DU DIPLOˆME DE PHILOSOPHIÆ DOCTOR (GE´NIE INFORMATIQUE) AVRIL 2015 ©c Thierry M. Lavoie, 2015.

}
}

@InProceedings{,
  Title                    = {Static Detection of Packet Injection Vulnerabilities ? A Case for Identifying Attacker-controlled Implicit},
  Author                   = {Information Leaks},
  Year                     = {2015},

  Abstract                 = {1. INTRODUCTION
},
  File                     = {:home/ccc/github/literature/article/Static Detection of Packet Injection Vulnerabilities?A Case for Identifying Attacker-controlled Implicit Information Leaks.pdf:PDF},
  Keywords                 = {with pointer analysis support. To handle the scalability challenge Network protocol security, Implicit information leakage, Static caused by such high sensitivity, we choose a data flow analysis},
  Review                   = {Static Detection of Packet Injection Vulnerabilities ? A Case for Identifying Attacker-controlled Implicit
Information Leaks
Qi Alfred Chen, Zhiyun Qian?, Yunhan Jack Jia, Yuru Shao, Z. Morley Mao University of Michigan, ?University of California, Riverside
alfchen@umich.edu, ?zhiyunq@cs.ucr.edu, {jackjia, yurushao, zmao}@umich.edu
ABSTRACT 1. INTRODUCTION
Off-path packet injection attacks are still serious threats to the In- The encryption coverage on today?s Internet is unfortunately still
ternet and network security. In recent years, a number of studies rather poor: only 30% [46]. Thus, off-path packet injection attacks
have discovered new variations of packet injection attacks, target- remain a serious threat to network security. Recently a number
ing critical protocols such as TCP. We argue that such recurring of such attacks and their variants have been reported including off-
problems need a systematic solution. In this paper, we design and path TCP packet injection [18,19,37,38] and DNS cache poisoning
implement PacketGuardian, a precise static taint analysis tool that attacks [34, 47]. These attacks jeopardize the integrity of network
comprehensively checks the packet handling logic of various net- communication, and lead to serious damage where personal data
work protocol implementations. The analysis operates in two steps. from unsuspecting users can be leaked when visiting a web site.
First, it identifies the critical paths and constraints that lead to ac- Despite application-layer encryption support (e.g., SSL and TLS),
cepting an incoming packet. If paths with weak constraints exist, network connections are still vulnerable. For instance, for HTTPS
a vulnerability may be revealed immediately. Otherwise, based on connections, the initial request sent by the browser may still be an
?secret? protocol states in the constraints, a subsequent analysis is unencrypted HTTP request, and the server subsequently redirects
performed to check whether such states can be leaked to an attacker. the client to the HTTPS site. As shown in a recent study [37], an
In the second step, observing that all previously reported leaks off-path attacker can inject a legitimate response to the very first
are through implicit flows, our tool supports implicit flow tainting, HTTP request. Furthermore, such packet injection attacks can re-
which is a commonly excluded feature due to high volumes of false sult in DoS, e.g., by injecting a reset (RST) packet with an inferred
alarms caused by it. To address this challenge, we propose the con- TCP sequence number.
cept of attacker-controlled implicit information leaks, and prioritize To combat such threats, the network stacks typically implement
our tool to detect them, which effectively reduces false alarms with- stringent checks on various fields to verify if an incoming packet is
out compromising tool effectiveness. We use PacketGuardian on 6 valid. In fact, a number of RFCs like RFC 5961 [39] are dedicated
popular protocol implementations of TCP, SCTP, DCCP, and RTP, to this purpose. However, two problems remain. First, the design
and uncover new vulnerabilities in Linux kernel TCP as well as 2 of an RFC may not be formally verified to be secure. Second, even
out of 3 RTP implementations. We validate these vulnerabilities if the design is secure, the actual implementation may not always
and confirm that they are indeed highly exploitable. conform to the design. In fact, the implementation is generally much more complex and difficult to get right. For instance, it has
Categories and Subject Descriptors been shown that TCP implementations on Linux and FreeBSD are significantly weaker than what the RFC recommends regarding the
D.4.6 [Operating Systems]: Security and Protection?Informa- mitigation against off-path attacks [38]. This calls for a systematic tion flow controls; C.2.5 [Computer-Communication Networks]: approach to verify protocol implementations. Local and Wide-Area Networks?Internet (e.g., TCP/IP) In this work, we fulfill this very need by developing an effective
and scalable static program analysis tool, PacketGuardian, which
General Terms can systematically evaluate the robustness (i.e., the level of secu- rity strength) of a network protocol implementation against off-path
Security, Program Analysis packet injection attacks. To ensure effectiveness and accuracy, our tool uses a precise context-, flow-, and field-sensitive taint analysis
Keywords with pointer analysis support. To handle the scalability challenge Network protocol security, Implicit information leakage, Static caused by such high sensitivity, we choose a data flow analysis
analysis, Side channel detection of summary-based approach, which is known to be more scalable compared to other frameworks [43], and is demonstrated to scale
Permission to make digital or hard copies of all or part of this work for personal or to very large programs like the Linux kernel [52].
classroom use is granted without fee provided that copies are not made or distributed At a high level, the tool operates by performing analysis in two for profit or commercial advantage and that copies bear this notice and the full cita- steps: (1) Find all paths leading to the program execution point tion on the first page. Copyrights for components of this work owned by others than of accepting an incoming packet. This helps identify the critical ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or re-
checks that a protocol implementation relies on to prevent packet publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org. injection, and may directly reveal a packet injection vulnerability
CCS?15, October 12?16, 2015, Denver, CO, USA. if any check is weak. (2) Motivated by the observation that strong
�c 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00. checks typically rely on certain hard-to-guess or ?secret? commu- DOI: http://dx.doi.org/10.1145/2810103.2813643.

}
}

@InProceedings{LearningAlgorithms2011,
  Title                    = {Automated Analysis of Source Code Patches using},
  Author                   = {Machine Learning and Algorithms},
  Year                     = {2011},

  Abstract                 = {An updated version of a tool for automated analysis of source code
},
  File                     = {:home/ccc/github/literature/article/Automated Analysis of Source Code Patches using Machine Learning Algorithms.pdf:PDF},
  Keywords                 = {automated, source code review, source code analysis, patch},
  Review                   = {Automated Analysis of Source Code Patches using Machine Learning Algorithms Antonio Castro Lechtaler1,2, Julio César Liporace1, Marcelo Cipriano1, Edith García1, Ariel Maiorano1, Eduardo Malvacio1, Néstor Tapia1 1 Grupo de Investigación en Criptografía y Seguridad Informática (GICSI), Instituto Universitario del Ejército (IUE), 2 Universidad Nacional de Chilecito (UNdeC), Argentina {antonio.castrolechtaler, edithxgarcia, jcliporace,maiorano, cipriano1.618, edumalvacio, tapianestor87}@gmail.com Abstract. An updated version of a tool for automated analysis of source code patches and branch differences is presented. The upgrade involves the use of machine learning techniques on source code, comments, and messages. It aims to help analysts, code reviewers, or auditors perform repetitive tasks continuously. The environment designed encourages collaborative work. It systematizes certain tasks pertaining to reviewing or auditing processes. Currently, the scope of the automated test is limited. Current work aims to increase the volume of source code analyzed per time unit, letting users focus on alerts automatically generated. The tool is distributed as open source software. This work also aims to provide arguments in support of the use of this type of tool. A brief overview of security problems in open source software is presented. It is argued that these problems were or may have been discovered reviewing patches and branch differences, released before the vulnerability was disclosed. Keywords: automated, source code review, source code analysis, patch analysis, machine learning, text mining, software quality. 1 Introduction This work presents a software tool for automated source code patches and branch differences analysis. It aims to systematize updates and source code reviews to alert on potential bugs, implying some degree of system security compromise, or vulnerabilities. The tool is distributed as an open source project and available at http://github.com/gicsi/aap. Currently, we have not found other open source tools available for the proposed functionality presented here. Other projects have been published but they largely deal with software engineering. For instance: trackable recoveries between source code and corrected bugs through patch analysis [5], or the use of patches for bug reporting. In the first case, a data processing tool was presented for Bugzilla systems, identifying the information with CVS tags. In the case of open source analysis with cryptographic functionalities, Android case studies show that, for instance, only 17% of 269 vulnerabilities - reported in the period between January 2011 and May 2014 - were attributed to glitches in the 

}
}

@Article{LeiLiLiuEtAl2010,
  Title                    = {Robustness testing for software components},
  Author                   = {Lei, Bin and Li, Xuandong and Liu, Zhiming and Morisset, Charles and Stolz, Volker},
  Journal                  = {Science of Computer Programming},
  Year                     = {2010},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {879鈥�897},
  Volume                   = {75},

  Doi                      = {10.1016/j.scico.2010.02.005},
  File                     = {:article\\Robustness testing for software components.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0167-6423},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.scico.2010.02.005}
}

@Article{LiZhangChen2014,
  Title                    = {Vulnerability mining of Cisco router based on fuzzing},
  Author                   = {Fengjiao Li and Luyong Zhang and Dianjun Chen},
  Journal                  = {The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)},
  Year                     = {2014},
  Pages                    = {649--53},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Router security analysis plays a vital role in maintaining network security. However, IOS, which runs in Cisco routers, has been proved carrying serious security risks. And in order to improve security, we need to conduct vulnerability mining on IOS. Currently, Fuzzing, as a simple and effective automated test technology, is widely used in vulnerability discovery. In this paper, we introduce a novel testing framework for Cisco routers. Based on this framework, we first generate test cases with Semi-valid Fuzzing Test Cases Generator (SFTCG), which considerably improves the test effectiveness and code coverage. After that, we develop a new Fuzzer based on SFTCG and then emulate Cisco router in Dynamips, which makes it easy to interact with GDB or IDA Pro for debugging. In order to supervise the Target, we employ a Monitor Module to check the status of the router regularly. Finally, through the experiment on ICMP protocol in IOS, we find the released vulnerabilities of Ping of Death and Denial of Service, which demonstrates the effectiveness of our proposed Fuzzer.},
  Bn                       = {978-1-4799-5458-2},
  Cl                       = {Shanghai, China},
  Ct                       = {2014 2nd International Conference on Systems and Informatics (ICSAI)},
  Cy                       = {15-17 Nov. 2014},
  Doi                      = {10.1109/ICSAI.2014.7009366},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:14852224},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{Li2010,
  Title                    = {KLOVER: A Symbolic Execution and Automatic Test Generation and Tool for C and Programs},
  Author                   = {Guodong Li},
  Year                     = {2010},

  Abstract                 = {We present the first symbolic execution and automatic test generation tool for C++ programs. First we describe our effort in extend- ing an existing symbolic execution tool for C programs to handle C++ programs. We then show how we made this tool generic, efficient and usable to handle real-life industrial applications. Novel features include extended symbolic virtual machine, library optimization for C and C++, object-level execution and reasoning, interfacing with specific type of ef- ficient solvers, and semi-automatic unit and component testing. This tool is being used to assist the validation and testing of industrial software as well as publicly available programs written using the C++ language.
},
  File                     = {:article\\KLOVER-A symbolic execution and automatic test generation tool for C++ programs.CAV11.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {KLOVER: A Symbolic Execution and Automatic Test Generation Tool for C++ Programs Guodong Li, Indradeep Ghosh, and Sreeranga P. Rajan Fujitsu Labs of America, CA {gli, ighosh, sree.rajan}@us.fujitsu.com Abstract. We present the first symbolic execution and automatic test generation tool for C++ programs. First we describe our effort in extend- ing an existing symbolic execution tool for C programs to handle C++ programs. We then show how we made this tool generic, efficient and usable to handle real-life industrial applications. Novel features include extended symbolic virtual machine, library optimization for C and C++, object-level execution and reasoning, interfacing with specific type of ef- ficient solvers, and semi-automatic unit and component testing. This tool is being used to assist the validation and testing of industrial software as well as publicly available programs written using the C++ language. 1 Introduction With the ubiquitous presence of software programs permeating almost all as- pects of daily life, providing robust and reliable software has become a necessity. Traditionally, software quality has been assured through manual testing which is tedious, difficult, and often gives poor coverage of the source code especially when availing of random testing approaches. This has led to much recent work in the formal validation arena [4, 1]. One such formal technique is symbolic execution which can be used to automatically generate test inputs with high structural coverage for the program under test. The widely used symbolic execution en- gines currently are able to handle C or Java programs only. So far there has been no such formal tool designed specifically for the automatic validation and test generation for C++ programs. Currently C++ is the language of choice for most low-level scientific and performance critical applications in academia and industry. This paper describes our efforts in creating the first industrially usable symbolic execution engine for C++ programs. Symbolic execution [4, 1] performs the execution of a program on symbolic (open) inputs. It characterizes each program path it explores with a path con- dition which denotes a series of branching decisions. The solutions to path con- ditions are the test inputs that will assure that the program under test runs along a particular concrete path during concrete execution. Typically a decision procedure such as a SMT (Satisfiability Modulo Theory) solver is used to find the solutions and prune out false paths. Exhaustive testing is achieved by explor- ing all true paths. Some sanity properties can also be checked such as memory out-of-bound access, divide-by-zero, and certain types of user-defined assertions.

}
}

@PhdThesis{Li2014,
  Title                    = {IMPROVING QUALITY OF SOFTWARE WITH FOREIGN and FUNCTION INTERFACES USING STATIC ANALYSIS},
  Author                   = {Siliang Li},
  Year                     = {2014},

  File                     = {:article\\IMPROVING QUALITY OF SOFTWARE  WITH FOREIGN FUNCTION INTERFACES  USING STATIC ANALYSIS.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {IMPROVING QUALITY OF SOFTWARE WITH FOREIGN FUNCTION INTERFACES USING STATIC ANALYSIS by Siliang Li Presented to the Graduate and Research Committee of Lehigh University in Candidacy for the Degree of Doctor of Philosophy in Computer Engineering Lehigh University May 2014

}
}

@Article{LiangDiYan-fengEtAl2012,
  Title                    = {Research on Integer Type Vulnerability Mining and Usage in Binary Code},
  Author                   = {Liu Liang and Peng Di and Yang Yan-feng and Wu Run-pu},
  Journal                  = {Journal of Sichuan University},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {123--6},
  Volume                   = {44},

  __markedentry            = {[ccc:6]},
  Abstract                 = {By researching the previous binary mining method, a new mining model based on fuzzing, reversing and symbolic execution technology was presented. This method used reversing analysis to locate integer vulnerabilities scope, obtain related data types by IDA disassembler and SDK development kit, detect safety-sensitive functions, build function control diagram, determine the related codes of potential integer vulnerabilities, and cover each code part related. The input and output relations were obtained by symbolic execution in assembly codes and adjusting the input for fuzzing. The proposed model greatly enhances the binary integer vulnerabilities mining accuracy and efficiency.},
  Groups                   = {Code Mining},
  Sn                       = {1001-8395},
  Tc                       = {0},
  Ut                       = {INSPEC:13301299},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{LingYuanNiEtAl2015,
  Title                    = {Clustering analysis of vulnerability information based on text mining},
  Author                   = {Gao Ling and Shen Yuan and Gao Ni and Lei Panting and Sun Qian},
  Journal                  = {Journal of Southeast University (Natural Science Edition)},
  Year                     = {2015},
  Number                   = {5},
  Pages                    = {845--50},
  Volume                   = {45},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In order to dig out the internal relationships of vulnerability and efficiently manage vulnerability information, text processing and clustering algorithm are applied to vulnerability mining. In the aspect of whole vulnerability database, the PSO-K-means algorithm based on text mining and particle swarm optimization (PSO) algorithm is proposed. First, the keyword space is obtained by text processing to code the vulnerability information description. Secondly, the PSO algorithm is used to obtain the global cluster centers for reducing the impact of local optimum and cluster centers' improper selection on clustering. Finally, the K-means algorithm is adopted to achieve clustering of vulnerability information, which can administer vulnerability information in classification and supply information for predicting unknown vulnerability characteristics. The experimental results show that the accuracy rate of the PSO-K-means algorithm is 90. 16% . Compared to the K-means algorithm, the average accuracy rate of the PSO-K-means algorithm is increased by about 5% and the average number of iteration is reduced about 45 times. Moreover, the PSO-K-means algorithm can predict three primary classes for unknown vulnerability and it is an effective vulnerability analysis method.},
  Doi                      = {10.3969/j.issn.1001-0505.2015.85.006},
  Groups                   = {Code Mining},
  Sn                       = {1001-0505},
  Tc                       = {0},
  Ut                       = {INSPEC:15909293},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{LiuShiCaiEtAl2012,
  Title                    = {Software Vulnerability Discovery Techniques: A Survey},
  Author                   = {Liu, Bingchang and Shi, Liang and Cai, Zhuhua and Li, Min},
  Journal                  = {2012 Fourth International Conference on Multimedia Information Networking and Security},
  Year                     = {2012},

  Month                    = {Nov},

  Doi                      = {10.1109/mines.2012.202},
  File                     = {:home/ccc/github/literature/article/Software Vulnerability Discovery Techniques\: A Survey .pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-0-7695-4852-4},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/MINES.2012.202}
}

@Article{LiuZhangWuEtAl2013,
  Title                    = {An effective taint-based software vulnerability miner},
  Author                   = {Zhi Liu and Xiaosong Zhang and Yue Wu and Ting Chen},
  Journal                  = {COMPEL: The International Journal for Computation and Mathematics in Electrical and Electronic Engineering},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {467--84},
  Volume                   = {32},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Purpose - The purpose of this paper is to propose an approach to detect Indirect Memory-Corruption Exploit (IMCE) at runtime on binary code, which is often caused by integer conversion error. Real-world attacks were evaluated for experimentation. Design/methodology/approach - Current dynamic analysis detects attacks by enforcing low level policy which can only detect control-flow hijacking attack. The proposed approach detects IMCE with high level policy enforcement using dynamic taint analysis. Unlike low-level policy enforced on instruction level, the authors' policy is imposed on memory operation routine. The authors implemented a fine-grained taint analysis system with accurate taint propagation for detection. Findings - Conversion errors are common and most of them are legitimate. Taint analysis with high-level policy can accurately block IMCE but have false positives. Proper design of data structures to maintain taint tag can greatly improve overhead. Originality/value - This paper proposes an approach to block IMCE with high-level policy enforcement using taint analysis. It has very low false negatives, though still causes certain false positives. The authors made several implementation contributions to strengthen accuracy and performance.},
  Doi                      = {10.1108/03321641311296873},
  Groups                   = {Code Mining},
  Sn                       = {0332-1649},
  Tc                       = {0},
  Ut                       = {INSPEC:13359341},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{LuLi-faHong-linEtAl2012,
  Title                    = {Study on the Analytical Technique of Exception in Fuzzing},
  Author                   = {Yu Lu and Wu Li-fa and Zhuang Hong-lin and Shen Yi},
  Journal                  = {Journal of Chinese Computer Systems},
  Year                     = {2012},
  Number                   = {7},
  Pages                    = {1472--6},
  Volume                   = {33},

  Abstract                 = {Fuzzing is a new automatic software testing technology based on fault-injection technique and it is widely used on the software testing and vulnerability mining field. However, when error occurs, it depends on reverse engineering to precisely locate the codes that trigger the error, which calls for a great amount of analysis workload and consequently leads to low analysis efficiency. Based on the current binary comparison technology, this paper raises the two-phase comparison algorithm for execution records by the run-time relationship of the target software's binary code. This algorithm compares the execution records to get all the same and different execution sequences on both function level and instruction level. Using the instruction sequence related information; analyzers can reduce the analysis workload and improve analysis efficiency.},
  Groups                   = {Code Mining},
  Sn                       = {1000-1220},
  Tc                       = {0},
  Ut                       = {INSPEC:13525836},
  Z1                       = {Fuzzingæµè¯ä¸­å¼å¸¸åæææ¯çç ç©¶},
  Z2                       = {äºçEOLEOLå´ç¤¼åEOLEOLåºæ´ªæEOLEOLæ²æ¯},
  Z3                       = {å°åå¾®åè®¡ç®æºç³»ç»},
  Z4                       = {Fuzzingæµè¯æ¯ä¸ç§åºäºç¼ºé·æ³¨å¥çèªå¨è½¯ä»¶æµè¯ææ¯,è¿å å¹´æ¥å¹¿æ³åºç¨äºè½¯ä»¶æµè¯ãå®å¨æ¼æ´ææç­é¢å.å½Fuzzingæµè¯åºç°å¼å¸¸æ¶,éè¦äººå·¥EOLEOLéåç²¾ç¡®å®ä½è§¦åæ¼æ´çä»£ç ä½ç½®,åæçå·¥ä½éå¤§,èä¸åææçè¾ä¸ºä½ä¸.è®ºæä»¥ç°æäºè¿å¶æ¯è¾ææ¯ä¸ºåºç¡,æ ¹æ®äºè¿å¶ç®æ è½¯ä»¶ä¸­ä»£ç è¿è¡çä¾èµå³ç³»,æEOLEOLåºä¸ç§éå¯¹æ§è¡è®°å½çä¸¤é¶æ®µæ¯è¾ç®æ³,å¨å½æ°çº§å«ååºæ¬åçº§å«ä¸å¯¹æä»¶æ­£å¸¸æ§è¡ååºç°å¼å¸¸çè·è¸ªè®°å½åå®¹è¿è¡æ¯è¾,å¾å°ææç¸ååä¸åçæä»¤æ§è¡åºå.ä¾EOLEOLææä»¤åºåä¿¡æ¯,å¯ä»¥ææè¾å©äººå·¥åæ,å¤§å¤§éä½åæçå·¥ä½é,æé«åææç.},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{LuLifaFanEtAl2011,
  Title                    = {Automatic Fault Localization for Fuzzing},
  Author                   = {Yu Lu and Wu Lifa and Pan Fan and Zhuang Honglin and Hong Zheng},
  Journal                  = {Proceedings of the 2011 First International Conference on Instrumentation, Measurement, Computer, Communication and Control (IMCCC 2011)},
  Year                     = {2011},
  Pages                    = {388--91},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Fuzzing has proved successful in finding security vulnerabilities in large binary programs. Traditionally, reversing engineering technologies are used to locate codes that may lead to exceptions in Fuzzing, and this may demand a great amount of human efforts and consequently gives rise to low efficiency. In this paper, an automatic fault localization method for Fuzzing is proposed together with an automatic vulnerability analysis system named Fuzz Loc. Fuzz Loc can filter key instructions that may directly cause exceptions. Starting from these key instructions, Fuzz Loc implements automatic fault localization by back tracing. With Fuzz Loc, a great deal of human efforts can be saved. Experiments show that Fuzz Loc can locate fault codes accurately with little human intervention and consequently improves efficiency of fault analysis and vulnerability mining.},
  Bn                       = {978-0-7695-4519-6},
  Cl                       = {Beijing, China},
  Ct                       = {2011 First International Conference on Instrumentation, Measurement,EOLEOLComputer, Communication and Control (IMCCC 2011)},
  Cy                       = {21-23 Oct. 2011},
  Doi                      = {10.1109/IMCCC.2011.104},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:12556534},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{Malhotra2015,
  Title                    = {A systematic review of machine learning techniques for software fault prediction},
  Author                   = {Malhotra, Ruchika},
  Journal                  = {Applied Soft Computing},
  Year                     = {2015},

  Month                    = {Feb},
  Pages                    = {504–518},
  Volume                   = {27},

  Doi                      = {10.1016/j.asoc.2014.11.023},
  File                     = {:home/ccc/github/literature/article/A systematic review of machine learning techniques for software fault prediction.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1568-4946},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.asoc.2014.11.023}
}

@Article{MassacciNguyen2014,
  Title                    = {An Empirical Methodology to Evaluate Vulnerability Discovery Models},
  Author                   = {Massacci, Fabio and Nguyen, Viet Hung},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2014},

  Month                    = {Dec},
  Note                     = {略读},
  Number                   = {12},
  Pages                    = {1147–1162},
  Volume                   = {40},

  __markedentry            = {[ccc:1]},
  Doi                      = {10.1109/tse.2014.2354037},
  File                     = {:home/ccc/github/literature/article/An Empirical Methodology to Validate Vulnerability Discovery Models.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2014.2354037}
}

@Article{MedeirosNevesCorreia2016,
  Title                    = {Detecting and Removing Web Application Vulnerabilities with Static Analysis and Data Mining},
  Author                   = {Medeiros, Iberia and Neves, Nuno and Correia, Miguel},
  Journal                  = {Ieee Transactions On Reliability},
  Year                     = {2016},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {54--69},
  Volume                   = {65},

  __markedentry            = {[ccc:6]},
  Abstract                 = {6Although a large research effort on web application security has been going on for more than a decade, the security of web applications continues to be a challenging problem. An important part of that problem derives from vulnerable source code, often written in unsafe languages like PHP. Source code static analysis tools are a solution to find vulnerabilities, but they tend to generate false positives, and require considerable effort for programmers to manually fix the code. We explore the use of a combination of methods to discover vulnerabilities in source code with fewer false positives. We combine taint analysis, which finds candidate vulnerabilities, with data mining, to predict the existence of false positives. This approach brings together two approaches that are apparently orthogonal: humans coding the knowledge about vulnerabilities (for taint analysis), joined with the seemingly orthogonal approach of automatically obtaining that knowledge (with machine learning, for data mining). Given this enhanced form of detection, we propose doing automatic code correction by inserting fixes in the source code. Our approach was implemented in the WAP tool, and an experimental evaluation was performed with a large set of PHP applications. Our tool found 388 vulnerabilities in 1.4 million lines of code. Its accuracy and precision were approximately 5% better than PhpMinerII's and 45% better than Pixy's.},
  Doi                      = {10.1109/TR.2015.2457411},
  Ei                       = {1558-1721},
  Groups                   = {Code Mining},
  Sn                       = {0018-9529},
  Tc                       = {0},
  Ut                       = {WOS:000372371100006},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{MedeirosNevesCorreia2016a,
  Title                    = {Detecting and Removing Web Application Vulnerabilities with Static Analysis and Data Mining},
  Author                   = {Medeiros, Iberia and Neves, Nuno and Correia, Miguel},
  Journal                  = {IEEE Trans. Rel.},
  Year                     = {2016},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {54–69},
  Volume                   = {65},

  __markedentry            = {[ccc:1]},
  Doi                      = {10.1109/tr.2015.2457411},
  File                     = {:home/ccc/github/literature/article/Decting and removing web application vulnerabilities with static analysis and data mining.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1558-1721},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TR.2015.2457411}
}

@Article{MelinaKulenovic2014,
  Title                    = {A survey of static code analysis methods for and security vulnerabilities detection},
  Author                   = {Melina Kulenovic, Dzenana Donko},
  Journal                  = {MIPRO 2014, 26-30 May 2014, Opatija, Croatia},
  Year                     = {2014},

  Abstract                 = {Software security is becoming highly important documented more than 680 software weaknesses that can lead for universal acceptance of applications for many kinds of to impacts like modified data, read data, denial of service transactions. Automated code analyzers can be utilized to detect (DOS) attacks, resource consumption, execution of security vulnerabilities during the development phase. This unauthorized code, gain of privileges, protection bypass or the paper is aimed to provide a survey on Static code analysis and hiding of activities. how it can be used to detect security vulnerabilities. The most recent findings and publications are summarized and presented Many security focused methodologies like the Open in this paper. This paper provides an overview of the gains, flows Software Assurance Maturity Model (Open SAMM) [3],The and algorithms of static code analyzers. It can be considered a Building Security In Maturity Model (BSIMM) [4], or stepping stone for further research in this domain. Microsoft’s Security Development Lifecycle (SDL) [5] are},
  File                     = {:article\\A survey of static code analysis methods for security vulnerabilities detection.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {static code analysis; security; vulnerability; research provided by Cenzic showed that 99% of all reviewed,literature,article},
  Read                     = {未读},
  Review                   = {MIPRO 2014, 26-30 May 2014, Opatija, Croatia A survey of static code analysis methods for security vulnerabilities detection Melina Kulenovic, Dzenana Donko Faculty of Electrical Engineering, University of Sarajevo Sarajevo, Bosnia and Herzegovina E-mails: mk14746@etf.una.ba , ddonko@etf.unsa.ba Abstract—Software security is becoming highly important documented more than 680 software weaknesses that can lead for universal acceptance of applications for many kinds of to impacts like modified data, read data, denial of service transactions. Automated code analyzers can be utilized to detect (DOS) attacks, resource consumption, execution of security vulnerabilities during the development phase. This unauthorized code, gain of privileges, protection bypass or the paper is aimed to provide a survey on Static code analysis and hiding of activities. how it can be used to detect security vulnerabilities. The most recent findings and publications are summarized and presented Many security focused methodologies like the Open in this paper. This paper provides an overview of the gains, flows Software Assurance Maturity Model (Open SAMM) [3],The and algorithms of static code analyzers. It can be considered a Building Security In Maturity Model (BSIMM) [4], or stepping stone for further research in this domain. Microsoft’s Security Development Lifecycle (SDL) [5] are designed to enforce developers to build security in. Still, the Keywords—static code analysis; security; vulnerability; research provided by Cenzic showed that 99% of all reviewed survey; applications in 2012 had at least one serious security I. I vulnerability [6].Conveniently, tools that are able to scan NTRODUCTION millions of lines of source code and detect defects and Millions of users accomplish their daily tasks using web vulnerabilities in relatively short time are improving rapidly. A and desktop applications on different electronic devices. The variety of such utilities is now available as commercial or free prevalence of software enforces the software industry to think tools that can be used either as a standalone tool or as a plug-in of how to build quality in. Most aspects of software quality are for a development framework. Automated code analysis tools related to the skills and knowledge of the development team can be utilized during the development phase and as part of the [1]. Unfortunately, developers make mistakes that lead to build process. The detection of defects in such an early phase vulnerable and defect software. Exploited security can reduce costs significantly. vulnerabilities lead to unreliable software that can become harmful for the user and software provider. The main characteristics of static code analysis tools, their gains and flows are outlined in section II. Section II also The CWE (Common Weakness Enumerations) initiative provides information on how to measure the contribution of provides a unified, measurable set of software weaknesses static analysis to vulnerabilities detection. Section III presents which can lead to serious security vulnerabilities [2].The top 25 the techniques and algorithms static code analysis is based on. vulnerabilities list contains the most widespread and critical This section contains an overview of the most recent work and errors that can lead to security vulnerabilities that can be easy a summary of related researches. The Conclusion is presented exploited by an attacker. Table I presents an overview and in section IV. description of the most common and harmful attacks. CWE has Table I. Top 5 security attacks [2] Name Description Effects SQL injection SQL – Injection is one of the major threats for data rich software. Every non Attackers use the commands to bypass validated textbox that utilizes user input and uses it in a SQL context can be authentication mechanisms or insert tainted data into used by an attacker to inject SQL commands. the database. OS command Applications are considered vulnerable to the OS command injection attack if Attackers attempt to execute system level commands injection they utilize non validated user input in a system level command what can lead in order to achieve the desired goal. to the invocation of scripts injected by the attacker. Buffer overflow Buffer overflows is an anomaly where a program, while writing data to a Buffer overflow may result in erratic program buffer, overruns the buffer's boundary and overwrites adjacent memory. It can behavior, including memory access errors, incorrect be triggered by non-validated inputs that are designed to execute code. results, a crash, or a breach of system security. Cross-site scripting Cross-site scripting is typically found in Web applications and it enables Attackers use it to bypass access controls, steal user attackers to inject client-side scripts into Web pages viewed by other users. sessions or to redirect users to a different website. Missing Missing authentication is a security vulnerability that occurs in software that Exposing critical functionality. Provides an attacker authentication does not perform any authentication for functionalities that require a provable with the privilege level of that functionality. user identity or consume a significant amount of resources. 1381}
}

@Article{MellegardFerwerdaLindEtAl2016,
  Title                    = {Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study},
  Author                   = {Mellegard, Niklas and Ferwerda, Adry and Lind, Kenneth and Heldal, Rogardt and Chaudron, Michel R. V.},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2016},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {245–260},
  Volume                   = {42},

  Doi                      = {10.1109/tse.2015.2479221},
  File                     = {:home/ccc/github/literature/article/Impact of Introducing Domain-Specific Modelling.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2015.2479221}
}

@Article{MenziesMiltonTurhanEtAl2010,
  Title                    = {Defect prediction from static code features: current results, limitations, new approaches},
  Author                   = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ay艧e},
  Journal                  = {Automated Software Engineering},
  Year                     = {2010},

  Month                    = {May},
  Number                   = {4},
  Pages                    = {375鈥�407},
  Volume                   = {17},

  Doi                      = {10.1007/s10515-010-0069-5},
  File                     = {:article\\Defect prediction from static code features current results, limitations, new approaches.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1573-7535},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10515-010-0069-5}
}

@InProceedings{MetricsTextMining4107,
  Title                    = {Predicting Vulnerable Components},
  Author                   = {Software Metrics and vs Text and Mining},
  Year                     = {4107},

  Abstract                 = {Building secure software is difficult, time- tend to reduce the ratio of publicly available vulnerabilities to consuming, and expensive. Prediction models that identify defects. vulnerability prone software components can be used to focus security efforts, thus helping to reduce the time and effort Developers and vulnerability researchers often limit pub- required to secure software. Several kinds of vulnerability lic disclosure of information about vulnerabilities to reduce prediction models have been proposed over the course of the opportunities for exploitation. There is much debate in the past decade. However, these models were evaluated with differing security community about how much information to disclose methodologies and datasets, making it difficult to determine about vulnerabilities [3]. Descriptions in vulnerability reports the relative strengths and weaknesses of different modeling frequently lack the details needed to correctly identify which techniques.
},
  File                     = {:home/ccc/github/literature/article/-Predicting Vulnerable Components\: Software Metrics vs Text Mining.pdf:PDF},
  Review                   = {Predicting Vulnerable Components: Software Metrics vs Text Mining James Walden Jeff Stuckman Riccardo Scandariato Department of Computer Science Department of Computer Science iMinds-DistriNet Northern Kentucky University University of Maryland KU Leuven Highland Heights, KY 41076 College Park, MD 20742 3001 Leuven, Belgium Email: waldenj@nku.edu Email: stuckman@umd.edu Email: riccardo.scandariato@cs.kuleuven.be Abstract—Building secure software is difficult, time- tend to reduce the ratio of publicly available vulnerabilities to consuming, and expensive. Prediction models that identify defects. vulnerability prone software components can be used to focus security efforts, thus helping to reduce the time and effort Developers and vulnerability researchers often limit pub- required to secure software. Several kinds of vulnerability lic disclosure of information about vulnerabilities to reduce prediction models have been proposed over the course of the opportunities for exploitation. There is much debate in the past decade. However, these models were evaluated with differing security community about how much information to disclose methodologies and datasets, making it difficult to determine about vulnerabilities [3]. Descriptions in vulnerability reports the relative strengths and weaknesses of different modeling frequently lack the details needed to correctly identify which techniques. software components and which versions were impacted by In this paper, we provide a high-quality, public dataset, the vulnerabilities [4]. Vulnerability fixing commits are often containing 223 vulnerabilities found in three web applications, not identified as such in code repositories. As a result of these to help address this issue. We used this dataset to compare attempts to limit exploitation, the quality of vulnerability data vulnerability prediction models based on text mining with models is typically lower than the quality of defect data found in using software metrics as predictors. We found that text mining software repositories. models had higher recall than software metrics based models for all three applications. Due in part to these differences, the field of vulnerability prediction is not as mature as the study of software defect pre- I. I diction. While hundreds of defect prediction studies have beenNTRODUCTION published, including multiple systematic literature reviews [5], Building secure software is difficult, time-consuming, and using a wide range of software metrics and predictive modeling expensive. Prediction models that identify software compo- techniques, only a few dozen vulnerability prediction studies nents that are prone to vulnerabilities can be used to focus have been published. Early defect prediction studies focused limited information assurance resources on a subset of the on individual models and software artifacts, while later studies source code, thereby reducing the time and effort needed to began comparing models and then developed standard datasets, mitigate vulnerabilities. such as the PROMISE repository [6], and methods of compar- ing models. There is an extensive literature on defect prediction in software engineering. While vulnerabilities are a specific type We expect the field of vulnerability prediction to evolve of software defect, the problem of finding vulnerabilities in along a similar trajectory. Many papers have been published software differs in significant ways from the more general on models using a single type of predictor data, while a problem of finding defects. The most obvious difference is few papers compare models built using different types of quantitative: there are typically many more defects than vulner- predictors. However, there are no standard data sets to use abilities in software, as one can see by comparing the number for comparison of vulnerability prediction models. With this of defects reported in a project’s defect tracker to the number paper, we hope to create some debate and momentum in order of vulnerabilities listed on a project’s security web page. to spur the transition towards comparing models with standard datasets. This paper makes two contributions: However, the qualitative differences between vulnerabilities and defects may be more important than the quantitative 1) It presents a comparison of using software metrics as differences. Different skills are needed to find vulnerabilities predictors with using text mining techniques to build than to find defects. Finding vulnerabilities requires an under- vulnerability prediction models in the context of web standing of both the software and the attacker’s mindset [1]. applications written in PHP. Furthermore, defects cause problems for users, who then report 2) It includes a novel, hand-curated dataset of vulnera- them to developers, while vulnerabilities provide opportunities bilities in PHP web applications that will be offered to hackers, who therefore often keep their knowledge of to the community. vulnerabilities secret. Opportunities presented by vulnerabil- ities include not only criminal activities but also the sale of This paper is organized into sections as follows. Section II vulnerabilities at increasingly high prices [2]. These effects discusses the collection and validation of the data set, while

}
}

@Article{MinVaradharajanTupakulaEtAl2013,
  Title                    = {Antivirus security: naked during updates},
  Author                   = {Byungho Min and Vijay Varadharajan and Udaya Tupakula and Michael Hitchens},
  Journal                  = {Softw. Pract. Exper.},
  Year                     = {2013},

  Month                    = {apr},
  Number                   = {10},
  Pages                    = {1201--1222},
  Volume                   = {44},

  Doi                      = {10.1002/spe.2197},
  File                     = {:article\\Antivirus security naked during updates.pdf:PDF},
  Groups                   = {binarary vulnerability},
  Publisher                = {Wiley-Blackwell},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {文章研究了杀毒软件在更新升级时的安全问题，另外还讨论了针对提权攻击可能的缓减技术。},
  Url                      = {http://dx.doi.org/10.1002/spe.2197}
}

@InProceedings{MinKashyapLeeEtAl2015,
  Title                    = {Cross-checking Semantic Correctness: The Case of Finding File System Bugs},
  Author                   = {Changwoo Min and Sanidhya Kashyap and Byoungyoung Lee and Chengyu Song and Taesoo Kim and Georgia Institute and of Technology},
  Booktitle                = {SOSP’15},
  Year                     = {2015},

  Abstract                 = {1. Introduction Today, systems software is too complex to be bug-free. To Systems software is buggy. On one hand, it is often imple- find bugs in systems software, developers often rely on code mented in unsafe, low-level languages (e.g., C) for achiev- checkers, like Linux’s Sparse. However, the capability of ing better performance or directly accessing the hardware, existing tools used in commodity, large-scale systems is thereby facilitating the introduction of tedious bugs. On the limited to finding only shallow bugs that tend to be introduced other hand, it is too complex. For example, Linux consists of by simple programmer mistakes, and so do not require a almost 19 million lines of pure code and accepts around 7.7 deep understanding of code to find them. Unfortunately, the patches per hour [18]. majority of bugs as well as those that are difficult to find are To help this situation, especially for memory corruption semantic ones, which violate high-level rules or invariants bugs, researchers often use memory-safe languages in the (e.g., missing a permission check). Thus, it is difficult for first place. For example, Singularity [34] and Unikernel [43] code checkers lacking the understanding of a programmer’s are implemented in C# and OCaml, respectively. However, true intention to reason about semantic correctness. in practice, developers largely rely on code checkers. For
},
  Doi                      = {.org/10.1145/2815400.2815422},
  File                     = {:article\\Cross-checking semantic correctness the case of finding file system bugs.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Cross-checking Semantic Correctness: The Case of Finding File System Bugs Changwoo Min Sanidhya Kashyap Byoungyoung Lee Chengyu Song Taesoo Kim Georgia Institute of Technology Abstract 1. Introduction Today, systems software is too complex to be bug-free. To Systems software is buggy. On one hand, it is often imple- find bugs in systems software, developers often rely on code mented in unsafe, low-level languages (e.g., C) for achiev- checkers, like Linux’s Sparse. However, the capability of ing better performance or directly accessing the hardware, existing tools used in commodity, large-scale systems is thereby facilitating the introduction of tedious bugs. On the limited to finding only shallow bugs that tend to be introduced other hand, it is too complex. For example, Linux consists of by simple programmer mistakes, and so do not require a almost 19 million lines of pure code and accepts around 7.7 deep understanding of code to find them. Unfortunately, the patches per hour [18]. majority of bugs as well as those that are difficult to find are To help this situation, especially for memory corruption semantic ones, which violate high-level rules or invariants bugs, researchers often use memory-safe languages in the (e.g., missing a permission check). Thus, it is difficult for first place. For example, Singularity [34] and Unikernel [43] code checkers lacking the understanding of a programmer’s are implemented in C# and OCaml, respectively. However, true intention to reason about semantic correctness. in practice, developers largely rely on code checkers. For To solve this problem, we present JUXTA, a tool that au- example, Linux has integrated static code analysis tools (e.g., tomatically infers high-level semantics directly from source Sparse) in its build process to detect common coding errors code. The key idea in JUXTA is to compare and contrast (e.g., checking whether system calls validate arguments that multiple existing implementations that obey latent yet im- come from userspace). Other tools such as Coverity [8] and plicit high-level semantics. For example, the implementation KINT [60] can find memory corruption and integer overflow of open() at the file system layer expects to handle an out- bugs, respectively. Besides these tools, a large number of of-space error from the disk in all file systems. We applied dynamic checkers are also available, such as kmemleak for JUXTA to 54 file systems in the stock Linux kernel (680K detecting memory leaks and AddressSanitizer [51] for finding LoC), found 118 previously unknown semantic bugs (one use-after-free bugs in Linux. bug per 5.8K LoC), and provided corresponding patches to Unfortunately, lacking a deep understanding of a program- 39 different file systems, including mature, popular ones like mer’s intentions or execution context, these tools tend to ext4, btrfs, XFS, and NFS. These semantic bugs are not easy discover shallow bugs. The majority of bugs, however, are se- to locate, as all the ones found by JUXTA have existed for mantic ones that violate high-level rules or invariants [15, 42]. over 6.2 years on average. Not only do our empirical results According to recent surveys of software bugs and patches, look promising, but the design of JUXTA is generic enough over 50% of bugs in Linux file systems are semantic bugs [42], to be extended easily beyond file systems to any software that such as incorrectly updating a file’s timestamps or missing a has multiple implementations, like Web browsers or protocols permission check. Without domain-specific knowledge, it is at the same layer of a network stack. extremely difficult for a tool to reason about the correctness or incorrectness of the code and discover such bugs. Thus, many tools used in practice are ineffective in detecting semantic vulnerabilities [15]. In this regard, a large body of research has been proposed to check and enforce semantic or system rules, which we Permission to make digital or hard copies of all or part of this work for personal or broadly classify into three categories: model checking, for- classroom use is granted without fee provided that copies are not made or distributed mal proof, and automatic testing. A common requirement for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the for these techniques is that developers should manually pro- author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or vide the correct semantics of code for checking: models to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. check and proofs of program properties. Unfortunately, cre- SOSP’15, October 4–7, 2015, Monterey, CA. ating such semantics is difficult, error-prone, and virtually Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3834-9/15/10. . . $15.00. infeasible for commodity systems like Linux. http://dx.doi.org/10.1145/2815400.2815422 361

}
}

@Article{MirakhorliCleland-Huang2016,
  Title                    = {Detecting, Tracing, and Monitoring Architectural Tactics in Code},
  Author                   = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2016},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {205–220},
  Volume                   = {42},

  Doi                      = {10.1109/tse.2015.2479217},
  File                     = {:home/ccc/github/literature/article/Detecting, Tracing, and Monitoring.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2015.2479217}
}

@InProceedings{MouLiLiuEtAl2014,
  Title                    = {Building Program Vector Representations for Deep Learning},
  Author                   = {Lili Mou and Ge Li and Yuxuan Liu and Hao Peng and Zhi Jin and Yan Xu and Lu Zhang and Software Institute and School of EECS and Peking University},
  Year                     = {2014},

  Abstract                 = {Deep learning has made significant breakthroughs Such striking results raise the interest of its applications in the in various fields of artificial intelligence. Advantages of deep field of program analysis. Using deep learning to automatically learning include the ability to capture highly complicated fea- capture program features is an interesting and prospective tures, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze research area. programs since deep architectures cannot be trained effectively Unfortunately, it has been practically infeasible for deep with pure back propagation. In this pioneering paper, we propose learning to analyze programs up till now. Since no proper the “coding criterion” to build program vector representations, “pretraining” method is proposed for programs, deep neural which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a networks cannot be trained effectively with pure back propa-
},
  File                     = {:article\\Building Program Vector Representations for Deep Learning.pdf:PDF},
  Review                   = {Building Program Vector Representations for Deep Learning Lili Mou, Ge Li∗, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang Software Institute, School of EECS, Peking University Beijing, 100871, P. R. China Email: {moull12, lige, zhijin, zhanglu}@sei.pku.edu.cn {liuyuxuan, penghao.pku, alandroxu}@gmail.com Abstract—Deep learning has made significant breakthroughs Such striking results raise the interest of its applications in the in various fields of artificial intelligence. Advantages of deep field of program analysis. Using deep learning to automatically learning include the ability to capture highly complicated fea- capture program features is an interesting and prospective tures, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze research area. programs since deep architectures cannot be trained effectively Unfortunately, it has been practically infeasible for deep with pure back propagation. In this pioneering paper, we propose learning to analyze programs up till now. Since no proper the “coding criterion” to build program vector representations, “pretraining” method is proposed for programs, deep neural which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a networks cannot be trained effectively with pure back propa- reality in this new field. We evaluate the learned vector represen- gation [13], [14], [15] because gradients would either vanish or tations both qualitatively and quantitatively. We conclude, based blow up through the deep architecture [16]. No useful features on the experiments, the coding criterion is successful in building can be extracted, and it results in very poor performance. program representations. To evaluate whether deep learning In this paper, we propose a novel “coding criterion” to build is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program vector representations based on abstract syntax trees program classification task than “shallow” methods, such as (ASTs). The vector representations are the premise of deep logistic regression and the support vector machine. This result architectures, and our method directly makes deep learning confirms the feasibility of deep learning to analyze programs. It a reality in the new field—program analysis. In such vector also gives primary evidence of its success in this new field. We representations, each node in ASTs (e.g. ID, Constant) is believe deep learning will become an outstanding technique for program analysis in the near future. mapped to a real-valued vector, with each element indicating a certain feature of the node. The vector representations I. INTRODUCTION serve as a “pretraining” method. They can emerge, through a deep architecture, high-level abstract features, and thus Machine learning-based program analysis has been studied benefit ultimate tasks. We analyze the learned representations long in the literature [1], [2], [3]. Hindle et al. compare both qualitatively and quantitatively. We conclude from the programming languages to natural languages and conclude experiments that the coding criterion is successful in building that programs also have rich statistical properties [4]. These program vector representations. properties are difficult for human to capture, but they justify To evaluate whether deep learning can be used to analyze using learning-based approaches to analyze programs. programs, we feed the learned representations to a deep neural The deep neural network, also known as deep learning, has network in the program classification task. We achieve higher become one of the prevailing machine learning approaches accuracy than “shallow” methods. The result confirms the since 2006 [5]. It has made significant breakthroughs in a feasibility of neural program analysis. It also sheds some light variety of fields, such as natural language processing [6], [7], on the future of this new area. image processing [8], [9], speech recognition [10], [11], etc. We publish all of our source codes, datasets, and learned Compared with traditional machine learning approaches, deep representations on our project website1 to promote future learning has the following major advantages: studies. The AST node representations can be used for further • The deep architecture can capture highly complicated researches in various applications of program analysis. The (non-linear) features efficiently. They are crucial to most source codes contain a versatile infrastructure of the feed- real-world applications. forward neural network, based on which one can build one’s • Very little human engineering and prior knowledge is own deep neural architectures. required. Interestingly, with even a unified model, deep To our best knowledge, this paper is the first to propose learning achieves better performance than state-of-the-art representation learning algorithms for programs. It is also the approaches in many heterogeneous tasks [12]. first to analyze programs by deep learning. This study is a ∗Corresponding author. 1 https://sites.google.com/site/learnrepresent/ arXiv:1409.3358v1 [cs.SE] 11 Sep 2014

}
}

@Article{MunsonNikoraSherif2006,
  Title                    = {Software faults: A quantifiable definition},
  Author                   = {Munson, John C. and Nikora, Allen P. and Sherif, Joseph S.},
  Journal                  = {Advances in Engineering Software},
  Year                     = {2006},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {327–333},
  Volume                   = {37},

  Doi                      = {10.1016/j.advengsoft.2005.07.003},
  File                     = {:home/ccc/github/literature/article/Software faults\: A quantifiable definition.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0965-9978},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.advengsoft.2005.07.003}
}

@Article{MurtazaKhreichHamou-LhadjEtAl2016,
  Title                    = {Mining trends and patterns of software vulnerabilities},
  Author                   = {Murtaza, Syed Shariyar and Khreich, Wael and Hamou-Lhadj, Abdelwahab and Bener, Ayse Basar},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2016},

  Month                    = {Jul},
  Pages                    = {218–228},
  Volume                   = {117},

  __markedentry            = {[ccc:1]},
  Doi                      = {10.1016/j.jss.2016.02.048},
  File                     = {:home/ccc/github/literature/article/-Mining trends and patterns of software vulnerabilities.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0164-1212},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.jss.2016.02.048}
}

@InProceedings{,
  Title                    = {Static Program Analysis},
  Author                   = {Anders M�ller and Michael I. Schwartzbach},
  Year                     = {2015},

  File                     = {:home/ccc/github/literature/article/Static Program Analysis(Anders M�ller).pdf:PDF},
  Review                   = {Static Program Analysis
Anders M�ller and Michael I. Schwartzbach
June 16, 2015

}
}

@InProceedings{NagappanShihabSoftwareEtAl2011,
  Title                    = {Future Trends in Software Engineering Research for Mobile Apps},
  Author                   = {Meiyappan Nagappan and Emad Shihab and Department of Software and Engineering Dept. of Computer and Science and Software and Engineering},
  Year                     = {2011},

  Abstract                 = {There has been tremendous growth in the use of cently researchers have begun to focus on software engineering mobile devices over the last few years. This growth has fueled issues for mobile apps. For example, the 2011 Mining Software the development of millions of software applications for these Repositories Challenge focused on studying the Android mobile mobile devices often called as ‘apps’. Current estimates indicate that there are hundreds of thousands of mobile app developers. platform [90]. Other work focused on issues related to code As a result, in recent years, there has been an increasing reuse in mobile apps [84], on mining mobile app data from amount of software engineering research conducted on mobile the app stores [34], testing mobile apps [70] and teaching apps to help such mobile app developers. In this paper, we programming on mobile devices [95]. Therefore, we feel it is discuss current and future research trends within the framework a perfect time to reflect on the accomplishments in the area of of the various stages in the software development life-cycle: requirements (including non-functional), design and development, Software Engineering research for mobile apps and to draw a testing, and maintenance. While there are several non-functional vision for its future. Note that we restrict to just the software requirements, we focus on the topics of energy and security in engineering topics for mobile apps in this paper, and even that our paper, since mobile apps are not necessarily built by large not exhaustively due to space restrictions (we skip topics like companies that can afford to get experts for solving these two usability or performance engineering since an entire paper can topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, be written on each of these topics). We do not discuss the we first present the recent advances done in these stages and then advancements in other areas of research for mobile apps such we present the challenges present in current work, followed by as cloud based solutions, or networking in mobile apps. the future opportunities and the risks present in pursuing such The purpose of this vision paper is to serve as a reference research. point for mobile app work. We start by providing some
},
  File                     = {:home/ccc/github/literature/article/Future-Trends-in-Software-Engineering-Research-for-Mobile-Apps.pdf:PDF},
  Review                   = {Future Trends in Software Engineering Research for Mobile Apps Meiyappan Nagappan Emad Shihab Department of Software Engineering Dept. of Computer Science and Software Engineering Rochester Institute of Technology Concordia University Rochester, NY, USA Montreal, QC, Canada mei@se.rit.edu eshihab@cse.concordia.ca Abstract—There has been tremendous growth in the use of cently researchers have begun to focus on software engineering mobile devices over the last few years. This growth has fueled issues for mobile apps. For example, the 2011 Mining Software the development of millions of software applications for these Repositories Challenge focused on studying the Android mobile mobile devices often called as ‘apps’. Current estimates indicate that there are hundreds of thousands of mobile app developers. platform [90]. Other work focused on issues related to code As a result, in recent years, there has been an increasing reuse in mobile apps [84], on mining mobile app data from amount of software engineering research conducted on mobile the app stores [34], testing mobile apps [70] and teaching apps to help such mobile app developers. In this paper, we programming on mobile devices [95]. Therefore, we feel it is discuss current and future research trends within the framework a perfect time to reflect on the accomplishments in the area of of the various stages in the software development life-cycle: requirements (including non-functional), design and development, Software Engineering research for mobile apps and to draw a testing, and maintenance. While there are several non-functional vision for its future. Note that we restrict to just the software requirements, we focus on the topics of energy and security in engineering topics for mobile apps in this paper, and even that our paper, since mobile apps are not necessarily built by large not exhaustively due to space restrictions (we skip topics like companies that can afford to get experts for solving these two usability or performance engineering since an entire paper can topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, be written on each of these topics). We do not discuss the we first present the recent advances done in these stages and then advancements in other areas of research for mobile apps such we present the challenges present in current work, followed by as cloud based solutions, or networking in mobile apps. the future opportunities and the risks present in pursuing such The purpose of this vision paper is to serve as a reference research. point for mobile app work. We start by providing some Index Terms—Mobile apps, Mining app markets. background information on mobile apps. Then, we discuss the current state-of-the-art in the field, relating it to each of the I. INTRODUCTION software development phases, i.e., requirements, development, In the context of this paper, a mobile app is defined as the testing, and maintenance as shown in Figure 1. We also talk application developed for the current generation of mobile about two non-functional requirements: energy use and security devices popularly known as smart phones. These apps are often of mobile apps. Finally, even though it is not one of the software distributed through a platform specific, and centralized app development phases, we talk about the software engineering market. In this paper, we sometimes refer to mobile apps simply challenges and recommendations for monetizing mobile apps. as apps. In the past few years we are observing an explosion in Along with a discussion of the state-of-the-art, we also present the popularity of mobile devices and mobile apps [17]. In fact, the challenges currently faced by the researchers/developers recent market studies show that the the centralized app market of mobile apps. Then we discuss our vision for the future of for Apple’s platform (iOS) and Google’s platform (Android), software engineering research for mobile apps and the risks each have more than 1.5 million apps [8]. These mobile app involved, based on our experiences. markets are extremely popular among developers due to the Our hope is that our vision paper will help newcomers flexibility and revenue potential. At the same time, mobile apps to quickly gain a background in the area of mobile apps. bring a whole slew of new challenges to software practitioners Moreover, we hope that our discussion of the vision for the area - such as challenges due to the highly-connected nature of these will inspire and guide future work and build a community of devices, the unique distribution channels available for mobile researchers with common goals regarding software engineering apps (i.e., app markets like Apple’s App Store and Google’s challenges for mobile apps. A word of caution though - the Google Play), and novel revenue models (e.g., freemium and discussion of the current state-of-the-art is not meant to be a subscription apps). systematic literature survey (for a more comprehensive study To date the majority of the software engineering research please refer to Sarro et al. [88]), and the future directions of has focused on traditional “shrink wrapped” software, such as research are based on our opinions that have been influenced Mozilla Firefox, Eclipse or Apache HTTP [79]. However, re- by our knowledge of the research in this community.

}
}

@InProceedings{NeuhausZimmermannHollerEtAl2007,
  Title                    = {Predicting Vulnerable Software Components},
  Author                   = {Stephan Neuhaus and Thomas Zimmermann and Christian Holler and Andreas Zeller and * Department and of Computer and Science and Department of Computer and Science},
  Year                     = {2007},

  Abstract                 = {tracking past vulnerabilities. The Mozilla project, for in- Where do most vulnerabilities occur in software? Our Vul- stance, maintains a vulnerability database which records
},
  File                     = {:home/ccc/github/literature/article/-Predicting Vulnerable software components.pdf:PDF},
  Keywords                 = {Software Security, Prediction},
  Review                   = {Predicting Vulnerable Software Components Stephan Neuhaus* Thomas Zimmermann+ Christian Holler* Andreas Zeller* * Department of Computer Science + Department of Computer Science Saarland University, Saarbrücken, Germany University of Calgary, Calgary, Alberta, Canada {neuhaus,holler,zeller}@st.cs.uni-sb.de tz@acm.org ABSTRACT tracking past vulnerabilities. The Mozilla project, for in- Where do most vulnerabilities occur in software? Our Vul- stance, maintains a vulnerability database which records ture tool automatically mines existing vulnerability databa- all incidents. However, these databases do not tell how ses and version archives to map past vulnerabilities to com- these vulnerabilities are distributed across the Mozilla code- ponents. The resulting ranking of the most vulnerable com- base. Our Vulture tool automatically mines a vulnerability ponents is a perfect base for further investigations on what database and associates the reports with the change history makes components vulnerable. to map vulnerabilities to individual components (Figure 1). In an investigation of the Mozilla vulnerability history, we Vulture’s result is a distribution of vulnerabilities across surprisingly found that components that had a single vulner- the entire codebase. Figure 2 shows this distribution for ability in the past were generally not likely to have further Mozilla: the darker a component, the more vulnerabilities vulnerabilities. However, components that had similar im- were ﬁxed in the past.The distribution is very uneven: Only ports or function calls were likely to be vulnerable. 4% of the 10,452 components were involved in security ﬁxes. Based on this observation, we were able to extend Vul- This raises the question: Are there speciﬁc code patterns that ture by a simple predictor that correctly predicts about half occur only in vulnerable components? of all vulnerable components, and about two thirds of all In our investigation, we were not able to determine code predictions are correct. This allows developers and project features such as, code complexity or buﬀer usage that would managers to focus their their eﬀorts where it is needed most: correlate with the number of vulnerabilities. What we found, “We should look at nsXPInstallManager because it is likely though, was that vulnerable components shared similar sets to contain yet unknown vulnerabilities.” of imports and function calls. In the case of Mozilla, for instance, we found that of the 14 components importing Categories and Subject Descriptors: D.2.4 [Software nsNodeUtils.h, 13 components (93%) had to be patched be- Engineering]: Software/Program Veriﬁcation—Statistical cause of security leaks. The situation is even worse for those methods; D.2.5 [Software Engineering]: Testing and De- 15 components that import nsIContent.h, nsIInterface- bugging—Testing tools; D.4.6 [Operating Systems]: Se- RequestorUtils.h and nsContentUtils.h together—they curity and Protection—Invasive software. all had vulnerabilities. This observation can be used for General Terms: Security, Experimentation, Measurement automatically predicting whether a new component will be Keywords: Software Security, Prediction 1. INTRODUCTION Code NewVulnerability Version Code component database archive Code Many software security problems are instances of general patterns, such as buﬀer overﬂow or format string vulnerabil- ities. Some problems, though, are speciﬁc to a single project or problem domain: JavaScript programs escaping their jails are a problem only in web browsers. To improve the secu- Vulture produces Predictor rity of software, we must therefore not only look for general problem patterns, but also learn speciﬁc patterns that apply maps vulnerabilities to components predicts vulnerability only to the software at hand. Modern software development usually does a good job in Component Component Component Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are Figure 1: How Vulture works. Vulture mines a vul- not made or distributed for profit or commercial advantage and that copies nerability database (e.g. a Bugzilla subset), a version bear this notice and the full citation on the first page. To copy otherwise, to archive (e.g. CVS), and a code base, and maps past republish, to post on servers or to redistribute to lists, requires prior specific vulnerabilities to components. The resulting predic- permission and/or a fee. CCS’07, October 29–No tor predicts the future vulnerabilities of new compo-vember 2, 2007, Alexandria, Virginia, USA. Copyright 2007 ACM 978-1-59593-703-2/07/0010...$5.00. nents, based on their imports or function calls. 529

}
}

@InProceedings{NewsomeSongjnewsome@ece.cmu.edudawnsong@cmu.edu2005,
  Title                    = {Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software},
  Author                   = {James Newsome and Dawn Song and jnewsome@ece.cmu.edu dawnsong@cmu.edu},
  Year                     = {2005},

  Abstract                 = {detected, we must quickly develop filters (a.k.a. attack signatures) that can be used to filter out attack packets ef- ficiently, and hence protect vulnerable hosts from compro-
},
  File                     = {:article\\TaintCheck_Dynamic taint analysis for automatic  detection, analysis, and signature generation of exploits on commodity software.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software James Newsome Dawn Song jnewsome@ece.cmu.edu dawnsong@cmu.edu Carnegie Mellon University Carnegie Mellon University Abstract detected, we must quickly develop filters (a.k.a. attack signatures) that can be used to filter out attack packets ef- ficiently, and hence protect vulnerable hosts from compro- Software vulnerabilities have had a devastating effect mise until the vulnerability can be patched. Because a new on the Internet. Worms such as CodeRed and Slammer worm can spread quickly, signature generation must be can compromise hundreds of thousands of hosts within automatic—no manual intervention can respond quickly hours or even minutes, and cause millions of dollars of enough to prevent a large number of vulnerable hosts from damage [26, 43]. To successfully combat these fast auto- being infected by a new fast-spreading worm. matic Internet attacks, we need fast automatic attack de- tection and filtering mechanisms. In this paper we propose dynamic taint analysis for au- We need fine-grained attack detectors for commodity tomatic detection of overwrite attacks, which include most software. Many approaches have been proposed to de- types of exploits. This approach does not need source code tect new attacks. These approaches roughly fall into two or special compilation for the monitored program, and categories: coarse-grained detectors, that detect anoma- hence works on commodity software. To demonstrate this lous behavior, such as scanning or unusual activity at a idea, we have implemented TaintCheck, a mechanism that certain port; and fine-grained detectors, that detect attacks can perform dynamic taint analysis by performing binary on a program’s vulnerabilities. Coarse-grained detectors rewriting at run time. We show that TaintCheck reliably may result in frequent false positives, and do not provide detects most types of exploits. We found that TaintCheck detailed information about the vulnerability and how it is produced no false positives for any of the many different exploited. Thus, it is desirable to develop fine-grained de- programs that we tested. Further, we describe how Taint- tectors that produce fewer false positives, and provide de- Check could improve automatic signature generation in tailed information about the vulnerability and exploit. several ways. Several approaches for fine-grained detectors have been proposed that detect when a program is exploited. Most of these previous mechanisms require source code or special 1. Introduction recompilation of the program, such as StackGuard [15], PointGuard [14], full-bounds check [20, 38], Libsafe- Software vulnerabilities such as buffer overruns and for- Plus [5], FormatGuard [13], and CCured [28]. Some of mat string vulnerabilities have had a devastating effect on them also require recompiling the libraries [20, 38], or the Internet. Worms such as CodeRed and Slammer ex- modifying the original source code, or are not compatible ploit software vulnerabilities and can compromise hun- with some programs [28, 14]. These constraints hinder the dreds of thousands of hosts within hours or even min- deployment and applicability of these methods, especially utes, and cause millions of dollars of damage [26, 43]. To for commodity software, because source code or specially successfully combat fast Internet worm attacks, we need recompiled binaries are often unavailable, and the addi- automatic detection and defense mechanisms. First, we tional work required (such as recompiling the libraries and need automatic detection mechanisms that can detect new modifying the original source code) makes it inconvenient attacks for previously unknown vulnerabilities. A detec- to apply these methods to a broad range of applications. tion mechanism should be easy to deploy, result in few Note that most of the large-scale worm attacks to date are false positives and few false negatives, and detect attacks attacks on commodity software. early, before a significant fraction of vulnerable systems Thus, it is important to design fine-grained detectors are compromised. Second, once a new exploit attack is that work on commodity software, i.e., work on arbitrary

}
}

@Article{NoMoreGotos:DecompilationUsingPatternIndependentControl-FlowStructuringandSemantics-PreservingTransformationsEtAl2015,
  Title                    = {No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations},
  Author                   = {NoMoreGotos:DecompilationUsingPattern- and IndependentControl-FlowStructuringand and Semantics-PreservingTransformations and CONFERENCEPAPER·FEBRUARY2015},
  Year                     = {2015},

  File                     = {:article\\dream_ndss2015.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/267762589 NoMoreGotos:DecompilationUsingPattern- IndependentControl-FlowStructuringand Semantics-PreservingTransformations CONFERENCEPAPER·FEBRUARY2015 READS 74 4AUTHORS,INCLUDING: KhaledYakdan UniversityofBonn 3PUBLICATIONS0CITATIONS SEEPROFILE Availablefrom:KhaledYakdan Retrievedon:12October2015

}
}

@Article{OzTopcuogluKandemirEtAl2012,
  Title                    = {Thread vulnerability in parallel applications},
  Author                   = {Oz, Isil and Topcuoglu, Haluk Rahmi and Kandemir, Mahmut and Tosun, Oguz},
  Journal                  = {Journal of Parallel and Distributed Computing},
  Year                     = {2012},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1171鈥�1185},
  Volume                   = {72},

  Doi                      = {10.1016/j.jpdc.2012.05.002},
  File                     = {:article\\Thread vulnerability in parallel applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0743-7315},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.jpdc.2012.05.002}
}

@Article{PadmanabhuniTan2016,
  Title                    = {Auditing buffer overflow vulnerabilities using hybrid static-dynamic analysis},
  Author                   = {Padmanabhuni, Bindu Madhavi and Tan, Hee Beng Kuan},
  Journal                  = {Iet Software},
  Year                     = {2016},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {54--61},
  Volume                   = {10},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Buffer overflow (BOF) vulnerabilities when present in code can be exploited to violate security objectives such as availability, confidentiality and integrity. They make up substantial portion of input manipulation attacks due to their common presence and ease of exploitation. In this study, the authors propose a hybrid approach combining static and dynamic program analysis with machine learning to audit BOFs. Simple rules to generate test data is proposed to confirm some of the vulnerabilities through dynamic analysis. Confirmed cases can be fixed by developers without further verification. Statements whose vulnerability is not confirmed by dynamic analysis are predicted by mining static code attributes. In the authors' evaluation using standard benchmarks, their best classifier achieved a recall over 93% and accuracy >94%. Dynamic analysis itself confirmed 34% of known vulnerabilities along with reporting six new bugs, thereby reducing by third, otherwise needed manual auditing effort.},
  Doi                      = {10.1049/iet-sen.2014.0185},
  Ei                       = {1751-8814},
  Groups                   = {Code Mining},
  Sn                       = {1751-8806},
  Tc                       = {0},
  Ut                       = {WOS:000374663800003},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{PadmanabhuniTan2015,
  Title                    = {Buffer Overflow Vulnerability Prediction from x86 Executables Using Static Analysis and Machine Learning},
  Author                   = {Padmanabhuni, Bindu Madhavi and Tan, Hee Beng Kuan},
  Journal                  = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  Year                     = {2015},

  Month                    = {Jul},
  Pages                    = {450--9},

  Abstract                 = {Mining static code attributes for predicting software vulnerabilities has received some attention recently. There are a number of approaches for detecting vulnerabilities from source code, but commercial off the shelf components are, in general, distributed in binary form. Before using such third-party components it is imperative to check for presence of vulnerabilities. We investigate the use of static analysis and machine learning for predicting buffer overflow vulnerabilities from binaries in this study. To mitigate buffer overflows, developers typically perform size checks and input validation. We propose static code attributes characterizing buffer usage and defense mechanisms implemented in the code for preventing buffer overflows. The proposed approach starts by identifying potential vulnerable statement constructs during binary program analysis and extracts static code attributes for each of them as per proposed characterization scheme to capture buffer usage patterns and defensive mechanisms employed in the code. Data mining methods are then used on these collected code attributes for predicting buffer overflows. Our experimental evaluation on standard buffer overflow benchmark binaries shows that the proposed static code attributes are effective in predicting buffer overflow vulnerabilities.},
  Cl                       = {Taichung, Taiwan},
  Ct                       = {2015 IEEE 39th Annual Computer Software and Applications ConferenceEOLEOL(COMPSAC)},
  Cy                       = {1-5 July 2015},
  Doi                      = {10.1109/compsac.2015.78},
  File                     = {:home/ccc/github/literature/article/Buffer Overflow Vulnerability Prediction from x86 executables using Static.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-1-4673-6564-2},
  Pn                       = {vol.2},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Tc                       = {0},
  Url                      = {http://dx.doi.org/10.1109/COMPSAC.2015.78},
  Ut                       = {INSPEC:15475995},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{PadmanabhuniTan2014,
  Title                    = {Auditing Buffer Overflow Vulnerabilities using Hybrid Static-Dynamic Analysis},
  Author                   = {Padmanabhuni, Bindu Madhavi and Tan, Hee Beng Kuan},
  Journal                  = {2014 Ieee 38th Annual International Computers, Software and Applications Conference (compsac)},
  Year                     = {2014},
  Pages                    = {IEEE; IEEE Comp Soc; IEEE Cloud Comp; Korean Inst Informat Scientists \&EOLEOLEngineers; Iowa State Univ Sci \& Technol; IPS; CCF; Malardalen UnivEOLEOLSweden; ABB; Missouri S \& T},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Despite being studied for more than two decades buffer overflow vulnerabilities are still frequently reported in programs. In this paper, we propose a hybrid approach that combines static and dynamic program analysis to audit buffer overflows. Using simple rules, test data are generated to automatically confirm some of the vulnerabilities through dynamic analysis and the remaining cases are predicted by mining static code attributes. Confirmed cases can be directly fixed without further verification whereas predicted cases need to be manually reviewed to confirm existence of vulnerabilities. Since our approach combines the strengths of static and dynamic analyses, it results in an overall accuracy improvement. In our evaluation of approach using the standard benchmark suite, our classifiers achieved a recall over 92% and precision greater than 81%. The dynamic analysis component confirmed 51% of known vulnerabilities along with reporting 2 new bugs, thereby reducing by half, otherwise needed manual auditing effort.},
  Be                       = {Chang, CKEOLEOLGao, YEOLEOLHurson, AEOLEOLMatskin, MEOLEOLMcMillin, BEOLEOLOkabe, YEOLEOLSeceleanu, CEOLEOLYoshida, K},
  Bn                       = {978-1-4799-3574-1},
  Cl                       = {Vasteras, SWEDEN},
  Ct                       = {IEEE 38th Annual International Computers, Software and ApplicationsEOLEOLConference (COMPSAC)},
  Cy                       = {JUL 27-29, 2014-2015},
  Doi                      = {10.1109/COMPSAC.2014.62},
  Groups                   = {Code Mining},
  Se                       = {Proceedings International Computer Software and Applications Conference},
  Sn                       = {0730-3157},
  Tc                       = {0},
  Ut                       = {WOS:000353962400048},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{PadmanabhuniTan2014a,
  Title                    = {Predicting Buffer Overflow Vulnerabilities through Mining Light-Weight Static Code Attributes},
  Author                   = {Padmanabhuni, Bindu Madhavi and Tan, Hee Beng Kuan},
  Journal                  = {2014 Ieee International Symposium On Software Reliability Engineering Workshops (issrew)},
  Year                     = {2014},
  Pages                    = {IEEE; IEEE Comp Soc; CISCO; DIETIUNINA; CRITIWARE s r l; ResilTech},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Static code attributes are widely used in defect prediction studies as an abstraction model because they capture general properties of the program. To counter buffer overflow exploits, programmers use buffer size checking and input validation schemes. In this paper, we propose lightweight static code attributes that can be extracted easily, to characterize buffer overflow safety mechanisms and input validation checks implemented in the code for predicting buffer overflows. We then use data mining methods on the collected static code attributes to predict buffer overflows in application programs. In our experiments across five applications, our best classifier could achieve a recall of 95% and precision over 80% suggesting that our proposed static code attributes are effective indicators in predicting buffer overflows.},
  Bn                       = {978-1-4799-7377-4},
  Cl                       = {Naples, ITALY},
  Ct                       = {25th IEEE International Symposium on Software Reliability EngineeringEOLEOLWorkshops (ISSREW)},
  Cy                       = {NOV 03-06, 2014},
  Doi                      = {10.1109/ISSREW.2014.26},
  Gp                       = {IEEE},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {WOS:000360286200071},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{Paolo CiancariniAlberto SillittiGiancarlo SucciEtAl2015,
  Title                    = {Advances in Intelligent Systems and Computing},
  Author                   = {Paolo Ciancarini and Alberto Sillitti and Giancarlo Succi and Angelo Messina and E and ditors},
  Year                     = {2015},

  File                     = {:home/ccc/github/literature/article/SEDA 2015.pdf:PDF},
  Review                   = {Advances in Intelligent Systems and Computing 422 Paolo Ciancarini Alberto Sillitti Giancarlo Succi Angelo Messina E ditors Proceedings of 4th International Conference in Software Engineering for Defence Applications SEDA 2015

}
}

@Article{Paul1994,
  Title                    = {A framework for source code search using program patterns （CR 135）.pdf},
  Author                   = {Santanu Paul},
  Journal                  = {IEEE Transactions},
  Year                     = {1994},

  File                     = {:article\\A framework for source code search using program patterns （CR 135）.pdf:PDF},
  Groups                   = {source code vulnerability},
  Owner                    = {c},
  Read                     = {未读},
  Timestamp                = {2015.10.19}
}

@MastersThesis{Penttilae2014,
  Title                    = {Improving C++ Software Quality with Static Code Analysis},
  Author                   = {Elias Penttilä},
  Year                     = {2014},

  File                     = {:article\\Improving C++ software quality with static code analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Aalto University School of Science Degree Programme in Computer Science and Engineering Elias Penttilä Improving C++ Software Quality with Static Code Analysis Master’s Thesis Espoo, May 16, 2014 Supervisor: Professor Lauri Malmi Advisor: Kaj Björklund, M.Sc. (Tech.)

}
}

@InProceedings{PerlArpFahlEtAl2015,
  Title                    = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits},
  Author                   = {Henning Perl and Daniel Arp and Sascha Fahl and Sergej Dechand and Fabian Yamaguchi and Yasemin Acar and Matthew Smith*† Konrad Rieck Saarland University and Germany},
  Booktitle                = {CCS'15},
  Year                     = {2015},

  Abstract                 = {Keywords Despite the security community’s best effort, the number Vulnerabilities; Static Analysis; Machine Learning of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. How- 1. INTRODUCTION ever, due to the enormous amount of code being produced, Despite the best effort of the security community, the as well as a the lack of manpower and expertise, not all code number of serious vulnerabilities discovered in deployed soft- is sufficiently audited. Thus, many vulnerabilities slip into ware is on the rise. The Common Vulnerabilities and Expo- production systems. A best-practice approach is to use a sures (CVE) database operated by MITRE tracks the most code metric analysis tool, such as Flawfinder, to flag poten- serious vulnerabilities. In 2000, around 1,000 CVEs were tially dangerous code so that it can receive special attention. registered. By 2010, there were about 4,500. In 2014, al- However, because these tools have a very high false-positive most 8,000 CVEs were registered. The trend seems to be rate, the manual effort needed to find vulnerabilities remains increasing in speed. overwhelming. While it is considered a best practice to perform code
},
  File                     = {:article\\VCCFinder Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits Henning Perl*, Daniel Arp, Sascha Fahl, Sergej Dechand†, Fabian Yamaguchi, Yasemin Acar Matthew Smith*† Konrad Rieck Saarland University, Germany * Fraunhofer FKIE, Germany University of Göttingen, † University of Bonn, Germany Germany ABSTRACT Keywords Despite the security community’s best effort, the number Vulnerabilities; Static Analysis; Machine Learning of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. How- 1. INTRODUCTION ever, due to the enormous amount of code being produced, Despite the best effort of the security community, the as well as a the lack of manpower and expertise, not all code number of serious vulnerabilities discovered in deployed soft- is sufficiently audited. Thus, many vulnerabilities slip into ware is on the rise. The Common Vulnerabilities and Expo- production systems. A best-practice approach is to use a sures (CVE) database operated by MITRE tracks the most code metric analysis tool, such as Flawfinder, to flag poten- serious vulnerabilities. In 2000, around 1,000 CVEs were tially dangerous code so that it can receive special attention. registered. By 2010, there were about 4,500. In 2014, al- However, because these tools have a very high false-positive most 8,000 CVEs were registered. The trend seems to be rate, the manual effort needed to find vulnerabilities remains increasing in speed. overwhelming. While it is considered a best practice to perform code In this paper, we present a new method of finding poten- reviews before code is released, as well as to retroactively tially dangerous code in code repositories with a significantly checking old code, there is often not enough manpower to lower false-positive rate than comparable systems. We com- rigorously review all the code that should be reviewed. Al- bine code-metric analysis with metadata gathered from code though open-source projects have the advantage that any- repositories to help code review teams prioritize their work. body can, in theory, look at all the source code, and although The paper makes three contributions. First, we conducted bug-bounty programs create incentives to do so, usually only the first large-scale mapping of CVEs to GitHub commits a small team of core developers reviews the code. in order to create a vulnerable commit database. Second, In order to support code reviewers in finding vulnerabili- based on this database, we trained a SVM classifier to flag ties, tools and methodologies that flag potentially dangerous suspicious commits. Compared to Flawfinder, our approach code are used to narrow down the search. For C-like lan- reduces the amount of false alarms by over 99 % at the same guages, a wide variety of code metrics can raise warning level of recall. Finally, we present a thorough quantitative flags, such as a variable assigned inside an if-statement or and qualitative analysis of our approach and discuss lessons unreachable cases in a switch-statement. The Clang static learned from the results. We will share the database as analyzer [1] as well as the dynamic analyzer Valgrind [3] and a benchmark for future research and will also provide our others, can pinpoint further pitfalls such as invalid mem- analysis tool as a web service. ory access. For the Linux kernel, the Trinity system call- fuzzer [2] has found and continues to find many bugs. Fi- nally, static analysis tools like Flawfinder [34] help find pos- Categories and Subject Descriptors sible security vulnerabilities. Most of these approaches operate on an entire software D.2.4 [Software Engineering]: Software/Program Verifi- project and deliver a (frequently very large) list of poten- cation; K.6.5 [Management of Computing and Infor- tially unsafe code. However, software grows incrementally mation Systems]: Security and Protection and it is desirable to have tools to assist in reviewing these increments as well as tools to check entire projects. Most open-source projects manage their source code with version Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed control systems (VCS) such as Git, Mercurial, CVS or Sub- for profit or commercial advantage and that copies bear this notice and the full cita- version. In such systems, code – including vulnerable code tion on the first page. Copyrights for components of this work owned by others than – is inserted into the software in the form of commits to the ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- repository. Therefore, the natural unit upon which to check publish, to post on servers or to redistribute to lists, requires prior specific permission whether new code is dangerous is the commit. However, and/or a fee. Request permissions from Permissions@acm.org. most existing tools cannot simply be executed on code snip- CCS’15, October 12–16, 2015, Denver, Colorado, USA. ©c 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00. pets contained within a commit. Thus, if a code reviewer DOI: http://dx.doi.org/10.1145/2810103.2813604. wants to check the security of a commit, the reviewer must

}
}

@Article{PewnySchusterBernhardEtAl2014,
  Title                    = {Leveraging semantic signatures for bug search in binary programs},
  Author                   = {Pewny, Jannik and Schuster, Felix and Bernhard, Lukas and Holz, Thorsten and Rossow, Christian},
  Journal                  = {Proceedings of the 30th Annual Computer Security Applications Conference on - ACSAC 鈥�14},
  Year                     = {2014},

  Doi                      = {10.1145/2664243.2664269},
  File                     = {:article\\Leveraging semantic signatures for bug search in binary programs.pdf:PDF},
  ISBN                     = {http://id.crossref.org/isbn/9781450330053},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/2664243.2664269}
}

@InProceedings{PredictingSeverityofSoftwareVulnerabilityBasedonGreySystemTheoryARTICLE·NOVEMBER20152927,
  Title                    = {Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication},
  Author                   = {PredictingSeverityofSoftwareVulnerability and BasedonGreySystemTheory and ARTICLE·NOVEMBER2015},
  Year                     = {2927},

  File                     = {:home/ccc/github/literature/article/-Predicting Severity of Software Vulnerability Based on Grey System Theory.pdf:PDF},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/292720691 PredictingSeverityofSoftwareVulnerability BasedonGreySystemTheory ARTICLE·NOVEMBER2015 READS 12 3AUTHORS,INCLUDING: JinkunGeng BeihangUniversity(BUAA) 2PUBLICATIONS0CITATIONS SEEPROFILE Allin-textreferencesunderlinedinbluearelinkedtopublicationsonResearchGate, Availablefrom:JinkunGeng lettingyouaccessandreadthemimmediately. Retrievedon:29March2016

}
}

@InProceedings{ProcessPerformingCodeEtAl1540,
  Title                    = {Basic Training Editors: Richard Ford, rford@se.fit.edu Michael A. Howard, mikehow@microsoft.com},
  Author                   = {A Process and for Performing and Security Code and Reviews},
  Booktitle                = {74 PUBLISHED BY THE IEEE COMPUTER SOCIETY ■},
  Year                     = {1540},
  Publisher                = {IEEE},

  Doi                      = {ng},
  File                     = {:home/ccc/github/literature/article/A Process for Performing Security Code Reviews.pdf:PDF},
  Review                   = {Basic Training Editors: Richard Ford, rford@se.fit.edu Michael A. Howard, mikehow@microsoft.com A Process for Performing Security Code Reviews o one really likes reviewing source code for se- • Old code. Older code tends to have N more security vulnerabilities thancurity vulnerabilities; it’s slow, tedious, and new code because newer codeoften reflects a better understand-mind-numbingly boring. Yet, code review is a ing of security issues. The defini-tion of old is hard to quantify, butcritical component of shipping secure software you should review in depth any code you consider “legacy.” to customers. Neglecting it isn’t an option. • Code that runs by default. Attackers often go after installed code that MICHAEL A. I get to review quite a bit of Of course, there’s no replace- runs by default. Therefore, such HOWARD code—not as much as I used to, but ment for experience, and so we at code must be better quality, and Microsoft enough to keep me busy helping Microsoft often pair up more senior hence reviewed earlier and deeper teams at Microsoft. Sometimes peo- code reviewers with apprentices to than code that doesn’t execute by ple just want my take on small snip- help pass on code review wisdom. default. Code running by default pets of perhaps 100 lines of code, and Learning from other people’s expe- increases an application’s attack other times I get hundreds of thou- rience is a critical component of ed- surface, which is a product of all sands of lines. ucation, and, frankly, it works better code accessible to attackers.1 People often ask how I review for some classes of people than sit- • Code that runs in elevated context. code for security vulnerabilities ting in a lecture-style environment. Code that runs in elevated identi- when faced with a massive amount If you’re new to this field, you’ll ties—Local System in Windows to review. At a very high level, my then want to review old security or root in *nix, for example— process is simple: vulnerabilities. Check out Security- must be of higher quality and re- Focus.com and sign up for Bugtraq. quires earlier and deeper review • Make sure you know what you’re Spend time understanding the core because code identity is another doing. issues and proposed remedies in the component of attack surface. • Prioritize. discussions. Also, look at security • Anonymously accessible code. An- • Review the code. vulnerabilities you and your col- other attack-surface element, code leagues have confronted. that anonymous users can access Although my approach might not should be reviewed in greater work for you, I’ve fine-tuned it over Prioritize depth than code that only valid the years based on comments and How do you prioritize a code re- users and administrators can access. lessons from people I consider much view effort when you have, say, • Code listening on a globally accessible better than me at reviewing code. 1,000 files to review? Various tactics network interface.Code that listens by Let’s look at each step in more detail. that gravitate around attack surface default on a network, especially the and potential bug density can pro- Internet, is obviously open to sub- Make sure you know vide a good place to start. Code stantial risk and must be reviewed in what you’re doing compiled into higher attack surface depth for security vulnerabilities. This sounds obvious, but unless you software requires deeper review • Code written in C/C++/assembly lan- know what a security bug is, you than code in lower attack surface guage. Because these languages have have no chance whatsoever of find- components. This isn’t to say that direct access to memory, buffer-ma- ing them while reviewing code. So you don’t need to review lower at- nipulation vulnerabilities within the the first-order task is to become edu- tack surface software; I’m simply code can lead to buffer overruns, cated on the issues. For some good talking about priorities. We can use which often lead to malicious code books on the subject, see the “Fur- the following heuristics to deter- execution. With this in mind, you ther reading” sidebar. mine code review priority: should review code written in these 74 PUBLISHED BY THE IEEE COMPUTER SOCIETY ■ 1540-7993/06/$20.00 © 2006 IEEE ■ IEEE SECURITY & PRIVACY 

}
}

@InProceedings{QiAlfredChen2015,
  Title                    = {Static Detection of Packet Injection Vulnerabilities – A Case for Identifying Attacker-controlled Implicit Information Leaks},
  Author                   = {Qi Alfred Chen,},
  Booktitle                = {CCS’15},
  Year                     = {2015},

  Abstract                 = {1. INTRODUCTION
},
  File                     = {:article\\Static Detection of Packet Injection Vulnerabilities–A Case for Identifying Attacker-controlled Implicit Information Leaks.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {with pointer analysis support. To handle the scalability challenge Network protocol security, Implicit information leakage, Static caused by such high sensitivity, we choose a data flow analysis,literature,article},
  Read                     = {未读},
  Review                   = {Static Detection of Packet Injection Vulnerabilities – A Case for Identifying Attacker-controlled Implicit Information Leaks Qi Alfred Chen, Zhiyun Qian†, Yunhan Jack Jia, Yuru Shao, Z. Morley Mao University of Michigan, †University of California, Riverside alfchen@umich.edu, †zhiyunq@cs.ucr.edu, {jackjia, yurushao, zmao}@umich.edu ABSTRACT 1. INTRODUCTION Off-path packet injection attacks are still serious threats to the In- The encryption coverage on today’s Internet is unfortunately still ternet and network security. In recent years, a number of studies rather poor: only 30% [46]. Thus, off-path packet injection attacks have discovered new variations of packet injection attacks, target- remain a serious threat to network security. Recently a number ing critical protocols such as TCP. We argue that such recurring of such attacks and their variants have been reported including off- problems need a systematic solution. In this paper, we design and path TCP packet injection [18,19,37,38] and DNS cache poisoning implement PacketGuardian, a precise static taint analysis tool that attacks [34, 47]. These attacks jeopardize the integrity of network comprehensively checks the packet handling logic of various net- communication, and lead to serious damage where personal data work protocol implementations. The analysis operates in two steps. from unsuspecting users can be leaked when visiting a web site. First, it identifies the critical paths and constraints that lead to ac- Despite application-layer encryption support (e.g., SSL and TLS), cepting an incoming packet. If paths with weak constraints exist, network connections are still vulnerable. For instance, for HTTPS a vulnerability may be revealed immediately. Otherwise, based on connections, the initial request sent by the browser may still be an “secret” protocol states in the constraints, a subsequent analysis is unencrypted HTTP request, and the server subsequently redirects performed to check whether such states can be leaked to an attacker. the client to the HTTPS site. As shown in a recent study [37], an In the second step, observing that all previously reported leaks off-path attacker can inject a legitimate response to the very first are through implicit flows, our tool supports implicit flow tainting, HTTP request. Furthermore, such packet injection attacks can re- which is a commonly excluded feature due to high volumes of false sult in DoS, e.g., by injecting a reset (RST) packet with an inferred alarms caused by it. To address this challenge, we propose the con- TCP sequence number. cept of attacker-controlled implicit information leaks, and prioritize To combat such threats, the network stacks typically implement our tool to detect them, which effectively reduces false alarms with- stringent checks on various fields to verify if an incoming packet is out compromising tool effectiveness. We use PacketGuardian on 6 valid. In fact, a number of RFCs like RFC 5961 [39] are dedicated popular protocol implementations of TCP, SCTP, DCCP, and RTP, to this purpose. However, two problems remain. First, the design and uncover new vulnerabilities in Linux kernel TCP as well as 2 of an RFC may not be formally verified to be secure. Second, even out of 3 RTP implementations. We validate these vulnerabilities if the design is secure, the actual implementation may not always and confirm that they are indeed highly exploitable. conform to the design. In fact, the implementation is generally much more complex and difficult to get right. For instance, it has Categories and Subject Descriptors been shown that TCP implementations on Linux and FreeBSD are significantly weaker than what the RFC recommends regarding the D.4.6 [Operating Systems]: Security and Protection—Informa- mitigation against off-path attacks [38]. This calls for a systematic tion flow controls; C.2.5 [Computer-Communication Networks]: approach to verify protocol implementations. Local and Wide-Area Networks—Internet (e.g., TCP/IP) In this work, we fulfill this very need by developing an effective and scalable static program analysis tool, PacketGuardian, which General Terms can systematically evaluate the robustness (i.e., the level of secu- rity strength) of a network protocol implementation against off-path Security, Program Analysis packet injection attacks. To ensure effectiveness and accuracy, our tool uses a precise context-, flow-, and field-sensitive taint analysis Keywords with pointer analysis support. To handle the scalability challenge Network protocol security, Implicit information leakage, Static caused by such high sensitivity, we choose a data flow analysis analysis, Side channel detection of summary-based approach, which is known to be more scalable compared to other frameworks [43], and is demonstrated to scale Permission to make digital or hard copies of all or part of this work for personal or to very large programs like the Linux kernel [52]. classroom use is granted without fee provided that copies are not made or distributed At a high level, the tool operates by performing analysis in two for profit or commercial advantage and that copies bear this notice and the full cita- steps: (1) Find all paths leading to the program execution point tion on the first page. Copyrights for components of this work owned by others than of accepting an incoming packet. This helps identify the critical ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- checks that a protocol implementation relies on to prevent packet publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. injection, and may directly reveal a packet injection vulnerability CCS’15, October 12–16, 2015, Denver, CO, USA. if any check is weak. (2) Motivated by the observation that strong ©c 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00. checks typically rely on certain hard-to-guess or “secret” commu- DOI: http://dx.doi.org/10.1145/2810103.2813643.

}
}

@Article{Qian2015,
  Title                    = {VULHUNTER: TOWARD and DISCOVERING and VULNERABILITIES IN and ANDROID},
  Author                   = {Chenxiong Qian},
  Journal                  = {IEEE Computer Society},
  Year                     = {2015},

  Booktitle                = {44 Published by the IEEE Computer Society},
  File                     = {:article\\VulHunter Toward Discovering Vulnerabilities in Android Applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {未读},
  Review                   = {................................................................................................................................................................................................................. VULHUNTER: TOWARD DISCOVERING VULNERABILITIES IN ANDROID APPLICATIONS ................................................................................................................................................................................................................. A NEW STATIC-ANALYSIS FRAMEWORK HELPS SECURITY ANALYSTS DETECT VULNERABLE APPLICATIONS. THE AUTHORS DESIGNED AN APP PROPERTY GRAPH (APG), CONDUCTED GRAPH TRAVERSALS OVER APGS, AND REDUCED THE MANUAL-VERIFICATION WORKLOAD. THEY IMPLEMENTED THE FRAMEWORK IN VULHUNTER AND MODELED FIVE TYPES OF VULNERABILITIES. RESULTS SHOWED THAT OF 557 RANDOMLY COLLECTED APPS WITH AT LEAST 1 MILLION INSTALLATIONS, 375 APPS (67.3 PERCENT) HAD AT LEAST ONE VULNERABILITY. ......With the mobile Internet’s pros- (ICFG), a method-call graph (MCG), and a Chenxiong Qian perity, recent years have witnessed an unpre- system dependency graph (SDG) to repre- cedented number of Android applications sent each app. Although the APG is moti- Xiapu Luo (“apps”) published and sold in app markets. vated by the code property graph (CPG),1 Yu Le However, short development cycles and the APG differs from the CPG due to the sig-insufficient security development guidelines nificant difference between apps and C have led to many vulnerable apps. After ana- source codes (see the “Related Work in Vul- Hong Kong Polytechnic lyzing 2,107 apps from companies on the nerability Discovery” sidebar for details). For Forbes Global 2000, HP research recently example, the APG employs the ICFG, University found that 90 percent of apps are vulnerable MCG, and SDG to characterize the frequent (http://zd.net/1FK7I5b). interprocedure and intercomponent commu- Guofei Gu Motivated by recent research,1 we propose nications in apps. The APG also incorporates a new static-analysis framework to facilitate permissions and other unique features in Texas A&M University vulnerability discovery for apps by extracting apps as properties. To ease the identification detailed and precise information from apps, process, we model common vulnerabilities of easing the identification process, and reduc- apps reported in the Common Vulnerabil- ing the manual-verification workload. More ities and Exposures (CVE) system as graph precisely, we design a novel data structure traversals and detect vulnerable apps by con- called the app property graph (APG), which ducting graph traversals over APGs. Note smoothly integrates abstract syntax trees that each app needs to be processed just once (ASTs), an interprocedure control-flow graph for extracting APG and then we can conduct ....................................................... 44 Published by the IEEE Computer Society 0272-1732/15/$31.00c 2015 IEEE

}
}

@Article{QiuSuMa2016,
  Title                    = {Using Reduced Execution Flow Graph to Identify Library Functions in Binary Code},
  Author                   = {Qiu, Jing and Su, Xiaohong and Ma, Peijun},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {187–202},
  Volume                   = {42},

  Doi                      = {10.1109/tse.2015.2470241},
  File                     = {:home/ccc/github/literature/article/Identify Library Functions in Binary Code.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2015.2470241}
}

@Article{RadjenovicHerickoTorkarEtAl2013,
  Title                    = {Software fault prediction metrics: A systematic literature review},
  Author                   = {Radjenović, Danijel and Heričko, Marjan and Torkar, Richard and Živkovič, Aleš},
  Journal                  = {Information and Software Technology},
  Year                     = {2013},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1397–1418},
  Volume                   = {55},

  Doi                      = {10.1016/j.infsof.2013.02.009},
  File                     = {:home/ccc/github/literature/article/Software fault prediction metrics\: A systematic literature review.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2013.02.009}
}

@PhdThesis{Rahimi2013,
  Title                    = {SECURITY VULNERABILITIES: DISCOVERY, PREDICTION, EFFECT, AND MITIGATION},
  Author                   = {Sanaz Rahimi},
  School                   = {Southern Illinois University},
  Year                     = {2013},

  File                     = {:article\\SECURITY VULNERABILITIES DISCOVERY, PREDICTION, EFFECT, AND MITIGATION.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {SECURITY VULNERABILITIES: DISCOVERY, PREDICTION, EFFECT, AND MITIGATION by Sanaz Rahimi M.S., Southern Illinois University, 2007 B.S., University of Southern Mississippi, 2003 A Dissertation Submitted in Partial Fulfillment of the Requirements for the Doctor of Philosophy Degree Department of Computer Science in the Graduate School Southern Illinois University Carbondale May, 2013

}
}

@Article{RahimiZargham2013,
  Title                    = {Vulnerability Scrying Method for Software Vulnerability Discovery Prediction Without a Vulnerability Database},
  Author                   = {Rahimi, S. and Zargham, M.},
  Journal                  = {IEEE Trans. Rel.},
  Year                     = {2013},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {395–407},
  Volume                   = {62},

  __markedentry            = {[ccc:1]},
  Doi                      = {10.1109/tr.2013.2257052},
  File                     = {:home/ccc/github/literature/article/-Vulnerability Scring Method for Software Vulnerability Discovery Prediction Without a Vulnerability Database.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1558-1721},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TR.2013.2257052}
}

@PhdThesis{Ramos2015,
  Title                    = {UNDER-CONSTRAINED SYMBOLIC EXECUTION CORRECTNESS CHECKING and FOR REAL and CODE},
  Author                   = {David A. Ramos},
  School                   = {Stanford University},
  Year                     = {2015},

  File                     = {:article\\UNDER-CONSTRAINED SYMBOLIC EXECUTION_correctness checking for real code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {UNDER-CONSTRAINED SYMBOLIC EXECUTION: CORRECTNESS CHECKING FOR REAL CODE A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY David A. Ramos June 2015

}
}

@InProceedings{Ramos2015a,
  Title                    = {Under-Constrained Symbolic Execution: Correctness Checking for Real Code},
  Author                   = {David A. Ramos},
  Booktitle                = {USENIX 2015},
  Year                     = {2015},

  File                     = {:article\\Under-constrained symbolic execution_correctness checking for real code sec15-paper-ramos.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {Under-Constrained Symbolic Execution: Correctness Checking for Real Code David A. Ramos and Dawson Engler, Stanford University https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/ramos This paper is included in the Proceedings of the 24th USENIX Security Symposium August 12–14, 2015 • Washington, D.C. ISBN 978-1-931971-232 Open access to the Proceedings of the 24th USENIX Security Symposium is sponsored by USENIX

}
}

@InProceedings{Ramos2011,
  Title                    = {Practical, low-effort equivalence verification of real code},
  Author                   = {David A. Ramos},
  Year                     = {2011},

  Abstract                 = {Verifying code equivalence is useful in many situations, such as checking: yesterday’s code against today’s, different implementations of the same (standardized) interface, or an optimized routine against a reference implementation. We present a tool designed to easily check the equivalence of two arbitrary C functions. The tool provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case. It automatically synthesizes inputs to the routines and uses bit-accurate, sound symbolic execution to verify that they produce equivalent outputs on a finite number of paths, even for rich, nested data structures. We show that the approach works well, even on heavily-tested code, where it finds interesting errors and gets high statement coverage, often exhausting all feasible paths for a given input size. We also show how the simple trick of checking equivalence of identical code turns the verification tool chain against itself, finding errors in the underlying compiler and verification tool.
},
  File                     = {:article\\Practical, low-effort equivalence verification of real code-ucklee-cav-2011.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {Practical, low-effort equivalence verification of real code David A. Ramos and Dawson R. Engler Stanford University ramos@cs.stanford.edu, engler@csl.stanford.edu Abstract. Verifying code equivalence is useful in many situations, such as checking: yesterday’s code against today’s, different implementations of the same (standardized) interface, or an optimized routine against a reference implementation. We present a tool designed to easily check the equivalence of two arbitrary C functions. The tool provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case. It automatically synthesizes inputs to the routines and uses bit-accurate, sound symbolic execution to verify that they produce equivalent outputs on a finite number of paths, even for rich, nested data structures. We show that the approach works well, even on heavily-tested code, where it finds interesting errors and gets high statement coverage, often exhausting all feasible paths for a given input size. We also show how the simple trick of checking equivalence of identical code turns the verification tool chain against itself, finding errors in the underlying compiler and verification tool. 1 Introduction Historically, code verification has been hard. Thus, implementors rarely make any effort to do it. We present uc-klee, a modified version of the klee [2] tool designed to make it easy to verify that two routines are equivalent. This ability is useful in many situations, such as checking: different implementations of the same (standardized) interface, different versions of the same implementation, op- timized routines against a reference implementation, and finding compiler bugs by comparing code compiled with and without optimization. Comparing identi- cal code against itself finds bugs in our own tool. Previously, cross checking code that takes inputs with complex invariants or complicated data structures required tediously constructing these inputs by hand. From experience, the non-trivial amount of code needed to do so can easily dwarf the size of the checked code (e.g., as happens when checking small library routines). Manual construction also leads to missed errors caused by over-specificity. For example, when manually building a linked list containing symbolic data, should it have one entry? Two? A hash table should have how many collisions and in which buckets? Creating all possible instances is usually difficult or even impossible. Further, manually specifying pointers (by assigning the concrete address returned by malloc) can limit paths that check relationships

}
}

@InProceedings{RasthoferArztBodden,
  Title                    = {A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks},
  Author                   = {Siegfried Rasthofer and Steven Arzt and Eric Bodden},

  File                     = {:home/ccc/github/literature/article/A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks-slides.pdf:PDF},
  Review                   = {A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks Siegfried Rasthofer, Steven Arzt, Eric Bodden    

}
}

@InProceedings{RasthoferStevenEricEtAl2014,
  Title                    = {A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks},
  Author                   = {Siegfried Rasthofer and \& Steven and Arzt Eric and Bodden and Secure Software and Engineering Group and Secure Software and Engineering Group},
  Year                     = {2014},

  Abstract                 = {Today’s smartphone users face a security dilemma: experience, they also create additional privacy concerns if used many apps they install operate on privacy-sensitive data, although for tracking or monitoring. they might originate from developers whose trustworthiness is hard to judge. Researchers have addressed the problem with To address this problem, researchers have proposed various more and more sophisticated static and dynamic analysis tools analysis tools to detect and react to data leaks, both statically as an aid to assess how apps use private user data. Those tools, [1]–[13] and dynamically [14]–[17]. Virtually all of these tools however, rely on the manual configuration of lists of sources of are configured with a privacy policy, usually defined in terms sensitive data as well as sinks which might leak data to untrusted of lists of sources of sensitive data (e.g., the user’s current observers. Such lists are hard to come by. location) and sinks of potential channels through which such
},
  Doi                      = {.org/doi-info-to-be-provided-later},
  File                     = {:home/ccc/github/literature/article/A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks-paper.pdf:PDF},
  Review                   = {A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks Siegfried Rasthofer & Steven Arzt Eric Bodden Secure Software Engineering Group Secure Software Engineering Group EC SPRIDE, Technische Universita¨t Darmstadt Fraunhofer SIT & Technische Universita¨t Darmstadt {firstname.lastname}@ec-spride.de eric.bodden@sit.fraunhofer.de Abstract—Today’s smartphone users face a security dilemma: experience, they also create additional privacy concerns if used many apps they install operate on privacy-sensitive data, although for tracking or monitoring. they might originate from developers whose trustworthiness is hard to judge. Researchers have addressed the problem with To address this problem, researchers have proposed various more and more sophisticated static and dynamic analysis tools analysis tools to detect and react to data leaks, both statically as an aid to assess how apps use private user data. Those tools, [1]–[13] and dynamically [14]–[17]. Virtually all of these tools however, rely on the manual configuration of lists of sources of are configured with a privacy policy, usually defined in terms sensitive data as well as sinks which might leak data to untrusted of lists of sources of sensitive data (e.g., the user’s current observers. Such lists are hard to come by. location) and sinks of potential channels through which such We thus propose SUSI, a novel machine-learning guided data could leak to an adversary (e.g., a network connection). As approach for identifying sources and sinks directly from the code an important consequence, no matter how good the tool, it can of any Android API. Given a training set of hand-annotated only provide security guarantees if its list of sources and sinks sources and sinks, SUSI identifies other sources and sinks in is complete. If a source is missing, a malicious app can retrieve the entire API. To provide more fine-grained information, SUSI its information without the analysis tool noticing. A similar further categorizes the sources (e.g., unique identifier, location problem exists for information written into unrecognized sinks. information, etc.) and sinks (e.g., network, file, etc.). This work focuses on Android. As we show, existing For Android 4.2, SUSI identifies hundreds of sources and analysis tools, both static and dynamic, focus on a handful of sinks with over 92% accuracy, many of which are missed by current information-flow tracking tools. An evaluation of about hand-picked sources and sinks, and can thus be circumvented by 11,000 malware samples confirms that many of these sources malicious applications with ease. It would be too simple, though, and sinks are indeed used. We furthermore show that SUSI to blame the developers of those tools. Android’s version 4.2, can reliably classify sources and sinks even in new, previously for instance, comprises about 110,000 public methods, which unseen Android versions and components like Google Glass or makes a manual classification of sources and sinks clearly the Chromecast API. infeasible. Furthermore, each new Android version includes new functionality (e.g., NFC in Android 2.3 or Restricted Profiles in the brand-new Android 4.3) which often also leads to new I. INTRODUCTION sources and sinks. This shows that a manual identification of sources and sinks is impractical. It would impose a high Current smartphone operating systems, such as Android or workload on the analyst and would have to be done again for iOS, allow users to run a multitude of applications developed every new Android version. Additionally, hand-picking is an by many independent developers available through various app error-prone task. markets. While this flexibility is very convenient for the user, as one will find a suitable application for almost every need, We therefore propose SUSI, an automated machine-learning it also makes it hard to determine the trustworthiness of these guided approach for identifying sources and sinks directly from applications. the code of an Android API. We have identified both semantic and syntactic features to train a model for sources and sinks on Smartphones are widely used to store and process highly a small subset of hand-classified Android API methods. SUSI sensitive information such as text messages, private and business can then use this model to classify arbitrarily large numbers contacts, calendar data, and more. Furthermore, while a large of previously unknown Android API methods. In the Android variety of sensors like GPS allow a context-sensitive user 4.2 operating system, SUSI finds several hundred sources and sinks, only a small fraction of which were previously known Permission to freely reproduce all or part of this paper for noncommercial from the scientific literature or included in configurations of purposes is granted provided that copies bear this notice and the full citation available analysis tools. on the first page. Reproduction for commercial purposes is strictly prohibited without the prior written consent of the Internet Society, the first-named author While SUSI is not able to identify each and every source (for reproduction of an entire paper only), and the author’s employer if the or sink, it resembles a practical best-effort solution that paper was prepared within the scope of employment. solves the problem to a large extent, which is a substantial NDSS ’14, 23-26 February 2014, San Diego, CA, USA Copyright 2014 Internet Society, ISBN 1-891562-35-5 improvement over existing hand-picked sets. In cross-validation, http://dx.doi.org/doi-info-to-be-provided-later SUSI achieves a precision and recall of over 92%, which means

}
}

@InProceedings{RaymondChangBudrisEtAl0419,
  Title                    = {LUMPED-MODEL ANALYSIS OF MICROCIRCUIT VULNERABILITY' by},
  Author                   = {J. P. Raymond and W. W. Chang and R. E. Budris and Northrop Corporate and Laboratories},
  Year                     = {0419},

  File                     = {:home/ccc/github/literature/article/Lumped-model analysis of microcircuit vulnerability.pdf:PDF},
  Review                   = {LUMPED-MODEL ANALYSIS OF MICROCIRCUIT VULNERABILITY' by J. P. Raymond, W. W. Chang and R. E. Budris Northrop Corporate Laboratories Hawthorne, California 90250 SunTnary Ionizing radiation effects in simple mon- p-type substrate, n-type transistor collector, olithic microcircuits have been predicted from p-type transistor base, and the n± transistor the microcircuit geometry, electrical measure - emitter. As shown in Figure 1, the resistor ments, and basic semiconductor parameters. element is formed from the transistor p-type Using the lumped-model technique, the base region, with the n-type transistor region mathematical model of the microcircuit is derived used as the resistor isolation tub. Diode directly from the electronic characteristics and elements, not shown, are fabricated as a special boundary values of each bulk semiconductor region. two-terminal case of the transistor element. Basic parameter values of the circuit model are obtained by correlating calculated results to the experimental data. Ionizing radiation effects are included as the radiation carrier generation per unit volume. Since the ionizing radiation effect is included in ternms of the bulk semiconductor parameters, the radiation-inclusive lumped model is a straightforward extension of the electrical model. Results are presented for (a) the large- TRANSISTOR signal electrical and radiation-induced response ELEMENT of junction-isolated and dielectric-isolated DTL gates and (b) the small-signal electrical and radiation-induced response of a junction-isolated differential amplifier. Calculation of the electrical and radiation-induced responses RESISTOR ELEMENTwas enabled with the use of SCEPTREI and ECAP2 circuit analysis computer codes, for the gate CROSS SECTION and amplifier analyses, respectively. Use of I lumped models in the classical circuit element con-puter codes was accomplished by deriving the analogous R-C-I networks corresponding to the lumped-models. Figure 1. Four-region microcircuit elements Representation of the electrical perform- ance and radiation-induced response of the micro- circuits was generally very good. The major The lumped-model technique uses adifferential-difference approximation to the source of error in the analysis is in the representation of the diffused resistor photo- partial differential equations describing carrier response, which in this case, did not include the distribution and motion in the semiconductor. effects of interleg coupling and conductivity Each bulk semiconductor region of a device is modulation. divided into a finite number of regions. Foreach region terminal current-carrier density relationships are written which involve the Introduction average geometric and bulk semiconductorparameters3. Boundary values on the bulk The monolithic integrated circuit is semiconductor regions are given by ohmica complex cQntacts, or by the relationship between thesemiconductor device formed from successive diffusions into a single semi- excess.carrier density and applied voltage at a conductor wafer. Resistor p-n junction.and transistor element geometry and cross section of a four-region, junction-isolated circuit General form of the transistor elementare illustrated in Figure 1. All elements lumped model used in the integrated circuitare formed from the four major bulk analysis is shown in Figure 2. The majorsemi- conductor regions: electrical properties of the transistor are represented by the two-lump approximation of 'This work was sponsored by DASA and the the base region. Combinances, Hcl and Hc2, U. S. Army Electronics Command under represent base minority carrier recombination, USAECOM Contract DA-AB07-67-C-0419. 271

}
}

@Unpublished{References1989,
  Title                    = {Secure Coding in C and C++, Second Edition},
  Author                   = {References},
  Year                     = {1989},

  File                     = {:article\\Secure Coding in C and C++.pdf:PDF},
  Groups                   = {source code vulnerability},
  Number                   = {Science},
  Publisher                = {IEEE},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Secure Coding in C and C++, Second Edition References [Aleph 1996] "Aleph One. Smashing the Stack for Fun and Profit." Phrack 7(49), 1996. [Alexander 2003] Alexander, I. "Misuse Cases: Use Cases with Hostile Intent." IEEE Software 20(1): 58–66, 2003. [Alexandrescu 2010] Alexandrescu, A. The D Programming Language. Upper Saddle River, NJ: Addison-Wesley, 2010. [Alhazmi 2005a] Alhazmi, O. H. and Y. K. Malaiya. Modeling the Vulnerability Discovery Process. In Proceedings of the 16th IEEE International Symposium on Software Reliability Engineering: ISSRE 2005, Chicago, November 8–11, 2005. Los Alamitos, CA: IEEE Computer Society Press, 2005. [Alhazmi 2005b] Alhazmi, O.; Y. K. Malaiya; & I. K. Ray. Security Vulnerabilities in Software Systems: A Quantitative Perspective Technical Report, CS T&R, AMR05. Fort Collins: Computer Science Department, Colorado State University, 2005. [Allen 2001] Allen, J. H. The CERT Guide to System and Network Security Practices. Boston: Addison- Wesley, 2001. [Amarasinghe 2007] Amarasinghe, S. Lecture 4, "Concurrent Programming," 6.189 IAP 2007. MIT, 2007. [Andersen 2004] Andersen, D., D. M. Cappelli, J. J. Gonzalez, M. Mojtahedzadeh, A. P. Moore, E. Rich, J. M. Sarriegui, T. J. Shimeall, J. M. Stanton, E. A. Weaver and A. Zagonel. Preliminary System Dynamics Maps of the Insider Cyber-Threat Problem. In Proceedings of the 22nd International Conference of the System Dynamics Society, Oxford, England, July 25–29, 2004. Albany, NY: System Dynamics Society, 2004. [ANSI 1989] American National Standards Institute. American National Standard for Information Systems: Programming Language C (X3.159-1989). Washington, DC: Author, 1989. 

}
}

@Article{Reps1998,
  Title                    = {Program analysis via graph reachability},
  Author                   = {Reps, Thomas},
  Journal                  = {Information and Software Technology},
  Year                     = {1998},

  Month                    = {Dec},
  Number                   = {11-12},
  Pages                    = {701鈥�726},
  Volume                   = {40},

  Doi                      = {10.1016/s0950-5849(98)00093-7},
  File                     = {:article\\Program analysis via graph reachability.pdf:PDF},
  Groups                   = {Software Basic Theory, source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1016/S0950-5849(98)00093-7}
}

@InProceedings{SamateandevaluatingstaticanalysistoolsARTICLE·JANUARY20072289,
  Title                    = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication},
  Author                   = {Samateandevaluatingstaticanalysistools and ARTICLE·JANUARY2007},
  Year                     = {2289},

  File                     = {:article\\Samate and evaluating static analysis tools.pdf:PDF},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/228996566 Samateandevaluatingstaticanalysistools ARTICLE·JANUARY2007 CITATIONS READS 4 36 1AUTHOR: PaulEvanBlack NationalInstituteofStandardsandTechnolo… 54PUBLICATIONS1,078CITATIONS SEEPROFILE Availablefrom:PaulEvanBlack Retrievedon:10November2015

}
}

@InProceedings{SaxenaAkhaweHannaEtAl2010,
  Title                    = {A Symbolic Execution Framework for JavaScript},
  Author                   = {Prateek Saxena and Devdatta Akhawe and Steve Hanna and Feng Mao and Stephen McCamant and Dawn Song and Computer Science Division and EECS Department},
  Year                     = {2010},

  Abstract                 = {As AJAX applications gain popularity, client-side XMLHttpRequest, and data from code running concur- JavaScript code is becoming increasingly complex. However, rently in other browser windows. Each kind of input string few automated vulnerability analysis tools for JavaScript exist. In this p has its own format, so developers use a combination of cus-aper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution tom routines and third-party libraries to parse and validate. To handle JavaScript code’s complex use of string operations, the inputs they receive. To effectively explore a program’s we design a new language of string constraints and implement execution space, a tool must be able to supply values for all a solver for it. We build an automatic end-to-end tool, Kudzu, of these different kinds of inputs and reason about how they and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 li are parsed and validated.ve web applications, Kudzu automatically discovers 2 previously unknown vulner- Approach. In this paper, we develop the first com- abilities and 9 more that were previously found only with a plete symbolic-execution based framework for client-side manually-constructed test suite. JavaScript code analysis. We build an automated, stand-
},
  File                     = {:article\\A Symbolic Execution Framework for JavaScript.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {web security; symbolic execution; string decision alone tool that, given a URL for a web application, automat- procedures ically generates high-coverage test cases to systematically},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {A Symbolic Execution Framework for JavaScript Prateek Saxena, Devdatta Akhawe, Steve Hanna, Feng Mao, Stephen McCamant, Dawn Song Computer Science Division, EECS Department University of California, Berkeley {prateeks, devdatta, sch, fmao, smcc, dawnsong}@cs.berkeley.edu Abstract—As AJAX applications gain popularity, client-side XMLHttpRequest, and data from code running concur- JavaScript code is becoming increasingly complex. However, rently in other browser windows. Each kind of input string few automated vulnerability analysis tools for JavaScript exist. In this p has its own format, so developers use a combination of cus-aper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution tom routines and third-party libraries to parse and validate. To handle JavaScript code’s complex use of string operations, the inputs they receive. To effectively explore a program’s we design a new language of string constraints and implement execution space, a tool must be able to supply values for all a solver for it. We build an automatic end-to-end tool, Kudzu, of these different kinds of inputs and reason about how they and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 li are parsed and validated.ve web applications, Kudzu automatically discovers 2 previously unknown vulner- Approach. In this paper, we develop the first com- abilities and 9 more that were previously found only with a plete symbolic-execution based framework for client-side manually-constructed test suite. JavaScript code analysis. We build an automated, stand- Keywords-web security; symbolic execution; string decision alone tool that, given a URL for a web application, automat- procedures ically generates high-coverage test cases to systematically I. INTRODUCTION explore its execution space. Automatically reasoning about the operations we see in real JavaScript applications requires Rich web applications have a significant fraction of their a powerful constraint solver, especially for the theory of code written in client-side scripting languages, such as strings. However, the power needed to express the semantics JavaScript. As an increasing fraction of code is found on of JavaScript operations is beyond what existing string the client, client-side security vulnerabilities (such as client- side code injection [20], [26]–[ constraint solvers [14], [18] offer. As a central contribution28]) are becoming a promi- of this work, we overcome this difficulty by proposing a nent threat. However, a majority of the research on web constraint language and building a practical solver (called vulnerabilities so far has focused on server-side application Kaluza) that supports the specification of boolean, machine code written in PHP and Java. There is a growing need for integer (bit-vector), and string constraints, including regular powerful analysis tools for the client-side components of expressions, over multiple variable-length string inputs. This web applications. This paper presents the first techniques language’s rich support for string operations is crucial for and system for automatically exploring the execution space reasoning about the parsing and validation checks that of client-side JavaScript code. To explore this execution JavaScript applications perform. space, our techniques generate new inputs to cover a pro- g To show the practicality of our constraint language, weram’s value space using dynamic symbolic execution of JavaScript, and to cover its event space by automatic GUI detail a translation from the most commonly used JavaScript string operations to our constraints. This translation also exploration. Dy harnesses concrete information from a dynamic executionnamic symbolic execution for JavaScript has numerous applications in web security. In this pap of the program in a way that allows the analysis to scale.er we focus on one of these applicati We analyze the theoretical expressiveness of the theory ofons: automatically finding client-side code injection vulnerabilities. A cli strings supported by our language (including in comparisonent-side code injection attack occurs when client-side code passes untrusted input t to existing constraint solvers), and bound its computationalo a dy complexity. We then give a sound and complete decisionnamic code evaluation construct, without proper validation or sanitization, allowing an attacker to inj procedure for the bounded-length version of the constraintect JavaScript code that runs with the privil language. We develop an end-to-end system, called Kudzu,eges of a web application. JavaScript executi that performs symbolic execution with this constraint solveron space exploration is challenging for many reasons. In particular, JavaScript applications accept at its core. many kinds of input, and those inputs are structured just End-to-end system. We identify further challenges in build- as strings. For instance, a typical application might take ing an end-to-end automated tool for rich web applications. user input from form fields, messages from its server via For instance, because JavaScript code interacts closely with a

}
}

@Article{SbirleaBurkeGuarnieriEtAl2013,
  Title                    = {Automatic detection of inter-application permission leaks in Android applications},
  Author                   = {Sbirlea, D. and Burke, M. G. and Guarnieri, S. and Pistoia, M. and Sarkar, V.},
  Journal                  = {IBM J. Res. \& Dev.},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {10:1–10:12},
  Volume                   = {57},

  Doi                      = {10.1147/jrd.2013.2284403},
  File                     = {:home/ccc/github/literature/article/Automatic detection of inter-application permission leaks in android applications.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0018-8646},
  Publisher                = {IBM},
  Url                      = {http://dx.doi.org/10.1147/JRD.2013.2284403}
}

@Article{ScandariatoWaldenHovsepyanEtAl2014,
  Title                    = {Predicting Vulnerable Software Components via Text Mining},
  Author                   = {Scandariato, Riccardo and Walden, James and Hovsepyan, Aram and Joosen, Wouter},
  Journal                  = {Ieee Transactions On Software Engineering},
  Year                     = {2014},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {993--1006},
  Volume                   = {40},

  __markedentry            = {[ccc:6]},
  Abstract                 = {This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e. g., to identify the components needing special scrutiny.},
  Doi                      = {10.1109/TSE.2014.2340398},
  Ei                       = {1939-3520},
  Groups                   = {Code Mining},
  Oi                       = {Ahirwal, Ashish/0000-0002-8247-9793},
  Ri                       = {Ahirwal, Ashish/B-8886-2015},
  Sn                       = {0098-5589},
  Tc                       = {1},
  Ut                       = {WOS:000343899100004},
  Z8                       = {0},
  Z9                       = {1},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{ScandariatoWaldenHovsepyanEtAl2014a,
  Title                    = {Predicting Vulnerable Software Components via Text Mining},
  Author                   = {Scandariato, Riccardo and Walden, James and Hovsepyan, Aram and Joosen, Wouter},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2014},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {993–1006},
  Volume                   = {40},

  Doi                      = {10.1109/tse.2014.2340398},
  File                     = {:home/ccc/github/literature/article/-Predicting Vulnerable Software Components via Text Mining.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/TSE.2014.2340398}
}

@InProceedings{Schmidt1998,
  Title                    = {Data Flow Analysis is Model Checking of Abstract Interpretations},
  Author                   = {David A. Schmidt},
  Year                     = {1998},

  Abstract                 = {busy expressions at a program point, p,
},
  File                     = {:article\\Data Flow Analysis is Model Checking of Abstract Interpretations.pdf:PDF},
  Groups                   = {source code vulnerability},
  Review                   = {Data Flow Analysis is Model Checking of Abstract Interpretations David A. Schmidt Computing and Information Sciences Department 0 Kansas State University Abstract busy expressions at a program point, p, VBE(p) = This expository paper simplies and claries Steen's T 0 Used(p) [ (notModied(p) \ ( VBE(p ))) 0 depiction of data ow analysis (d.f.a. ) as model p 2succ p checking: By employing abstract interpretation (a.i. ) is mechanically reformatted into a proposition in the to generate program traces and by utilizing Kozen's modal mu-calculus [37] that states the very-business of modal mu-calculus to express trace properties, we ex- an expression, e, at a program point: press in simplest possible terms that a d.f.a. is a model check of a program's a.i. trace. In particular, the clas- isVBE(e) = Z: isUsed(e) _ (:isModied(e) ^2Z) sic ow equations for bit-vector-based d.f.a. s reformat trivially into modal mu-calculus formulas. A surprising When the latter is model checked on a program's consequence is that two of the classical d.f.a. s are ex- control- ow graph (which is the simplest possible safe posed as unsound; this problem is analyzed and simply a.i.of the program), one obtains exactly the informa- repaired. In the process of making the above discover- tion calculated by the iterative d.f.a. The other classic ies, we clarify the relationship between a.i. and d.f.a. in d.f.a. s decode similarly, and two of them (live variables terms of the often-misunderstood notion of collecting analysis and reaching denitions) are found to be un- semantics and we highlight how the research areas of sound. This surprising development is explained and ow analysis, abstract interpretation, and model check- repaired in a simple way. ing have grown together. In the process of developing the above results, we observe that (i) model checking can be applied to any a.i. , not just a control- ow graph; and (ii) an iterative 1 Introduction d.f.a. can be dened by any modal-mucalculus formula, not just a bit-vector-based one. These facts become Folklore tells us that abstract interpretation (a.i. ) is obvious because we use trace-based abstract interpre- the \theory" of static analysis and data ow analysis tation [55, 53], an operational-semantics-based version (d.f.a. ) is the \practice." This isn't quite so, and of abstract intepretation that represents a program's this paper claries the relation between a.i. and d.f.a. in a.i.as a computation tree of traces [44]. (As noted terms of a third concept, model checking. above, a program's simplest a.i. -based computation Utilizing Steen's crucial observation that one can tree is its control ow graph; of course, there are many dene a ow analysis by a modal logic formula [59, more interesting abstract computation trees.) Using 60], we demonstrate in simplest possible terms that an trace-based a.i. s, we can explain simply the crucial iterative d.f.a. is a model check of a modal logic formula and often-misunderstood concept of collecting seman- on a program's a.i. Here is one famous example: The tics; we note that there exist multiple versions of col- bit-vector ow equation that denes the set of very- lecting semantics for a given a.i. , and we employmodal mu-calculus formulas to extract particular instances of 0 234 Nichols Hall, Manhattan, KS 66506 USA. collecting semantics. From here, it is easy to see how schmidt@cis.ksu.edu. Supported by NSF CCR-9633388. iterative d.f.a. s compute representations of collecting This paper will appear in the 1998 ACM Symp. Principles of Programming Languages; copyright belongs to ACM. semantics dened by modal mu-calculus formulas. The developments in this paper should make clear that the classic d.f.a. s merely scratch the surface as to the forms of iterative static analysis one can perform| the combination of the abstract interpretation design space and the modal mu-calculus denes the design

}
}

@Article{Schryen2011,
  Title                    = {Is open source security a myth?},
  Author                   = {Schryen, Guido},
  Journal                  = {Commun. ACM},
  Year                     = {2011},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {130},
  Volume                   = {54},

  Doi                      = {10.1145/1941487.1941516},
  File                     = {:home/ccc/github/literature/article/-CACM_-_Is_open_source_security_a_myth.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0001-0782},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/1941487.1941516}
}

@Article{ScienceDirect2015,
  Title                    = {SIGMA A Semantic Integrated Graph Matching Approach for identifying reused functions in binary code},
  Author                   = {Contents lists available at ScienceDirect},
  Journal                  = {Digital Investigation},
  Year                     = {2015},

  Doi                      = {.org/10.1016/j.diin.2015.01.011},
  File                     = {:article\\SIGMA A Semantic Integrated Graph Matching Approach for identifying reused functions in binary code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Function identiﬁcation The capability of efﬁciently recognizing reused functions for binary code is critical to many a b s t r a c t DFRWS 2015 Europe,literature,article},
  Read                     = {未读},
  Review                   = {文中提出了一个Semantic Integrated Graph}
}

@InProceedings{,
  Title                    = {Science of Computer Programming 97 (2015)},
  Author                   = {Contents lists available at ScienceDirect},
  Year                     = {2015},

  Doi                      = {.org/10.1016/j.scico.2013.11.010},
  File                     = {:home/ccc/github/literature/article/Source-code queries with graph databases?with application to programming language usage and evolution.pdf:PDF},
  Review                   = {Science of Computer Programming 97 (2015) 127?134
Contents lists available at ScienceDirect
S to
R
Co
h
? ? ?
a
Ar Re Ac Av
Ke Pr So Gr
01 ht Science of Computer Programming
www.elsevier.com/locate/scico
ource-code queries with graph databases?with application programming language usage and evolution
aoul-Gabriel Urma, Alan Mycroft
mputer Laboratory, University of Cambridge, United Kingdom
i g h l i g h t s
A scalable source-code querying system which stores full source-code detail. Queries on syntax, types and data ?ow via overlay relations (database views). Based on the graph data model, with prototype (?Wiggle?) using Neo4j.
r t i c l e i n f o a b s t r a c t
ticle history: Program querying and analysis tools are of growing importance, and occur in two main ceived 30 September 2013 variants. Firstly there are source-code query languages which help software engineers to cepted 5 November 2013 explore a system, or to ?nd code in need of refactoring as coding standards evolve. These ailable online 12 November 2013 also enable language designers to understand the practical uses of language features and
ywords: idioms over a software corpus. Secondly there are program analysis tools in the style of
ogramming language evolution Coverity which perform deeper program analysis searching for bugs as well as checking
urce-code queries and DSLs adherence to coding standards such as MISRA. aph databases The former class are typically implemented on top of relational or deductive databases
and make ad-hoc trade-offs between scalability and the amount of source-code detail held?with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-speci?c languages to specify analyses. We argue that a graph data model and associated query language provides a unifying conceptual model and gives e?cient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-?ow-graph or data-?ow levels. We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.
� 2013 Elsevier B.V. All rights reserved.
On the occasion of his 65th birthday, we thank Paul Klint for his contributions to knowledge, for his role as co-founder and initial president of the European Association of Programming Languages and Systems (eapls.org), for his recommen- dations on software patents in Europe, and for generally being a good guy. Happy Birthday! Congratulations!
E-mail addresses: raoul.urma@cl.cam.ac.uk (R.-G. Urma), alan.mycroft@cl.cam.ac.uk (A. Mycroft).
67-6423/$ ? see front matter � 2013 Elsevier B.V. All rights reserved. tp://dx.doi.org/10.1016/j.scico.2013.11.010

}
}

@InProceedings{,
  Title                    = {Information and Software Technology 55 (2013)},
  Author                   = {Contents lists available at SciVerse and ScienceDirect },
  Year                     = {2013},

  Doi                      = {.org/10.1016/j.infsof.2012.11.007},
  File                     = {:home/ccc/github/literature/article/A comparison of the efficiency and effectiveness of vulnerability discovery techniques.pdf:PDF},
  Keywords                 = {Security Vulnerability Static analysis Penetration testing Black box testing White box testing},
  Review                   = {Information and Software Technology 55 (2013) 1279?1288
Contents lists available at SciVerse ScienceDirect
of
.e l
iv
?
Article history: Context: Security vulnerabilities discovered later in the development cycle are more expensive to ?x than Available online 8 December 2012
Keywords: Security Vulnerability Static analysis Penetration testing Black box testing White box testing
guidance on the use of these techniques. However, similar empir- tected using each and comparing their ef?ciencies. four vulnerability ecord (EH alyzed inc
ners lack evidence-based guidance on the use of vulnerability dis- exploratory manual penetration testing, systematic manua covery techniques. tration testing, automated penetration testing, and automat
In his book Software Security: Building Security In, Gary McGraw tic analysis. The ?rst author used these four techniques on draws on his experience as a security researcher and claims: ??Secu- Electronic Clinician Health Record (eCHR)1 and OpenEMR.2 These rity problems evolve, grow, and mutate, just like species on a con- two systems are currently used within the United States to store pa- tinent. No one technique or set of rules will ever perfectly detect all tient records. Tolven eCHR and OpenEMR are web-based systems. security vulnerabilities?? [1]. Instead, he advocates using a variety This paper adds the same analysis conducted on an additional
? Corresponding author. 1 http://sourceforge.net/projects/tolven/. E-mail addresses: andrew_austin@ncsu.edu (A. Austin), cmholmgr@ncsu.edu 2 http://www.oemr.org/. (C. Holmgreen), williams@csc.ncsu.edu (L. Williams). ical results on the effectiveness and ef?ciency of vulnerability dis- In previous work [2], the ?rst author analyzed covery techniques, such as security-focused automated static discovery techniques on two electronic health r analysis and penetration testing are sparse. As a result, practitio- tems. The vulnerability discovery techniques an 0950-5849/$ - see front matter  2012 Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.infsof.2012.11.007 R) sys- luded: l pene- ed sta- Tolven 1. Introduction of vulnerability discovery and prevention techniques throughout the software development lifecycle. McGraw?s claim, however, is
Results of decades of empirical research on effectiveness and based upon his experience and is not substantiated with empirical ef?ciency of fault and failure discovery techniques, such as unit evidence. The objective of this research is to aid in the selection of vul- testing and inspections, can be used to provide evidence-based nerability discovery techniques by comparing the vulnerabilities de- those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities dif?cult. A number of vulnerability discovery techniques are available, each with their own strengths. Objective: The objective of this research is to aid in the selection of vulnerability discovery techniques by comparing the vulnerabilities detected by each and comparing their ef?ciencies. Method: We conducted three case studies using three electronic health record systems to compare four vulnerability discovery techniques: exploratory manual penetration testing, systematic manual penetra- tion testing, automated penetration testing, and automated static analysis. Results: In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered that the speci?c set of vulnerabilities identi?ed by one tool was largely orthogonal to that of other tools. Systematic manual penetration testing found the most design ?aws, while automated static analysis found the most implementation bugs. The most ef?cient discovery tech- nique in terms of vulnerabilities discovered per hour was automated penetration testing. Conclusion: The results show that employing a single technique for vulnerability discovery is insuf?cient for ?nding all types of vulnerabilities. Each technique identi?ed only a subset of the vulnerabilities, which, for the most part were independent of each other. Our results suggest that in order to discover the greatest variety of vulnerability types, at least systematic manual penetration testing and automated static analysis should be performed.
 2012 Elsevier B.V. All rights reserved. a r t i c l e i n f o a b s t r a c t Information and S
journal homepage: www
A comparison of the ef?ciency and effect discovery techniques
Andrew Austin, Casper Holmgreen, Laurie Williams Department of Computer Science, North Carolina State University, Raleigh 27695, USA tware Technology
sevier .com/locate / infsof
eness of vulnerability

}
}

@InProceedings{,
  Title                    = {Computers and Mathematics with Applications 63 (2012)},
  Author                   = {Contents lists available at SciVerse and ScienceDirect },
  Year                     = {2012},

  Doi                      = {10.1016/j.camwa.2011.08.001},
  File                     = {:home/ccc/github/literature/article/Static program analysis assisted dynamic taint tracking for software vulnerability discovery.pdf:PDF},
  Keywords                 = {The evolution of computer science has exposed us to the growing gravity of security Taint analysis problems and threats. Dynamic taint analysis is a prevalent approach to protect a program Software vulnerability Code coverage from malicious behaviors, but fails to provide any information about the code which Data flow analysis is not executed. This paper describes a novel approach to overcome the limitation of},
  Review                   = {Computers and Mathematics with Applications 63 (2012) 469?480
Contents lists available at SciVerse ScienceDirect
Computers and Mathematics with Applications
journal homepage: www.elsevier.com/locate/camwa
Static program analysis assisted dynamic taint tracking for software vulnerability discovery Ruoyu Zhang a,?, Shiqiu Huang a, Zhengwei Qi a, Haibing Guan b a School of Software, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China b School of Electronic Information and Electrical Engineering, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, China
a r t i c l e i n f o a b s t r a c t
Keywords: The evolution of computer science has exposed us to the growing gravity of security Taint analysis problems and threats. Dynamic taint analysis is a prevalent approach to protect a program Software vulnerability Code coverage from malicious behaviors, but fails to provide any information about the code which Data flow analysis is not executed. This paper describes a novel approach to overcome the limitation of
traditional dynamic taint analysis by integrating static analysis into the system and presents framework SDCF to detect software vulnerabilities with high code coverage. Our experiments show that SDCF is not only able to provide efficient runtime protection by introducing an overhead of 4.16� based on the taint tracing technique, but is also capable of discovering latent software vulnerabilities which have not been exploited, and achieve code coverage of more than 90%.
� 2011 Elsevier Ltd. All rights reserved.
1. Introduction
1.1. Motivation
In the last decade, software vulnerabilities have threatened computer security severely. Malicious users are able to gain access to confidential information inside the target program, even take control of it by taking advantage of design flaws. Take notorious buffer overflow as an instance, attackers can exploit this software vulnerability bymanipulating the software input, and cause an overwrite in the stack in order to control the execution stream of the program [1].
Taint analysis is a prevalent approach to detect malicious behavior in recent years. Based on the concept that some data (such as the input from the user) is not trustworthy, taint analysis is proposed to keep track of the data which can be used to harm the software, and monitor suspicious actions. There exists several researches concerned with taint analysis and the details will be described in Section 2.
In current researches, the taint analysis technique is usually realized as a runtime method and referred to as dynamic taint analysis. Regardless of the implementation of the dynamic taint analysis, there is one limitation in current researches. Due to the fact that the dynamic taint analysis can only detect software vulnerabilities when the attack has been launched, it is impossible to locate the latent software weak spots, which is very desirable in many cases.
An example is presented in Fig. 1 to show the motivation of our approach. This vulnerability is exposed as CVE-2007- 6454, and described with C++ code for clarity while our system is implemented to handle binary. PeerCast is an open source streaming media multicast tool. HandshakeHTTP is a procedure in PeerCast to handle http packages. The input package is controlled by users. The programmer attempts to copy the password and other information from the input package into two
? Corresponding author. E-mail addresses: holmeszry@sjtu.edu.cn (R. Zhang), hsqfire@sjtu.edu.cn (S. Huang), qizhwei@sjtu.edu.cn (Z. Qi), hbguan@sjtu.edu.cn (H. Guan).
0898-1221/$ ? see front matter� 2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.camwa.2011.08.001

}
}

@InProceedings{ScrumLargeProjectsEtAl1110,
  Title                    = {Egyptian Computer Science Journal (ISSN-1110-2586) Volume 40 – Issue1 January},
  Author                   = {Paired Scrum and for Large and Projects and Fatma A. El-Licy},
  Year                     = {1110},

  File                     = {:home/ccc/github/literature/article/Paired Scrum for Large Projects.pdf:PDF},
  Keywords                 = {Software Engineering, Agile Development, Scaling Scrum Framework, Scrum for},
  Review                   = {Egyptian Computer Science Journal (ISSN-1110-2586) Volume 40 – Issue1 January 2016 Paired Scrum for Large Projects Fatma A. El-Licy Department of Computer and Information Sciences, Institute of Statistical Studies and Research Cairo University, Giza Why2fatma@yahoo.com Abstract Software has become part of all aspects of our lives, and organizations are increasingly conceiving extremely large and complex software projects. Agile development is a method of building software by empowering and trusting people, acknowledging change as norm, and promoting constant delivery of small pieces of working code. Yet, for large project, scaling Agile Scrum framework requires multi level management to coordinate the efforts and integrate the smaller pieces of the working code. An iterative paired Scrum framework is proposed to manage the development process of large project while exploiting the benefits of Agile methodology. Top-down requirement categorization and modulation is achieved to allow for the short cycle of the sprint in Agile Scrum framework. Requirement modularity while employing Scrum development methodology will, not only, help managing large project but also facilitating and acceleration its deployment. The different categories of the requirements constitute the Sprint modules, which iteratively reformed into production architecture. The objective of this research paper is to establish a framework model that pairs modularity with Scrum methodology. The framework employs three levels of process development and management to facilitate and minimize deployment efforts, without, compromising the agility characteristics of the Agile Scrum framework. Keywords: Software Engineering, Agile Development, Scaling Scrum Framework, Scrum for Large projects, Requirement Modulation, Testing and Deployment. 1. Introduction Agile approaches embrace task simplicity, fast turnaround for smaller pieces of working code and human interaction over email/document/process oriented interaction. Simpler tasks make it harder for developers to misunderstand what the code is meant to do [1, 2]. The most popular approach of Agile today, is Agile with Scrum [3, 4]. Each characteristic of this approach has been selected to support agility. For example short development ‘sprints’ are used rather than the 18-month cycles common to traditional Waterfall development [5, 6, 7]. In scrum, requirements are broken down into Sprints which are one-to-four week coding cycles with specific tasks to be performed (not necessarily features to be developed). For large project, however, number of sprints could be hard to manage as individual tasks given limited number of developers. Several approaches were presented in the literature to improve the scalability of Agile approach in general, focusing in deferent aspects of software engineering practices, [8, 9,10,11, 12, 13,14] and Scum in particular [8, 9, 15, 16, 17, 18]. -1- 

}
}

@Article{ShahMehtre2015,
  Title                    = {An overview of vulnerability assessment and penetration testing techniques},
  Author                   = {Shah, Sugandh and Mehtre, Babu M},
  Journal                  = {Journal of Computer Virology and Hacking Techniques},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {27--49},
  Volume                   = {11},

  Groups                   = {Predication Vulnerability},
  Publisher                = {Springer}
}

@Article{ShahmehriMammarMontesdeOcaEtAl2012,
  Title                    = {An advanced approach for modeling and detecting software vulnerabilities},
  Author                   = {Shahmehri, Nahid and Mammar, Amel and Montes de Oca, Edgardo and Byers, David and Cavalli, Ana and Ardi, Shanai and Jimenez, Willy},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},

  Month                    = {Sep},
  Number                   = {9},
  Pages                    = {997鈥�1013},
  Volume                   = {54},

  Doi                      = {10.1016/j.infsof.2012.03.004},
  File                     = {:article\\An advanced approach for modeling and detecting software vulnerabilities.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2012.03.004}
}

@Article{ShahriarWeldemariamZulkernineEtAl2014,
  Title                    = {Effective detection of vulnerable and malicious browser extensions},
  Author                   = {Shahriar, Hossain and Weldemariam, Komminist and Zulkernine, Mohammad and Lutellier, Thibaud},
  Journal                  = {Computers \& Security},
  Year                     = {2014},

  Month                    = {Nov},
  Pages                    = {66鈥�84},
  Volume                   = {47},

  Doi                      = {10.1016/j.cose.2014.06.005},
  File                     = {:article\\Effective detection of vulnerable and malicious browser extensions.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0167-4048},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.cose.2014.06.005}
}

@Article{ShankarTalwarFosterEtAl2001,
  Title                    = {Detecting Format String Vulnerabilities with Type Qualifiers},
  Author                   = {Umesh Shankar and Kunal Talwar and Jeffrey S. Foster and David Wagner and fushankar and kunal and jfoster and dawg@cs.berkeley.edu},
  Year                     = {2001},

  Abstract                 = {The first argument to printf() is a format string that specifies the number and types of the other arguments.
},
  File                     = {:article\\Detecting Format String Vulnerabilities with Type Qualifiers..pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Detecting Format String Vulnerabilities with Type Qualifiers Umesh Shankar Kunal Talwar Jeffrey S. Foster David Wagner fushankar,kunal,jfoster,dawg@cs.berkeley.edu University of California at Berkeley May 11, 2001 Abstract The first argument to printf() is a format string that specifies the number and types of the other arguments. We present a new system for automatically detecting for No checking is done, either at run-time or compile-time,- mat string security vulnerabilities in C programs using to verify that printf()was indeed called with the cor- a constraint-based type-inference engine. We describe rect number and types of arguments. Thus the following new techniques for presenting the results of such an anal innocuous-looking simplification of the above call can- ysis to the user in a form that makes bugs easier to find be dangerous: and to fix. The system has been implemented and tested printf(buf); (may be incorrect!) on several real-world software packages. Our tests show that the system is very effective, detecting several bugs If buf contains a format specifier (e.g., “%s”), previously unknown to the authors and exhibiting a low printf() will naively attempt to read nonexistent ar- rate of false positives in almost all cases. Many of our guments off the stack, most likely causing the program techniques are applicable to additional classes of secu- to crash. The C standard library contains a number rity vulnerabilities, as well as other type- and constraint- of other, similar primitives that put the programmer at based systems. risk for format string bugs. Other examples include the message-logging syslog() function, as well as set- proctitle(), which sets the X window name asso- 1 Introduction ciated with the current process. Securing systems that interact with malicious parties can A perhaps unexpected consequence of format string be a tremendous challenge. Indeed, systems written in C bugs is that they can be devastating to security. When are especially difficult to secure, given C’s tendency to a knowledgeable adversary has control of the value of sacrifice safety for efficiency. One of the more subtle pit the format string s involved in a format string bug, they- falls facing implementors is the so-called format string can use s to write to arbitrary memory locations. For ex- vulnerability. Since the discovery of this failure mode ample, including the “%n” specifier in a format string in the past year, security experts have identified for causes printf-like functions to store the number of- mat string vulnerabilities in dozens of widely-deployed characters printed so far into a location pointed to by the security associated argument. When combined with other tricks,-critical systems [2, 4, 5, 8, 9, 10, 11, 22, 23, 24, 25, 27, 30, 35, 43], and attackers have begun exploiting this often leads to a complete compromise of security. these security holes on a large scale [10, 27], gaining Techniques for exploiting format string bugs have been root access on vulnerable systems. It seems likely that described elsewhere [30]; for the purposes of this paper, many legacy applications still contain undiscovered for the details are unimportant.- mat string vulnerabilities. The main contribution of this paper is to describe a sys- Format string bugs arise from design misfeatures in the tem for automatically detecting format string bugs at C standard library combined with a problematic imple compile-time. Our system applies static, type-theoretic- mentation of variable-argument functions. Consider a analysis techniques from the programming languages typical usage of format strings: literature to the task of detecting potential security holes. We have implemented our system as a tool built on top printf("%s", buf); (correct) of an extensible type qualifier framework [19]. We have tested our tool on a number of real-world software sys- This research was supported in part by the National Science Foun- dation CAREER Award No. CCR-0093337, NSF CCR-9457812 tems, in the process independently re-discovering sev-, NASA Contract No. NAG2-1210, NSF CCR eral format string bugs that were unknown to the authors-0085949, a UC Regents Fellowship, and an equipment donation from Intel. at the time.

}
}

@Article{SharBriand2013,
  Title                    = {Web Application Vulnerability Prediction using Hybrid Program Analysis and Machine Learning},
  Author                   = {Lwin Khin Shar and Lionel C. Briand},
  Journal                  = {IEEE Transactions on Dependable and Secure Computing},
  Year                     = {2013},

  Abstract                 = {Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical
},
  Doi                      = {10.1109/TDSC.2014.2373377,},
  File                     = {:article\\Web Application Vulnerability Prediction using   Hybrid Program Analysis and Machine Learning.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {SKIM},
  Review                   = {This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2014.2373377, IEEE Transactions on Dependable and Secure Computing Web Application Vulnerability Prediction using Hybrid Program Analysis and Machine Learning Lwin Khin Shar and Lionel C. Briand Interdisciplinary Centre for Security, Reliability and Trust University of Luxembourg, Luxembourg {lwinkhin.shar, lionel.briand}@uni.lu Hee Beng Kuan Tan Block S2, School of Electrical and Electronic Engineering Nanyang Technological University Nanyang Avenue, Singapore 639798 ibktan@ntu.edu.sg Abstract—Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77% recall and 5% probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24% higher recall and 3% lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing. IndexTerms—Vulnerability prediction, security measures, input validation and sanitization, program analysis, empirical study. 1 INTRODUCTION Web applications play an important role in many of our daily activities such as social networking, email, banking, shopping, registrations, and so on. As web software is also highly accessible, web application vulnerabilities arguably have greater impact than vulnerabilities in other types of software. Web developers are directly responsible for the security of web applications. Unfortunately, they often have limited time to follow up with new arising security issues and are often not provided with adequate security training to become aware of state-of-the-art web security techniques. According to OWASP’s Top Ten Project [1], SQL injection (SQLI), cross site scripting (XSS), remote code execution (RCE), and file inclusion (FI) are among the most common and serious web application vulnerabilities threatening the privacy and security of both clients and applications nowadays. To address these security threats, many web vulnerability detection 1 1545-5971 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

}
}

@Article{SharBriandTan2015,
  Title                    = {Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning},
  Author                   = {Shar, Lwin Khin and Briand, Lionel C. and Tan, Hee Beng Kuan},
  Journal                  = {IEEE Trans. Dependable and Secure Comput.},
  Year                     = {2015},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {688–707},
  Volume                   = {12},

  __markedentry            = {[ccc:1]},
  Doi                      = {10.1109/tdsc.2014.2373377},
  File                     = {:home/ccc/github/literature/article/Web application vulnerability prediction using hybrid program analysis and machine learning.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1545-5971},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TDSC.2014.2373377}
}

@Article{SharTan2013,
  Title                    = {Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns},
  Author                   = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  Journal                  = {Information and Software Technology},
  Year                     = {2013},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {1767--1780},
  Volume                   = {55},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Context: SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure. Objective: This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS. Method: From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities. Results: We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities. Conclusion: The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities. (C) 2013 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.infsof.2013.04.002},
  Groups                   = {Code Mining},
  Sn                       = {0950-5849},
  Tc                       = {5},
  Ut                       = {WOS:000323298000007},
  Z8                       = {0},
  Z9                       = {5},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SharTan2013a,
  Title                    = {Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns},
  Author                   = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  Journal                  = {Information and Software Technology},
  Year                     = {2013},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1767鈥�1780},
  Volume                   = {55},

  Abstract                 = {SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure.This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS.From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities.We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities.The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities. [All rights reserved Elsevier]. },
  Doi                      = {10.1016/j.infsof.2013.04.002},
  File                     = {:article\\Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Sn                       = {0950-5849},
  Tc                       = {0},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2013.04.002},
  Ut                       = {INSPEC:14356420},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SharTan2012,
  Title                    = {Mining input sanitization patterns for predicting SQL injection and cross site scripting vulnerabilities},
  Author                   = {Lwin Khin Shar and Hee Beng Kuan Tan},
  Journal                  = {2012 Proceedings of the 34th International Conference on Software Engineering (ICSE 2012)},
  Year                     = {2012},
  Pages                    = {ACM (SIGSOFT); IEEE Comp. Soc. Tech. Council Softw. Eng., SI-SE; Univ.EOLEOLZurich, Dept. Inf.},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities.},
  Be                       = {Glinz, M.EOLEOLMurphy, G.EOLEOLPezze, M.},
  Bn                       = {978-1-4673-1066-6},
  Cl                       = {Zurich, Switzerland},
  Ct                       = {2012 34th International Conference on Software Engineering (ICSE 2012)},
  Cy                       = {2-9 June 2012},
  Doi                      = {10.1109/ICSE.2012.6227096},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:12847650},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SharTan2012a,
  Title                    = {Mining Input Sanitization Patterns for Predicting SQL Injection and Cross Site Scripting Vulnerabilities},
  Author                   = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  Journal                  = {2012 34th International Conference On Software Engineering (icse)},
  Year                     = {2012},
  Pages                    = {ACM Special Interest Grp Software Engn (SIGSOFT); IEEE Comp Soc TechEOLEOLCouncil Software Engn (TCSE); Special Interest Grp Software Engn SwissEOLEOLInformat Soc (SI-SE); Univ Zurich, Dept Informat; ACM; IEEE Comp Soc},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities.},
  Be                       = {Glinz, MEOLEOLMurphy, GEOLEOLPezze, M},
  Bn                       = {978-1-4673-1067-3},
  Cl                       = {Zurich, SWITZERLAND},
  Ct                       = {34th International Conference on Software Engineering (ICSE)},
  Cy                       = {JUN 02-09, 2012},
  Groups                   = {Code Mining},
  Se                       = {International Conference on Software Engineering},
  Sn                       = {0270-5257},
  Tc                       = {10},
  Ut                       = {WOS:000312908700136},
  Z8                       = {0},
  Z9                       = {10},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SharTan2012b,
  Title                    = {Automated removal of cross site scripting vulnerabilities in web applications},
  Author                   = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {467鈥�478},
  Volume                   = {54},

  Doi                      = {10.1016/j.infsof.2011.12.006},
  File                     = {:article\\Automated removal of cross site scripting vulnerabilities in web applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0950-5849},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.infsof.2011.12.006}
}

@Article{SharTanBriand2013,
  Title                    = {Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis},
  Author                   = {Lwin Khin Shar and Hee Beng Kuan Tan and Briand, L. C.},
  Journal                  = {2013 35th International Conference on Software Engineering (ICSE)},
  Year                     = {2013},
  Pages                    = {642--51},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data.},
  Bn                       = {978-1-4673-3073-2},
  Cl                       = {San Francisco, CA, USA},
  Ct                       = {2013 35th International Conference on Software Engineering (ICSE)},
  Cy                       = {18-26 May 2013},
  Doi                      = {10.1109/ICSE.2013.6606610},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:13799183},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SharTanBriand2013a,
  Title                    = {Mining SQL Injection and Cross Site Scripting Vulnerabilities using Hybrid Program Analysis},
  Author                   = {Shar, Lwin Khin and Tan, Hee Beng Kuan and Briand, Lionel C.},
  Journal                  = {Proceedings of the 35th International Conference On Software Engineering (icse 2013)},
  Year                     = {2013},
  Pages                    = {Assoc Comp Machinery; ACM Special Interest Grp Software Engn; IEEE CompEOLEOLSoc; Tech Council Software Engn},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data.},
  Be                       = {Notkin, DEOLEOLCheng, BHCEOLEOLPohl, K},
  Bn                       = {978-1-4673-3076-3},
  Cl                       = {San Francisco, CA},
  Ct                       = {35th International Conference on Software Engineering (ICSE)},
  Cy                       = {MAY 18-26, 2013},
  Groups                   = {Code Mining},
  Tc                       = {2},
  Ut                       = {WOS:000333965800065},
  Z8                       = {1},
  Z9                       = {3},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{ShekharDietzWallachEtAl2012,
  Title                    = {AdSplit: Separating smartphone advertising from applications},
  Author                   = {Shashi Shekhar and Michael Dietz and Dan S. Wallach and shashi.shekhar@rice.edu mdietz@rice.edu dwallach@rice.edu},
  Year                     = {2012},

  Abstract                 = {cottage industry of companies offer advertising services A wide variety of smartphone applications today rely on for smartphone application developers. third-party advertising services, which provide libraries Today, these services are simply pre-compiled code li- that are linked into the hosting application. This situ- braries, linked and shipped together with the application. ation is undesirable for both the application author and This means that a remote advertising server has no way the advertiser. Advertising libraries require their own to validate a request it receives from a user legitimately permissions, resulting in additional permission requests clicking on an advertisement. A malicious application to users. Likewise, a malicious application could sim- could easily forge these messages, generating revenue for ulate the behavior of the advertising library, forging the its developer while hiding the advertisement displays in user’s interaction and stealing money from the advertiser. their entirety. To create a clear trust boundary, advertis- This paper describes AdSplit, where we extended An- ers would benefit from running separately from their host droid to allow an application and its advertising to run applications. as separate processes, under separate user-ids, eliminat- In Android, applications must request permission at ing the need for applications to request permissions on install time for any sensitive privileges they wish to ex- behalf of their advertising libraries, and providing ser- ercise. Such privileges include access to the Internet, ac- vices to validate the legitimacy of clicks, locally and re- cess to coarse or fine location information, or even ac- motely. AdSplit automatically recompiles apps to extract cess to see what other apps are installed on the phone. their ad services, and we measure minimal runtime over- Advertisers want this information to better profile users head. AdSplit also supports a system resource that allows and thus target ads at them; in return, advertisers may advertisements to display their content in an embedded pay more money to their hosting applications’ develop- HTML widget, without requiring any native code. ers. Consequently, many applications which require no
},
  File                     = {:article\\Separating smartphone advertising from applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {AdSplit: Separating smartphone advertising from applications Shashi Shekhar Michael Dietz Dan S. Wallach shashi.shekhar@rice.edu mdietz@rice.edu dwallach@rice.edu Abstract cottage industry of companies offer advertising services A wide variety of smartphone applications today rely on for smartphone application developers. third-party advertising services, which provide libraries Today, these services are simply pre-compiled code li- that are linked into the hosting application. This situ- braries, linked and shipped together with the application. ation is undesirable for both the application author and This means that a remote advertising server has no way the advertiser. Advertising libraries require their own to validate a request it receives from a user legitimately permissions, resulting in additional permission requests clicking on an advertisement. A malicious application to users. Likewise, a malicious application could sim- could easily forge these messages, generating revenue for ulate the behavior of the advertising library, forging the its developer while hiding the advertisement displays in user’s interaction and stealing money from the advertiser. their entirety. To create a clear trust boundary, advertis- This paper describes AdSplit, where we extended An- ers would benefit from running separately from their host droid to allow an application and its advertising to run applications. as separate processes, under separate user-ids, eliminat- In Android, applications must request permission at ing the need for applications to request permissions on install time for any sensitive privileges they wish to ex- behalf of their advertising libraries, and providing ser- ercise. Such privileges include access to the Internet, ac- vices to validate the legitimacy of clicks, locally and re- cess to coarse or fine location information, or even ac- motely. AdSplit automatically recompiles apps to extract cess to see what other apps are installed on the phone. their ad services, and we measure minimal runtime over- Advertisers want this information to better profile users head. AdSplit also supports a system resource that allows and thus target ads at them; in return, advertisers may advertisements to display their content in an embedded pay more money to their hosting applications’ develop- HTML widget, without requiring any native code. ers. Consequently, many applications which require no particular permissions, by themselves, suffer permission 1 Introduction bloat—being forced to request the privileges required by their advertising libraries in addition to any of their own The smartphone and tablet markets are growing in leaps needed privileges. Since users might be scared away and bounds, helped in no small part by the availability of by detailed permission requests, application developers specialized third-party applications (“apps”). Whether would also benefit if ads could be hosted in separate ap- on the iPhone or Android platforms, apps often come plications, which might then make their own privilege in two flavors: a free version, with embedded adver- requests or be given a suitable one-size-fits-all policy. tising, and a pay version without. Both models have Finally, separating applications from their advertise- been successful in the marketplace. To pick one exam- ments creates better fault isolation. If the ad system fails ple, the popular Angry Birds game at one point brought or runs slowly, the host application should be able to in roughly equal revenue from paid downloads on Ap- carry on without inconveniencing the user. Addressing ple iOS devices and from advertising-supported free these needs requires developing a suitable software ar- downloads on Android devices [10]. They now offer chitecture, with OS assistance to make it robust. advertising-supported free downloads on both platforms. The rest of the paper is organized as follows: in Sec- We cannot predict whether free or paid apps will dom- tion 2 we present a survey of thousands of Android ap- inate in the years to come, but advertising-supported ap- plications, and estimate the degree of permission bloat plications will certainly remain prominent. Already, a caused by advertisement libraries. Section 3 discusses

}
}

@Article{ShinMeneelyWilliamsEtAl2011,
  Title                    = {Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities},
  Author                   = {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2011},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {772–787},
  Volume                   = {37},

  Doi                      = {10.1109/tse.2010.81},
  File                     = {:home/ccc/github/literature/article/Evaluating complexity, code churn, developer activity metrics as indicators of software vulnerabilities-正式.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0098-5589},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2010.81}
}

@InProceedings{SlicingDependenceGraphs1990,
  Title                    = {Interprocedural Slicing and Using Dependence and Graphs},
  Author                   = {Interprocedural Slicing and Using Dependence and Graphs},
  Year                     = {1990},

  File                     = {:article\\Interprocedural slicing using dependence graph.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {- 1 - Interprocedural Slicing Using Dependence Graphs SUSAN HORWITZ, THOMAS REPS, and DAVID BINKLEY University of Wisconsin-Madison hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic paral- lelization, and program integration. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing—generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system depen- dence graph, which extends previous dependence representations to incorporate collections of procedures (with pro- cedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: Rather than per- mitting a program to be sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data-dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar’s nonterminals. CR Categories and Subject Descriptors: D.3.3 [Programming Languages]: Language Constructs − control structures, procedures, functions, and subroutines; D.3.4 [Programming Languages]: Processors − compilers, optimization General Terms: Algorithms, Design Additional Key Words and Phrases: attribute grammar, control dependence, data dependence, data-flow analysis, flow- insensitive summary information, program debugging, program dependence graph, program integration, program slic- ing, subordinate characteristic graph hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1. INTRODUCTION The slice of a program with respect to program point p and variable x consists of all statements and predi- cates of the program that might affect the value of x at point p. This concept, originally discussed by Mark Weiser in [22], can be used to isolate individual computation threads within a program. Slicing can help a hhhhhhhhhhhhhhhhhhhhhhhhhhhhh This work was supported in part by a David and Lucile Packard Fellowship for Science and Engineering, by the National Science Foundation under grants DCR-8552602, DCR-8603356, and CCR-8958530, by the Defense Advanced Research Projects Agency, monitored by the Office of Naval Research under contract N00014-88-K-0590, as well as by grants from IBM, DEC, and Xerox. Authors’ address: Computer Sciences Department, University of Wisconsin − Madison, 1210 W. Dayton St., Madison, WI 53706. An earlier version of this paper appeared in abridged form in the Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, (Atlanta, GA, June 22-24, 1988), ACM SIGPLAN Notices 23(7) (July 1988) [9]. ACM TOPLAS vol 12 no 1 (January 1990)

}
}

@InProceedings{SnowMonroseDaviEtAl2013,
  Title                    = {Just-In-Time Code Reuse: On the Effectiveness of Fine-Grained Address Space Layout Randomization},
  Author                   = {K. Z. Snow and F. Monrose and L. Davi and A. Dmitrienko and C. Liebchen and A. Sadeghi},
  Booktitle                = {2013 {IEEE} Symposium on Security and Privacy},
  Year                     = {2013},
  Month                    = {may},
  Publisher                = {{IEEE}},

  Doi                      = {10.1109/sp.2013.45},
  File                     = {:article\\Just-In-Time Code Reuse.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/SP.2013.45}
}

@Article{SrivastavaBondMcKinleyEtAl2011,
  Title                    = {A Security Policy Oracle: Detecting Security Holes Using Multiple API Implementations},
  Author                   = {Srivastava, Varun and Bond, Michael D. and McKinley, Kathryn S. and Shmatikov, Vitaly},
  Journal                  = {Acm Sigplan Notices},
  Year                     = {2011},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {343--354},
  Volume                   = {46},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct. In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong! Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering "deep" bugs in them.},
  Doi                      = {10.1145/1993316.1993539},
  Groups                   = {Code Mining},
  Sn                       = {0362-1340},
  Tc                       = {1},
  Ut                       = {WOS:000294609500030},
  Z8                       = {0},
  Z9                       = {1},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SrivastavaBondMcKinleyEtAl2011a,
  Title                    = {A Security Policy Oracle: Detecting Security Holes Using Multiple API Implementations},
  Author                   = {Srivastava, V. and Bond, M. D. and McKinley, K. S. and Shmatikov, V.},
  Journal                  = {SIGPLAN Notices},
  Year                     = {2011},
  Number                   = {6},
  Pages                    = {343--54},
  Volume                   = {46},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct. In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flowand context sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong! Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering "deep" bugs in them.},
  Groups                   = {Code Mining},
  Sn                       = {0362-1340},
  Tc                       = {0},
  Ut                       = {INSPEC:12609854},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{SrivastavaBondMcKinleyEtAl2011b,
  Title                    = {A Security Policy Oracle: Detecting Security Holes Using Multiple API Implementations},
  Author                   = {Srivastava, Varun and Bond, Michael D. and McKinley, Kathryn S. and Shmatikov, Vitaly},
  Journal                  = {Pldi 11: Proceedings of the 2011 Acm Conference On Programming Language Design and Implementation},
  Year                     = {2011},
  Pages                    = {ACM SIGPLAN},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct. In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations Of the same API must enforce the same policy, or at least one of them is wrong! Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun. Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering "deep" bugs in them.},
  Bn                       = {978-1-4503-0663-8},
  Cl                       = {San Jose, CA},
  Ct                       = {32nd ACM SIGPLAN Conference on Programming Language Design andEOLEOLImplementation (PLDI 11)},
  Cy                       = {JUN 04-08, 2011},
  Gp                       = {ACM SIGPLAN},
  Groups                   = {Code Mining},
  Tc                       = {3},
  Ut                       = {WOS:000297656700030},
  Z8                       = {0},
  Z9                       = {3},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{StaticMalwareUsingEtAl2015,
  Title                    = {30 80 IEEE LATIN AMERICA TRANSACTIONS, VOL. 13, NO. 9, SEPTEMBER},
  Author                   = {Integrating Static and Dynamic Malware and Analysis Using and Machine Learning},
  Year                     = {2015},
  Publisher                = {IEEE},

  Abstract                 = {Malware Analysis and Classification Systems use máquina têm sido utilizadas em conjunto com algoritmos de
},
  File                     = {:home/ccc/github/literature/article/Integrating static and dynamic malware analysis using machine learning.pdf:PDF},
  Keywords                 = {Information Security, Malware, Static Analysis, Dynamic Analysis, Unified Analysis, Machine Learning. Com o objetivo de contribuir com a melhoria do processo de},
  Review                   = {30 80 IEEE LATIN AMERICA TRANSACTIONS, VOL. 13, NO. 9, SEPTEMBER 2015 Integrating Static and Dynamic Malware Analysis Using Machine Learning R. J. Mangialardo and J. C. Duarte Abstract— Malware Analysis and Classification Systems use máquina têm sido utilizadas em conjunto com algoritmos de static and dynamic techniques, in conjunction with machine coleta estática e dinâmica para a identificação e classificação de learning algorithms, to automate the task of identification and classification of malicious codes. Both techniques have weaknesses malwares, de tal forma a automatizar a classificação e reduzir that allow the use of analysis evasion techniques, hampering the o tempo de desenvolvimento de softwares de antivírus para identification of malwares. In this work, we propose the unification combater os códigos maliciosos. Trabalhos de análise de of static and dynamic analysis, as a method of collecting data from malwares realizados destacam normalmente a aplicação de malware that decreases the chance of success for such evasion técnicas isoladas de análise de malwares. A tabela I apresenta techniques. From the data collected in the analysis phase, we use the alguns trabalhos relacionados de análise de malwares e o C5.0 and Random Forest machine learning algorithms, implemented inside the FAMA framework, to perform the método que foi utilizado: identification and classification of malwares into two classes and multiple categories. In our experiments, we showed that the TABELA I. TRABALHOS RELACIONADOS. accuracy of the unified analysis achieved an accuracy of 95.75% for AUTOR ABORDAGEM PRECISÃO the binary classification problem and an accuracy value of 93.02% [16] DINÂMICA Random Forest, 93,6% for the multiple categorization problem. In all experiments, the [23] DINÂMICA kNN, 83,90% [24] DINÂMICA Random Forest, 98,3% unified analysis produced better results than those obtained by [26] ESTÁTICA Árvores de Decisão, 98% static and dynamic analyzes isolated. [27] ESTÁTICA Random Forest, 92,34% [28] ESTÁTICA SVM, 98,73% Keywords— Information Security, Malware, Static Analysis, Dynamic Analysis, Unified Analysis, Machine Learning. Com o objetivo de contribuir com a melhoria do processo de I. INTRODUÇÃO análise e, assim, diminuir o impacto das atividades de evasão, contribuindo com a segurança da informação, este trabalho M apresenta um método para a construção de uma base dados ALWARES (softwares maliciosos) são programas unificada para a análise e classificação automática de desenvolvidos para executar ações danosas em um malwares. Outra inovação do trabalho é a classificação em tipos computador. Os principais motivos que levam ao de malwares. Classificamos os artefatos experimentando e desenvolvimento e a propagação de códigos maliciosos são a introduzindo o algoritmo de aprendizado de máquina C5.0 na obtenção de vantagens financeiras, a coleta de informações classificação automática e comparamos seus resultados com os confidenciais, o desejo de autopromoção e o vandalismo. Além obtidos pelo algoritmo Random Forest. disto, os códigos maliciosos são muitas vezes usados como métodos intermediários e possibilitam a prática de golpes, a I I. ANÁLISE DE MALWARES realização de ataques e a disseminação de spam [1]. A detecção e a análise de códigos maliciosos são atividades cruciais para qualquer mecanismo de defesa contra estes A análise de malwares é o processo de identificação e ataques. Para identificar os malwares duas técnicas podem ser classificação de códigos maliciosos. Com o objetivo de empregadas: a análise estática e a análise dinâmica de identificar malwares, especialistas de segurança adotam um malwares. A análise estática permite extrair características do método geral de investigação, que se divide normalmente em 5 código sem executá-lo e a análise dinâmica permite extrair fases [3]: informações quando o código é executado [15]. Os • Fase 1: preservação forense e exame dos dados voláteis; desenvolvedores de códigos maliciosos utilizam técnicas • Fase 2: exame da memória do sistema infectado; antianálise ou de evasão das análises estática e dinâmica para • Fase 3: análise forense: compreende o exame dos meios de evitar que informações sobre os malwares sejam obtidas, armazenamento permanentes (disco rígido, fitas de backup, dificultando ou impedindo a sua identificação. Além deste etc.); problema, ainda existe a demanda de tempo para classificar o • Fase 4: verificação das características do arquivo código como um malware pelo especialista de segurança. Esta desconhecido; tarefa não é simples e para que seja possível são necessários o • Fase 5: análise estática e dinâmica do arquivo suspeito. planejamento e desenvolvimento de sistemas que executem a As fases 4 e 5 são utilizadas para obter as características dos tarefa de forma automatizada, reduzindo o tempo de resposta de códigos contidos em arquivos. Estes arquivos podem ter vários reação à ação dos malwares. Técnicas de aprendizado de formatos e, neste trabalho, para manter uma padronização, R. J. Mangialardo, Instituto Militar de Engenharia (IME), Rio de Janeiro, J. C. Duarte, Instituto Militar de Engenharia (IME), Rio de Janeiro, Rio de Rio de Janeiro, Brasil, mangialardo@ime.eb.br Janeiro, Brasil, duarte@ime.eb.br 

}
}

@InProceedings{,
  Title                    = {AUTOMATIC VULNERABILITY DETECTION},
  Author                   = {USING STATIC and SOURCE CODE},
  Year                     = {2005},

  File                     = {:home/ccc/github/literature/article/AUTOMATIC VULNERABILITY DETECTION USING STATIC CODE ANALYSIS.pdf:PDF},
  Review                   = {AUTOMATIC VULNERABILITY DETECTION
USING STATIC SOURCE CODE
ANALYSIS
by
ALEXANDER IVANOV SOTIROV
A THESIS
Submitted in partial fulfillment of the requirements for the degree of Master of Science
in the Department of Computer Science in the Graduate School of The University of Alabama
TUSCALOOSA, ALABAMA
2005

}
}

@InProceedings{Static-AnalysisIndicator2012,
  Title                    = {STATIC ANALYSIS},
  Author                   = {SAVI: Static-Analysis and Vulnerability Indicator},
  Booktitle                = {32 May/June 2012 Copublished by the IEEE Computer and Reliability Societies},
  Year                     = {2012},
  Publisher                = {IEEE},

  File                     = {:home/ccc/github/literature/article/SAVI\: Static-Analysis Vulnerability Indicator.pdf:PDF},
  Review                   = {STATIC ANALYSIS SAVI: Static-Analysis Vulnerability Indicator James Walden and Maureen Doyle | Northern Kentucky University Open source software presents new opportunities for software acquisition but comes with the risk of introducing vulnerabilities. SAVI combines several types of static-analysis data to rank application vulnerability. B ecause of Web applications’ widespread use, they’re less popular applications won’t have a public history frequently the target of att ackers and have become of past vulnerabilities. However, security scanners can the largest source of security vulnerabilities.1 Even Web examine any installed Web application, and manual applications such as blogs and wikis that don’t store con- code review or automated static analysis can evaluate fi dential data can distribute malware to compromise vis- any application with source code. iting Web browsers. Identity theft , phishing, malware, Because of open source Web applications’ popular- and other computer crimes cost consumers and corpora- ity, their security is particularly critical. An estimated tions billions of US dollars and erode the requisite trust 87 percent of US businesses use open source soft ware,2 for people to be willing to do business online. as do most blogs and content management systems We’re developing metrics to evaluate Web applica- deployed in 2011.3 Although open source Web applica- tion security risks on the basis of static analysis of the tions’ popularity creates thriving communities, widely application’s source code. We validate these metrics by deployed soft ware is susceptible to epidemic att acks. testing whether they accurately predict vulnerabilities. Att ackers use point-and-click interfaces to launch mass As part of this research, we developed SAVI (Static- SQL injection att acks that compromise hundreds of Analysis Vulnerability Indicator), which combines thousands of Web applications at a time.4 For example, several static-analysis metrics and ranks Web applica- the WordPress blog engine experienced multiple mal- tions’ vulnerability. ware outbreaks in a single month. Organizations choosing among open source appli- Evaluating Risk cations face an important challenge. For most tasks, When introducing an application to an organization, it’s many diff erent applications exist. For example, hun- important to evaluate the application’s security risks in dreds of open source content management systems are addition to its functionality. A popular way to assess this available, ranging from popular ones such as Drupal, risk is to evaluate the application’s vulnerabilities (see which powers www.whitehouse.gov and many other the “Related Work in Evaluating Application Vulnera- large sites, to dozens of systems used by only a handful bilities” sidebar). Whereas well-known applications will of sites. Although the set of desired application features likely have a history of published vulnerabilities in the is unique to each organization, all organizations need to National Vulnerability Database (NVD) or elsewhere, be concerned about security risks. 32 May/June 2012 Copublished by the IEEE Computer and Reliability Societies 1540-7993/12/$31.00 © 2012 IEEE

}
}

@InProceedings{SuiXueComputerScienceEngineeringEtAl,
  Title                    = {SVF: Interprocedural Static Value-Flow Analysis in LLVM},
  Author                   = {Yulei Sui and Jingling Xue and and School of Computer ScienceEngineering and UNSW Australia},

  Abstract                 = {analysis and a context-sensitive tabulation-based slicer. Recently, This paper presents SVF, a tool that enables scalable and precise in- Heros [7] also includes an IFDS/IDE [32] solver for analysing
},
  File                     = {:home/ccc/github/literature/article/SVF\: Interprocedural Static Value-Flow Analysis in LLVM.pdf:PDF},
  Review                   = {SVF: Interprocedural Static Value-Flow Analysis in LLVM Yulei Sui Jingling Xue School of Computer Science and Engineering, UNSW Australia Abstract analysis and a context-sensitive tabulation-based slicer. Recently, This paper presents SVF, a tool that enables scalable and precise in- Heros [7] also includes an IFDS/IDE [32] solver for analysing terprocedural Static Value-Flow analysis for C programs by lever- single- and multi-threaded code in the Soot framework [25]. Some aging recent advances in sparse analysis. SVF, which is fully im- industry static analysis tools that use program dependence analysis plemented in LLVM (version 3.7.0), allows value-flow construc- include Coverity [6] from Synopsys, Parfait [13] from Oracle, and tion and pointer analysis to be performed in an iterative manner, SLAM (built on top of Microsoft’s in-house compiler) [5]. thereby providing increasingly improved precision for both. SVF For the mainstream open-source compilers (e.g., GCC and accepts points-to information generated by any pointer analysis LLVM), most of program dependence analyses used are intraproce- (e.g., Andersen’s analysis) and constructs an interprocedural mem- dural with limited alias analysis support, as is the case for LLVM’s ory SSA form, in which the def-use chains of both top-level and memory dependence analysis. However, many client applications, address-taken variables are captured. Such value-flows can be sub- such as memory leak detection, require value-flows to be analysed sequently exploited to support various forms of program analysis or across the procedural boundaries, for which interprocedural analy- enable more precise pointer analysis (e.g., flow-sensitive analysis) sis is essential. to be performed sparsely. By dividing a pointer analysis into three The traditional iterative approach for computing interprocedu- loosely coupled components: Graph, Rules and Solver, SVF pro- ral value-flows is costly and unscalable for large programs. Recent vides an extensible interface for users to write their own solutions progresses in sparse analysis [19, 30, 36, 38, 44, 45] provide a easily. Moreover, our memory SSA design allows users to make promising solution for analysing large programs scalably and pre- scalability and precision trade-offs by defining their own memory cisely. To avoid expensive propagation of data-flow facts across a partitioning strategies. We discuss some usage scenarios and our program’s control flow graph, sparse analysis is usually conducted previous experiences in using SVF in several client applications. in stages: a pre-analysis is first applied to over-approximate a pro- SVF is publicly available at http://unsw-corg.github.io/ gram’s def-use chains, which are then refined by performing a data- SVF. flow analysis sparsely, i.e., only along such pre-computed def-use relations. In this paper, we present SVF, a tool that enables scalable 1. Introduction and precise interprocedural analysis for C programs by leverag- ing recent advances in sparse analysis. SVF allows value-flow con- Due to the sheer complexity of modern software systems, finding struction and pointer analysis to be performed iteratively, thereby and fixing software bugs is far cheaper earlier in the software devel- providing increasingly improved precision for both. SVF accepts opment life cycle (e.g., during the coding stage) than later (e.g., dur- points-to information generated by any pointer analysis (e.g., An- ing the testing stage), resulting in higher quality software [9, 39]. dersen’s analysis) and builds an interprocedural memory SSA Static analysis, which approximates the runtime behaviour of a pro- (Static-Single Assignment) form so that the def-use chains of both gram at compile time, is a fundamental approach to helping devel- top-level and address-taken variables are captured. These value- opers catch bugs effectively in early stages of software develop- flows can be subsequently exploited to support various forms of ment. program analysis or enable more precise pointer analysis (e.g., In static analysis, a fundamental research problem is to resolve flow-sensitive analysis) to be performed sparsely. program dependencies (aka value-flows). The more precisely the SVF is fully implemented in an industry-strength compiler value-flows are resolved, the more effective static analysis will LLVM (in its latest version 3.7.0). The LLVM platform [26] is be. By improving the precision and scalability of static value-flow designed as a set of reusable libraries with a well-defined IR (In- analysis, we can significantly improve the effectiveness of virtually termediate Representation). It has been recognised as a common all other forms of program analysis on detecting a variety of bugs, infrastructure to support analysis and transformation with many such as memory leaks [11, 36], uninitialised variables [43], security front-ends (e.g., C/C++, Objective-C/C++, OpenMP, Java [14] and vulnerabilities [28], and tainted information flow [4, 8]. Javascript [3]). By using the LLVM IR as input, SVF can poten- Static value-flow analysis resolves both the data and control de- tially tap into LLVM’s front-ends to handle programs written in pendences of a program. It was initially adopted in software debug- other languages (in addition to C). ging [40, 41] and optimising compilers [17, 33] by providing ex- plicit definition-use relations of program variables. This fundamen- tal technique has subsequently been used widely for program analy- 2. Design Overview sis and verification in many open-source and commercial tools. The Our SVF framework is depicted in Figure 1. The source code of Wisconsin program-slicing project [22] is a well-known research a program is first compiled by clang into bit-code files, which prototype that supports both forward and backward slicing on its are merged by LLVM Gold Plugin at link time stage (LTO) to program dependence graph. Later, the tool was integrated into the produce a whole-program bc file. Then the “Pointer Analysis” commercial product CodeSurfer [2]. WALA [23] is an open-source module is invoked. Based on the points-to information obtained, the Java analysis framework that provides interprocedural data-flow “Value-Flow Construction” module puts the program in memory

}
}

@InProceedings{,
  Title                    = {Static Memory Leak Detection Using Full-Sparse Value-Flow Analysis},
  Author                   = {Yulei Sui and Ding Ye and Jingling Xue},
  Year                     = {4503},

  Abstract                 = {Table 1: Comparing Saber with other static detec-
},
  File                     = {:home/ccc/github/literature/article/Static Memory Leak Detection Using Full-Sparse Value-Flow Analysis.pdf:PDF},
  Keywords                 = {by both top-level and address-taken pointers in the program. Memory Leaks, Static Analysis, Sparse Value-Flow Analysis In order to be scalable and accurate, its underlying pointer},
  Review                   = {Static Memory Leak Detection Using Full-Sparse Value-Flow Analysis
Yulei Sui Ding Ye Jingling Xue
Programming Languages and Compilers Group School of Computer Science and Engineering, UNSW, Australia
ABSTRACT Table 1: Comparing Saber with other static detec-
We introduce a static detector, Saber, for detecting mem- tors on analysing the 15 SPEC2000 C programs. ory leaks in C programs. Leveraging recent advances on The data for Clang, which stands here for its static sparse pointer analysis, Saber is the first to use a full-sparse analysis tool, and Saber are from this paper while value-flow analysis for leak detection. Saber tracks the flow the data for the other three tools are from the pa- of values from allocation to free sites using a sparse value- pers cited. Saturn [20], which has no published data flow graph (SVFG) that captures def-use chains and value on SPEC2000, runs at 50 LOC/sec with a false pos- flows via assignments for all memory locations represented itive rate of 10% [2]. by both top-level and address-taken pointers. By exploiting Speed Bug False Positive field-, flow- and context-sensitivity during different phases Leak Detector (LOC/sec) Count Rate (%) of the analysis, Saber detects leaks in a program by solving
Contradiction [14] 300 26 56 a graph reachability problem on its SVFG.
Clang [8] 400 27 25 Saber, which is fully implemented in Open64, is effec-
Sparrow [9] 720 81 16 tive at detecting 211 leaks in the 15 SPEC2000 C programs
Fastcheck [2] 37,900 59 14 and five applications, while keeping the false positive rate at 18.5%. We have also compared Saber with Fastcheck Saber 10,220 83 19
(which analyzes allocated objects flowing only into top-level pointers) and Sparrow (which handles all allocated objects memory leaks in C programs. Table 1 compares Saber with using abstract interpretation) using the 15 SPEC2000 C pro- several other static detectors based on published and self- grams. Saber is as accurate as Sparrow but is 14.2X faster produced data on their scalability and accuracy in analysing and reports 40.7% more bugs than Fastcheck at a slightly the 15 SPEC2000 C programs (totalling 620 KLOC). These higher false positive rate but is only 3.7X slower. results, together with those reported later on analysing five
applications (totalling 1.7 MLOC), show that Saber has Categories and Subject Descriptors met its design objectives, as discussed below. D 2.4 [Software/Program Verification]: Reliability; D 3.4 [Processors]: Compilers, Memory Management; F 3.2 1.1 Motivations and Objectives [Semantics of Programming Languages]: Program Anal- To find memory leaks statically in a C program, a leak ysis analysis reasons about a source-sink property : every object
General Terms created at an allocation site (a source) must eventually reacha free site (a sink) during any execution of the program. The Algorithms, Languages, Verification analysis involves tracking the flow of values from sources to
sinks through a sequence of memory locations represented Keywords by both top-level and address-taken pointers in the program. Memory Leaks, Static Analysis, Sparse Value-Flow Analysis In order to be scalable and accurate, its underlying pointer
analysis must also be scalable and accurate.
1. INTRODUCTION Current static detection techniques include Contradic- tion [14] (data-flow analysis), Saturn [20] (Boolean satis-
This paper introduces a new static detector, Saber, which fiability), Sparrow [9] (abstract interpretation), Clang [8] is fully implemented in the Open64 compiler, for detecting (symbolic execution) and Fastcheck [2] (sparse value-flow
analysis). Two approaches exist: iterative data-flow analysis and sparse value-flow analysis. The former tracks the flow
Permission to make digital or hard copies of all or part of this work for of values iteratively at each point through the control flow personal or classroom use is granted without fee provided that copies are while the latter tracks the flow of values sparsely through not made or distributed for profit or commercial advantage and that copies def-use chains or SSA form. The latter is faster as the infor- bear this notice and the full citation on the first page. To copy otherwise, to mation is computed only where necessary using a sparse rep- republish, to post on servers or to redistribute to lists, requires prior specific resentation of value flows. Among all published static leak permission and/or a fee. ISSTA ?12, July 15-20, 2012, Minneapolis, MN, USA detectors, Fastcheck is the only one in the latter category
Copyright 12 ACM 978-1-4503-1454-1/12/07 ...$10.00. and all the others fall into the former category. However,

}
}

@Article{SutherlandKalbBlythEtAl2006,
  Title                    = {An empirical examination of the reverse engineering process for binary files},
  Author                   = {Iain Sutherland and George E. Kalb and Andrew Blyth and Gaius Mulley},
  Journal                  = {Computers {\&} Security},
  Year                     = {2006},

  Month                    = {may},
  Number                   = {3},
  Pages                    = {221--228},
  Volume                   = {25},

  Doi                      = {10.1016/j.cose.2005.11.002},
  Publisher                = {Elsevier {BV}},
  Url                      = {http://dx.doi.org/10.1016/j.cose.2005.11.002}
}

@Article{TakataAkiyamaYagiEtAl2015,
  Title                    = {MineSpider: Extracting URLs from Environment-Dependent Drive-by Download Attacks},
  Author                   = {Takata, Y. and Akiyama, M. and Yagi, T. and Hariu, T. and Goto, S.},
  Journal                  = {2015 IEEE 39th Annual Computer Software and Applications Conference (COMPSAC). Proceedings},
  Year                     = {2015},
  Pages                    = {444--9},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Drive-by download attacks force users to automatically download and install malware by redirecting them to malicious URLs that exploit vulnerabilities of the user's web browser. Attackers profile the information on the user's environment such as the name and version of the browser and browser plugins and launch a drive-by download attack on only certain targets by changing the destination URL. When malicious content detection and collection techniques such as honey clients are used that do not match the specific environment of the attack target, they cannot detect the attack because they are not redirected. We propose here a method to exhaustively analyze Java Script code relevant to redirections and to extract the destination URLs in the code. Our method facilitates the detection of attacks by extracting a large number of URLs while controlling the analysis overhead by excluding code not relevant to redirections. We implemented our method in a browser emulator called Mine Spider that automatically extracts potential URLs from websites. We validated it by using communication data with malicious websites captured during a three-year period. The experimental results demonstrated that Mine Spider extracted 30,000 new URLs from websites in a few seconds that existing techniques missed.},
  Cl                       = {Taichung, Taiwan},
  Ct                       = {2015 IEEE 39th Annual Computer Software and Applications ConferenceEOLEOL(COMPSAC)},
  Cy                       = {1-5 July 2015},
  Doi                      = {10.1109/COMPSAC.2015.76},
  Groups                   = {Code Mining},
  Pn                       = {vol.2},
  Tc                       = {0},
  Ut                       = {INSPEC:15476006},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{TAKHMA2015,
  Title                    = {Third-party Source Code Compliance using Early Static Code Analysis},
  Author                   = {Youness TAKHMA},
  Booktitle                = {CTS 2015},
  Year                     = {2015},
  Publisher                = {IEEE},

  Abstract                 = {Thispaperpresentsageneric toolforStaticCode 2SHUDWLQJ 6\VWHPV DQG RWKHU DQWLLQWUXVLRQ WKLUGSDUW\ AnalysisforMyICPhonedevelopercommunity.Itsmajoraim DSSOLFDWLRQV DW UXQ WLPH LW UHPDLQV WKDW PXFK RI WKH istoverify,earlyduringdevelopmentcycle,thecomplianceof LQHIILFLHQWRUSUREOHPDWLFFRGHFDQEHLGHQWLILHGEHIRUHHYHQ third­party software with theMyIC phone platform coding FRPSLODWLRQRULQWHUSUHWDWLRQDQGGHDOWZLWKDWGHYHORSPHQW standards,ensuring successfuldeployment through theMyIC VWDJH$GGLWLRQDOO\PDQ\SODWIRUPSURYLGHUVDQGWKLVLVWKH PhoneAppStore. BuiltasanextendableEclipseplug­in,our toolfacilitatescollaborativesoftwareacceptancetestsimposed FDVH IRU $OFDWHO /XFHQW IRUZKLFK WKLV VWDWLF DQDO\]HU ZDV by the target platform provider.  Our approach to code ZULWWHQ UHTXLUH WKLUGSDUW\ FRPPXQLW\FRGH WREHZULWWHQ compliance isbasedon static codeanalysis,which consists in DFFRUGLQJWRFHUWDLQVWDQGDUGVEHIRUHLWVLQFOXVLRQLQWKH$6 theconstructionofanabstractmodelofthesourcecodeofthe *LYHQWKHVKHHUVL]HRIDSSOLFDWLRQVWKHFRQIRUPLW\WDVNFDQ application under analysis. The abstract model is then RQO\EHSHUIRUPHGZLWKWKHKHOSRIDXWRPDWLFWRROVVXFKDV traversedinordertofindthepotentialnoncompliancesbased FRGHDQDO\]HUV>@>@DWGHYHORSPHQWVWDJH1RWRQO\GRHV onthesetofrulessetbytheplatformprovider,andwhichare HDUO\ DXWRPDWLF VRIWZDUH FRPSOLDQFH WKURXJK VWDWLF FRGH distributedasXMLfiles,andloadedbythedeveloperintothe DQDO\VLV UHGXFH WKH WLPH WRPDUNHW IRUQHZVRIWZDUHEXW LW Eclipseenvironmentuponprojectinstantiation.Thegenerated DOVR LQFUHDVHV JUHDWO\ WKH HIILFLHQF\ RI WKH LQWULQVLFDOO\ resultsoftheanalysisarerepresented inatreeviewwith line code highlighted to be easily accessed by the developer. FROODERUDWLYHQDWXUHRIFRPSOLDQFHWHVWLQJ Statisticsthatrelatetoconformitywiththerulesarecalculated ,QWKLVSDSHUZHSUHVHQWDFRGHDQDO\VLVDSSURDFKIRU0\ anddisplayedinapiechartforconsiderationbythedeveloper. ,& 3KRQH >@ WKLUGSDUW\ GHYHORSHU FRPPXQLW\ 2XU FRGH
},
  File                     = {:article\\Third-party source code compliance using early static code analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Static code analysis, Software quality, DQDO\]HU DXWRPDWLFDOO\ YHULILHV DQG DVVHVVHV GHYHORSHG Software conformity, Software compliance, Software DSSOLFDWLRQV HJ :HE DSSOLFDWLRQV ZULWWHQ LQ -DYD6FULSW  verification,Collaborativeacceptancetests. DJDLQVW D VHW RI FRPSOLDQFH WHVWV SURYLGHU E\ WKH SODWIRUP,literature,article},
  Read                     = {未读},
  Review                   = {7KLUGSDUW\6RXUFH&RGH&RPSOLDQFHXVLQJ(DUO\  6WDWLF&RGH$QDO\VLV <RXQHVV7$.+0$˺7DMMHHGGLQH5$&+,',˻+DPLG+$5528'˼0RKDPHG5LGXDQ$%,'˽1DVVHU$66(0˾ 6FKRRORI6FLHQFHDQG(QJLQHHULQJ $O$NKDZD\Q8QLYHUVLW\LQ,IUDQH ,IUDQH0RURFFR ˺<7DNKPD#DXLPD˻75DFKLGL#DXLPD˼++DUURXG#DXLPD˽5$ELG#DXLPD˾1$VVHP#DXLPD  Abstract²Thispaperpresentsageneric toolforStaticCode 2SHUDWLQJ 6\VWHPV DQG RWKHU DQWLLQWUXVLRQ WKLUGSDUW\ AnalysisforMyICPhonedevelopercommunity.Itsmajoraim DSSOLFDWLRQV DW UXQ WLPH LW UHPDLQV WKDW PXFK RI WKH istoverify,earlyduringdevelopmentcycle,thecomplianceof LQHIILFLHQWRUSUREOHPDWLFFRGHFDQEHLGHQWLILHGEHIRUHHYHQ third­party software with theMyIC phone platform coding FRPSLODWLRQRULQWHUSUHWDWLRQDQGGHDOWZLWKDWGHYHORSPHQW standards,ensuring successfuldeployment through theMyIC VWDJH$GGLWLRQDOO\PDQ\SODWIRUPSURYLGHUVDQGWKLVLVWKH PhoneAppStore. BuiltasanextendableEclipseplug­in,our toolfacilitatescollaborativesoftwareacceptancetestsimposed FDVH IRU $OFDWHO /XFHQW IRUZKLFK WKLV VWDWLF DQDO\]HU ZDV by the target platform provider.  Our approach to code ZULWWHQ UHTXLUH WKLUGSDUW\ FRPPXQLW\FRGH WREHZULWWHQ compliance isbasedon static codeanalysis,which consists in DFFRUGLQJWRFHUWDLQVWDQGDUGVEHIRUHLWVLQFOXVLRQLQWKH$6 theconstructionofanabstractmodelofthesourcecodeofthe *LYHQWKHVKHHUVL]HRIDSSOLFDWLRQVWKHFRQIRUPLW\WDVNFDQ application under analysis. The abstract model is then RQO\EHSHUIRUPHGZLWKWKHKHOSRIDXWRPDWLFWRROVVXFKDV traversedinordertofindthepotentialnoncompliancesbased FRGHDQDO\]HUV>@>@DWGHYHORSPHQWVWDJH1RWRQO\GRHV onthesetofrulessetbytheplatformprovider,andwhichare HDUO\ DXWRPDWLF VRIWZDUH FRPSOLDQFH WKURXJK VWDWLF FRGH distributedasXMLfiles,andloadedbythedeveloperintothe DQDO\VLV UHGXFH WKH WLPH WRPDUNHW IRUQHZVRIWZDUHEXW LW Eclipseenvironmentuponprojectinstantiation.Thegenerated DOVR LQFUHDVHV JUHDWO\ WKH HIILFLHQF\ RI WKH LQWULQVLFDOO\ resultsoftheanalysisarerepresented inatreeviewwith line code highlighted to be easily accessed by the developer. FROODERUDWLYHQDWXUHRIFRPSOLDQFHWHVWLQJ Statisticsthatrelatetoconformitywiththerulesarecalculated ,QWKLVSDSHUZHSUHVHQWDFRGHDQDO\VLVDSSURDFKIRU0\ anddisplayedinapiechartforconsiderationbythedeveloper. ,& 3KRQH >@ WKLUGSDUW\ GHYHORSHU FRPPXQLW\ 2XU FRGH Keywords±Static code analysis, Software quality, DQDO\]HU DXWRPDWLFDOO\ YHULILHV DQG DVVHVVHV GHYHORSHG Software conformity, Software compliance, Software DSSOLFDWLRQV HJ :HE DSSOLFDWLRQV ZULWWHQ LQ -DYD6FULSW verification,Collaborativeacceptancetests. DJDLQVW D VHW RI FRPSOLDQFH WHVWV SURYLGHU E\ WKH SODWIRUP SURYLGHU7KHRXWSXWRIWKHDQDO\]HULVDQHYDOXDWLRQRI WKH , ,1752'8&7,21 GHJUHHRIFRQIRUPLW\RI D WKLUGSDUW\DSSOLFDWLRQ WRFRGLQJ 1RZDGD\V,7LVEHFRPLQJXELTXLWRXVDQGSHUYDVLYH:H VWDQGDUGV EHVW FRGLQJSUDFWLFHV DQGRWKHU FRQYHQWLRQV VHW UHO\RQ LWRQDGDLO\EDVLVFKHFNLQJHPDLOVRUGHULQJ LWHPV IRU 0\,& 3KRQH DSSOLFDWLRQV DQG ZKLFK HQVXUH VHFXULW\ RQ WKH ZHE XVLQJ *36 ORFDOL]DWLRQ HWF 7KLV FRXOG QRW DQG SHUIRUPDQFH :KLOH RXU VWDWLF DQDO\]HU KDQGOHV KDSSHQ ZLWKRXW WKH HYHU HYROYLQJ KDUGZDUH DQG FRPPRQFRGLQJIODZVLWLVWREHQRWHGWKDWLWZDVGHVLJQHG DFFRPSDQ\LQJVRIWZDUH DQGEXLOWDVDJHQHULFWRROWKDWVSHFLILFDOO\WDUJHWVWKH0\,& 3KRQH SODWIRUP DQG LWV DFFRPSDQ\LQJ OLEUDULHV $ NH\ ,QGHHGDVPRUHKDUGZDUHLQSXWRQWKHPDUNHWWKHUHLVD IHDWXUHRIRXU DQDO\]HU LV WKDW LWZDVGHVLJQHGDVDSOXJLQ UHPDUNDEOHLQFUHDVHLQWKHGHPDQGRQVRIWZDUH7KLVLQWXUQ IRU (FOLSVH >@ 'HYHORSPHQW (QYLURQPHQW IDFLOLWDWLQJ JHQHUDWHV D JURZLQJ GHPDQG RQ SURJUDPPHUV WR SURGXFH FROODERUDWLYH VRIWZDUH GHYHORSPHQW EHWZHHQ WKH WDUJHW PRUH VRIWZDUH ZKLOH ZDUUDQWLQJ LWV TXDOLW\ >@ PDLQO\ SODWIRUPSURYLGHU$OFDWHODQGWKLUGSDUW\GHYHORSHUV HIILFLHQF\ PDLQWDLQDELOLW\ DQG VHFXULW\ 7KH DPRXQW RI VRIWZDUH SURJUDPV KDV UDSLGO\ LQFUHDVHG IURP  ELOOLRQ 7KHUHVWRI WKLVSDSHU LVRUJDQL]HGDVIROORZV6HFWLRQ OLQHVRIFRGHLQWRDSSUR[LPDWHO\ELOOLRQLQ TXLFNO\ JLYHV WKH VDOLHQW IHDWXUHV RI WKH 0\,& SKRQH ,QWKHQH[WWHQ\HDUVLWLVH[SHFWHGWKDWWKHJOREDOFRGHEDVH SODWIRUP6HFWLRQUHODWHVWKLVZRUNWRH[LVWLQJDSSURDFKHV WRSVWKHRQHWULOOLRQOLQHVRIFRGH>@ 6HFWLRQVWDONVDERXWFROODERUDWLYHVRIWZDUHFRPSOLDQFHLQ ,QWRGD\¶VRSHQFRPSXWLQJZRUOGVXFKDVWRGD\¶VPRELOH WKH QHZ PRELOH DSSOLFDWLRQ VRIWZDUH HQJLQHHULQJ PRGHO6HFWLRQ  GHVFULEHV WKH DXWRPDWLF DQDO\VLV DSSURDFK WKLUG SDUW\ DSSOLFDWLRQV GHOLYHUHG WKURXJK $SSOLFDWLRQ 6HFWLRQSUHVHQWV WKHRYHUDOODUFKLWHFWXUHRI WKHDXWRPDWLF 6WRUHV $6 VHFXULW\ SHUIRUPDQFH DQG FRPSOLDQFH ZLWK VRXUFH FRGH YHULILFDWLRQ DSSURDFK ,Q 6HFWLRQ  D SODWIRUP DQG EDFN HQG V\VWHPV WKH FORXG DUH PDMRU SURWRW\SLFDOVFHQDULRWKDWGHPRQVWUDWHVWKHHIIHFWLYHQHVVRI FRQFHUQV ,IPXFKRI WKHVHFRQFHUQVFDQEHHQVXUHGE\ WKH  978-1-4673-7648-8/15/$31.00 ©2015 IEEE 132

}
}

@Article{TangZhaoYangEtAl2015,
  Title                    = {Predicting Vulnerable Components via Text Mining or Software Metrics? An Effort-Aware Perspective},
  Author                   = {Tang, Yaming and Zhao, Fei and Yang, Yibiao and Lu, Hongmin and Zhou, Yuming and Xu, Baowen},
  Journal                  = {2015 IEEE International Conference on Software Quality, Reliability and Security},
  Year                     = {2015},

  Month                    = {Aug},
  Pages                    = {27--36},

  Abstract                 = {In order to identify vulnerable software components, developers can take software metrics as predictors or use text mining techniques to build vulnerability prediction models. A recent study reported that text mining based models have higher recall than software metrics based models. However, this conclusion was drawn without considering the sizes of individual components which affects the code inspection effort to determine whether a component is vulnerable. In this paper, we investigate the predictive power of these two kinds of prediction models in the context of effort-aware vulnerability prediction. To this end, we use the same data sets, containing 223 vulnerabilities found in three web applications, to build vulnerability prediction models. The experimental results show that: (1) in the context of effort-aware ranking scenario, text mining based models only slightly outperform software metrics based models, (2) in the context of effort-aware classification scenario, text mining based models perform similarly to software metrics based models in most cases, and (3) most of the effect sizes (i.e. the magnitude of the differences) between these two kinds of models are trivial. These results suggest that, from the viewpoint of practical application, software metrics based models are comparable to text mining based models. Therefore, for developers, software metrics based models are practical choices for vulnerability prediction, as the cost to build and apply these models is much lower.},
  Bn                       = {978-1-4673-7989-2},
  Cl                       = {Vancouver, BC, Canada},
  Ct                       = {2015 IEEE International Conference on Software Quality, Reliability andEOLEOLSecurity (QRS)},
  Cy                       = {3-5 Aug. 2015},
  Doi                      = {10.1109/qrs.2015.15},
  File                     = {:home/ccc/github/literature/article/-Predicting Vulnerable Components via Text Mining or Software Metrics? An Effort-aware perspective.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-1-4673-7989-2},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Tc                       = {0},
  Url                      = {http://dx.doi.org/10.1109/QRS.2015.15},
  Ut                       = {INSPEC:15475895},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Booklet{Technologies2014,
  Title                    = {SECURWARE 2014},
  Author                   = {Technologies},
  Year                     = {2014},

  File                     = {:article\\Test Case Generation Assisted by Control Dependence Analysis.securware_2014_full.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {SECURWARE 2014 The Eighth International Conference on Emerging Security Information, Systems and Technologies ISBN: 978-1-61208-376-6 November 16 - 20, 2014 Lisbon, Portugal SECURWARE 2014 Editors Rainer Falk, Siemens AG - München, Germany Carlos Becker Westphall, Federal University of Santa Catarina, Brazil 1 / 231

}
}

@InProceedings{Thesis2015,
  Title                    = {University of Passau},
  Author                   = {Bachelor Thesis},
  Year                     = {2015},

  File                     = {:home/ccc/github/literature/article/Fuchs_DoS_in_nodeRED.pdf:PDF},
  Review                   = {University of Passau Bachelor Thesis DoS Detection in NodeRED Author: Supervisor: Per Fuchs Juan David Parra November 22, 2015

}
}

@InProceedings{,
  Author                   = { This and is an author produced version of a paper published in Information and Software and Technology },
  Year                     = {1016},

  Doi                      = {.org/10.1016/j.infsof.2013.02.009},
  File                     = {:home/ccc/github/literature/article/Software Fault Prediction Metrics\: A Systematic Literature Review.pdf:PDF},
  Review                   = {� �
 
 
 This is an author produced version of a paper published in Information and Software Technology 
 This paper has been peer-reviewed but does not include the final publisher proof- corrections or journal pagination. 
 
 Citation for the published paper: 
 
 Radjenovic, D; Hericko, M; Torkar, R; Zivkovic, A: 
 Software fault prediction metrics: A systematic literature review 
 Information and Software Technology, 55 ( 8 ) s. 1397-1418 
 http://dx.doi.org/10.1016/j.infsof.2013.02.009 
 
 
 Access to the published version may require subscription. Published with permission from: Elsevier 
�
GUP Gothenburg�University�Publications�
http://gup.ub.gu.se�

}
}

@Article{ThummalapentaXieTillmannEtAl2009,
  Title                    = {MSeqGen: Object-Oriented Unit-Test Generation via Mining Source Code},
  Author                   = {Suresh Thummalapenta and Tao Xie and Nikolai Tillmann and Jonathan de Halleux and Wolfram Schulte and 1Department of Computer Science and North Carolina State University and Raleigh},
  Year                     = {2009},

  File                     = {:article\\MSeqGen=Object-oriented unit-test generation via mining source .codeesecfse09-mseqgen.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Object-oriented testing, Sequence mining create and mutate objects. These sequences help generate,literature,article},
  Read                     = {未读},
  Review                   = {MSeqGen: Object-Oriented Unit-Test Generation via Mining Source Code∗ Suresh Thummalapenta1, Tao Xie1, Nikolai Tillmann2, Jonathan de Halleux2, Wolfram Schulte2 1Department of Computer Science, North Carolina State University, Raleigh 2Microsoft Research, One Microsoft Way, Redmond 1{sthumma, txie}@ncsu.edu 2, {nikolait, jhalleux, schulte}@microsoft.com ABSTRACT //UndirectedDFS: Short form of UndirectedDepthFirstSearch 00:class UndirectedDFS { An objective of unit testing is to achieve high structural cov- 01: IVertexAndEdgeListGraph VisitedGraph; ... erage of the code under test. Achieving high structural cov- 02: public UndirectedDFS(IVertexAndEdgeListGraph g) { erage of object-oriented code requires desirable method-call 03: ... 04: } sequences that create and mutate objects. These sequences 05: public void Compute(IVertex s) { help generate target object states such as argument or re- 06: //init vertices ceiver object states (in short as target states) of a method 07: foreach(IVertex u in VisitedGraph.Vertices) { under test. Automatic generation of sequences for achieving 08: Colors[u]=GraphColor.White; target states is often challenging due to a large search space 09: if (InitializeVertex != null) of possible sequences. On the other hand, code bases us- 10: InitializeVertex(this, new VertexEventArgs(u)); 11: } ing object types (such as receiver or argument object types) 12: //init edges include sequences that can be used to assist automatic test- 13: foreach(IEdge e in VisitedGraph.Edges) { generation approaches in achieving target states. In this 14: EdgeColors[e]=GraphColor.White; } paper, we propose a novel approach, called MSeqGen, that 15: //use start vertex mines code bases and extracts sequences related to receiver 16: if (s != null) { or argument object types of a method under test. Our 17: if (StartVertex != null) 18: StartVertex(this,new VertexEventArgs(s)); approach uses these extracted sequences to enhance two 19: Visit(s); } state-of-the-art test-generation approaches: random testing 20: // visit vertices and dynamic symbolic execution. We conduct two evalua- 21: foreach(IVertex v in VisitedGraph.Vertices) { tions to show the effectiveness of our approach. Using se- 22: if (Colors[v] == GraphColor.White) { quences extracted by our approach, we show that a random 23: if (StartVertex != null) testing approach achieves 8.7% (with a maximum of 20.0% 24: StartVertex(this,new VertexEventArgs(v)); 25: Visit(v); } for one namespace) higher branch coverage and a dynamic- 26: } symbolic-execution-based approach achieves 17.4% (with a 27: } maximum of 22.5% for one namespace) higher branch cover- 28:} age than without using our approach. Such an improvement Figure 1: A method under test from the Quick- is significant as the branches that are not covered by these Graph library [20]. state-of-the-art approaches are generally quite difficult to cover. 1. INTRODUCTION Categories and Subject Descriptors: D.2.3 [Software A primary objective of unit testing is to achieve high Engineering]: Coding Tools and Techniques—Object-oriented structural coverage such as branch coverage. Achieving high programming ; D.2.6 [Software Engineering]: Programming structural coverage with passing tests gives high confidence Environments—Integrated environments; in the quality of the code under test. To achieve high struc- General Terms: Languages, Experimentation tural coverage of object-oriented code, unit testing requires desirable method-call sequences (in short as sequences) that Keywords: Object-oriented testing, Sequence mining create and mutate objects. These sequences help generate ∗This work is supported in part by NSF grant CCF-0725190 target object states (in short as target states) of the receiver and Army Research Office grant W911NF-08-1-0443. or arguments of the method under test (MUT). As a real example for a target state, the Compute MUT is shown in Figure 1. The MUT performs a depth-first search on an undirected graph. A target state for reaching Statement 8, Permission to make digital or hard copies of all or part of this work for 14, or 22 requires that a graph object used in test execution personal or classroom use is granted without fee provided that copies are has a non-empty set of vertices and edges. not made or distributed for profit or commercial advantage and that copies To generate target states, there exist four major categories bear this notice and the full citation on the first page. To copy otherwise, to of sequence-generation approaches: bounded-exhaustive [13, republish, to post on servers or to redistribute to lists, requires prior specific p 30], evolutionary [11,27], random [5,12,19], and heuristic ap-ermission and/or a fee. ESEC-FSE’09, August 23–28, 2009, Amsterdam, The Netherlands proaches [25]. Bounded-exhaustive approaches generate se-. Copyright 2009 ACM 978-1-60558-001-2/09/08 ...$5.00 quences exhaustively up to a small bound of sequence length..

}
}

@Article{ThungLuciaLoEtAl2014,
  Title                    = {To what extent could we detect field defects? An extended empirical study of false negatives in static bug-finding tools},
  Author                   = {Thung, Ferdian and Lucia and Lo, David and Jiang, Lingxiao and Rahman, Foyzur and Devanbu, Premkumar T.},
  Journal                  = {Automated Software Engineering},
  Year                     = {2014},

  Month                    = {Sep},
  Number                   = {4},
  Pages                    = {561鈥�602},
  Volume                   = {22},

  Doi                      = {10.1007/s10515-014-0169-8},
  File                     = {:article\\An extended empirical study of false negatives in static bug-finding tools.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1573-7535},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10515-014-0169-8}
}

@PhdThesis{DAI2015,
  Title                    = {Detection and Prevention of Misuse of Software Components},
  Author                   = {DAI TING},
  Year                     = {2015},

  File                     = {:article\\Detection and Prevention of Misuse of Software  Components.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Detection and Prevention of Misuse of Software Components DAI TING (B.Eng., TSINGHUA UNIVERSITY) A THESIS SUBMITTED FOR THE DEGREE OF DOCTOR OF PHILOSOPHY DEPARTMENT OF COMPUTER SCIENCE NATIONAL UNIVERSITY OF SINGAPORE 2015

}
}

@Article{TomasEscalonaMejias2013,
  Title                    = {Open source tools for measuring the Internal Quality of Java software products. A survey},
  Author                   = {Tomas, P. and Escalona, M.J. and Mejias, M.},
  Journal                  = {Computer Standards \& Interfaces},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {1},
  Pages                    = {244–255},
  Volume                   = {36},

  Doi                      = {10.1016/j.csi.2013.08.006},
  File                     = {:home/ccc/github/literature/article/Open source tools for measuring the Internal Quality of Java software products. A survey.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0920-5489},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.csi.2013.08.006}
}

@Article{TorchianoRiccaMarchetto2010,
  Title                    = {Are web applications more defect-prone than desktop applications?},
  Author                   = {Torchiano, Marco and Ricca, Filippo and Marchetto, Alessandro},
  Journal                  = {Int J Softw Tools Technol Transfer},
  Year                     = {2010},

  Month                    = {Nov},
  Number                   = {2},
  Pages                    = {151鈥�166},
  Volume                   = {13},

  Doi                      = {10.1007/s10009-010-0182-6},
  File                     = {:article\\Are web applications more defect-prone than desktop applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1433-2787},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s10009-010-0182-6}
}

@TechReport{TrippResearchUSAEtAl9319,
  Title                    = {A Bayesian Approach to Privacy Enforcement in Smartphones},
  Author                   = {Omer Tripp and IBM Research and USA; Julia Rubin and IBM Research and Israel},
  Year                     = {9319},

  File                     = {:home/ccc/github/literature/article/A Bayesian Approach to Privacy Enforcement in Smartphones.pdf:PDF},
  Review                   = {A Bayesian Approach to Privacy Enforcement in Smartphones Omer Tripp, IBM Research, USA; Julia Rubin, IBM Research, Israel https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/tripp This paper is included in the Proceedings of the 23rd USENIX Security Symposium. August 20–22, 2014 • San Diego, CA ISBN 978-1-931971-15-7 Open access to the Proceedings of the 23rd USENIX Security Symposium is sponsored by USENIX

}
}

@Article{TurhanKocakBener2009,
  Title                    = {Data mining source code for locating software bugs: A case study in telecommunication industry},
  Author                   = {Turhan, Burak and Kocak, Gozde and Bener, Ayse},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},

  Month                    = {Aug},
  Number                   = {6},
  Pages                    = {9986鈥�9990},
  Volume                   = {36},

  Doi                      = {10.1016/j.eswa.2008.12.028},
  File                     = {:article\\Data mining source code for locating software bugs  A case study in telecommunication industry.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0957-4174},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.eswa.2008.12.028}
}

@Article{UrmaMycroft2015,
  Title                    = {Source-code queries with graph databases-with application to programming language usage and evolution},
  Author                   = {Urma, Raoul-Gabriel and Mycroft, Alan},
  Journal                  = {Science of Computer Programming},
  Year                     = {2015},

  Month                    = {Jan},
  Pages                    = {127鈥�134},
  Volume                   = {97},

  Doi                      = {10.1016/j.scico.2013.11.010},
  File                     = {:article\\Source-code queries with graph databases—with application to programming language usage and evolution.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0167-6423},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.scico.2013.11.010}
}

@InProceedings{Vanegue2013,
  Title                    = {Towards practical reactive security audit using and extended static checkers},
  Author                   = {Julien Vanegue},
  Booktitle                = {SP 2013},
  Year                     = {2013},
  Publisher                = {IEEE},

  Abstract                 = {This paper describes our experience of performing process of performing effective audit for variants of known reactive security audit of known security vulnerabilities in core op- vulnerabilities, over a large code base. Although the current erating system and browser COM components, using an extended practices are effective, they leave a lot to be desired in terms static checker HAVOC-LITE. We describe the extensions made to the tool to be applicable on such large C++ components, along of scalability and conﬁdence obtained after the reviews. with our experience of using an extended static checker in the In this work, we explore the use of extended static checking large. We argue that the use of such checkers as a conﬁgurable tools towards improving the productivity of auditors perform- static analysis in the hands of security auditors can be an effective ing reactive security audit, and increasing the conﬁdence of the tool for ﬁnding variations of known vulnerabilities. The effort has led to ﬁnding and ﬁxing around 70 previously unknown security audit
},
  Doi                      = {10.1109/SP.2013.12},
  File                     = {:article\\Towards practical reactive security audit using  extended static checkers.pdf:PDF},
  Groups                   = {source code vulnerability},
  Journal                  = {2013 IEEE Symposium on Security and Privacy},
  Keywords                 = {security audit; program veriﬁcation; static analysis; provide the user the ability to write contracts (speciﬁcations extended static checkers of procedures) in the form of preconditions, postconditions,literature,article},
  Read                     = {未读},
  Review                   = {2013 IEEE Symposium on Security and Privacy Towards practical reactive security audit using extended static checkers Julien Vanegue Shuvendu K. Lahiri Bloomberg L.P. Microsoft Research New York, NY, USA Redmond, WA, USA jvanegue@bloomberg.net shuvendu@microsoft.com Abstract—This paper describes our experience of performing process of performing effective audit for variants of known reactive security audit of known security vulnerabilities in core op- vulnerabilities, over a large code base. Although the current erating system and browser COM components, using an extended practices are effective, they leave a lot to be desired in terms static checker HAVOC-LITE. We describe the extensions made to the tool to be applicable on such large C++ components, along of scalability and conﬁdence obtained after the reviews. with our experience of using an extended static checker in the In this work, we explore the use of extended static checking large. We argue that the use of such checkers as a conﬁgurable tools towards improving the productivity of auditors perform- static analysis in the hands of security auditors can be an effective ing reactive security audit, and increasing the conﬁdence of the tool for ﬁnding variations of known vulnerabilities. The effort has led to ﬁnding and ﬁxing around 70 previously unknown security audit 2. Extended static checking tools (such as ESC/Java [1], vulnerabilities in over 10 millions lines operating system and HAVOC [2]) offer a potential to develop conﬁgurable static browser code. analysis tools with high coverage guarantees. These tools Keywords-security audit; program veriﬁcation; static analysis; provide the user the ability to write contracts (speciﬁcations extended static checkers of procedures) in the form of preconditions, postconditions, assertions and discharge them using modern Satisﬁability I. INTRODUCTION Modulo Theories (SMT) solvers [3]. The semantics of the source language is precisely deﬁned once by the tool (and does Ensuring security of software has become of paramount im- not vary by the property being checked), and the assumptions portance to the software industry. Every software development are well documented. Many such tools are also equipped group, representing either a small team of developers or an with simple yet robust user-guided contract inference tools entire company, mandates extensive testing and analysis to (such as Houdini [4]) to reduce the manual overhead of safeguard against security breaches. However, security ﬂaws writing simple intermediate contracts. Unlike full functional will remain a part of life, at least in the case of legacy correctness veriﬁers (such as VCC [5]), they make pragmatic applications that cannot be redesigned from scratch. In such assumptions to reduce the complexity of proofs, and provide cases, effective defense mechanisms are required to mitigate a lot more automation in the form of inference. Although the the impact of security vulnerabilities. In particular, ﬁnding all use of extended static checkers had been proposed for ensuring possible variants of a known security vulnerability can go security a decade back [6], not much success has been reported a long way in safeguarding the known attack surface of a in practical usage. Our conjecture is that the absence of a software system. usable, robust and scalable tool for the space of core operating The Microsoft Security Response Center (MSRC) identiﬁes, system and browser implementations has been one of the main monitors, resolves, and responds to security incidents and Mi- causes for the lack of adoption. crosoft software security vulnerabilities. The following quote In this paper, we present a case study of using an extended summarizes some of the activities that the team performs in static checker HAVOC-LITE3 for checking variants of security conjunction with product teams to mitigate future occurrences of known vulnerabilities that merit security bulletins:1 vulnerabilities in Microsoft Windows and Internet Explorer. We document the challenges encountered in deploying the pre- ”...The MSRC engineering team investigates the vious version of the tool (henceforth called HAVOC) and the surrounding code and design and searches for other extensions needed to apply the tool in a realistic setting. The variants of that threat that could affect customers.” extensions include modeling most common C++ language fea- This is an expensive and arduous process that involves a mix tures used typically in such applications, scaling the contract of manual testing, fuzzing and a large amount of manual inference to be applicable to modules with several hundred security audit. Such audits need to be responsive and timely in thousand procedures, and early annotation validation. We then order to prevent attackers from crafting similar attacks in the near future. We deﬁne the term reactive security audit as this 2The work was done when the ﬁrst author was employed at Microsoft. 3HAVOC-LITE is the new version of HAVOC [2] developed to meet the 1http://www.microsoft.com/security/msrc/whatwedo/updatecycle.aspx needs of this deployment. 1081-6011/13 $26.00 © 2013 IEEE 33 DOI 10.1109/SP.2013.12

}
}

@Article{VelichetiFeiockPeirisEtAl2014,
  Title                    = {Towards modeling the behavior of static code analysis tools},
  Author                   = {Velicheti, Lakshmi Manohar Rao and Feiock, Dennis C. and Peiris, Manjula and Raje, Rajeev and Hill, James H.},
  Journal                  = {Proceedings of the 9th Annual Cyber and Information Security Research Conference on - CISR 鈥�14},
  Year                     = {2014},

  Doi                      = {10.1145/2602087.2602101},
  File                     = {:article\\Towards modeling the behavior of static code analysis tools.pdf:PDF},
  ISBN                     = {http://id.crossref.org/isbn/9781450328128},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/2602087.2602101}
}

@Article{VenkatesanChellappanVengattaramanEtAl2010,
  Title                    = {Advanced mobile agent security models for code integrity and malicious availability check},
  Author                   = {Venkatesan, S. and Chellappan, C. and Vengattaraman, T. and Dhavachelvan, P. and Vaish, Anurika},
  Journal                  = {Journal of Network and Computer Applications},
  Year                     = {2010},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {661鈥�671},
  Volume                   = {33},

  Doi                      = {10.1016/j.jnca.2010.03.010},
  File                     = {:article\\Advanced mobile agent security models for code integrity and malicious availability check.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {1084-8045},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.jnca.2010.03.010}
}

@Article{ViegaBlochKohnoEtAl2000,
  Title                    = {ITS4: A Static Vulnerability Scanner for C and C++ Code},
  Author                   = {John Viega and J.T. Bloch and Yoshi Kohno and Gary McGraw and Reliable Software and Technologies},
  Year                     = {2000},

  Abstract                 = {flow attacks are perhaps the most common security flaw in applications today. The technical details of these attacks are
},
  File                     = {:article\\ITS4  A Static Vulnerability Scanner for C and C++ Code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {ITS4: A Static Vulnerability Scanner for C and C++ Code John Viega, J.T. Bloch, Yoshi Kohno, Gary McGraw Reliable Software Technologies Dulles, Virginia fjviega, jtbloch, kohno, gemg@rstcorp.com http://www.rstcorp.com Abstract flow attacks are perhaps the most common security flaw in applications today. The technical details of these attacks are We describe ITS4, a tool for statically scanning widely discussed in the security community [6, 17]. security-critical C source code for vulnerabilities. Com- In practice, discovering gets in a program usually in- pared to other approaches, our scanning technique stakes dicates a security problem. Nevertheless, this function out a new middle ground between accuracy and efficiency. still resides in the standard C library, along with other This method is efficient enough to offer real-time feedback problematic constructs. Some well-known “gotchas” in- to developers during coding while producing few false neg- clude sprintf, strcpy and strcat. Wagner et. al. atives. Unlike other techniques, our method is also simple discuss more subtle buffer overflow problems with com- enough to scan C++ code despite the complexities inher- mon C functions, including the so-called “safe” alternatives ent in the language. Using ITS4 we found new remotely- snprintf, strncpy and strncat [17]. exploitable vulnerabilities in a widely distributed software Buffer overflow vulnerabilities are not the only known package as well as in a major piece of e-commerce soft- security problems in C and C++ programs. For example, ware. The ITS4 source distribution is available at http: system and popen, two library calls for running pro- //www.rstcorp.com/its4. grams through the command shell, are both notoriously difficult to use correctly. Nonetheless, these functions are commonly used in security-critical applications together 1. Introduction with the unsafe string operations listed above. For exam- ple, sendmail version 8.9.3 boasts 285 individual calls to The C and C++ programming languages and supporting strcpy alone. If these problems are so well known, why libraries make it extremely easy for programmers to inad- are they still so prevalent? vertently introduce security vulnerabilities into their code. There are several reasons: For example, the standard C library defines the gets rou- tine which takes as a parameter a pointer to a character s 1. Well known problems are not universally recognized.. gets reads text from the standard input, placing the first Furthermore, even programmers who know about a character in the location specified by s, and subsequent problem may not focus on the issue when employing a data consecutively in memory. Reading continues until a questionable routine; many programmers consider se- newline or end of file character is reached, at which point curity after writing all the code. the buffer is terminated with a null character. The program- mer has no way to specify the size of the buffer passed to 2. Programmers often know a particular call introduces gets. As a result, when the buffer is n bytes an attacker potential vulnerabilities without understanding the de- trying to write n+m bytes into the buffer will always suc- tails about these problems. ceed if the data excludes newlines. This example illustrates two significant risks. First, vari- 3. Programmers are often unaware of what corrections ables adjacent to the buffer in memory may be overwritten. will eliminate a known problem. If these variables store security-critical data such as an ac- cess control list, then an attacker can modify the data. The 4. Programmers may hope that hazardous constructs are second risk is that an attacker could overflow the stack and not exploitable or that no one will discover vulnerabil- trick the program into running arbitrary code. Stack over- ities (the “security through obscurity” approach).

}
}

@InProceedings{VisserPa˘sa˘reanuKhurshidEtAl2004,
  Title                    = {Test Input Generation with Java PathFinder},
  Author                   = {Willem Visser and Corina S. Pa˘sa˘reanu and Sarfraz Khurshid and RIACS/USRA Kestrel and Technology UT and ARISE},
  Booktitle                = {ISSTA’04},
  Year                     = {2004},

  Abstract                 = {(unstructured) data. In this paper we’ll address the problem We show how model checking and symbolic execution can of doing test input generation for code that manipulates
},
  Doi                      = {ng},
  File                     = {:article\\Test input generation with Java PathFinder.JPF-issta04.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Testing Object-oriented Programs, Model Check- programs manipulating complex data, augmenting it with,literature,article},
  Read                     = {未读},
  Review                   = {Test Input Generation with Java PathFinder Willem Visser Corina S. Pa˘sa˘reanu Sarfraz Khurshid RIACS/USRA Kestrel Technology UT ARISE NASA Ames Research Center NASA Ames Research Center University of Texas at Austin Moffett Field, CA 94035, USA Moffett Field, CA 94035, USA Austin, Texas 78712, USA wvisser@email.arc.nasa.gov pcorina@email.arc.nasa.gov khurshid@ece.utexas.edu ABSTRACT (unstructured) data. In this paper we’ll address the problem We show how model checking and symbolic execution can of doing test input generation for code that manipulates be used to generate test inputs to achieve structural cover- complex data structures. The main research challenge in age of code that manipulates complex data structures. We this area is how to do eﬃcient test input generation that focus on obtaining branch-coverage during unit testing of will obtain high code coverage — we will show how symbolic some of the core methods of the red-black tree implementa- execution over complex data can address this problem. tion in the Java TreeMap library, using the Java PathFinder Model checking [13] has been hugely popular for the last model checker. Three diﬀerent test generation techniques two decades. More recently the application of model check- will be introduced and compared, namely, straight model ing to the analysis of software programs has also come to the checking of the code, model checking used in a black-box fore [7,15,27,43]. Model checking programs however is hard fashion to generate all inputs up to a ﬁxed size, and lastly, due to the complexity of the code and it often cannot com- model checking used during white-box test input generation. pletely analyze the program’s state space since it runs out of The main contribution of this work is to show how eﬃcient memory. For this reason some of the most popular program white-box test input generation can be done for code manip- model checkers rely on (predicate) abstractions [7,27] to re- ulating complex data, taking into account complex method duce the size of the state space, but these techniques are preconditions. not well suited for handling code that manipulates complex data — they introduce too many predicates, making the Categories and Subject Descriptors: D.2.4 [Software abstraction process ineﬃcient. We will show that although Engineering]: Testing and Debugging—Symbolic Execution a program model checker (without relying on abstraction) General Terms: Algorithms, Veriﬁcation cannot always achieve good code coverage when dealing with Keywords: Testing Object-oriented Programs, Model Check- programs manipulating complex data, augmenting it with ing, Symbolic Execution, Coverage, Red-Black Trees symbolic execution (which can be seen as a form of abstrac- tion), can result in the generation of tests that will achieve high code coverage. 1. INTRODUCTION There has been an active research community investigat- Software testing, the most commonly used technique for ing the generation of test inputs with the use of model check- validating the quality of software, is a labor intensive pro- ing [3,4,21,26,28] — the focus is on speciﬁcation-based test cess, and typically accounts for about half the total cost input generation (i.e. black-box testing) where coverage of of software development and maintenance [9]. Automating the speciﬁcation is the goal. Model checking lends itself testing would not only reduce the cost of producing software to test input generation, since one simply speciﬁes as a set but also increase the reliability of modern software. A recent of (temporal) properties that a speciﬁc coverage cannot be report by the National Institute of Standards and Technol- achieved and the model checker will ﬁnd counterexamples, ogy estimates that software failures currently cost the US if they exist, that then can easily be transformed into test economy about $60 billion every year, and that improve- inputs to achieve the stated coverage goal. ments in software testing infrastructure might save one-third Symbolic execution has long been advocated as a means of this cost [1]. for doing eﬃcient test input generation [31], but most of the Automated test case generation has been well studied in ensuing research has focused on generating tests for simple the literature (see Section 6), but most of this work has data types (integers for the most part). focused on the generation of test inputs containing simple In previous work [30] we developed a veriﬁcation frame- work based on symbolic execution and model checking that handles dynamically allocated structures (e.g. lists and trees), simple (primitive) data (e.g. integers and strings) and con- Permission to make digital or hard copies of all or part of this work for currency. The framework uses method preconditions to ini- personal or classroom use is granted without fee provided that copies are tialize ﬁelds only with valid values and method postcondi- not made or distributed for profit or commercial advantage and that copies tions as test oracles to test a method’s correctness. bear this notice and the full citation on the first page. To copy otherwise, to We show here how we used and extended the symbolic republish, to post on servers or to redistribute to lists, requires prior specific permission execution framework from [30] to perform automated testand/or a fee. ISSTA input generation for unit testing of Java programs. To gen-’04, July 11–14, 2004, Boston, Massachusetts, USA. Copyright 2004 ACM 1-58113-820-2/04/0007 ...$5.00. 97

}
}

@InProceedings{VisualLanguagesComputingEtAl2012,
  Title                    = {Contents lists available at SciVerse ScienceDirect Journal of Visual Languages and Computing 23 (2012)},
  Author                   = {Journal of Visual and Languages and Computing and $ Thi and n Corr},
  Year                     = {2012},

  Doi                      = {10.1},
  File                     = {:home/ccc/github/literature/article/1-s2.0-S1045926X12000146-main.pdf:PDF}
}

@Article{VossYamaguchiGreenstadtEtAl2014,
  Title                    = {De-anonymizing Programmers via Code Stylometry},
  Author                   = {Clare Voss and Fabian Yamaguchi and Rachel Greenstadt and ∗Drexel University and ac993@drexel.edu and greenie@cs.drexel.edu and †ARL and richard.e.harang.civ@mail.mil and clare.r.voss.civ@mail.mil},
  Year                     = {2014},

  Abstract                 = {Source code authorship attribution could provide written in a scripting language or source code downloaded into proof of authorship in court, automate the process of finding a the breached system for compilation. cyber criminal from the source code left in an infected system, or The second application is determining whether or not a aid in resolving copyright, copyleft and plagiarism issues in the programming fields. In this work, we investigate methods to de- piece of code was written by its claimed author. This takes anonymize source code authors of C++ using coding style. We various forms including detection of ghostwriting, a form cast source code authorship attribution as a machine learning of plagiarism, and investigation of copyright disputes. We problem using natural language processing techniques to extract elaborate in Section II. the necessary features. The Code Stylometry Feature Set is a We can formulate code stylometry as a machine-learning novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax task consisting of feature extraction followed by classification. trees. Such a unique representation of coding style has not been Prior works have used primarily two types of features: layout used before in code attribution. features such as whitespace that don’t change the meaning of a
},
  File                     = {:article\\De-anonymizing Programmers via Code Stylometry.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {De-anonymizing Programmers via Code Stylometry Aylin Caliskan-Islam∗, Richard Harang†, Andrew Liu‡, Arvind Narayanan§, Clare Voss†, Fabian Yamaguchi¶, and Rachel Greenstadt∗ ∗Drexel University, ac993@drexel.edu, greenie@cs.drexel.edu †ARL, richard.e.harang.civ@mail.mil, clare.r.voss.civ@mail.mil ‡UMD, aliu1@umd.edu §Princeton University, arvindn@cs.princeton.edu ¶ University of Goettingen, fabs@goesec.de Abstract—Source code authorship attribution could provide written in a scripting language or source code downloaded into proof of authorship in court, automate the process of finding a the breached system for compilation. cyber criminal from the source code left in an infected system, or The second application is determining whether or not a aid in resolving copyright, copyleft and plagiarism issues in the programming fields. In this work, we investigate methods to de- piece of code was written by its claimed author. This takes anonymize source code authors of C++ using coding style. We various forms including detection of ghostwriting, a form cast source code authorship attribution as a machine learning of plagiarism, and investigation of copyright disputes. We problem using natural language processing techniques to extract elaborate in Section II. the necessary features. The Code Stylometry Feature Set is a We can formulate code stylometry as a machine-learning novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax task consisting of feature extraction followed by classification. trees. Such a unique representation of coding style has not been Prior works have used primarily two types of features: layout used before in code attribution. features such as whitespace that don’t change the meaning of a Our random forest and abstract syntax tree-based approach program, and lexical features such as those that count various attributes more authors (250) with significantly higher accuracy types of tokens in the language. The latter, roughly speaking, (95%) on a larger data set (Google Code Jam) than has been previously attempted. Furthermore these novel features are more are features that can be obtained via lexical program analysis robust than previous approaches, and are still able to attribute rather than parsing. authors even when code is run through commercial obfuscation Contributions. Our first contribution is the use of syntactic with no significant change in accuracy. This analysis also pro- features for code stylometry. Extracting such features requires duces interesting insights relevant to software engineering. We parsing of incomplete source code using a fuzzy parser to find that (i) the code resulting from difficult programming tasks is easier to attribute than easier tasks and (ii) skilled programmers generate an abstract syntax tree. These features add a com- (who can complete the more difficult tasks) are easier to attribute ponent to code stylometry that has so far remained almost than less skilled programmers. completely unexplored. We also provide evidence that these features are more fundamental and harder to obfuscate. Our I. INTRODUCTION complete feature set consists of a comprehensive set of layout- based, lexical, and syntactic features totalling around 20,000. Do programmers leave fingerprints in their source code? With our complete feature set we are able to achieve a That is, does each programmer have a distinctive “coding dramatic increase in accuracy compared to previous work. style”? Perhaps a programmer has a preference for spaces over Each year more than 15,000 contestants from all around the tabs, or while loops over for loops, or, more subtly, modular world compete in the international programming competition rather than monolithic code. Can elements of coding style be “Google Code Jam”. We scraped the correct solutions of extracted computationally, and if so, what features are most problems implemented in C++ with ground truth authorship informative? Can these extracted styles be used to distinguish information from the available competition data. A bagging one programmer’s code from another, or to determine the (portmanteau of “bootstrap aggregating”) classifier - random author of an unattributed piece of code? forest was used to attribute authors to source code by making Affirmative answers to these questions would have at least use of the relevant features. Our classifiers reach 95% accuracy two sets of applications. The first is software forensics, i.e., in a 250-class closed world task, 99% accuracy on average examining the artifacts on a system after an intrusion to obtain in a two-class task, and 93% accuracy in attributing the test evidence for a criminal prosecution. Often, the adversary instances of the training class in a two-class/one-class task. leaves behind code after an intrusion, either a backdoor or Finally, we analyze various attributes of authors, type of pro- a payload. If we are able to identify the code’s author — for gramming task, and types of features that appear to influence example, by a stylistic comparison of the code with various the success of stylometric identification. We identified the most authors in online code repositories — it may give us clues important 137 features out of 20,000 and 18% of them are about the adversary’s identity. A careful adversary may only syntactic, 4% are layout-based and the rest of the features leave binaries, but a less careful one may leave behind code are lexical. 8 training files with an average of 70 lines of

}
}

@InProceedings{Vulnerabilities2010,
  Title                    = {An Empirical Study on Using the National Vulnerability Database to Predict Software},
  Author                   = {Vulnerabilities},
  Year                     = {2010},

  Abstract                 = {Software vulnerabilities represent a major cause of cyber- security problems. The National Vulnerability Database (NVD) is a pub- lic data source that maintains standardized information about reported software vulnerabilities. Since its inception in 1997, NVD has published information about more than 43,000 software vulnerabilities affecting more than 17,000 software applications. This information is potentially valuable in understanding trends and patterns in software vulnerabil- ities, so that one can better manage the security of computer systems that are pestered by the ubiquitous software security flaws. In particular, one would like to be able to predict the likelihood that a piece of software contains a yet-to-be-discovered vulnerability, which must be taken into account in security management due to the increasing trend in zero-day attacks. We conducted an empirical study on applying data-mining tech- niques on NVD data with the objective of predicting the time to next vulnerability for a given software application. We experimented with var- ious features constructed using the information available in NVD, and applied various machine learning algorithms to examine the predictive power of the data. Our results show that the data in NVD generally have poor prediction capability, with the exception of a few vendors and soft- ware applications. By doing a large number of experiments and observing the data, we suggest several reasons for why the NVD data have not pro- duced a reasonable prediction model for time to next vulnerability with our current approach.
},
  Doi                      = {ng},
  File                     = {:home/ccc/github/literature/article/-An_Empirical_Study_on_Using_the_National_Vulnerability_Database_to_Predict_Software_Vulnerabilities.pdf:PDF},
  Keywords                 = {data mining, cyber-security, vulnerability prediction},
  Review                   = {An Empirical Study on Using the National Vulnerability Database to Predict Software Vulnerabilities Su Zhang, Doina Caragea, and Xinming Ou Kansas State University, {zhangs84,dcaragea,xou}@ksu.edu Abstract. Software vulnerabilities represent a major cause of cyber- security problems. The National Vulnerability Database (NVD) is a pub- lic data source that maintains standardized information about reported software vulnerabilities. Since its inception in 1997, NVD has published information about more than 43,000 software vulnerabilities affecting more than 17,000 software applications. This information is potentially valuable in understanding trends and patterns in software vulnerabil- ities, so that one can better manage the security of computer systems that are pestered by the ubiquitous software security flaws. In particular, one would like to be able to predict the likelihood that a piece of software contains a yet-to-be-discovered vulnerability, which must be taken into account in security management due to the increasing trend in zero-day attacks. We conducted an empirical study on applying data-mining tech- niques on NVD data with the objective of predicting the time to next vulnerability for a given software application. We experimented with var- ious features constructed using the information available in NVD, and applied various machine learning algorithms to examine the predictive power of the data. Our results show that the data in NVD generally have poor prediction capability, with the exception of a few vendors and soft- ware applications. By doing a large number of experiments and observing the data, we suggest several reasons for why the NVD data have not pro- duced a reasonable prediction model for time to next vulnerability with our current approach. Keywords: data mining, cyber-security, vulnerability prediction 1 Introduction Each year a large number of new software vulnerabilities are discovered in var- ious applications (see Figure 1). Evaluation of network security has focused on known vulnerabilities and their effects on the hosts and networks. However, the potential for unknown vulnerabilities (a.k.a. zero-day vulnerabilities) cannot be ignored because more and more cyber attacks utilize these unknown security holes. A zero-day vulnerability could last a long period of time (e.g. in 2010 Microsoft confirmed a vulnerability in Internet Explorer, which affected some versions that were released in 2001). Therefore, in order to have more accurate

}
}

@InProceedings{VulnerabilitiesSoftware3060,
  Title                    = {An Automated Approach for Identifying},
  Author                   = {Potential Vulnerabilities and in Software},
  Year                     = {3060},

  Abstract                 = {databases. Process maturity models and formally ver-
},
  File                     = {:home/ccc/github/literature/article/1998-An Automated Approach for Identifying Potential Vulnerabilities in Software.pdf:PDF},
  Review                   = {An Automated Approach for Identifying  Potential Vulnerabilities in Software Anup K. Ghosh, Tom O'Connor, & Gary McGraw Reliable Software Technologies Corporation 21515 Ridgetop Circle, #250, Sterling, VA 20166 faghosh,toconnor,gemg@rstcorp.com http://www.rstcorp.com Abstract databases. Process maturity models and formally ver- ied protocols play a necessary and important role in This paper presents results from analyzing the vul- developing secure systems. It is important to note, nerability of security-critical software applications to however, that even the most rigorous processes can malicious threats and anomalous events using an au- produce poor quality software [17]. Likewise, even the tomated fault injection analysis approach. The work most rigorously and formally analyzed protocol speci- is based on the well-understood premise that a largecation can be poorly implemented. In practice, mar- proportion of security violations result from errors in ket pressures tend to dominate the engineering and software source code and conguration. The method- development of software, often at the expense of for- ology employs software fault injection to force anoma- mal verication and even testing activities. This is lous program states during the execution of software especially true of commercial grade software for use and observes their corresponding eects on system se- by consumers. The result is a software product em- curity. If insecure behavior is detected, the perturbed ployed in security-critical applications (such as Inter- location that resulted in the violation is isolated for net clients and servers) whose behavioral attributes in further analysis and possibly retrotting with fault- relation to security are largely unknown. tolerant mechanisms. The objective of the approach presented here is 1 Analyzing the behavior of software to provide the capability to analyze software pro- It is now well understood that a vast majority of grams for potential vulnerabilities that can be lever- security intrusions are made possible by aws in soft- aged into security intrusions. Software developers cur- ware. One need only look at the annals of Bugtraq for rently have at their disposal a number of techniques 1 empirical evidence of this assertion. To address this for aiding in the development of quality software, in- problem, computer security researchers and practi- cluding: program debugging, conguration manage- tioners have created mature software engineering pro- ment, memory leak detection, performance proling, cesses such as the TCSEC and the Systems Security load testing, analysis of structural metrics, test case Engineering Capability Maturity Model (SSECMM) generation, and code coverage analysis. While all of [13] to improve the likelihood of producing more se- these tools if properly used can result in higher quality cure systems. software, none are specically oriented toward analy- Another body of research focuses on producing se- sis of security properties. The analysis technique pre- cure protocols for transporting and accessing con- sented in this paper is specically oriented towards dential data across insecure networks and in shared identifying portions of software that if awed can result  This work is sponsored under the Defense Advanced Re- in security violations. This paper presents an auto- search Projects Agency (DARPA) Contract F30602-95-C-0282. mated approach and tool for simulating program aws the views and conclusions contained in this document to determine their potential eect on system security. are those of the authors and should not be interpreted Results from applying the automated fault injection as representing the official policies, either expressed or implied, of the defense advanced research projects analysis on ve common network service daemons are agency or the u.s. government. presented. 1 Bugtraq can be viewed on-line at http://www.netspace.org/lsv-archive/bugtraq.html The approach described here focuses on the behav-

}
}

@Article{WagnerDeanBerkeleyEtAl2001,
  Title                    = {Intrusion Detection via Static Analysis},
  Author                   = {David Wagner and Drew Dean and U.C. Berkeley and Xerox PARC},
  Year                     = {2001},

  Abstract                 = {gram’s execution to be consistent with the program’s source code. We assume that the program was written with be-
},
  File                     = {:article\\Intrusion Detection via Static Analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Publisher                = {IEEE},
  Read                     = {未读},
  Review                   = {Intrusion Detection via Static Analysis David Wagner Drew Dean U.C. Berkeley Xerox PARC daw@cs.berkeley.edu ddean@parc.xerox.com Abstract gram’s execution to be consistent with the program’s source code. We assume that the program was written with be- One of the primary challenges in intrusion detection is nign intent. This approach deals with attacks (such as buffer modelling typical application behavior, so that we can rec- overflows) that cause a program to behave in a manner in- ognize attacks by their atypical effects without raising too consistent with its author’s intent. These are the most preva- many false alarms. We show how static analysis may be lent security problems. Of course, some security problems used to automatically derive a model of application behav- are directly attributable to faulty application logic, such as ior. The result is a host-based intrusion detection system programs that fail to check authentication information be- with three advantages: a high degree of automation, pro- fore proceeding, and one limitation of our intrusion detec- tection against a broad class of attacks based on corrupted tion system is that it does not detect attacks that exploit code, and the elimination of false alarms. We report on logic errors. Application logic bugs, however, are dwarfed our experience with a prototype implementation of this tech- in practice by buffer overflow problems and other vulnera- nique. bilities that allow for execution of arbitrary machine code of the attacker’s choice [8, 35], and it is the latter type of vulnerability on which we focus. 1 The rest of this paper is organized as follows: Section 2. Introduction discusses related work, Section 3 discusses our framework, Section 4 discusses the models we use, Section 5 discusses Computer security has undergone a major renaissance in our implementation, Section 6 evaluates our results, Sec- the last five years. Beginning with Sun’s introduction of the tion 7 discusses future work, and Section 8 concludes. Java language and its support of mobile code in 1995, pro- gramming languages have been a major focus of security research. Many papers have been published applying pro- 2 Related Work gramming language theory to protection problems [25, 24], especially information flow [17]. Security, however, is a Early work on intrusion detection was due to Ander- many-faceted topic, and protection and information flow ad- son [1] and Denning [9]. Since then, it has become a very dress only a subset of the problems faced in building and de- active field. Most intrusion detection systems (IDS) are ploying secure systems. As attackers and defenders are in based on one of two methodologies: either they generate an arms race, deploying a system with strictly static but in- a model of a program’s or system’s behavior from observ- complete security measures is doomed to failure: this gives ing its behavior on known inputs (e.g., [14]), or they require the attacker the last move, and therefore victory. the generation of a rule base (e.g., [3]). In both cases, these Formal methods, alone, are insufficient for building and systems then monitor execution of the deployed program or deploying secure systems. Intrusion detection systems have system and raise an alarm if the execution diverges from the been developed to provide an online auditing capability to model. The current model-based approaches all share one alert the defender that something appears to be wrong. Un- common problem: a truly robust intrusion detection system fortunately, most intrusion detection systems suffer from must solve a special case of the machine learning problem, a major problems as described in Section 2. We take a classic AI problem. That is, to prevent false alarms, the IDS new approach to the problem that eliminates many of these must be able to infer, from statistical data, whether the cur- drawbacks. rent execution of the system is valid or not. The false alarm Our approach constrains the system call trace of a pro- rate of present systems is a major problem in practice [2]. 0-7695-1046-9(C) 2001 IEEE

}
}

@Article{WagnerFosterBrewerEtAl1999,
  Title                    = {A First Step Towards Automated Detection of Buffer Overrun Vulnerabilities},
  Author                   = {David Wagner and Jeffrey S. Foster and Eric A. Brewer and Alexander Aiken and University of California and Berkeley},
  Year                     = {1999},

  File                     = {:article\\A First Step Towards Automated Detection of Buffer Overrun Vulnerabilities.pdf:PDF},
  Groups                   = {binarary vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {设计一种约束语言将C源码转换给整形约束，然后解约束发现漏洞。}
}

@InProceedings{Wagner2004,
  Title                    = {Finding User/Kernel Pointer Bugs With Type Inference},
  Author                   = {Rob Johnson David Wagner},
  Booktitle                = {USENIX 2004},
  Year                     = {2004},

  Abstract                 = {Kernel version Bugs found Linux 2.4.20 11 Linux 2.4.23 10
},
  File                     = {:article\\Finding User Kernel Pointer Bugs With Type Inference.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Finding User/Kernel Pointer Bugs With Type Inference Rob Johnson David Wagner University of California at Berkeley Abstract Kernel version Bugs found Linux 2.4.20 11 Linux 2.4.23 10 Today’s operating systems struggle with vulnerabil- ities from careless handling of user space pointers. User/kernel pointer bugs have serious consequences for Table 1: User/kernel bugs found by CQUAL. Each of security: a malicious user could exploit a user/kernel these bugs represents an exploitable security vulnerabil- pointer bug to gain elevated privileges, read sensitive ity. Four bugs were common to both 2.4.20 and 2.4.23, data, or crash the system. We show how to detect for a total of 17 unique bugs. Eight of the bugs in Linux user/kernel pointer bugs using type-qualifier inference, 2.4.23 were also in Linux 2.5.63. and we apply this method to the Linux kernel using CQUAL, a type-qualifier inference tool. We extend the basic type-inference capabilities of CQUAL to support User/kernel pointer bugs are unfortunately all too com- context-sensitivity and greater precision when analyz- mon. In an attempt to avoid these bugs, the Linux pro- ing structures so that CQUAL requires fewer annota- grammers have created several easy-to-use functions for tions and generates fewer false positives. With these accessing user pointers. As long as programmers use enhancements, we were able to use CQUAL to find 17 these functions correctly, the kernel is safe. Unfortu- exploitable user/kernel pointer bugs in the Linux kernel. nately, almost every device driver must use these func- Several of the bugs we found were missed by careful tions, creating thousands of opportunities for error, and hand audits, other program analysis tools, or both. as a result, user/kernel pointer bugs are endemic. This class of bugs is not unique to Linux. Every version of Unix and Windows must deal with user pointers inside the OS kernel, so a method for automatically checking 1 Introduction an OS kernel for correct user pointer handling would be a big step in developing a provably secure and depend- able operating system. Security critical programs must handle data from un- trusted sources, and mishandling of this data can lead We introduce type-based analyses to detect and elimi- to security vulnerabilities. Safe data-management is par- nate user/kernel pointer bugs. In particular, we augment ticularly crucial in operating systems, where a single bug the C type system with type qualifiers to track the prove- can expose the entire system to attack. Pointers passed nance of all pointers, and then we use type inference to as arguments to system calls are a common type of un- automatically find unsafe uses of user pointers. Type trusted data in OS kernels and have been the cause of qualifier inference provides a principled and semanti- many security vulnerabilities. Such user pointers oc- cally sound way of reasoning about user/kernel pointer cur in many system calls, including, for example, read, bugs. write, ioctl, and statfs. These user pointers must be handled very carefully: since the user program and We implemented our analyses by extending CQUAL[7], operating system kernel reside in conceptually differ- a program verification tool that performs type qualifier ent address spaces, the kernel must not directly derefer- inference. With our tool, we discovered several pre- ence pointers passed from user space, otherwise security viously unknown user/kernel pointer bugs in the Linux holes can result. By exploiting a user/kernel bug, a ma- kernel. In our experiments, we discovered 11 user/kernel licious user could take control of the operating system pointer bugs in Linux kernel 2.4.20 and 10 such bugs in by overwriting kernel data structures, read sensitive data Linux 2.4.23. Four bugs were common to 2.4.20 and out of kernel memory, or simply crash the machine by 2.4.23, for a total of 17 different bugs, and eight of these corrupting kernel data. 17 were still present in the 2.5 development series. We

}
}

@InProceedings{WangChenWang2012,
  Title                    = {Signing Me onto Your Accounts through Facebook and Googlea：Traffic-Guided Security Study of Commercially Deployed Single-Sign-On Web Services},
  Author                   = {Rui Wang and Shuo Chen and XiaoFeng Wang},
  Booktitle                = {2012 {IEEE} Symposium on Security and Privacy},
  Year                     = {2012},
  Month                    = {may},
  Publisher                = {{IEEE}},

  Doi                      = {10.1109/sp.2012.30},
  File                     = {:article\\Signing Me onto Your Accounts through Facebook and Google.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/SP.2012.30}
}

@InProceedings{WangWeiGuEtAl2010,
  Title                    = {TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection},
  Author                   = {Tielei Wang and Tao Wei and Guofei Gu and Wei Zou},
  Booktitle                = {2010 {IEEE} Symposium on Security and Privacy},
  Year                     = {2010},
  Publisher                = {{IEEE}},

  Doi                      = {10.1109/sp.2010.37},
  File                     = {:article\\TaintScope_ A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1109/SP.2010.37}
}

@Article{Wang2010,
  Title                    = {BEST: An Assembler Structural Representation Tool Based On Flow Analysis},
  Author                   = {Wei Wang},
  Journal                  = {2010 International Conference on Management and Service Science (MASS 2010)},
  Year                     = {2010},
  Pages                    = {IEEE Wuhan Sect.},

  __markedentry            = {[ccc:6]},
  Abstract                 = {When mining security vulnerabilities in software, a structural intermediate representation of binary code should be obtained first. In this paper, we propose PANDA, a vulnerability-mining-oriented intermediate language and a series of algorithms for assembler understanding based on flow analysis. We implement a lightweight prototype system named BEST for assembler structural representation. The system uses control flow analysis and data flow analysis techniques to identify common control structures, analyze executive flow of a program, reconstruct expressions and functions, find data dependency, finally transform the assembler into a structured PANDA intermediate language program. Experiment results show that our system produces high quality intermediate language programs for further automatic security analysis.},
  Bn                       = {978-1-4244-5325-2},
  Cl                       = {Wuhan, China},
  Ct                       = {2010 International Conference on Management and Service Science (MASSEOLEOL2010)},
  Cy                       = {24-26 Aug. 2010},
  Doi                      = {10.1109/ICMSS.2010.5575669},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:11540074},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{WooKilSamanthaEtAl2013,
  Title                    = {Scheduling Black-box Mutational Fuzzing},
  Author                   = {Maverick Woo and Sang Kil and Cha Samantha and Gottlieb David and Brumley and Carnegie Mellon and University},
  Booktitle                = {CCS'13},
  Year                     = {2013},

  Abstract                 = {program? Above all, how can she maximize her likelihood of success within the given time budget?
},
  Doi                      = {.org/10.1145/2508859.2516736.},
  File                     = {:article\\Scheduling blackbox mutational fuzzing.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {automatic analysis. A fuzz campaign takes this list as inputand reports each new (previously unseen) bug when it is Software Security; Fuzz Configuration Scheduling discovered. As a simplification, we also assume that the},
  Read                     = {未读},
  Review                   = {Scheduling Black-box Mutational Fuzzing Maverick Woo Sang Kil Cha Samantha Gottlieb David Brumley Carnegie Mellon University {pooh,sangkilc,sgottlie,dbrumley}@cmu.edu ABSTRACT program? Above all, how can she maximize her likelihood of success within the given time budget? Black-box mutational fuzzing is a simple yet effective tech- nique to find bugs in software. Given a set of program-seed In this paper, we focus on the setting where our analyst pairs, we ask how to schedule the fuzzings of these pairs in has chosen to find bugs via black-box mutational fuzzing. At order to maximize the number of unique bugs found at any a high level, this technique takes as input a program p and a point in time. We develop an analytic framework using a seed s that is usually assumed to be a well-formed input for p. mathematical model of black-box mutational fuzzing and Then, a program known as a black-box mutational fuzzer is use it to evaluate 26 existing and new randomized online used to fuzz the program p with the seed s, i.e., execute p on scheduling algorithms. Our experiments show that one of a potentially malformed input x obtained by randomly mu- our new scheduling algorithms outperforms the multi-armed tating s in a precise manner to be described in §2. Through bandit algorithm in the current version of the CERT Basic repeated fuzzings, we may discover a number of inputs that Fuzzing Framework (BFF) by finding 1.5× more unique bugs crash p. These crashing inputs are then passed to down- in the same amount of time. stream analyses to triage each crash into a corresponding bug, test each newly-discovered bug for exploitability, and Categories and Subject Descriptors generate exploits when possible. D.2.5 [Software Engineering]: Testing and Debugging— Intuitively, our analyst may try to improve her chances Testing Tools by finding the greatest number of unique bugs among the programs to be analyzed within the given time budget. To General Terms model this, let us introduce the notion of a fuzz campaign. We assume our analyst has already obtained a list of program- Security seed pairs (pi, si) to be fuzzed through prior manual and/or Keywords automatic analysis. A fuzz campaign takes this list as inputand reports each new (previously unseen) bug when it is Software Security; Fuzz Configuration Scheduling discovered. As a simplification, we also assume that the fuzz campaign is orchestrated in epochs. At the beginning of 1 Introduction each epoch we select one program-seed pair based only on information obtained during the campaign, and we fuzz that A General (or professor) walks into a cramped cubicle, telling pair for the entire epoch. This latter assumption has two the lone security analyst (or graduate student) that she has subtle but important implications. First, though it does not one week to find a zero-day exploit against a certain popular limit us to fuzzing with only one computer, it does require OS distribution, all the while making it sound as if this task that every computer in the campaign fuzz the same program- is as easy as catching the next bus. Although our analyst seed pair during an epoch. Second, while our definition of has access to several program analysis tools for finding bugs a fuzz configuration in §2 is more general than a program- [8, 10, 11, 21] and generating exploits [4, 9], she still faces a seed pair, we also explain our decision to equate these two harsh reality: the target OS distribution contains thousands concepts in our present work. As such, what we need to of programs, each with potentially tens or even hundreds select for each epoch is really a fuzz configuration, which of yet undiscovered bugs. What tools should she use for gives rise to our naming of the Fuzz Configuration Scheduling this mission? Which programs should she analyze, and in (FCS) problem. what order? How much time should she dedicate to a given To find the greatest number of unique bugs given the above problem setting, our analyst must allocate her time wisely. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed Since initially she has no information on which configuration for profit or commercial advantage and that copies bear this notice and the full cita- will yield more new bugs, she should explore the configu- tion on the first page. Copyrights for components of this work owned by others than rations and reduce her risk by fuzzing each configuration ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- for an adequate amount of time. As she starts to identify publish, to post on servers or to redistribute to lists, requires prior specific permission some configurations that she believes may yield more new and/or a fee. Request permissions from permissions@acm.org. bugs in the future, she should also exploit this information CCS’13, November 4–8, 2013, Berlin, Germany. Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00. by increasing the time allocated to fuzz these configurations. http://dx.doi.org/10.1145/2508859.2516736. Of course, any increase in exploitation reduces exploration,

}
}

@MastersThesis{Wright2014,
  Title                    = {SOFTWARE VULNERABILITIES: LIFESPANS, METRICS, AND CASE STUDY},
  Author                   = {Jason L. Wright},
  Year                     = {2014},

  File                     = {:article\\SOFTWARE VU LNERABILITIES LIFESPANS, METRICS, AND CASE STUDY.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {SOFTWARE VULNERABILITIES: LIFESPANS, METRICS, AND CASE STUDY A Thesis Presented in Partial Fulfillment of the Requirements for the Degree of Master of Science with a Major in Computer Science in the College of Graduate Studies University of Idaho by Jason L. Wright May 2014 Major Professor: Milos Manic, Ph.D.

}
}

@InProceedings{WuGraceZhouEtAl2013,
  Title                    = {The impact of vendor customizations on android security},
  Author                   = {Lei Wu and Michael Grace and Yajin Zhou and Chiachih Wu and Xuxian Jiang},
  Booktitle                = {Proceedings of the 2013 {ACM} {SIGSAC} conference on Computer {\&} communications security - {CCS} {\textquotesingle}13},
  Year                     = {2013},
  Publisher                = {{ACM} Press},

  Doi                      = {10.1145/2508859.2516728},
  File                     = {:article\\The Impact of Vendor Customizations on Android Security.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1145/2508859.2516728}
}

@InProceedings{XuSai¨diAndersonEtAl2011,
  Title                    = {Aurasium: Practical Policy Enforcement for Android Applications},
  Author                   = {Rubin Xu and Hassen Saı¨di and Ross Anderson and Computer Laboratory and Computer Science and Laboratory Computer and Laboratory},
  Year                     = {2011},

  Abstract                 = {MacAfee [29], making it the most assaulted mobile op- The increasing popularity of Google’s mobile platform erating system during that period. While much of the Android makes it the prime target of the latest surge in initial wave of Android malware consisted of trojans that mobile malware. Most research on enhancing the plat- masquerade as legitimate applications and leak a user’s form’s security and privacy controls requires extensive personal information or send SMS messages to premium modification to the operating system, which has signif- numbers, recent malware samples indicate an escalation icant usability issues and hinders efforts for widespread in the capability and stealth of Android malware. In par- adoption. We develop a novel solution called Aurasium ticular, attempts are made to gain root access on the de- that bypasses the need to modify the Android OS while vice through escalation of privilege [37] to establish a providing much of the security and privacy that users de- stealthy permanent presence on the device or to bypass sire. We automatically repackage arbitrary applications Android permission checks. to attach user-level sandboxing and policy enforcement Fighting malware and securing Android-powered de- code, which closely watches the application’s behavior vices has focused on three major directions. The first for security and privacy violations such as attempts to re- one consists of statically [20] and dynamically [12, 36] trieve a user’s sensitive information, send SMS covertly analyzing application code to detect malicious activities to premium numbers, or access malicious IP addresses. before the application is loaded onto the user’s device. Aurasium can also detect and prevent cases of privilege The second consists of modifying the Android OS to in- escalation attacks. Experiments show that we can apply sert monitoring modules at key interfaces to allow the this solution to a large sample of benign and malicious interception of malicious activity as it occurs on the de- applications with a near 100 percent success rate, with- vice [19, 27, 17, 33, 13]. The third approach consists of out significant performance and space overhead. Aura- using virtualization to implement rigorous separation of sium has been tested on three versions of the Android domains ranging from lightweight isolation of applica- OS, and is freely available. tions on the device [35] to running multiple instances of
},
  File                     = {:article\\Aurasium_Practical Policy Enforcement for Android Applications.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Aurasium: Practical Policy Enforcement for Android Applications Rubin Xu Hassen Saı¨di Ross Anderson Computer Laboratory Computer Science Laboratory Computer Laboratory University of Cambridge SRI International University of Cambridge Cambridge, UK Menlo Park, USA Cambridge, UK Rubin.Xu@cl.cam.ac.uk hassen.saidi@sri.com Ross.Anderson@cl.cam.ac.uk Abstract MacAfee [29], making it the most assaulted mobile op- The increasing popularity of Google’s mobile platform erating system during that period. While much of the Android makes it the prime target of the latest surge in initial wave of Android malware consisted of trojans that mobile malware. Most research on enhancing the plat- masquerade as legitimate applications and leak a user’s form’s security and privacy controls requires extensive personal information or send SMS messages to premium modification to the operating system, which has signif- numbers, recent malware samples indicate an escalation icant usability issues and hinders efforts for widespread in the capability and stealth of Android malware. In par- adoption. We develop a novel solution called Aurasium ticular, attempts are made to gain root access on the de- that bypasses the need to modify the Android OS while vice through escalation of privilege [37] to establish a providing much of the security and privacy that users de- stealthy permanent presence on the device or to bypass sire. We automatically repackage arbitrary applications Android permission checks. to attach user-level sandboxing and policy enforcement Fighting malware and securing Android-powered de- code, which closely watches the application’s behavior vices has focused on three major directions. The first for security and privacy violations such as attempts to re- one consists of statically [20] and dynamically [12, 36] trieve a user’s sensitive information, send SMS covertly analyzing application code to detect malicious activities to premium numbers, or access malicious IP addresses. before the application is loaded onto the user’s device. Aurasium can also detect and prevent cases of privilege The second consists of modifying the Android OS to in- escalation attacks. Experiments show that we can apply sert monitoring modules at key interfaces to allow the this solution to a large sample of benign and malicious interception of malicious activity as it occurs on the de- applications with a near 100 percent success rate, with- vice [19, 27, 17, 33, 13]. The third approach consists of out significant performance and space overhead. Aura- using virtualization to implement rigorous separation of sium has been tested on three versions of the Android domains ranging from lightweight isolation of applica- OS, and is freely available. tions on the device [35] to running multiple instances of Android on the same device through the use of a hyper- 1 Introduction visor [26, 30, 11]. Two fundamental and intertwined problems plague Google’s Android OS is undoubtedly the fastest grow- these approaches. The first is that the definition of ma- ing mobile operating system in the world. In July 2011, licious behavior in an Android application is hard to as- Nielsen placed the market share of Android in the U.S. certain. Access to privacy- and security-relevant parts at 38 percent of all active U.S. smartphones [9]. Weeks of Android’s API is controlled by an install-time appli- later, for the period ending in August, Nielsen found that cation permission system. Android users are informed Android has risen to 43 percent. More important, among about what data and resources an application will have those who bought their phones in June, July, or August, access to, and user consent is required before the appli- Google had a formidable 56 percent market share. This cation can be installed. These explicit permissions are unprecedented growth in popularity, together with the declared in the application package. Install-time permis- openness of its application ecosystem, has attracted ma- sions provide users with control over their privacy, but licious entities to aggressively target Android. Attacks are often coarse-grained. A permission granted at install on Android by malware writers have jumped by 76 per- time is granted as long as the application is installed on cent over the past three months according to a report by the device. While an application might legitimately re-

}
}

@Article{YakdanEschweilerGerhards-PadillaEtAl2015,
  Title                    = {No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations},
  Author                   = {Khaled Yakdan and Sebastian Eschweiler and Elmar Gerhards-Padilla and Matthew Smith and ∗University of Bonn and Germany},
  Year                     = {2015},

  Abstract                 = {Decompilation is important for many security appli- effective countermeasures and mitigation strategies requires a cations; it facilitates the tedious task of manual malware reverse thorough understanding of functionality and actions performed engineering and enables the use of source-based security tools on by the malware. Although many automated malware analysis binary code. This includes tools to find vulnerabilities, discover techniques have been developed, security analysts often have bugs, and perform taint tracking. Recovering high-level control to resort to manual reverse engineering, which is difficult and constructs is essential for decompilation in order to produce structured code that is suitable for human analysts and source- time-consuming. Decompilers that can reliably generate high- based program analysis techniques. State-of-the-art decompilers level code are very important tools in the fight against malware: rely on structural analysis, a pattern-matching approach over they speed up the reverse engineering process by enabling the control flow graph, to recover control constructs from malware analysts to reason about the high-level form of code binary code. Whenever no match is found, they generate goto instead of its low-level assembly form. statements and thus produce unstructured decompiled output. Those statements are problematic because they make decompiled Decompilation is not only beneficial for manual analy- code harder to understand and less suitable for program analysis. sis, but also enables the application of a wealth of source-
},
  Doi                      = {.org/10.14722/ndss.2015.23185},
  File                     = {:article\\No More Gotos Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations Khaled Yakdan∗, Sebastian Eschweiler†, Elmar Gerhards-Padilla†, Matthew Smith∗ ∗University of Bonn, Germany {yakdan, smith}@cs.uni-bonn.de †Fraunhofer FKIE, Germany {sebastian.eschweiler, elmar.gerhards-padilla}@fkie.fraunhofer.de Abstract—Decompilation is important for many security appli- effective countermeasures and mitigation strategies requires a cations; it facilitates the tedious task of manual malware reverse thorough understanding of functionality and actions performed engineering and enables the use of source-based security tools on by the malware. Although many automated malware analysis binary code. This includes tools to find vulnerabilities, discover techniques have been developed, security analysts often have bugs, and perform taint tracking. Recovering high-level control to resort to manual reverse engineering, which is difficult and constructs is essential for decompilation in order to produce structured code that is suitable for human analysts and source- time-consuming. Decompilers that can reliably generate high- based program analysis techniques. State-of-the-art decompilers level code are very important tools in the fight against malware: rely on structural analysis, a pattern-matching approach over they speed up the reverse engineering process by enabling the control flow graph, to recover control constructs from malware analysts to reason about the high-level form of code binary code. Whenever no match is found, they generate goto instead of its low-level assembly form. statements and thus produce unstructured decompiled output. Those statements are problematic because they make decompiled Decompilation is not only beneficial for manual analy- code harder to understand and less suitable for program analysis. sis, but also enables the application of a wealth of source- based security techniques in cases where only binary code In this paper, we present DREAM, the first decompiler is available. This includes techniques to discover bugs [5], to offer a goto-free output. DREAM uses a novel pattern- apply taint tracking [10], or find vulnerabilities such as RICH independent control-flow structuring algorithm that can recover all control constructs in binary programs and produce structured [7], KINT [38], Chucky [42], Dowser [24], and the property decompiled code without any goto statement. We also present graph approach [41]. These techniques benefit from the high- semantics-preserving transformations that can transform unstruc- level abstractions available in source code and therefore are tured control flow graphs into structured graphs. We demonstrate faster and more efficient than their binary-based counterparts. the correctness of our algorithms and show that we outperform For example, the average runtime overhead for the source- both the leading industry and academic decompilers: Hex-Rays based taint tracking system developed by Chang et al. [10] is and Phoenix. We use the GNU coreutils suite of utilities as a 0.65% for server programs and 12.93% for compute-bound benchmark. Apart from reducing the number of goto statements applications, whereas the overhead of Minemu, the fastest to zero, DREAM also produced more compact code (less lines of binary-based taint tracker, is between 150% and 300% [6]. code) for 72.7% of decompiled functions compared to Hex-Rays and 98.8% compared to Phoenix. We also present a comparison One of the essential steps in decompilation is control-flow of Hex-Rays and DREAM when decompiling three samples from structuring, which is a process that recovers the high-level Cridex, ZeusP2P, and SpyEye malware families. control constructs (e.g., if-then-else or while loops) from the program’s control flow graph (CFG) and thus plays a vital I. INTRODUCTION role in creating code which is readable by humans. State-of- the-art decompilers such as Hex-Rays [22] and Phoenix [33] Malicious software (malware) is one of the most serious employ structural analysis [31, 34] (§II-A3) for this step. At a threats to the Internet security today. The level of sophistication high level, structural analysis is a pattern-matching approach employed by current malware continues to evolve significantly. that tries to find high-level control constructs by matching For example, modern botnets use advanced cryptography, com- regions in the CFG against a predefined set of region schemas. plex communication and protocols to make reverse engineering When no match is found, structural analysis must use goto harder. These security measures employed by malware authors statements to encode the control flow inside the region. As a are seriously hampering the efforts by computer security result, it is very common for the decompiled code to contain researchers and law enforcement [4, 32] to understand and many goto statements. For instance, the de facto industry stan- take down botnets and other types of malware. Developing dard decompiler Hex-Rays (version v2.0.0.140605) produces 1,571 goto statements for a peer-to-peer Zeus sample (MD5 hash 49305d949fd7a2ac778407ae42c4d2ba) that consists of Permission to freely reproduce all or part of this paper for noncommercial purposes is granted provided that copies bear this notice and the full citation 997 nontrivial functions (functions with more than one basic on the first page. Reproduction for commercial purposes is strictly prohibited block). The decompiled malware code consists of 49,514 lines without the prior written consent of the Internet Society, the first-named author of code. Thus, on average it contains one goto statement for (for reproduction of an entire paper only), and the author’s employer if the each 32 lines of code. This high number of goto statements paper was prepared within the scope of employment. makes the decompiled code less suitable for both manual NDSS ’15, 8-11 February 2015, San Diego, CA, USA Copyright 2015 Internet Society, ISBN 1-891562-38-X and automated program analyses. Structured code is easier to http://dx.doi.org/10.14722/ndss.2015.23185 understand [16] and helps scale program analysis [31]. The

}
}

@InProceedings{Yamaguchi2015,
  Title                    = {Pattern-Based Vulnerability and Discovery},
  Author                   = {Fabian Yamaguchi},
  Year                     = {2015},

  File                     = {:article\\Pattern-Based Vulnerability Discovery.pdf:PDF},
  Review                   = {Georg-August-Universita¨t Go¨ttingen Pattern-Based Vulnerability Discovery Dissertation zur Erlangung des mathematisch-naturwissenschaftlichen Doktorgrades “Doctor rerum naturalium” der Georg-August-Universita¨t Go¨ttingen im PhD Programmme in Computer Science (PCS) der Georg-August University School of Science (GAUSS) vorgelegt von Fabian Yamaguchi aus Bochum Go¨ttingen 2015

}
}

@InProceedings{Yamaguchi2013,
  Title                    = {Chucky- Exposing missing checks in source code for Vulnerability Discovery（CR 12）},
  Author                   = {Fabian Yamaguchi},
  Year                     = {2013},

  Doi                      = {10.1145/2508859.2516665},
  File                     = {:article\\Chucky- Exposing missing checks in source code for Vulnerability Discovery（CR 12）.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {rank5},
  Read                     = {未读},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/258903333 Chucky:ExposingMissingChecksinSource CodeforVulnerabilityDiscovery CONFERENCEPAPER·OCTOBER2013 DOI:10.1145/2508859.2516665 CITATIONS READS 3 58 4AUTHORS: FabianYamaguchi ChristianWressnegger Georg-August-UniversitätGöttingen Georg-August-UniversitätGöttingen 6PUBLICATIONS24CITATIONS 7PUBLICATIONS9CITATIONS SEEPROFILE SEEPROFILE HugoGascon KonradRieck Georg-August-UniversitätGöttingen Georg-August-UniversitätGöttingen 10PUBLICATIONS34CITATIONS 59PUBLICATIONS883CITATIONS SEEPROFILE SEEPROFILE Availablefrom:HugoGascon Retrievedon:19October2015

}
}

@Article{YamaguchiGoldeArpEtAl2014,
  Title                    = {Modeling and discovering vulnerabilities with code property graphs},
  Author                   = {Yamaguchi, F. and Golde, N. and Arp, D. and Rieck, K.},
  Journal                  = {2014 IEEE Symposium on Security and Privacy. Proceedings},
  Year                     = {2014},
  Pages                    = {590--604},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method to effectively mine large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
  Bn                       = {978-1-4799-4686-0},
  Cl                       = {San Jose, CA, USA},
  Ct                       = {2014 IEEE Symposium on Security and Privacy (SP)},
  Cy                       = {18-21 May 2014},
  Doi                      = {10.1109/SP.2014.44},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:14773642},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{YamaguchiGoldeArpEtAl2013,
  Title                    = {Modeling and Discovering Vulnerabilities with Code Property Graphs},
  Author                   = {Fabian Yamaguchi and Nico Golde and Daniel Arp and Konrad Rieck and ∗University of Go¨ttingen and Germany},
  Year                     = {2013},

  Abstract                 = {The vast majority of security breaches encountered incarnations in speciﬁc software projects is often still not today are a direct result of insecure code. Consequently, the possible without signiﬁcant expert knowledge [16]. protection of computer systems critically depends on the rigorous identiﬁcation of vulnerabilities in software, a tedious and error- As a result of this situation, security research has initially},
  File                     = {:article\\Modeling and Discovering Vulnerabilities with Code Property Graphs.pdf:PDF;:note\\《Modeling and Discovering Vulnerabilities with Code Property Graphs》\\《Modeling and Discovering Vulnerabilities with Code Property Graphs》 V3.pptx:PowerPoint 2007+;:note\\《Modeling and Discovering Vulnerabilities with Code Property Graphs》\\《Modeling and Discovering Vulnerabilities with Code Property Graphs》 -note.docx:Word 2007+},
  Groups                   = {source code vulnerability},
  Keywords                 = {Vulnerabilities, Static Analysis, Graph Databases analysis with expert knowledge and can thereby guide thesearch for vulnerabilities [e.g., 39, 43, 44]. In this paper, we continue this direction of research and, rank5},
  Rd                       = {Y},
  Read                     = {精读},
  Review                   = {Modeling and Discovering Vulnerabilities with Code Property Graphs Fabian Yamaguchi∗, Nico Golde†, Daniel Arp∗ and Konrad Rieck∗ ∗University of Go¨ttingen, Germany †Qualcomm Research Germany Abstract—The vast majority of security breaches encountered incarnations in speciﬁc software projects is often still not today are a direct result of insecure code. Consequently, the possible without signiﬁcant expert knowledge [16]. protection of computer systems critically depends on the rigorous identiﬁcation of vulnerabilities in software, a tedious and error- As a result of this situation, security research has initially prone process requiring signiﬁcant expertise. Unfortunately, a focused on statically ﬁnding speciﬁc types of vulnerabilities, single ﬂaw sufﬁces to undermine the security of a system and such as ﬂaws induced by insecure library functions [6], buffer thus the sheer amount of code to audit plays into the attacker’s overﬂows [45], integer overﬂows [40] or insufﬁcient validation cards. In this paper, we present a method to effectively mine of input data [18]. Based on concepts from software testing, large amounts of source code for vulnerabilities. To this end, a broader detection of vulnerabilities has then been achieved we introduce a novel representation of source code called a using dynamic program analysis, ranging from simple fuzz code property graph that merges concepts of classic program testing [e.g., 38, 42] to advanced taint tracking and symbolic analysis, namely abstract syntax trees, control ﬂow graphs and execution [e.g., 2, 35]. While these approaches can discover program dependence graphs, into a joint data structure. This different types of ﬂaws, they are hard to operate efﬁciently in comprehensive representation enables us to elegantly model tem- plates for common vulnerabilities with graph traversals that, for practice and often fail to provide appropriate results due to instance, can identify buffer overﬂows, integer overﬂows, format either prohibitive runtime or the exponential growth of execu- string vulnerabilities, or memory disclosures. We implement our tion paths to consider [16, 21]. As a remedy, security research approach using a popular graph database and demonstrate its has recently started to explore approaches that assist an analyst efﬁcacy by identifying 18 previously unknown vulnerabilities in during auditing instead of replacing her. The proposed methods the source code of the Linux kernel. accelerate the auditing process by augmenting static program Keywords—Vulnerabilities; Static Analysis; Graph Databases analysis with expert knowledge and can thereby guide thesearch for vulnerabilities [e.g., 39, 43, 44]. In this paper, we continue this direction of research and I. INTRODUCTION present a novel approach for mining large amounts of source The security of computer systems fundamentally depends code for vulnerabilities. Our approach combines classic con- on the quality of its underlying software. Despite a long series cepts of program analysis with recent developments in the ﬁeld of research in academia and industry, security vulnerabilities of graph mining. The key insight underlying our approach is regularly manifest in program code, for example as failures that many vulnerabilities can only be adequately discovered to account for buffer boundaries or as insufﬁcient validation by jointly taking into account the structure, control ﬂow and of input data. Consequently, vulnerabilities in software remain dependencies of code. We address this requirement by intro- one of the primary causes for security breaches today. For ducing a novel representation of source code denoted as code example, in 2013 a single buffer overﬂow in a universal plug- property graph. This graph combines properties of abstract and-play library rendered over 23 million routers vulnerable syntax trees, control ﬂow graphs and program dependence to attacks from the Internet [26]. Similarly, thousands of graphs in a joint data structure. This comprehensive view on users currently fall victim to web-based malware that exploits code enables us to elegantly model templates for common different ﬂaws in the Java runtime environment [29]. vulnerabilities using graph traversals. Similar to the query in a The discovery of software vulnerabilities is a classic yet database, a graph traversal passes over the code property graph challenging problem of security. Due to the inability of a and inspects the code structure, the control ﬂow, and the data program to identify non-trivial properties of another program, dependencies associated with each node. This joint access to the generic problem of ﬁnding software vulnerabilities is different code properties enables crafting concise templates for undecidable [33]. As a consequence, current means for spotting several types of ﬂaws and thereby helps to audit large amounts security ﬂaws are either limited to speciﬁc types of vulnera- of code for vulnerabilities. bilities or build on tedious and manual auditing. In particular, We implement our approach using a popular graph database securing large software projects, such as an operating system and demonstrate its practical merits by designing graph traver- kernel, resembles a daunting task, as a single ﬂaw may sals for several well-known vulnerability types, such as buffer undermine the security of the entire code base. Although some overﬂows, integer overﬂows, format string vulnerabilities, or classes of vulnerabilities reoccurring throughout the software memory disclosures. As a show case, we analyze the source landscape exist for a long time, such as buffer overﬂows code of the Linux kernel—a large and well-audited code and format string vulnerabilities, automatically detecting their base. We ﬁnd that almost all vulnerabilities reported for}
}

@InProceedings{YamaguchiLindnerRieckEtAl2010,
  Title                    = {Vulnerability Extrapolation: Assisted Discovery of Vulnerabilities using Machine Learning},
  Author                   = {Fabian Yamaguchi and Felix ’FX’ Lindner and Konrad Rieck and 1Recurity Labs GmbH and Germany},
  Year                     = {2010},

  Abstract                 = {fundamental inability of a program to completely anal- Rigorous identification of vulnerabilities in program yse another program’s code however, determining vul- code is a key to implementing and operating secure sys- nerabilities automatically has proved to be an involved tems. Unfortunately, only some types of vulnerabilities and often daunting task. Current tools for automatic code can be detected automatically. While techniques from analysis, such as Fortify 360 and Microsoft PREfast, are software testing can accelerate the search for security thus limited to detecting vulnerabilities following well- flaws, in the general case discovery of vulnerabilities is known programming patterns. While techniques derived a tedious process that requires significant expertise and from software testing, such as fuzz testing [32], taint time. In this paper, we propose a method for assisted analysis [20] and symbolic execution [3, 29], may accel- discovery of vulnerabilities in source code. Our method erate analysis of program code, the general discovery of proceeds by embedding code in a vector space and auto- vulnerabilities still rests on tedious manual auditing that matically determining API usage patterns using machine requires considerable expertise and resources. learning. Starting from a known vulnerability, these As a remedy, we propose a method for assisted dis- patterns can be exploited to guide the auditing of code covery of vulnerabilities in source code. Instead of and to identify potentially vulnerable code with similar struggling with the limitations of automatic analysis, our characteristics—a process we refer to as vulnerability ex- method aims at rendering manual auditing more effective trapolation. We empirically demonstrate the capabilities by assisting and guiding the inspection of source code. of our method in different experiments. In a case study To this end, the method embeds code in a vector space, with the library FFmpeg, we are able to narrow the search such that typical patterns of API usage can be determined for interesting code from 6,778 to 20 functions and dis- automatically using machine learning techniques. These cover two security flaws, one being a known flaw and the patterns implicitly capture semantics of the code and al- other constituting a zero-day vulnerability. low to “extrapolate” known vulnerabilities by identifying
},
  File                     = {:article\\Vulnerability Extrapolation-Assisted Discovery of Vulnerabilities using Machine Learning2011-woot.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {rank5},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Vulnerability Extrapolation: Assisted Discovery of Vulnerabilities using Machine Learning Fabian Yamaguchi1, Felix ’FX’ Lindner1, and Konrad Rieck2 1Recurity Labs GmbH, Germany 2Technische Universita¨t Berlin, Germany Abstract fundamental inability of a program to completely anal- Rigorous identification of vulnerabilities in program yse another program’s code however, determining vul- code is a key to implementing and operating secure sys- nerabilities automatically has proved to be an involved tems. Unfortunately, only some types of vulnerabilities and often daunting task. Current tools for automatic code can be detected automatically. While techniques from analysis, such as Fortify 360 and Microsoft PREfast, are software testing can accelerate the search for security thus limited to detecting vulnerabilities following well- flaws, in the general case discovery of vulnerabilities is known programming patterns. While techniques derived a tedious process that requires significant expertise and from software testing, such as fuzz testing [32], taint time. In this paper, we propose a method for assisted analysis [20] and symbolic execution [3, 29], may accel- discovery of vulnerabilities in source code. Our method erate analysis of program code, the general discovery of proceeds by embedding code in a vector space and auto- vulnerabilities still rests on tedious manual auditing that matically determining API usage patterns using machine requires considerable expertise and resources. learning. Starting from a known vulnerability, these As a remedy, we propose a method for assisted dis- patterns can be exploited to guide the auditing of code covery of vulnerabilities in source code. Instead of and to identify potentially vulnerable code with similar struggling with the limitations of automatic analysis, our characteristics—a process we refer to as vulnerability ex- method aims at rendering manual auditing more effective trapolation. We empirically demonstrate the capabilities by assisting and guiding the inspection of source code. of our method in different experiments. In a case study To this end, the method embeds code in a vector space, with the library FFmpeg, we are able to narrow the search such that typical patterns of API usage can be determined for interesting code from 6,778 to 20 functions and dis- automatically using machine learning techniques. These cover two security flaws, one being a known flaw and the patterns implicitly capture semantics of the code and al- other constituting a zero-day vulnerability. low to “extrapolate” known vulnerabilities by identifying potentially vulnerable code with similar characteristics. 1 Introduction This process of vulnerability extrapolation can suggest candidates for investigation to the analyst as well as ease The security of computer systems critically depends on the browsing of source code during auditing. the quality and security of its underlying program code We empirically demonstrate the capabilities of this. Unfortunately, there is a persistent deficit of security method to identify usage patterns and to accelerate code awareness in software development [37] and often the auditing in different experiments. In a case study with pressure of business competition rules out the design and the popular library FFmpeg and a known vulnerability implementation of secure software. As a result, there ex (CVE-2010-3429), our method narrows the search for in-- ist numerous examples of programming flaws that have teresting code from 6,778 to 20 functions. Out of these led to severe security incidents and the proliferation of 20 functions, we can identify two security flaws, one be- malicious software [e.g., 11, 19, 24] ing another known weakness and the other constituting. Often these flaws emerge as zero-day vulnerabilities, rendering defense us a zero-day vulnerability. We prove the relevance of this- ing reactive security tools almost impossible finding by providing a working exploit.. From its early days, computer security has been con- The rest of this paper is structured as follows: we cerned with developing methods for discovery and elim- introduce our method for vulnerability extrapolation in ination of vulnerabilities in program code. Due to the Section 2. An evaluation and a case study with FFmpeg

}
}

@Article{YamaguchiLottmannRieckEtAl2012,
  Title                    = {Generalized Vulnerability Extrapolation using Abstract Syntax Trees},
  Author                   = {Fabian Yamaguchi and Markus Lottmann and Konrad Rieck and University of Göttingen and Technische Universität and Berlin University and of Göttingen and Göttingen and Germany Berlin and Germany Göttingen and Germany},
  Year                     = {2012},

  Abstract                 = {search has focused on devising methods for identifying spe- The discovery of vulnerabilities in source code is a key for cific types of vulnerabilities.
},
  File                     = {:article\\Generalized Vulnerability Extrapolation using  Abstract Syntactic Trees-2012-acsac（CR17）.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature, article, rank5},
  Read                     = {未读},
  Review                   = {Generalized Vulnerability Extrapolation using Abstract Syntax Trees Fabian Yamaguchi Markus Lottmann Konrad Rieck University of Göttingen Technische Universität Berlin University of Göttingen Göttingen, Germany Berlin, Germany Göttingen, Germany ABSTRACT search has focused on devising methods for identifying spe- The discovery of vulnerabilities in source code is a key for cific types of vulnerabilities. securing computer systems. While specific types of security Several approaches have been proposed that statically iden- flaws can be identified automatically, in the general case the tify patterns of specific vulnerabilities [e.g., 4, 18, 28, 32], process of finding vulnerabilities cannot be automated and such as the use of certain insecure functions. Moreover, con- vulnerabilities are mainly discovered by manual analysis. In cepts from the area of software verification have been suc- this paper, we propose a method for assisting a security an- cessfully adapted for tracking vulnerabilities, for example, alyst during auditing of source code. Our method proceeds in form of fuzz testing [27], taint analysis [22] and symbolic by extracting abstract syntax trees from the code and de- execution [1, 8]. Many of these approaches, however, are termining structural patterns in these trees, such that each limited to specific conditions and types of vulnerabilities. function in the code can be described as a mixture of these The discovery of vulnerabilities in practice still mainly rests patterns. This representation enables us to decompose a on tedious manual auditing that requires considerable time known vulnerability and extrapolate it to a code base, such and expertise. that functions potentially suffering from the same flaw can In this paper, we propose a method for assisting a security be suggested to the analyst. We evaluate our method on the analyst during auditing of source code. Instead of striving source code of four popular open-source projects: LibTIFF, for an automated solution, we aim at rendering manual au- FFmpeg, Pidgin and Asterisk. For three of these projects, diting more effective by guiding the search for vulnerabili- we are able to identify zero-day vulnerabilities by inspecting ties. Based on the idea of vulnerability extrapolation [33], our only a small fraction of the code bases. method proceeds by extracting abstract syntax trees from the source code and determining structural patterns in these trees, such that each function in the code can be described as a mixture of the extracted patterns. The patterns contain 1. INTRODUCTION subtrees with nodes corresponding to types, functions and The security of computer systems critically depends on the syntactical constructs of the code base. This representation quality of its underlying code. Even minor flaws in a code enables our method to decompose a known vulnerability and base can severely undermine the security of a computer sys- to suggest code with similar properties—potentially suffer- tem and make it an easy victim for attackers. There exist ing from the same flaw—to the analyst for auditing. several examples of vulnerabilities that have led to security We evaluate the efficacy of our method using the source incidents and the proliferation of malicious code in the past code of four popular open-source projects: LibTIFF, FFm- [e.g. 21, 26]. A drastic case is the malware Stuxnet [7] that peg, Pidgin and Asterisk. We first demonstrate in an quan- featured code for exploiting four unknown vulnerabilities in titative evaluation how functions are decomposed into struc- the Windows operating system, rendering conventional de- tural patterns and how similar code can be identified auto- fense techniques ineffective in practice. matically. In a controlled experiment we are able to narrow The discovery of vulnerabilities in source code is a cen- the search for a given vulnerability to 8.7% of the code base tral issue of computer security. Unfortunately, the process and consistently outperform non-structured approaches for of finding vulnerabilities cannot be automated in the gen- vulnerability extrapolation. We also study the discovery of eral case. According to Rice’s theorem a computer pro- real vulnerabilities in a qualitative evaluation, where we are gram is unable to generally decide whether another program able to discover 10 zero-day vulnerabilities in the source code contains vulnerable code [10]. Consequently, security re- of the four open-source projects. In summary, we make the following contributions: • Generalized vulnerability extrapolation: We present a general approach to the extrapolation of vulnerabili- Permission to make digital or hard copies of all or part of this work for ties, allowing both the content and structure of code personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies to be considered for finding similar flaws in a code base. bear this notice and the full citation on the first page. To copy otherwise, to • Structural comparison of code: We present a method republish, to post on servers or to redistribute to lists, requires prior specific for robust extraction and analysis of abstract syntax permission and/or a fee. ACSAC ’12 Dec. 3-7, 2012, Orlando, Florida USA trees that allows for automatic comparison of code Copyright 2012 ACM 978-1-4503-1312-4/12/12 ...$15.00. with respect to structural patterns.

}
}

@InProceedings{,
  Title                    = {Generalized Vulnerability Extrapolation using Abstract Syntax Trees},
  Author                   = {Fabian Yamaguchi and Markus Lottmann and Konrad Rieck and University of G�ttingen and Technische Universit�t and Berlin University and of G�ttingen and G�ttingen and Germany Berlin and Germany G�ttingen and Germany},
  Year                     = {2012},

  Abstract                 = {search has focused on devising methods for identifying spe- The discovery of vulnerabilities in source code is a key for cific types of vulnerabilities.
},
  File                     = {:home/ccc/github/literature/article/Generalized Vulnerability Extrapolation using  Abstract Syntactic Trees-2012-acsac?CR17?.pdf:PDF},
  Review                   = {Generalized Vulnerability Extrapolation using Abstract Syntax Trees
Fabian Yamaguchi Markus Lottmann Konrad Rieck University of G�ttingen Technische Universit�t Berlin University of G�ttingen G�ttingen, Germany Berlin, Germany G�ttingen, Germany
ABSTRACT search has focused on devising methods for identifying spe- The discovery of vulnerabilities in source code is a key for cific types of vulnerabilities.
securing computer systems. While specific types of security Several approaches have been proposed that statically iden-
flaws can be identified automatically, in the general case the tify patterns of specific vulnerabilities [e.g., 4, 18, 28, 32],
process of finding vulnerabilities cannot be automated and such as the use of certain insecure functions. Moreover, con-
vulnerabilities are mainly discovered by manual analysis. In cepts from the area of software verification have been suc-
this paper, we propose a method for assisting a security an- cessfully adapted for tracking vulnerabilities, for example,
alyst during auditing of source code. Our method proceeds in form of fuzz testing [27], taint analysis [22] and symbolic
by extracting abstract syntax trees from the code and de- execution [1, 8]. Many of these approaches, however, are
termining structural patterns in these trees, such that each limited to specific conditions and types of vulnerabilities.
function in the code can be described as a mixture of these The discovery of vulnerabilities in practice still mainly rests
patterns. This representation enables us to decompose a on tedious manual auditing that requires considerable time
known vulnerability and extrapolate it to a code base, such and expertise.
that functions potentially suffering from the same flaw can In this paper, we propose a method for assisting a security
be suggested to the analyst. We evaluate our method on the analyst during auditing of source code. Instead of striving
source code of four popular open-source projects: LibTIFF, for an automated solution, we aim at rendering manual au-
FFmpeg, Pidgin and Asterisk. For three of these projects, diting more effective by guiding the search for vulnerabili-
we are able to identify zero-day vulnerabilities by inspecting ties. Based on the idea of vulnerability extrapolation [33], our
only a small fraction of the code bases. method proceeds by extracting abstract syntax trees from the source code and determining structural patterns in these trees, such that each function in the code can be described as a mixture of the extracted patterns. The patterns contain
1. INTRODUCTION subtrees with nodes corresponding to types, functions and The security of computer systems critically depends on the syntactical constructs of the code base. This representation
quality of its underlying code. Even minor flaws in a code enables our method to decompose a known vulnerability and
base can severely undermine the security of a computer sys- to suggest code with similar properties?potentially suffer-
tem and make it an easy victim for attackers. There exist ing from the same flaw?to the analyst for auditing.
several examples of vulnerabilities that have led to security We evaluate the efficacy of our method using the source
incidents and the proliferation of malicious code in the past code of four popular open-source projects: LibTIFF, FFm-
[e.g. 21, 26]. A drastic case is the malware Stuxnet [7] that peg, Pidgin and Asterisk. We first demonstrate in an quan-
featured code for exploiting four unknown vulnerabilities in titative evaluation how functions are decomposed into struc-
the Windows operating system, rendering conventional de- tural patterns and how similar code can be identified auto-
fense techniques ineffective in practice. matically. In a controlled experiment we are able to narrow
The discovery of vulnerabilities in source code is a cen- the search for a given vulnerability to 8.7% of the code base
tral issue of computer security. Unfortunately, the process and consistently outperform non-structured approaches for
of finding vulnerabilities cannot be automated in the gen- vulnerability extrapolation. We also study the discovery of
eral case. According to Rice?s theorem a computer pro- real vulnerabilities in a qualitative evaluation, where we are
gram is unable to generally decide whether another program able to discover 10 zero-day vulnerabilities in the source code
contains vulnerable code [10]. Consequently, security re- of the four open-source projects. In summary, we make the following contributions:
? Generalized vulnerability extrapolation: We present a general approach to the extrapolation of vulnerabili-
Permission to make digital or hard copies of all or part of this work for ties, allowing both the content and structure of code personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies to be considered for finding similar flaws in a code base.
bear this notice and the full citation on the first page. To copy otherwise, to ? Structural comparison of code: We present a method republish, to post on servers or to redistribute to lists, requires prior specific for robust extraction and analysis of abstract syntax permission and/or a fee. ACSAC ?12 Dec. 3-7, 2012, Orlando, Florida USA trees that allows for automatic comparison of code
Copyright 2012 ACM 978-1-4503-1312-4/12/12 ...$15.00. with respect to structural patterns.

}
}

@InProceedings{YamaguchiMaierGasconEtAl2015,
  Title                    = {Automatic Inference of Search Patterns for Taint-Style Vulnerabilities},
  Author                   = {Fabian Yamaguchi and Alwin Maier and Hugo Gascon and Konrad Rieck and University of Go¨ttingen and Germany},
  Year                     = {2015},

  Abstract                 = {Taint-style vulnerabilities are a persistent problem for detection of web application vulnerabilities have been in software development, as the recently discovered “Heartbleed” proposed, for example for SQL injection flaws [e.g., 10, 26], vulnerability strikingly illustrates. In this class of vulnerabil- cross-site scripting [e.g., 31, 48] and missing authorization ities, attacker-controlled data is passed unsanitized from an input source to a sensitive sink. While simple instances of this checks [19, 51]. More recently, several researchers have rec- vulnerability class can be detected automatically, more subtle ognized that many common vulnerabilities in both, system defects involving data flow across several functions or project- software and web applications, share an underlying theme specific APIs are mainly discovered by manual auditing. Different rooted in information flow analysis: data propagates from an techniques have been proposed to accelerate this process by attacker-controlled input source to a sensitive sink without searching for typical patterns of vulnerable code. However, all of these approaches require a security expert to manually model undergoing prior sanitization, a class of vulnerabilities referred and specify appropriate patterns in practice. to as taint-style vulnerabilities [see 9, 10, 26, 63].
},
  File                     = {:article\\Automatic Inference of Search Patterns for Taint-Style Vulnerabilities.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {rank5},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Automatic Inference of Search Patterns for Taint-Style Vulnerabilities Fabian Yamaguchi, Alwin Maier, Hugo Gascon, and Konrad Rieck University of Go¨ttingen, Germany Abstract—Taint-style vulnerabilities are a persistent problem for detection of web application vulnerabilities have been in software development, as the recently discovered “Heartbleed” proposed, for example for SQL injection flaws [e.g., 10, 26], vulnerability strikingly illustrates. In this class of vulnerabil- cross-site scripting [e.g., 31, 48] and missing authorization ities, attacker-controlled data is passed unsanitized from an input source to a sensitive sink. While simple instances of this checks [19, 51]. More recently, several researchers have rec- vulnerability class can be detected automatically, more subtle ognized that many common vulnerabilities in both, system defects involving data flow across several functions or project- software and web applications, share an underlying theme specific APIs are mainly discovered by manual auditing. Different rooted in information flow analysis: data propagates from an techniques have been proposed to accelerate this process by attacker-controlled input source to a sensitive sink without searching for typical patterns of vulnerable code. However, all of these approaches require a security expert to manually model undergoing prior sanitization, a class of vulnerabilities referred and specify appropriate patterns in practice. to as taint-style vulnerabilities [see 9, 10, 26, 63]. In this paper, we propose a method for automatically inferring Different approaches have been devised that enable mining search patterns for taint-style vulnerabilities in C code. Given a for taint-style vulnerabilities using description languages that security-sensitive sink, such as a memory function, our method automatically identifies corresponding source-sink systems and allow dangerous programming patterns to be precisely en- constructs patterns that model the data flow and sanitization in coded [30, 35, 63]. In theory, this idea bares the possibility to these systems. The inferred patterns are expressed as traversals construct a large database of patterns for known vulnerabilities in a code property graph and enable efficiently searching for that can be easily matched against source code. Unfortunately, unsanitized data flows—across several functions as well as similar to signature-based intrusion detection systems, con- with project-specific APIs. We demonstrate the efficacy of this approach in different experiments with 5 open-source projects. structing effective search patterns for vulnerabilities requires The inferred search patterns reduce the amount of code to inspect a security expert to invest a considerable amount of manual for finding known vulnerabilities by 94.9% and also enable us work. Starting from a security-sensitive sink, the expert needs to uncover 8 previously unknown vulnerabilities. to identify related input sources, data flows and corresponding Index Terms—Vulnerabilities; Clustering; Graph Databases; sanitizations checks, which often involves a profound under- standing of project-specific functions and interfaces. I. INTRODUCTION In this paper, we present a method for automatically in- The discovery and elimination of vulnerabilities in software ferring search patterns for taint-style vulnerabilities from C is a fundamental problem of computer security. Unfortunately, source code. Given a sensitive sink, such as a memory or even subtle defects, such as a single missing authorization network function, our method automatically identifies corre- check or a slightly insufficient sanitization of data can al- sponding source-sink systems in a code base, analyzes the ready lead to severe security vulnerabilities in software. The data flow in these systems and generates search patterns that necessity for development of more effective approaches for reflect the characteristics of taint-style vulnerabilities. To this the discovery of such vulnerabilities has been made strikingly end, we combine techniques from static program analysis and obvious by the recent “Heartbleed” vulnerability in the cryp- unsupervised machine learning that enable us to construct tographic library OpenSSL [1] and the “Shellshock” vulnera- patterns that are usually identified by manual analysis and that bility in GNU Bash [2]. As programs are constantly modified allow for pinpointing insufficient sanitization, even if the data and the properties of the platforms they operate on change, flow crosses several function boundaries and involves project- new vulnerabilities regularly emerge. In effect, vulnerability specific APIs. Analysts can employ this method to generate discovery becomes an on-going process, requiring experts with patterns for API functions known to commonly be associated a deep understanding of the software in question and all the with vulnerabilities, as well as to find instances of the same technologies its security relies upon. vulnerability spread throughout the code base. Due to the diversity of vulnerable programming practices, We implement our approach by extending the analysis plat- security research has largely focused on detecting specific form Joern1 to support interprocedural analysis and developing types of vulnerabilities. For example, fuzz testing [e.g., 20, 53] a plugin for extracting and matching of search patterns, that and symbolic execution [e.g., 49, 59] have been successfully is, robust descriptions of syntax, control flow and data flow applied to find memory corruption vulnerabilities, such as that characterize a vulnerability. The platform is build on buffer overflows, integer overflows and format string vulner- abilities. In line with this research, a variety of approaches 1A Robust Code Analysis Platform for C/C++, http://mlsec.org/joern

}
}

@InProceedings{ Yarochkin Sinica TsungEtAl2013,
  Title                    = {Hunting the Shadows：In Depth Analysis of Escalated APT Attacks-slides},
  Author                   = {Fyodor  Yarochkin and  Academia  Sinica and Pei  Kan  PK  Tsung and  Academia  Sinica},
  Booktitle                = {Blackhat USA 2013},
  Year                     = {2013},

  File                     = {:article\\US-13-Yarochkin-In-Depth-Analysis-of-Escalated-APT-Attacks-Slides.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Hun$ng  the  Shadows:   In  Depth  Analysis  of  Escalated  APT  A=acks   Fyodor  Yarochkin,  Academia  Sinica   Pei  Kan  PK  Tsung,  Academia  Sinica   Ming-­‐Chang  Jeremy  Chiu,  Xecure  Lab   Ming-­‐Wei  Benson  Wu,  Xecure  Lab   1 

}
}

@Article{YiFan-pingMei-chao2011,
  Title                    = {Fuzzing Technique Based on Dynamic Input Tracking},
  Author                   = {Huang Yi and Zeng Fan-ping and Zhang Mei-chao},
  Journal                  = {Computer Engineering},
  Year                     = {2011},
  Number                   = {6},
  Pages                    = {44--5,},
  Volume                   = {37},

  __markedentry            = {[ccc:6]},
  Abstract                 = {This paper proposes a new fuzzing technique based on input path tracking technology on disassembly code, which is combined with code-coverage-based test data generation and snapshot-recovery-based fault injection techniques. It is a new method for automatic software security vulnerability discovering and solves a number of limitations of traditional fuzzing techniques. A test system based on this method is designed and implemented and the method is validated by vulnerabilities discovering experiment on example software.},
  Doi                      = {10.3969/j.issn.1000-3428.2011.06.016},
  Groups                   = {Code Mining},
  Sn                       = {1000-3428},
  Tc                       = {0},
  Ut                       = {INSPEC:12059420},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{YounisMalaiyaAndersonEtAl2016,
  Title                    = {To Fear or Not to Fear That is the Question},
  Author                   = {Younis, Awad and Malaiya, Yashwant and Anderson, Charles and Ray, Indrajit},
  Journal                  = {Proceedings of the Sixth ACM on Conference on Data and Application Security and Privacy - CODASPY ’16},
  Year                     = {2016},

  Doi                      = {10.1145/2857705.2857750},
  File                     = {:home/ccc/github/literature/article/-To Fear or Not to Fear That is the Question\: Code Characteristics of a Vulnerable Functionwith an Existing Exploit.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/9781450339353},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/2857705.2857750}
}

@Article{YounisMalaiyaAndersonEtAl2016a,
  Title                    = {To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Function with an Existing Exploit},
  Author                   = {Younis, Awad and Malaiya, Yashwant and Anderson, Charles and Ray, Indrajit},
  Journal                  = {Proceedings of the Sixth ACM on Conference on Data and Application Security and Privacy - CODASPY ’16},
  Year                     = {2016},

  Doi                      = {10.1145/2857705.2857750},
  File                     = {:home/ccc/github/literature/article/-To Fear or Not to Fear That is the Question\: Code.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/9781450339353},
  Publisher                = {Association for Computing Machinery (ACM)},
  Url                      = {http://dx.doi.org/10.1145/2857705.2857750}
}

@Article{YounisMalaiyaRay2015,
  Title                    = {Assessing vulnerability exploitability risk using software properties},
  Author                   = {Younis, Awad and Malaiya, Yashwant K. and Ray, Indrajit},
  Journal                  = {Software Quality Journal},
  Year                     = {2015},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {159–202},
  Volume                   = {24},

  Doi                      = {10.1007/s11219-015-9274-6},
  File                     = {:home/ccc/github/literature/article/Assessing vulnerability exploitablility risk using software properties.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1573-1367},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/s11219-015-9274-6}
}

@Article{ZhangTanZhangEtAl2011,
  Title                    = {Checking enforcement of integrity constraints in database applications based on code patterns},
  Author                   = {Zhang, Hongyu and Tan, Hee Beng Kuan and Zhang, Lu and Lin, Xi and Wang, Xiaoyin and Zhang, Chun and Mei, Hong},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2011},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {2253--2264},
  Volume                   = {84},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Integrity constraints (including key, referential and domain constraints) are unique features of database applications. Integrity constraints are crucial for ensuring accuracy and consistency of data in a database. It is important to perform integrity constraint enforcement (ICE) at the application level to reduce the risk of database corruption. We have conducted an empirical analysis of open-source PHP database applications and found that ICE does not receive enough attention in real-world programming practice. We propose an approach for automatic detection of ICE violations at the application level based on identification of code patterns. We define four patterns that characterize the structures of code implementing integrity constraint enforcement. Violations of these patterns indicate the missing of integrity constraint enforcement. Our work contributes to quality improvement of database applications. Our work also demonstrates that it is feasible to effectively identify bugs or problematic code by mining code patterns in a specific domain/application area. (C) 2011 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.jss.2011.06.044},
  Groups                   = {Code Mining},
  Sn                       = {0164-1212},
  Tc                       = {2},
  Ut                       = {WOS:000296415100017},
  Z8                       = {0},
  Z9                       = {3},
  Zb                       = {0},
  Zr                       = {1},
  Zs                       = {0}
}

@InProceedings{ZhangDuanYinEtAl2014,
  Title                    = {Semantics-Aware Android Malware Classification Using Weighted Contextual {API} Dependency Graphs},
  Author                   = {Mu Zhang and Yue Duan and Heng Yin and Zhiruo Zhao},
  Booktitle                = {Proceedings of the 2014 {ACM} {SIGSAC} Conference on Computer and Communications Security - {CCS} {\textquotesingle}14},
  Year                     = {2014},
  Publisher                = {{ACM} Press},

  Doi                      = {10.1145/2660267.2660359},
  File                     = {:article\\Semantics-Aware Android Malware Classification Using.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1145/2660267.2660359}
}

@TechReport{ZhangPrakashLiEtAl2005,
  Title                    = {Identifying and Analyzing Pointer Misuses for Sophisticated Memory-corruption Exploit Diagnosis},
  Author                   = {Mingwei Zhang and Aravind Prakash and Xiaolei Li and Zhenkai Liang and Heng Yin and 1School of Computing and National University of Singapore and 2Department of Computer Science and Syracuse University},
  Year                     = {2005},

  Abstract                 = {address-space-layout randomization (ASLR) [3, 9], attack- ers are no longer able to directly execute malicious code
},
  File                     = {:article\\Pointer Misuses_Yin Heng.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {非源码}
}

@Article{ZhangHuangQiEtAl2012,
  Title                    = {Static program analysis assisted dynamic taint tracking for software vulnerability discovery},
  Author                   = {Zhang, Ruoyu and Huang, Shiqiu and Qi, Zhengwei and Guan, Haibing},
  Journal                  = {Computers \& Mathematics with Applications},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {2},
  Pages                    = {469–480},
  Volume                   = {63},

  Doi                      = {10.1016/j.camwa.2011.08.001},
  File                     = {:home/ccc/github/literature/article/Static program analysis assisted dynamic taint tracking for software vulnerability discovery-.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {0898-1221},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.camwa.2011.08.001}
}

@Article{ZhangCarageaOu2011,
  Title                    = {An Empirical Study on Using the National Vulnerability Database to Predict Software Vulnerabilities},
  Author                   = {Zhang, Su and Caragea, Doina and Ou, Xinming},
  Journal                  = {Database and Expert Systems Applications},
  Year                     = {2011},
  Pages                    = {217–231},

  Doi                      = {10.1007/978-3-642-23088-2_15},
  File                     = {:home/ccc/github/literature/article/An Empirical Study on Using the National Vulnerability Database to Predict Software Vulnerabilities.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-3-642-23088-2},
  ISSN                     = {1611-3349},
  Publisher                = {Springer Science + Business Media},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-23088-2_15}
}

@Article{ZhangLoXiaEtAl2015,
  Title                    = {Combining software metrics and text features for vulnerable file prediction},
  Author                   = {Yun Zhang and Lo, D. and Xin Xia and Bowen Xu and Jianling Sun and Shanping Li},
  Journal                  = {2015 20th International Conference on Engineering of Complex Computer Systems (ICECCS). Proceedings},
  Year                     = {2015},
  Pages                    = {40--9},

  __markedentry            = {[ccc:6]},
  Abstract                 = {In recent years, to help developers reduce time and effort required to build highly secure software, a number of prediction models which are built on different kinds of features have been proposed to identify vulnerable source code files. In this paper, we propose a novel approach VULPREDICTOR to predict vulnerable files, it analyzes software metrics and text mining together to build a composite prediction model. VULPREDICTOR first builds 6 underlying classifiers on a training set of vulnerable and non-vulnerable files represented by their software metrics and text features, and then constructs a meta classifier to process the outputs of the 6 underlying classifiers. We evaluate our solution on datasets from three web applications including Drupal, PHPMyAdmin and Moodle which contain a total of 3,466 files and 223 vulnerabilities. The experiment results show that VULPREDICTOR can achieve F1 and EffectivenessRatio@20% scores of up to 0.683 and 75%, respectively. On average across the 3 projects, VULPREDICTOR improves the F1 and EffectivenessRatio@20% scores of the best performing state-of-the-art approaches proposed by Walden et al. by 46.53% and 14.93%, respectively.},
  Bn                       = {978-1-4673-8581-7},
  Cl                       = {Gold Coast, QLD, Australia},
  Ct                       = {2015 20th International Conference on Engineering of Complex ComputerEOLEOLSystems (ICECCS)},
  Cy                       = {9-12 Dec. 2015},
  Doi                      = {10.1109/ICECCS.2015.15},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15721091},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{Zhaoyang2014,
  Title                    = {Smart grid cyber security},
  Author                   = {Dong Zhaoyang},
  Journal                  = {2014 13th International Conference on Control Automation Robotics \& Vision (ICARCV). Proceedings},
  Year                     = {2014},
  Pages                    = {1--2},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Summary form only given. The trend of integrating power systems with advanced computer and communication technologies has introduced serious cyber security concerns, especially in a smart grid environment where the cyber system is no longer regarded as 100% reliable to support power system communications and control as before. Power system security therefore extends to potential cyber security domain in the smart grid era. Risks from the cyber system as well as non-conventional physical power system contingencies start to contributing to the overall grid security. This will be particularity important considering the potential risks from targeted attacks on vulnerable system components which may bring done the overall system. The presentation gives an overview of the work done by the research team on power system security, including conventional stability as well as cyber security assessment. A framework for smart grid cyber security and vulnerability assessment will be illustrated as well. The framework includes two main components, which are respectively cyber system security assessment and fast power system security assessment. Complex networks theory and data mining based approaches are also employed to identify the vulnerable components of the physical power system. The proposed cyber system models can be integrated with existing power system models to study the complex interactions between the cyber and physical parts of the smart grid. Advanced modeling tools are proposed to model cybThe trend of integrating power systems with advanced computer and communication technologies has introduced serious cyber security concerns, especially in a smart grid environment where the cyber system is no longer regarded as 100% reliable to support power system communications and control as before. Power system security therefore extends to potential cyber security domain in the smart grid era. Risks from the cyber system as well as non-conventional physical power system contingencies start to contributing to the overall grid security. This will be particularity important considering the potential risks from targeted attacks on vulnerable system components which may bring done the overall system. The presentation gives an overview of the work done by the research team on power system security, including conventional stability as well as cyber security assessment. A framework for smart grid cyber security and vulnerability assessment will be illustrated as well. The framework includes two main components, which are respectively cyber system security assessment and fast power system security assessment. Complex networks theory and data mining based approaches are also employed to identify the vulnerable components of the physical power system. The proposed cyber system models can be integrated with existing power system models to study the complex interactions between the cyber and physical parts of the smart grid. Advanced modeling tools are proposed to model cyber attacks and evaluate their impacts on smart grid security have been developed as well.er attacks and evaluate their impacts on smart grid security have been developed as well.},
  Bn                       = {978-1-4799-5199-4},
  Cl                       = {Singapore, Singapore},
  Ct                       = {2014 13th International Conference on Control, Automation, Robotics &EOLEOLVision (ICARCV)},
  Cy                       = {10-12 Dec. 2014},
  Doi                      = {10.1109/ICARCV.2014.7064485},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:15001247},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@Article{Zhou2013,
  Title                    = {Identity，Location，Disease and More: Inferring Your Secrets from Android Public Resources},
  Author                   = {Xiaoyong Zhou},
  Year                     = {2013},

  Booktitle                = {Proceedings of the 2013 {ACM} {SIGSAC} conference on Computer {\&} communications security - {CCS} {\textquotesingle}13},
  Doi                      = {10.1145/2508859.2516661},
  File                     = {:article\\Inferring Your Secrets from Android Public Resources.pdf:PDF},
  Groups                   = {source code vulnerability},
  Publisher                = {{ACM} Press},
  Rd                       = {N},
  Read                     = {未读},
  Url                      = {http://dx.doi.org/10.1145/2508859.2516661}
}

@Article{ZhouXiangChen2016,
  Title                    = {Metamorphic Testing for Software Quality Assessment: A Study of Search Engines},
  Author                   = {Zhou, Zhi Quan and Xiang, Shaowen and Chen, Tsong Yueh},
  Journal                  = {IIEEE Trans. Software Eng.},
  Year                     = {2016},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {260–280},
  Volume                   = {42},

  Doi                      = {10.1109/tse.2015.2478001},
  File                     = {:home/ccc/github/literature/article/Metamorphic Testing for Software Quality.pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISSN                     = {1939-3520},
  Publisher                = {Institute of Electrical \& Electronics Engineers (IEEE)},
  Url                      = {http://dx.doi.org/10.1109/TSE.2015.2478001}
}

@Article{ZhuLiuWangEtAl2015,
  Title                    = {Dytaint_ The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs},
  Author                   = {Zhu, Erzhou and Liu, Feng and Wang, Zuo and Liang, Alei and Zhang, Yiwen and Li, Xuejian and Li, Xuejun},
  Journal                  = {Computers \& Security},
  Year                     = {2015},

  Month                    = {Jul},
  Pages                    = {51鈥�69},
  Volume                   = {52},

  Doi                      = {10.1016/j.cose.2015.03.008},
  File                     = {:article\\Dytaint_ The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs.pdf:PDF},
  Groups                   = {source code vulnerability},
  ISSN                     = {0167-4048},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.cose.2015.03.008}
}

@Article{ZhuWuAtwood2011,
  Title                    = {A New Fuzzing Method Using Multi Data Samples Combination},
  Author                   = {Xueyong Zhu and Zhiyong Wu and Atwood, J. W.},
  Journal                  = {Journal of Computers},
  Year                     = {2011},

  Month                    = may,
  Number                   = {5},
  Pages                    = {881--8},
  Volume                   = {6},

  __markedentry            = {[ccc:6]},
  Abstract                 = {Knowledge-based Fuzzing technologies have been applied successfully in software vulnerability mining, however, its current methods mainly focus on Fuzzing target software using a single data sample with one or multi-dimension input mutation [1], and thus the vulnerability mining results are not stable, false negatives of vulnerability are high and the selection of data sample depends on human analysis. To solve these problems, this paper proposes a model named Fuzzing Test Suite Generation model using multi data sample combination (FTSGc), which can automatically select multi data samples combination from a large scale data sample set to fuzz target software and generate the test cases that can cover more codes of the software vulnerabilities. To solve Data Sample Coverage Problem (DSCP) in the proposed FTSGc, a method of covering maximum nodes' semantic attributes with minimum running cost is put forward and a theorem named Maximum Coverage Theorem is given to select the data sample combination. We conclude that DSCP is actually the Set Covering Problem (SCP). Practical experimental results show that the proposed Fuzzing method works much better than the other current Fuzzing method on the Ability of Vulnerability Mining (AVM).},
  Doi                      = {10.4304/jcp.6.5.881-888},
  Groups                   = {Code Mining},
  Sn                       = {1796-203X},
  Tc                       = {0},
  Ut                       = {INSPEC:11991023},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{ZimmermannNachiappanNagappanEtAl,
  Title                    = {Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista},
  Author                   = {Thomas Zimmermann and 1 Nachiappan and Nagappan and Laurie Williams and 2 and tzimmer@microsoft.com nachin@microsoft.com williams@csc.ncsu.edu},

  Abstract                 = {Many factors are believed to increase the reliable (i.e., works as expected) but not secure or a software vulnerability of software system; for example, the more widely system can be secure (e.g., adopting threat modeling deployed or popular is a software system the more likely it is to effectively, eliminating buffer overflows programmatically, be attacked. Early identification of defects has been a widely etc.) but not reliable (does not work as expected). To better investigated topic in software engineering research. Early address both security and reliability, it is essential to identification of software vulnerabilities can help mitigate understand differences and similarities between these two these attacks to a large degree by focusing better security fields. verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities Towards that end, we leverage existing metrics that have are, most often, few in number and introduce significant bias been used in prior research for defect prediction by creating a sparse dataset in the population. As a result, [17][18][19][20] to understand and investigate the efficacy vulnerability prediction can be thought of us preverbally of these metrics for vulnerability prediction. More formally, “searching for a needle in a haystack.” In this paper, we our research hypothesis is to investigate and report on the present a large-scale empirical study on Windows Vista, where ability of classical defect prediction metrics to be used as we empirically evaluate the efficacy of classical metrics like predictors for vulnerability prediction. For this purpose, complexity, churn, coverage, dependency measures, and we study Windows Vista, which is a large and widely-used organizational structure of the company to predict commercial operating system from Microsoft Corporation. A vulnerabilities and assess how well these software measures statistical challenge in our study is motivated by the fact that correlate with vulnerabilities. We observed in our experiments vulnerabilities are few and widely distributed in the dataset that classical software measures predict vulnerabilities with a akin to searching for a needle in a haystack. In our study for high precision but low recall values. The actual dependencies, example, only 66 advisories have been recorded for Vista however, predict vulnerabilities with a lower precision but (40 Million plus lines of code) in the National Vulnerability substantially higher recall. Database (NVD) [21], and only few of the Windows binaries 
},
  File                     = {:home/ccc/github/literature/article/-Searching for a Needle in a Haystack\: Predicting Security Vulnerabilities for Windows Vista.pdf:PDF},
  Keywords                 = {Vulnerabilities, Prediction, Metrics, Complexity, This statistical challenge involves identifying which of Churn, Coverage, Dependencies, Organizational Structure the classical metrics related to code quality can predict},
  Review                   = {Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista Thomas Zimmermann 1 Nachiappan Nagappan 1 Laurie Williams 2 tzimmer@microsoft.com nachin@microsoft.com williams@csc.ncsu.edu 1 Microsoft Research, Redmond, WA, USA 2 Department of Computer Science, North Carolina State University, Raleigh, NC, USA Abstract—Many factors are believed to increase the reliable (i.e., works as expected) but not secure or a software vulnerability of software system; for example, the more widely system can be secure (e.g., adopting threat modeling deployed or popular is a software system the more likely it is to effectively, eliminating buffer overflows programmatically, be attacked. Early identification of defects has been a widely etc.) but not reliable (does not work as expected). To better investigated topic in software engineering research. Early address both security and reliability, it is essential to identification of software vulnerabilities can help mitigate understand differences and similarities between these two these attacks to a large degree by focusing better security fields. verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities Towards that end, we leverage existing metrics that have are, most often, few in number and introduce significant bias been used in prior research for defect prediction by creating a sparse dataset in the population. As a result, [17][18][19][20] to understand and investigate the efficacy vulnerability prediction can be thought of us preverbally of these metrics for vulnerability prediction. More formally, “searching for a needle in a haystack.” In this paper, we our research hypothesis is to investigate and report on the present a large-scale empirical study on Windows Vista, where ability of classical defect prediction metrics to be used as we empirically evaluate the efficacy of classical metrics like predictors for vulnerability prediction. For this purpose, complexity, churn, coverage, dependency measures, and we study Windows Vista, which is a large and widely-used organizational structure of the company to predict commercial operating system from Microsoft Corporation. A vulnerabilities and assess how well these software measures statistical challenge in our study is motivated by the fact that correlate with vulnerabilities. We observed in our experiments vulnerabilities are few and widely distributed in the dataset that classical software measures predict vulnerabilities with a akin to searching for a needle in a haystack. In our study for high precision but low recall values. The actual dependencies, example, only 66 advisories have been recorded for Vista however, predict vulnerabilities with a lower precision but (40 Million plus lines of code) in the National Vulnerability substantially higher recall. Database (NVD) [21], and only few of the Windows binaries are affected by security updates. Keywords—Vulnerabilities, Prediction, Metrics, Complexity, This statistical challenge involves identifying which of Churn, Coverage, Dependencies, Organizational Structure the classical metrics related to code quality can predict vulnerabilities. We extract complexity, churn, coverage, I. INTRODUCTION dependency metrics for Vista and used them to predict the vulnerabilities that are found and fixed in Vista as dependent Software security is a critical part of the software variable. Our results are as follows: development process. While there is a significant body of  Metrics correlate with vulnerabilities; however the work on predicting defects, unfortunately little is known effect is only small (Section IV). about the field of vulnerability prediction. Some recent work focused on this topic in the open source domain [9][15][22].  Most metrics can predict vulnerabilities with an In this paper, we focus on vulnerability prediction for a average to good precision; however the recall is very proprietary commercial product (Windows Vista). We define low (Section V.B). a component to be vulnerable if it has been changed as part  Alternative techniques such as using the actual of a security update after it was released publically. dependencies of a binary to predict vulnerabilities have Software security and reliability are two crucial aspects better recall values (Section V.C). of software engineering research. Software security research The paper is organized as follows. Section II describes spans several domains ranging from better programming the metrics that we collected and used in our experiment. language design suited for security to the use of processes Section III characterizes our vulnerabilities based on public like penetration testing, design and use of robust access data available in the NVD database. Section IV discusses the control policies [10]. For example, a software systems can be correlation results between the collected metrics and 

}
}

@Article{ZitserLippmannLeekEtAl2004,
  Title                    = {Testing Static Analysis Tools using Exploitable Buffer Overflows from Open Source Code},
  Author                   = {Misha Zitser and Richard Lippmann and Tim Leek and D. E. Shaw and Group MIT and Lincoln Laboratory and MIT Lincoln and Laboratory},
  Year                     = {2004},

  Abstract                 = {Five modern static analysis tools (ARCHER, BOON, Poly- Space C Verifier, Splint, and UNO) were evaluated using source code examples containing 14 exploitable buffer over- flow vulnerabilities found in various versions of Sendmail, BIND, andWU-FTPD. Each code example included a “BAD” case with and a “OK” case without buffer overflows. Buffer overflows varied and included stack, heap, bss and data buffers; access above and below buffer bounds; access us- ing pointers, indices, and functions; and scope differences between buffer creation and use. Detection rates for the “BAD” examples were low except for PolySpace and Splint which had average detection rates of 87% and 57%, respec- tively. However, average false alarm rates were high and roughly 50% for these two tools. On patched programs these Figure 1: Cumulative buffer overflow vulnerabilities two tools produce one warning for every 12 to 46 lines of found in BIND, WU-FTPD, and Sendmail server source code and neither tool accurately distinguished be- software since 1996 tween vulnerable and patched code.
},
  File                     = {:article\\Testing Static Analysis Tools using Exploitable Buffer Overflows from Open Source Code.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {Buffer overflow vulnerabilities often permit remote attack-ers to run arbitrary code on a victim server or to crash Security, buffer overflow, static analysis, evaluation, exploit, server software and perform a denial of service (DoS) attack. test, detection, false alarm, source code They account for roughly 1/3 of all the severe remotely ex,literature,article},
  Read                     = {未读},
  Review                   = { Testing Static Analysis Tools using Exploitable Buffer Overflows from Open Source Code ∗ Misha Zitser Richard Lippmann Tim Leek D. E. Shaw Group MIT Lincoln Laboratory MIT Lincoln Laboratory New York, NY Lexington, MA Lexington, MA zitserm@deshaw.com rpl@ll.mit.edu tleek@ll.mit.edu ABSTRACT Five modern static analysis tools (ARCHER, BOON, Poly- Space C Verifier, Splint, and UNO) were evaluated using source code examples containing 14 exploitable buffer over- flow vulnerabilities found in various versions of Sendmail, BIND, andWU-FTPD. Each code example included a “BAD” case with and a “OK” case without buffer overflows. Buffer overflows varied and included stack, heap, bss and data buffers; access above and below buffer bounds; access us- ing pointers, indices, and functions; and scope differences between buffer creation and use. Detection rates for the “BAD” examples were low except for PolySpace and Splint which had average detection rates of 87% and 57%, respec- tively. However, average false alarm rates were high and roughly 50% for these two tools. On patched programs these Figure 1: Cumulative buffer overflow vulnerabilities two tools produce one warning for every 12 to 46 lines of found in BIND, WU-FTPD, and Sendmail server source code and neither tool accurately distinguished be- software since 1996 tween vulnerable and patched code. Categories and Subject Descriptors 1. INTRODUCTION D.2.4 [Software Engineering]: [Software/Program Verifi- The Internet is constantly under attack as witnessed by cation]; D.2.5 [Software Engineering]: [Testing and De- recent Blaster and Slammer worms that infected more than bugging]; K.4.4 [Computers and Society]: [Electronic 200,000 computers in a few hours [19, 25]. These, and many Commerce] past worms and attacks exploit buffer overflow vulnerabili- ties in server software. The term buffer overflow is used in General Terms this paper to describe all types of out-of-bound buffer ac- Measurement, Performance, Security, Verification cesses including accessing above the upper limit or below the lower limit of a buffer. Keywords Buffer overflow vulnerabilities often permit remote attack-ers to run arbitrary code on a victim server or to crash Security, buffer overflow, static analysis, evaluation, exploit, server software and perform a denial of service (DoS) attack. test, detection, false alarm, source code They account for roughly 1/3 of all the severe remotely ex- ∗ ploitable vulnerabilities listed in the NIST ICAT vulnerabil-This work was sponsored by the Advanced Research and ity database [22]. The often-suggested approach of patching Development Activity under Air Force Contract F19628-00- software as quickly as possible after buffer overflow vulner- C-0002. Opinions, interpretations, conclusions, and recom- mendations are those of the authors and are not necessarily abilities are announced is clearly not working given the ef- endorsed by the United States Government. fectiveness of recent worms. Figure 1 shows the dates that new remotely exploitable buffer overflow vulnerabilities were announced in three popular Internet server software applica- tions (BIND, WU-FTP, and Sendmail) and the cumulative Permission to make digital or hard copies of all or part of this work for number of these vulnerabilities. For just these three servers, peCrosopnyarilgohrt c2l0a0ss4r oAosmsoucsieatiosng rfaonrt eCdowmipthuotiuntgf eMe apcrhoivniedreyd. tAhaCtMco pacieksnaorwe- there have been from one to six remotely exploitable buffer- noletdmgeasd ethoart dthisitsr icbountetrdibfuotriopnr owfiat so rauctohmormede rocira cl oa-davuatnhtoargeed abnyd at hcaotnctorapcietosr overflow vulnerabilities announced each year, no reduction beoar ratfhfislinatoet icoef athned tUh.eSf.u Gllocviteartniomneonnt. tAhes fisruscthp,a tghee. TGoovcoerpnymoethnet rrwetiasein, sto a in the rate of new vulnerabilities, and a total of 24 vulnera- renpounbelxischlu, stoivpeo, srtooynalstyer-vfreeres orirgthot rteod ipsutrbilbiushte otor rliesptsro, dreuqcuei rtehsisp rairotircslpe,e coirfi tco pe bilities published since 1996.arllmowiss oiothnearsn dto/o droa sfoe,e .for Government purposes only. SISGIGSOSOFFT’T0’40/4F/SFES-E1-21,2O, cOt.ct3. 13–1N–Novo.v6. ,62,0 20040,4N, eNwepwoprtoBrte Baceha,cCh,A C,AU,S UAS. A. A detailed review of approaches that have been devel- CCopoypryirgihgth2t 020404A ACMCM1 -15-85181131-38-5855-5-/054/0/040/0101.0...$..5$.50.0.0. oped to counter buffer overflow exploits is available in [30]. 97

}
}

@InProceedings{半斤八兩2014,
  Title                    = {纯手工秒杀 VM,SE等虚拟机 Handle},
  Author                   = {半斤八兩},
  Year                     = {2014},

  File                     = {:article\\纯手工秒杀VM,SE等虚拟机Handle.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {纯手工秒杀 VM,SE等虚拟机 Handle /************************************** /* 作者:半斤八兩 /* 博客:http://cnblogs.com/bjblcracked /* 日期:2014-12-08 19:01 /************************************** 只是感兴趣，没有其他目的。失误之处敬请诸位大侠赐教! 最近在研究虚拟机还原,偶然发现这个快速寻找 Handle的方法. 现在就把这个方法分享给大家 :) 破解或者逆向的时候经常遇到程 序加了虚拟机保护.每当遇到虚拟机时,新人多半就是直接放弃,有经 验的就是头疼着搞,边搞边头疼 :( VMProtect的Handle非常好找, 从壳的EP处起, 只要人肉一 会就能找到Handle. Vmprotect无论编译选择是设成怎样, 一般 都不会超过 200条指令就能找到 Handle. 我们先来看看如何快速寻找 Vmprotect Handle. 首先就是 OD加载 Vmprotect.exe, 本例直接拿 Vmprotect 2.13.5 主程序做测试.}
}

@Article{卢锡城，李根2010,
  Title                    = {面向高可信软件的整数溢出错误的自动化测试},
  Author                   = {卢锡城，李根},
  Journal                  = {软件学报},
  Year                     = {2010},

  Abstract                 = {：aTphepirseseanntasutomattesitcimnegthod．DAIDTau(tdoymnaitanitmceigcer-overflow detecatintodensting)f，ifnodrintgeogevrerfflatoabwlugisnbinacroyde．DCAaIltlDhToroutgeshtlhybeinary},
  Doi                      = {rn：gE-mail：superligen@Igmail．com},
  File                     = {:article\\面向高可信软件的整数溢出错误的自动化测试.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {ISS1N000．9825R，CUOXDEUNEW E·mail：jos@iscas．ac．cn JouronfaSloftware，"C01．21，N2o0．120，，FPePb．r1ua7r9y-193 hap：／／www．jos．org．cn doi：10．3724／SPA．1001．2010．03785 TeL，Fax：+86．10．62562563 obyInstiotfuSteoftwaCrhe．itnAheecseadoefmSyciencreisg．hrAtelslerved． 面向高可信软件的整数溢出错误的自动化测试宰 卢锡城，李根+，卢凯，张英 (国防科学技术大学计算机学院，湖南长沙410073) High—-Trusted·-SoftAwaurteo·—mOTaretisietncfitoenIrdgnteOgveerrfBluogws LUXi—ChenGg，eLnI+，KLaUi，ZHYAinNgG (SchooflComputer。NationalUniversityofDefenseTechnology，Changsha410073，China) +Correspaounthdoirn：gE-mail：superligen@Igmail．com LuXC，LGi，Llu(，ZhaYn．gHigh-Trusted-Sofatuwtaormeta—etOsirtcifnoegirnntteedgoevrerfbluogsw． JouronfaSloftware，2010，21(2)：179—193．http：／／www．jos．org．cn／1000·9825／3785．htm Abstractp：aTphepirseseanntasutomattesitcimnegthod．DAIDTau(tdoymnaitanitmceigcer-overflow detecatintodensting)f，ifnodrintgeogevrerfflatoabwlugisnbinacroyde．DCAaIltlDhToroutgeshtlhybeinary codaendautomatificnaudlnlkynionwtneogevrerfbluogwsithoneuctesskarniolwyinsgy也mebthoabllesi．sIt formaplrloyvientdhipsaptehraDtAIcDaTntheoretidceatlealc1yt1thehigh-riinstkeogevrerfbluogwsitnho falpsoesitaivnednsofalsne gativeasd．dIintionbalu，gfasinnybdyDAIcDaTbnereplayedde．mTonsthreate effectioveftnhietshseory,InhtaHbsueneitnmeprlementhaesfd．oIut4nndehwigrhisiknteogveerrfbluogiwsn thelatersetleaosfetshreheigh-traupspteldicatiMoincsr(otsWwoIofNtsServiicneWsind2o0w0sa0nd2003 Server,HBiIanisdtuManetssagteers)teibnaygcfho2r4hours．Tohftrheeebseugasllaorwbiucaordeyexecution andhavreeceicvoendfivrumlenderabinluimtbiesrs，CVE一2009-192f3r，oCMmViEc·r2oSs0eo0cf9ut-r1i9ty24 RespoCnesnetaenrCdVE·2008—f6r4oB4ma4idu． Kewyords：integoevrerflow；ionvteergfvelruoiwnerabili够a；udtyonmaatmetisictc88geeneration；taint analysis；seyxmebcoultiicon 摘要： 面向高可信软件提出了一种二进制级高危整数溢出错误的全自动测试方法(dynaamuitocmatic integer-odveetrefclatoniwodtnesting，简称DAIDT)．该方法无需任何源码甚至是符号表支持，即可对二进制应用程序 进行全面测试，并自动发现高危整数溢出错误．在理论上形式化证明了该技术对高危整数溢出错误测试与发掘的无 漏报性、零误报性与错误可重现特性．为了验证该方法的有效性，实现了IntHonter原型系统．IntHunter对3个最新版 本的高可信应用程序(微软公司Wind2o0w0s3和2S0e0r0ver的WINS服务、百度公司的即时通讯H软i件)BaiDu 分别进行了24小时测试，共发现了4个高危整数溢出错误．其中3个错误可导致任意代码执行，其中两个由微软安全 响应中心分配漏洞编号CVE．2009．1923，CVE．2009．1924。另一个由百度公司分配漏洞编号CVE．2008．6444． 关键词：整数溢出；高危整数溢出错误；动态自动测试用例生成；污点分析；符号化执行 中图法分类号：TP3l1 文献标识码：A ·SuppobryttehNdeatioHniaglh·TReeesheaarncDdheveloPplmaoenfntChuindaGeraNnot．2007AA010301(国家高技术研究 发展计划(863))N；atthieoBnaaslRiecseaPrrcohgorfaCmhuindaGerraNnot．2005CB321801(国家重点基础研究发展计划(973)) Recei2v0e0d9-06—15；2R0e0v9i—s0e9·d11；A2c0c0e9p·t1e2d—07 万方数据},
  Unknown                  = {Read}
}

@Article{孙浩2012,
  Title                    = {基于信息流的整数漏洞插装和验证},
  Author                   = {孙浩},
  Journal                  = {软件学报},
  Year                     = {2012},

  Doi                      = {：10．3724／SP．J．1001．201h3t．t0p4：3／8／5w]wwjos．org．cn},
  File                     = {:article\\基于信息流的整数漏洞插装和验证.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {软件学报ISS1N000．9825R．CUOXDEUNEW E-mail：jOS@iscas．ac．cn JouronfaSloftware，2013，24(12)：2767—2781【doi：10．3724／SP．J．1001．201h3t．t0p4：3／8／5w]wwjos．org．cn ◎中国科学院软件研究所版权所有． Tbl／Fax：+86．10—62562563 基于信息流的整数漏洞插装和验证幸 孙浩1，一，李会朋1,2，曾庆凯1,2 1(计算机软件新技术国家重点实验室(南京大学)，江苏南京210093) 2(南京大学计算机科学与技术系，江苏南京210093) 通讯作者：曾庆凯，E-mail：zqk@nju．edu．cn 摘要： 为降低整数漏洞插装验证的运行开销，提出基于信息流的整数漏洞插装方法．从限定分析对象范围的角 度出发，将分析对象约减为污染信息流路径上的所有危险整数操作，以降低静态插装密度．在GCC平台上，实现了原 型系统DRIVER(adnerdtuen．ctticmheecinkteger-vublanserdabwiliitithniefsormaftlioown)．实验结果表明，该方法 具有精度高、开销低、定位精确等优点． 关键词： 整数漏洞；信息流；污点分析；插装 中图法分类号：TP31 1 文献标识码：A 中文引用格式：孙浩，李会朋，曾庆凯．基于信息流的整数漏洞插装和验证．软件学报，2013，24(12)：2767—2781．http：／／www．jos org．cn／1000-9825／4385．htm 英文引用格式：SuHn，LHiP，ZeQnKg．Statidceatlealcnytdrim—tcimheeicnktegerv-ubianseerdabwiliitithnifesormfaltoiwon RuaJinaXnuBeao／JouorfnSaolftware，2013，24(1C2h)i：n2e7se6)7．—h2t7t8pl：／(／iwnww．jos．org．cn／1000-9825／4385．htm StaticDaeltleyacntRdun-TCihmeeIcnktegerV-uBlanseerdabwiiftiIhtnifeosrmFatlioown SUHNa01，_H，LuIi．Pen91”Q，inZgE．NKaGil，2 1(StaKteLyaboraftoNroroyvSeolftwTaercehnologyU(nNiavnerjsiintgy)2，1N0a0n9j3i，nCghina) 2(DeparotfCmoemntpuStceireanncTdeechnologyU，nNiavnejrisnigty，21N0a0n9j3i，nCghina) Correspaountdhiongr：QZiEngN-GKai，E—mail：zqk@nju．edu．ca Abstracatp：pArontaodcehtecitnitneggerv-ublanseerdabislpirtoipeosbsaesdoendinformatiaonna-lfyislniosrwdteorimprtohvee run-tpiemreformatnhcieas．pIpnroacht，houenlsyaifneteogpeerratoinotanisnitnedformaftliopowanths，wChaibncehcontroblyled userasnidnvolivnesdensitoipveerationtso，bneeiendstrumweintrheudn—ticmheeckode，tShOabtotthhedensiotfsytatic instrumeantnapdteirofnoromvaenrcheaerareedduced．0B1at1hsieasdpproapcrho，taostyspetceamllDeRdIVER(adnerdtuen—cttime cheicnktegerv-ublanseerdabwiilitithnifeosrmafltoiwoi)nimsplemaesannteexdtentsoitohGne CcCompialnetdrestoendanumboefr real·waoprlpdlicatioenxsp．eTrhiemreenstuslatlhsotwhathiaspproiasecfhfective，scalablea，nlcdiagphatob-flwleoicgahttihneg rooctause． Kewyords：integveurln-ebraasbeidlity；filnofwo；rtmaanitaniltoynsis；instrumentation 整型变量是高级语言中常用的基本数据类型，在程序中常用于表示整数数值、内存地址、数组下标、循环 计数、标志位或者参与算术运算．由于整数表示形式的局限性和C语言类型不安全的特点，整数运算结果失真 或由于对整数的不一致解释将导致数据丢失或歧义，这种现象称为整数漏洞．尽管整数错误操作并不会直接对 内存做越界修改，但攻击者可通过多种方式间接借助整数漏洞来实现恶意攻击．目前，由整数错误操作导致的脆 弱性已遍布于内核代码、实用程序和各种应用中，如Apache、OpenSSH、Sendmail、Snort、BSD内核、Linux ·基金项目：国家自然科学基金(61170070，90818022，61021062)；国家科技支撑计划(2012BAK26801)；国家高技术研究发展计 划(863)1(A20A1lA202) 收稿时间：2012—08．3l：修改时间：2012—12—03；定稿时间：2013．02—04 万方数据

}
}

@Article{张仕金尚赵伟2015,
  Title                    = {Cppcheck的软件缺陷模式分析与定位},
  Author                   = {张仕金 尚赵伟},
  Journal                  = {计算机工程与应用Computer Engineering and Applications},
  Year                     = {2015},

  Abstract                 = {The C/C++ program which is compiled well does not always guarantee that there are no defects in the code. There may still contain defects relativing to securities, design and code style, therefore it may result in memory leak or misuse
},
  Doi                      = {：10.3778/j.issn.1002-8331.1304-0138},
  File                     = {:article\\Software defect pattern analysis and location based on Cppcheck.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Computer Engineering and Applications计算机工程与应用 2015，51（3） 69 Cppcheck的软件缺陷模式分析与定位 张仕金，尚赵伟 ZHANG Shijin, SHANG Zhaowei 重庆大学 计算机学院，重庆 400030 Computer School of Chongqing University, Chongqing 400030, China ZHANG Shijin, SHANG Zhaowei. Software defect pattern analysis and location based on Cppcheck. Computer Engi- neering and Applications, 2015, 51（3）：69-73. Abstract：The C/C++ program which is compiled well does not always guarantee that there are no defects in the code. There may still contain defects relativing to securities, design and code style, therefore it may result in memory leak or misuse of pointers so that it is difficult to accomplish the expected goal of software requirements. Aiming to software defection tool of Cppcheck’s insufficiency for open software defect, this paper mainly analyses the Cppcheck architecture, defect pattern representation and implementation, as well as on the basis of summarizing 350 defect patterns to improve Cppcheck. It makes two relevant experiments to verify the effectiveness of improved Cppcheck. Key words：software defect; defect pattern; Cppcheck 摘 要：能通过编译的C/C++程序代码可能依然隐含安全、设计或风格上缺陷，从而导致运行时出现内存泄露、运行 异常等现象，难以完成软件需求所预期的目标。针对开源软件缺陷检测工具Cppcheck软件存在的不足，主要分析了 Cppcheck架构、缺陷模式表示与实现，在对已收集 350个缺陷模式分析总结基础上，对其完善，提高缺陷检测能力。 通过两组实验验证改进Cppcheck工作的有效性。 关键词：软件缺陷；缺陷模式；Cppcheck 文献标志码：A 中图分类号：TP391 doi：10.3778/j.issn.1002-8331.1304-0138 1 引言 成软件的使用相对繁琐。在开源领域，具有代表的工具 软件缺陷是存在于软件中、不期望或不可接受的偏 有 ITS4，Splint，Cppcheck等等。ITS4仅基于词法分析， 差，以静态形式存在于软件的内部，是软件开发过程中 无法分析代码的语义，即无法分析一个标识符代表何种 人为错误的结果，当软件运行于某一特定条件时将出现 意义，相比之下，Splint基于模糊静态分析，加上用户添 软件故障（即软件缺陷被激活）[1]。所以软件缺陷检测一 加的扩展标记，检测准确率比 ITS4要好，但目前只针对 直是软件测试技术发展的瓶颈。随着软件测试项目不 于 C语言，无法检测出 C++中缺陷，并且依赖用户添加 断增多，积累的软件缺陷数据越来越多，如何利用缺陷 的扩展标记，给用户带来了严重负担[3-5]。Cppcheck（http：// 数据，抽象通用的缺陷模式，并用于静态缺陷自动检测 Cppcheck.sourceforge.net/）是针对 C/C++语言且基于缺 工具，对排除软件缺陷具有重要意义[2]。 陷模式的静态软件缺陷检测工具，因其开源和使用简单 无论是商业软件领域还是开源社区，都有C/C++代 等特点在软件测试中得到广泛应用。但Cppcheck只内 码缺陷自动检测工具的开发。在商业领域，具有代表性 置了部分缺陷模式，无法满足实际工程需求，所以如何 的工具有 PC-Lint和 Parasoft C/C++Test工具包，其采用 提高 Cppcheck的检测能力，对于提高软件缺陷检测具 的技术相对成熟，属于非开源软件，技术内幕没有公开， 有重要意义。 且配置规则和规则扩展方面需要用户编写脚本程序，造 首先对开源软件Cppcheck进行了分析和研究，总结 基金项目：国家自然科学重点基金（No.91118005）；国家自然科学基金（No.61173130）；重庆市自然科学基金（CSTC-2010BB2217）。 作者简介：张仕金（1987—），男，硕士研究生，主要从事软件缺陷研究、模式识别研究；尚赵伟（1968—），男，副教授，主要从事数字 图像处理、软件缺陷研究、模式识别研究。E-mail：928025455@qq.com 收稿日期：2013-04-11 修回日期：2013-06-07 文章编号：1002-8331（2015）03-0069-05 CNKI网络优先出版：2013-06-26，http://www.cnki.net/kcms/detail/11.2127.TP.20130626.1540.017.html}
}

@InProceedings{成荣，张方国2014,
  Title                    = {安全的程序混淆研究综述},
  Author                   = {成荣，张方国},
  Year                     = {2014},

  Abstract                 = {Program obfuscation is a compiler that transfers the original program into an unintelligible form while preserving the functionality. The concept of obfuscation was first introduced in code obfuscation, which is used for software protection, digital watermarking, etc. However, it lacks formal analysis and security proof. Obfuscation for cryptographic purposes was proposed by Barak et al., and they gave the formal definition of `virtual black-box obfuscation and its security requirements. General obfuscation of cryptographic functions has important meaning in theoretical research and has close relation with other cryptographic primitives such as random oracle, fully homomorphic encryption, zero knowledge, etc. Besides, secure obfuscation of specific cryptographic functions has practical use in cloud computing and delegate computing. In recent years, secure program obfuscation has become one of the hottest topics in the progress of cryptographic research. As obfuscation of general function families was proved impossible under Barak’s standard definition, thus following researches are mainly focused on realizing secure obfuscation of specific families of functions, new definition models of obfuscation, and relations and applications of obfuscation in other cryptographic primitives. In this paper, we give an overview on the study of secure obfuscation, which includes constructions of secure obfuscation of specific cryptographic functions, studies on special models of obfuscation and generalization and applications of secure obfuscation. 
},
  Doi                      = {：10.3969/j.issn.1671-1122.2014.08.002},
  File                     = {:article\\安全的程序混淆研究综述.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {技 术 研 究 2014年第08期 doi：10.3969/j.issn.1671-1122.2014.08.002 安全的程序混淆研究综述 成荣，张方国 （中山大学信息科学与技术学院，广东广州510006） 摘　要：程序混淆可以理解为一个编译器，它将源程序转化成一种不被理解的形式，但 依然保持其功能特性。混淆的概念最早在代码混淆领域被提出，在软件保护、数字水印等领 域有着实际的应用，但缺乏严格的安全分析与证明。混淆在密码学领域的研究最早由 Barak 等人引入，并提出了虚拟黑盒混淆的形式化定义及安全性要求。对密码函数的安全通用混淆 研究具有非常重要的理论意义，其与随机预言机、全同态加密、零知识证明等其他密码原语 有着紧密的联系。对具体密码函数的安全混淆在云计算、代理计算等领域也有着实际的应用 价值。近年来，安全的程序混淆研究成为当前密码研究领域的一个热点。由于在 Barak 提出 的标准定义下已证明不存在通用的安全混淆，因此后续的程序混淆方面的研究工作主要集中 在 3 个方面 ：对具体函数类的混淆实现、混淆的新模型研究以及混淆与其他密码模型的关系 研究及应用。文章给出了安全的程序混淆的一个研究综述，对对具体函数类的安全混淆、混 淆模型的研究以及混淆的推广和应用都分别给出了一个较为详细的介绍。 关键词：密码学 ；程序混淆 ；虚拟黑盒特性 中图分类号：TP309 文献标识码： A 文章编号：1671-1122（2014）08-0006-11 An Overview on the Secure Program Obfuscation CHENG Rong, ZHANG Fang-guo (School of Information Science and Technology, Sun Yat-sen University, Guangzhou Guangdong 510006, China) Abstract: Program obfuscation is a compiler that transfers the original program into an unintelligible form while preserving the functionality. The concept of obfuscation was first introduced in code obfuscation, which is used for software protection, digital watermarking, etc. However, it lacks formal analysis and security proof. Obfuscation for cryptographic purposes was proposed by Barak et al., and they gave the formal definition of `virtual black-box obfuscation and its security requirements. General obfuscation of cryptographic functions has important meaning in theoretical research and has close relation with other cryptographic primitives such as random oracle, fully homomorphic encryption, zero knowledge, etc. Besides, secure obfuscation of specific cryptographic functions has practical use in cloud computing and delegate computing. In recent years, secure program obfuscation has become one of the hottest topics in the progress of cryptographic research. As obfuscation of general function families was proved impossible under Barak’s standard definition, thus following researches are mainly focused on realizing secure obfuscation of specific families of functions, new definition models of obfuscation, and relations and applications of obfuscation in other cryptographic primitives. In this paper, we give an overview on the study of secure obfuscation, which includes constructions of secure obfuscation of specific cryptographic functions, studies on special models of obfuscation and generalization and applications of secure obfuscation. Key words: cryptography; program obfuscation; virtual black-box property 收稿日期： 2014-07-21 基金项目：国家自然科学基金 [61379154,U1135001]、高等学校博士学科点（博导类）专项科研基金 [20120171110027] 作者简介：成荣（1987-），女，湖北，博士研究生，主要研究方向 ：密码学与程序混淆；张方国（1972-），男，山东，博士生导师，教授，博士， 主要研究方向 ：密码学与信息安全。 6

}
}

@InProceedings{方滨兴，陆天波，李超2007,
  Title                    = {软件确保研究进展},
  Author                   = {方滨兴，陆天波，李超},
  Year                     = {2007},

  File                     = {:article\\软件确保研究进展.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读}
}

@Article{李文明2014,
  Title                    = {缓存区溢出研究与发展},
  Author                   = {李文明},
  Journal                  = {第３１卷第９期 计 算 机 应 用 研 究 Ｖｏｌ．３１Ｎｏ．９ ２０１４年９月　 ＡｐｐｌｉｃａｔｉｏｎＲｅｓｅａｒｃｈｏｆＣｏｍｐｕｔｅｒｓ Ｓｅｐ},
  Year                     = {2014},

  File                     = {:article\\缓存区溢出研究与发展.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {第３１卷第９期 计 算 机 应 用 研 究 Ｖｏｌ．３１Ｎｏ．９ ２０１４年９月　 ＡｐｐｌｉｃａｔｉｏｎＲｅｓｅａｒｃｈｏｆＣｏｍｐｕｔｅｒｓ Ｓｅｐ．２０１４ 缓存区溢出研究与发展 李文明，陈　哲，李绪蓉 （南京航空航天大学 计算机科学与技术学院，南京 ２１００１６） 摘　要：为检测出缓存区溢出的发生和预防攻击者利用缓存区溢出漏洞进行攻击，研究人员提出了各种各样的 检测和预防技术。首先介绍了缓存区溢出的四种攻击方式，将主流的缓存区溢出检测和预防技术进行了分类， 介绍了每一类的原理、发展历程和优缺点；然后对这些检测和预防技术进行了综合讨论；最后对缓存区溢出检测 和预防技术的未来发展趋势进行了分析与预测。 关键词：缓存区溢出；缓存区溢出攻击；检测；预防 中图分类号：ＴＰ３１１　　　文献标志码：Ａ　　　文章编号：１００１３６９５（２０１４）０９２５６１０６ ｄｏｉ：１０．３９６９／ｊ．ｉｓｓｎ．１００１３６９５．２０１４．０９．００１ Ｒｅｓｅａｒｃｈａｎｄｄｅｖｅｌｏｐｍｅｎｔｏｆｂｕｆｅｒｏｖｅｒｆｌｏｗ ＬＩＷｅｎｍｉｎｇ，ＣＨＥＮＺｈｅ，ＬＩＸｕｒｏｎｇ （ＣｏｌｅｇｅｏｆＣｏｍｐｕｔｅｒＳｃｉｅｎｃｅ＆Ｔｅｃｈｎｏｌｏｇｙ，ＮａｎｊｉｎｇＵｎｉｖｅｒｓｉｔｙｏｆＡｅｒｏｎａｕｔｉｃｓ＆Ａｓｔｒｏｎａｕｔｉｃｓ，Ｎａｎｊｉｎｇ２１００１６，Ｃｈｉｎａ） Ａｂｓｔｒａｃｔ：Ｉｎｏｒｄｅｒｔｏｄｅｔｅｃｔｂｕｆｅｒｏｖｅｒｆｌｏｗａｎｄｐｒｅｖｅｎｔｂｕｆｅｒｏｖｅｒｆｌｏｗａｔａｃｋ，ｔｈｅｒｅｓｅａｒｃｈｅｒｓｈａｖｅｐｒｏｐｏｓｅｄａｖａｒｉｅｔｙｏｆｄｅ ｔｅｃｔｉｏｎａｎｄｐｒｅｖｅｎｔｉｏｎｔｅｃｈｎｏｌｏｇｉｅｓ．Ｆｉｒｓｔ，ｔｈｉｓｐａｐｅｒｉｎｔｒｏｄｕｃｅｄｆｏｕｒｋｉｎｄｓｏｆｂｕｆｅｒｏｖｅｒｆｌｏｗａｔａｃｋ．Ｉｔｃｌａｓｓｉｆｉｅｄｔｈｅｍａｉｎｓｔｒｅａｍ ｏｆｂｕｆｅｒｏｖｅｒｆｌｏｗｄｅｔｅｃｔｉｏｎａｎｄｐｒｅｖｅｎｔｉｏｎｔｅｃｈｎｏｌｏｇｉｅｓ，ａｎｄｒｅｓｅａｒｃｈｅｄｅａｃｈｇｒｏｕｐ’ｓｐｒｉｎｃｉｐｌｅｓ，ａｄｖａｎｔａｇｅｓ，ｄｉｓａｄｖａｎｔａｇｅｓ ａｎｄｄｅｖｅｌｏｐｍｅｎｔ．Ｔｈｅｎｉｔｍａｄｅａｃｏｍｐｒｅｈｅｎｓｉｖｅｄｉｓｃｕｓｓｉｏｎａｂｏｕｔｔｈｅｓｅｇｒｏｕｐｓ．Ｆｉｎａｌｙ，ｉｔａｎａｌｙｚｅｄａｎｄｆｏｒｅｃａｓｔｅｄｔｈｅｂｕｆｅｒ ｏｖｅｒｆｌｏｗｄｅｔｅｃｔｉｏｎａｎｄｐｒｅｖｅｎｔｉｏｎｔｅｃｈｎｏｌｏｇｙｉｎｔｈｅｆｕｔｕｒｅｄｅｖｅｌｏｐｍｅｎｔｔｒｅｎｄ． Ｋｅｙｗｏｒｄｓ：ｂｕｆｅｒｏｖｅｒｆｌｏｗ；ｂｕｆｅｒｏｖｅｒｆｌｏｗａｔａｃｋ；ｄｅｔｅｃｔｉｏｎ；ｐｒｅｖｅｎｔｉｏｎ　　缓存区溢出存在于各种电脑程序中，特别是广泛存在于用 变量缓存区溢出来修改返回地址值和栈指针，从而改变程序的 Ｃ、Ｃ＋＋等这些本身不提供内存越界检测功能的语言编写的程 执行流。 序中。现在Ｃ、Ｃ＋＋作为程序设计基础语言的地位还没发生改 *++ "#$% 0/1/ ,++ &'( 变，它们仍然被广泛应用于操作系统、商业软件的编写中，每年 -++ .++ 2345 都会有很多缓存区溢出漏洞被人们从已发布和还在开发的软件 /++ 0++ 中发现出来。从ＣＥＲＴ漏洞数据库和国家漏洞数据库ＮＶＤ中 1++ 678 )++ 统计２００１—２０１２年每一年发现的缓存区溢出漏洞数如图１所 2++ + 9:()* 示。虽然从图上看出缓存区溢出数相比２００７年已经大幅减 3 4 1 0 / . - , * !+ !! !4 ! ;<=>?6@' ) ! " #$%&' ! "#$% &'( 少，但在２０１１年的 ＣＷＥ／ＳＡＮＳ最危险的软件漏洞排行榜上， ABCD ()*+,-./ “没进行输入大小检测的缓存区复制”漏洞排名第三。可见， ２）破坏堆数据 如何检测和预防缓存区溢出漏洞仍然是一个非常棘手的问题。 程序运行时，用户用 Ｃ、Ｃ＋＋内存操作库函数如 ｍａｌｏｃ、　缓存区溢出攻击的方式 ｆｒｅｅ等在堆内存空间分配存储和释放删除用户数据，对内存的 ! 使用情况如内存块的大小、它前后指向的内存块用一个链接类 为实现缓存区溢出攻击，攻击者必须在程序的地址空间里 的数据结构予以记录管理，管理数据同样存放于堆中，且管理 安排适当的代码及进行适当的初始化寄存器和内存，让程序跳 数据与用户数据是相邻的。这样，攻击者可以像破坏活动记录 转到入侵者安排的地址空间执行。控制程序转移到攻击代码 一样来溢出堆内存中分配的用户数据空间，从而破坏管理数 的方法有如下几种： 据。因为堆内存数据中没有指针信息，所以即使破坏了管理数 １）破坏活动记录 据也不会改变程序的执行流，但它还是会使正常的堆操作出 函数调用发生时，调用者会在栈中留下函数的活动记录， 错，导致不可预知的结果。 包含当前被调函数的参数、返回地址、前栈指针、变量缓存区等 ３）更改函数指针 值，它们在栈中的存放顺序如图２所示。 指针在Ｃ、Ｃ＋＋等程序语言中使用得非常频繁，空指针可 由它们在栈中的存放顺序可知，返回地址、栈指针与变量 以指向任何对象的特性使得指针的使用更加灵活，但同时也需 缓存区紧邻，且返回地址指向函数结束后要执行的下一条指 要人们对指针的使用更加谨慎小心，特别是空的函数指针，它 令。栈指针指向上一个函数的活动记录，这样攻击者可以利用 可以使程序执行转移到任何地方。攻击者充分利用了指针的 收稿日期：２０１３１１０５；修回日期：２０１３１２２０　　基金项目：国家自然科学基金资助项目（６１１０００３４，６１１７００４３） 作者简介：李文明（１９８９），男，硕士研究生，主要研究方向为软件工程与软件验证；陈哲（１９８１），男（通信作者），副教授，主要研究方向为软件 工程与软件验证（ｚｈｅｃｈｅｎ＠ｎｕａａ．ｅｄｕ．ｃｎ）；李绪蓉（１９７５），女，副教授，主要研究方向为软件工程与软件验证． 书

}
}

@MastersThesis{李朝君2010,
  Title                    = {二进制代码安全性分析},
  Author                   = {李朝君},
  Year                     = {2010},

  File                     = {:article\\二进制代码安全性分析.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {中国科学技术大学 硕士学位论文 二进制代码安全性分析 姓名：李朝君 申请学位级别：硕士 专业：计算机软件与理论 指导教师：蒋凡 20100601

}
}

@PhdThesis{李根2010,
  Title                    = {基于动态测试用例生成的二进制软件缺陷自动发掘技术研究},
  Author                   = {李根},
  Year                     = {2010},

  File                     = {:article\\基于动态测试用例生成的二进制软件缺陷自动发掘技术研究.pdf:PDF},
  Groups                   = {binarary vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {IZ42586 分类号 学号 UDC}
}

@InProceedings{混淆算法研究综述1678,
  Title                    = {混淆算法研究综述},
  Author                   = {混淆算法研究综述},
  Year                     = {1678},

  File                     = {:article\\混淆算法研究综述.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {第 !! 卷第 " 期 同 济 大 学 学 报（自 然 科 学 版） 1678 !! *68 " #$$% 年 " 月 &’()*+, ’- .’*/&0 (*012)30.4（*+.()+, 3502*52） 9 &:;8 #$$% 混淆算法研究综述 史9 扬，曹立明，王小平 （同济大学 计算机科学与技术系，上海9 #$$$@#） 摘要：首先对混淆算法的力量、弹性、执行代价和隐蔽性等性能及其度量作了介绍8随后对各种混淆变换作了简要 的叙述和评论8混淆变换主要分为以下几种：第一，词法变换；第二，控制流变换，包括分支插入变换、循环条件插入 变换、将可化简的控制流转换为不可化简的控制流，取消循环和控制流恶化；第三，数据变换，包括数组重构变换、 变量重组变换、将静态数据转换为与程序相关的数据和数值变量混合变换；第四，类结构变换，包括类熔合、类分裂 和类型隐藏8此外，对使用对象和别名的不透明谓词和利用并行技术构建不透明谓词进行了简介，并综述了对混淆 算法的攻击方法以及混淆算法的应用8最后对反混淆和对混淆算法的攻击进行了总结，并提出了若干可能的研究 方向8 关键词：混淆变换；移动代码保护；知识产权保护 中图分类号：.K !!>9 9 9 9 9 9 文献标识码：+9 9 9 9 9 9 9 9 文章编号：$#%! = !?<L（#$$%）$" = $M>! = $? !"#$%& ’( )%*%+#,- ’. /0."*,+12(3 453’#21-6* !"# $%&’，()* +,-.,&’，/)01 2,%3-4,&’ （NOPCQRBO;R 6S 56BP:ROQ 3EDO;EO C;T .OEG;676IH，.6;IUD (;DVOQFDRH，3GC;IGCD #$$$@#，5GD;C） 40*1#+,1：WOCF:QOF 6S P6RO;EH，QOFD7DO;EO，FROC7RG C;T OXOE:RD6; E6FR CQO D;RQ6T:EOT CR RGO YOID;;D;I8 .GO; C FH;6PFDF C;T C QOVDOZ 6S S6776ZD;I ECROI6QDOF 6S 6YS:FECRD;I RQC;FS6QBCRD6;F CQO IDVO;8 .GO SDQFR DF 7OXDEC7 RQC;FS6QBCRD6;8 .GO FOE6;T DF E6;RQ67 S76Z RQC;FS6QBCRD6;F D;E7:TD;I YQC;EG D;FOQRD6;，OXRO;TD;I 766P E6;TDRD6;F，E6;VOQRD;I C QOT:EDY7O S76Z IQCSR R6 C ;6; = QOT:EDY7O 6;O，QOB6VD;I 766PF，TOROQD6QCRD6; 6S E6;RQ67 S76Z C;T TCRCS76Z RQC;FS6QBCRD6;8 .GO RGDQT DF TCRC RQC;FS6QBCRD6;F D;E7:TD;I QOFRQ:ER:QD;I CQQCHF， QOE6;FRQ:ERD;I VCQDCY7OF，E6;VOQRD;I FRCRDE TCRC R6 PQ6EOT:QC7 TCRC C;T BOQID;I FEC7CQ VCQDCY7OF8 .GO S6:QRG DF E7CFF FRQ:ER:QO RQC;FS6QBCRD6;F D;E7:TD;I E7CFF E6C7OFED;I，E7CFF FP7DRRD;I C;T RHPO GDTD;I8 -:QRGOQB6QO， CRRCE[F CICD;FR 6YS:FECRD;I C7I6QDRGBF C;T TO6YS:FECRD6; CQO F:BBCQD\OT8 36BO F:IIOFRD6;F CY6:R S:R:QO Z6Q[ CQO PQ6P6FOT D; RGO O;T8 7%& 8’#9*：6YS:FECRD;I RQC;FS6QBCRD6;F；B6YD7O E6TO PQ6ROERD6;；D;RO77OER:C7 PQ6POQRH PQ6ROERD6; 9 9 混淆算法是一种可以用于对移动代码和软件知 护移动代码方面得到了广泛的应用8同时，&CVC 语言 识产权进行保护的程序变换技术8 随着移动代码技 的发展既促进了跨平台应用的推广，也带来了保护 术，尤其是 W6YD7O +IO;R的广泛使用，混淆算法在保 知识产权方面的新问题：&CVC 语言是经过预编译后 收稿日期：#$$< = $> = ># 基金项目：国家自然科学基金资助项目（?$>?>$">） 作者简介：史9 扬（>@?? =），男，江苏南京人，博士生8 2ABCD7：E;FGDHC;IJ HCG668 E6B8 E; 万方数据

}
}

@PhdThesis{玄跻峰2013,
  Title                    = {面向软件 Bug仓库的数据分析及其应用},
  Author                   = {玄跻峰},
  School                   = {大连理工大学},
  Year                     = {2013},

  File                     = {:article\\面向软件bug仓库的数据分析及其应用.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = { 博 士 学 位 论 文 面向软件 Bug仓库的数据分析及其应用 Data Analysis with Applications of Software Bug Repositories 作 者 姓 名： 玄跻峰 学科、 专业： 计算数学 学 号： 10901034 指 导 教 师： 江贺 完 成 日 期： 大连理工大学 Dalian University of Technology 

}
}

@InProceedings{王旭2008,
  Title                    = {二进制代码混淆关键技术研究},
  Author                   = {王旭},
  Year                     = {2008},

  Abstract                 = {Binary code obfuscation protects software from cracking and alteration by means of code reorganization and deformation. This paper analyses several cutting-edge code obfuscation algorithms, including the code out of order, insert opaque predicate and control flow flattening. Moreover, this paper shows the code obfuscation technology research at present through comparing these algorithms¶ advantages and disadvantages. At last, this paper will make a summary pointing out that code obfuscation technology in the theoretical studies still lack a complete argument, but some of which like control flow flattening has already used well in software anti-reverse. Keywords: Code Obfuscation; Program Control Flow Obfuscation; Reverse analysis.
},
  File                     = {:article\\二进制代码混淆关键技术研究.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Research of the Key Technologies in Binary Code Obfuscation Xu Wang1, Wen Qing Fan2, Wei Huang2 1. Information Security Center, Beijing University of Posts and Telecommunications, Beijing 100876, China 2. Communication University of China, Beijing 100024, China Abstract: Binary code obfuscation protects software from cracking and alteration by means of code reorganization and deformation. This paper analyses several cutting-edge code obfuscation algorithms, including the code out of order, insert opaque predicate and control flow flattening. Moreover, this paper shows the code obfuscation technology research at present through comparing these algorithms¶ advantages and disadvantages. At last, this paper will make a summary pointing out that code obfuscation technology in the theoretical studies still lack a complete argument, but some of which like control flow flattening has already used well in software anti-reverse. Keywords: Code Obfuscation; Program Control Flow Obfuscation; Reverse analysis. Ҽ䘋ࡦԓ⸱␧⏶ޣ䭞ᢰᵟ⹄ウ ⦻ ᰝ 1ˈ㤳᮷ᒶ 2ˈ哴⧞ 2 1. ेӜ䛞⭥བྷᆖؑ᚟ᆹޘѝᗳˈेӜˈѝഭˈ100876 2. ѝഭՐჂབྷᆖˈेӜˈѝഭˈ100024 ᪈ 㾱˖Ҽ䘋ࡦԓ⸱␧⏶ᢰᵟ䙊䗷ሩ〻ᒿⲴԓ⸱䘋㹼䟽㓴઼ਈᖒㅹ᡻⇥ሩ〻ᒿⲴҼ䘋ࡦԓ⸱䘋㹼؍ᣔˈ䱢→䖟Ԧ Ⲵ⹤䀓઼㈑᭩Ǆᵜ᮷ѫ㾱⹄ウҶࠐ⿽ࡽ⋯Ⲵԓ⸱␧⏶㇇⌅ˈवᤜԓ⸱ҡᒿˈᨂޕн䘿᰾䉃䇽઼ᒣኅ᧗ࡦ⍱ㅹˈ䙊 䗷∄䖳䘉Ӌ㇇⌅ⲴՈ઼࣯н䏣ኅ⽪Ҷᖃлԓ⸱␧⏶ᢰᵟⲴ⹄ウ⧠⣦Ǆᴰਾሩԓ⸱␧⏶ᢰᵟڊࠪҶᙫ㔃ˈᤷࠪԓ⸱ ␧⏶ᢰᵟ㲭❦൘⨶䇪⹄ウкቊ㕪ѿᆼ༷䇪䇱 նˈᱟԕᒣኅ᧗ࡦ⍱Ѫԓ㺘Ⲵ␧⏶ᢰᵟ൘㊫լ䖟Ԧ৽䘶ੁⲴᓄ⭘൪Ჟ лᐢ㓿ާ༷ᇎ⭘ᙗǄ ޣ䭞䇽˖ԓ⸱␧⏶˗᧗ࡦ⍱␧⏶˗䘶ੁ࠶᷀Ǆ 1 ᕅ䀰 ᧗ࡦ⍱࠶઼᷀ᮠᦞ⍱࠶᷀ᱟ〻ᒿ࠶᷀亶ฏѝє⿽ѫ 䳀㭭䖟Ԧ˄surreptitious software˅ᱟ䘁ॱࠐᒤᶕ䇑 㾱Ⲵ〻ᒿ࠶᷀ᯩ⌅ˈањᲞ䙊ᵚ㓿࣐ᇶ઼␧⏶ⲴҼ䘋ࡦ ㇇ᵪᆹޘ⹄ウ亶ฏᯠޤⲴањ࠶᭟Ǆ൘䳀㭭䖟ԦⲴ⹄ウ ԓ⸱ˈ䙊䗷䘉є⿽ᯩ⌅Ⲵ֯⭘ˈᖰᖰ㜭ཏ൘⸝ᰦ䰤޵㻛 䗷〻ѝнӵ䴰㾱ُ䢤䇑㇇ᵪᆹޘᯩ䶒Ⲵᢰᵟˈ䘈Պ⭘ࡠ ࠶઼᷀⹤䀓ˈ䙐ᡀԓ⸱Ⲵࢭコ઼㈑᭩Ǆ 䇑㇇、ᆖަԆ亶ฏⲴབྷ䟿ᢰᵟˈྲᇶ⸱ᆖˈᮠᆇ≤ঠˈ 2.1 ᧗ࡦ⍱࠶᷀ ԓ⸱␧⏶ˈ䘶ੁᐕ〻ԕ৺㕆䈁ಘՈॆㅹǄᡁԜ֯⭘䘉Ӌ ᡰ䉃᧗ࡦ⍱࠶᷀ᤷⲴᱟሩҼ䘋ࡦԓ⸱৽≷㕆ѻਾ䘋 ᢰᵟᶕ┑䏣൘䇑㇇ᵪ〻ᒿѝᆹޘᆈۘ〈ᇶؑ᚟Ⲵ䴰≲ˈ 㹼ⲴสҾ〻ᒿ⁑ඇ䰤䈳⭘ޣ㌫Ⲵа⿽࠶᷀ᯩ⌅[8]Ǆ᧗ࡦ ⴞⲴᱟ䱢→ԆӪࢭコ䖟ԦѝⲴᲪ࣋ᡀ᷌[4]Ǆ ⍱࠶᷀Ⲵޣ䭞ᱟሩ〻ᒿ䘋㹼৽≷㕆ѻਾ ˈࡂ࠶ࠪสᵜඇˈ ԓ⸱␧⏶ᢰᵟᱟа⿽䖟Ԧ؍ᣔᢰᵟǄ␧⏶ᱟᤷሩᐢ สᵜඇ⭡а㌫ࡇ亪ᒿᢗ㹼ⲴᤷԔᶴᡀˈᆳⲴ㔃ቮ䙊ᑨᱟ ਁᐳ䖟ԦⲴҼ䘋ࡦԓ⸱䘋㹼࠶઼᷀䟽ᯠ㓴㓷ˈ֯ᗇ༴⨶ ањ䐣䖜ᤷԔǄสᵜඇⲴ⢩ᙗᱟԓ⸱ᢗ㹼Ⲵᆼᮤᙗˈҏ ਾⲴԓ⸱൘䙫䗁࣏㜭к઼༴⨶ࡽⲴԓ⸱⴨਼ˈնᱟ൘㺘 ቡᱟ䈤൘〻ᒿᢗ㹼䗷〻ѝˈаᰖᢗ㹼ҶสᵜඇⲴㅜаᶑ ⧠ᖒᔿкՊӗ⭏ᖸབྷⲴਈॆˈ䘉⿽ਈॆ֯ᗇ䖟ԦⲴԓ⸱ ᤷԔˈ䛓ѸаᇊՊ亪ᒿᢗ㹼↔สᵜඇⲴᡰᴹᤷԔˈⴤࡠ ণ֯㻛䘶ੁ࠶᷀Ӫઈ৽㕆䈁ˈҏሶ䳮ԕ䰵䈫ˈӾ㘼ᴹ᭸ ᢗ㹼ᆼ↔สᵜඇⲴᴰਾаᶑᤷԔѻਾ᡽㜭⿫ᔰǄ ൠ؍ᣔҶ䖟Ԧ൘⸝ᰦ䰤޵㻛⹤䀓ˈ䱽վҶ䖟Ԧ㻛ⴇ⡸઼ ࡂ࠶สᵜඇѻਾˈቡ䴰㾱⨶␵สᵜඇѻ䰤Ⲵ㓴㓷ޣ ㈑᭩Ⲵਟ㜭ᙗ[7]Ǆ ㌫ˈҏቡᱟ䙊䗷ᴹੁമⲴᖒᔿሶสᵜඇѢ㚄䎧ᶕˈᖒᡀ ᵜ᮷ሶӻ㓽ԓ⸱␧⏶Ⲵสᵜᯩ⌅઼ㆆ⮕ˈ⵰䟽ሩԓ ᧗ࡦ⍱മ˄CFG˅Ǆਟ㿱ˈ᧗ࡦ⍱മᱟањйݳ㓴 G= ⸱␧⏶ѝสҾ᧗ࡦ⍱␧⏶Ⲵ㤕ᒢ㇇⌅䘋㹼⹄ウ઼࠶᷀ˈ ˄Nˈ Aˈ s˅ˈަѝ Nԓ㺘สᵜඇˈAԓ㺘สᵜඇѻ ∄䖳䘉Ӌ㇇⌅ѻ䰤ⲴՈ઼࣯н䏣ˈᒦᨀࠪԓ⸱␧⏶ᢰᵟ 䰤Ⲵᴹੁ䗩ˈ㘼 sԓ㺘᧗ࡦ⍱മⲴ䎧࿻สᵜඇ[1]ǄᴹҶ ൘ӺਾⲴ⹄ウਁኅᯩੁǄ ᧗ࡦ⍱മቡਟԕ⺞ᇊ〻ᒿ䘀㹼ᰦ᧗ࡦ⍱ᱟྲօ⍱ࣘⲴˈ 2 〻ᒿ࠶᷀Ⲵㆆ⮕ ഐ↔᧗ࡦ⍱മᱟԓ⸱ᡰᴹਟᢗ㹼䐟ᖴⲴањ䎵䳶Ǆ 

}
}

@Article{王雅文2012,
  Title                    = {一种基于代码静态分析的缓冲区溢出检测算法},
  Author                   = {王雅文},
  Journal                  = {计算机研究与发展},
  Year                     = {2012},

  File                     = {:article\\一种基于代码静态分析的缓冲区溢出检测算法.pdf:PDF},
  Groups                   = {source code vulnerability},
  Read                     = {未读},
  Review                   = {!"#$%&'( )* + ,- ./012345 6 89 :; <= >?@ A B C 7 G HIJKL MNO P Q R S T U V W X YZ[\ ]^_` ab cd ef gh i jkl mnop q rstu v wx yz{| } ~                   ¡¢ £¤ ¥¦ D E F 书}
}

@Article{王雷2011,
  Title                    = {基于约束分析与模型检测的代码安全漏洞检测方法研究},
  Author                   = {王雷},
  Journal                  = {计算机研究与发展},
  Year                     = {2011},

  File                     = {:article\\基于约束分析与模型检测的代码安全漏洞检测方法研究.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {!"#$%&'( )* + ,- ./012345 6 89 :; <= >?@ AB CD 7 H J K L I M N O P Q S R T U V W X Y Z[ \ ]^ _` ab c de fgh ij klmno pq r s tu v wx y z { |} ~               ¡¢ £¤¥ ¦ § ¨ E F G 书

}
}

@Article{甘水滔，秦晓军，陈左宁，王林章2015,
  Title                    = {一种基于特征矩阵的软件脆弱性代码克隆检测方法},
  Author                   = {甘水滔，秦晓军，陈左宁，王林章},
  Journal                  = {软件学报},
  Year                     = {2015},

  Abstract                 = {This article proposes a clone detection method based on a program characteristic metrics. Though analyzing the syntax and semantic characteristics of vulnerabilities, this detection method abstracts certain key nodes which describe different forms of vulnerability type from syntax parser tree, and expands four basic types of code clone to auxiliary classes. The characteristic metrics of the code then is finalized by obtaining the number of key nodes which are calculated via scanning corresponding code segment in the syntax parser tree. The clone detection based on a characteristic metrics creates basic knowledge base by extracting partial instances of open vulnerability database, and precisely locates the vulnerability codes by performing cluster calculation on the same codes responding to multiple types of code clone. Comparing with the detection method based on single characteristic vector, the proposed method produces more precise description about vulnerability. This detection method also offers a remedy to the drawbacks of formal detection method on its vulnerability type covering ability. Nine vulnerabilities are detected in an android-kernel system test. Testing on software of different code sizes shows that the performance of this method is linear with the size of the code. Key words: vulnerabilitydetection; codeclone; syntax parser tree; metrics of characteristics 
},
  File                     = {:article\\一种基于特征矩阵的软件脆弱性代码克隆检测方法.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {软件学报 ISSN 1000-9825, CODEN RUXUEW E-mail: jos@iscas.ac.cn Journal of Software,2015,26(2):348−363 [doi: 10.13328/j.cnki.jos.004786] http://www.jos.org.cn ©中国科学院软件研究所版权所有. Tel: +86-10-62562563 一种基于特征矩阵的软件脆弱性代码克隆检测方法∗ 甘水滔 1, 秦晓军 1, 陈左宁 1, 王林章 2 1(数学工程与先进计算国家重点实验室(无锡江南计算技术研究所),江苏 无锡 214083) 2(计算机软件新技术国家重点实验室(南京大学),江苏 南京 210023) 通讯作者: 甘水滔, E-mail: ganshuitao@gmail.com 摘 要: 提出了一种基于特征矩阵的软件代码克隆检测方法.在此基础上,实现了针对多类脆弱性的检测模型.基 于对脆弱代码的语法和语义特征分析,从语法分析树抽取特定的关键节点类型描述不同的脆弱性类型,将 4 种基本 克隆类型细化拓展到更多类,通过遍历代码片段对应的语法分析树中关键节点的数量,构造对应的特征矩阵.从公开 漏洞数据库中抽取部分实例作为基本知识库,通过对代码进行基于多种克隆类型的聚类计算,达到了从被测软件代 码中检测脆弱代码的目的.与基于单一特征向量的检测方法相比,对脆弱性特征的描述更加精确,更具有针对性,并 且弥补了形式化检测方法在脆弱性类型覆盖能力上的不足.在对 android-kernel代码的测试中发现了 9个脆弱性.对 不同规模软件代码的测试结果表明,该方法的时间开销和被测代码规模成线性关系. 关键词: 脆弱性检测;代码克隆;语法分析树;特征矩阵 中图法分类号: TP311 中文引用格式: 甘水滔 ,秦晓军,陈左宁 ,王林章.一种基于特征矩阵的软件脆弱性代码克隆检测方法.软件学报,2015,26(2): 348−363. http://www.jos.org.cn/1000-9825/4786.htm 英文引用格式: Gan ST, Qin XJ, Chen ZN, Wang LZ. Software vulnerability code clone detection method based on characteristic metrics. Ruan Jian Xue Bao/Journal of Software, 2015,26(2):348−363 (in Chinese). http://www.jos.org.cn/1000-9825/4786.htm Software Vulnerability Code Clone Detection Method Based on Characteristic Metrics GAN Shui-Tao1, QIN Xiao-Jun1, CHEN Zuo-Ning1, WANG Lin-Zhang2 1(State Key Laboratory of Mathematical Engineering and Advanced Computing (Jiangnan Institute of Computing Technique), Wuxi 214083, China) 2(State Key Laboratory for Novel Software Technology (Nanjing University), Nanjing 210023, China) Abstract: This article proposes a clone detection method based on a program characteristic metrics. Though analyzing the syntax and semantic characteristics of vulnerabilities, this detection method abstracts certain key nodes which describe different forms of vulnerability type from syntax parser tree, and expands four basic types of code clone to auxiliary classes. The characteristic metrics of the code then is finalized by obtaining the number of key nodes which are calculated via scanning corresponding code segment in the syntax parser tree. The clone detection based on a characteristic metrics creates basic knowledge base by extracting partial instances of open vulnerability database, and precisely locates the vulnerability codes by performing cluster calculation on the same codes responding to multiple types of code clone. Comparing with the detection method based on single characteristic vector, the proposed method produces more precise description about vulnerability. This detection method also offers a remedy to the drawbacks of formal detection method on its vulnerability type covering ability. Nine vulnerabilities are detected in an android-kernel system test. Testing on software of different code sizes shows that the performance of this method is linear with the size of the code. Key words: vulnerabilitydetection; codeclone; syntax parser tree; metrics of characteristics ∗ 基金项目: 国家自然科学基金(91318301, 61170066, 6147179) 收稿时间: 2014-07-09; 修改时间: 2014-10-31; 定稿时间: 2014-11-26 

}
}

@Article{聂楚江2011,
  Title                    = {一种微观漏洞数量预测模型},
  Author                   = {聂楚江},
  Journal                  = {计算机研究与发展},
  Year                     = {2011},

  Doi                      = {tchtenumboefvrulnerab订isivteireuyssefiunlsome},
  File                     = {:article\\一种微观漏洞数量预测模型.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {计算机研究与发展 lsS1N000一123191／—c1N777／TP JournoafClompuRteesreaarncIdh)evelopment 48(7)：1279一1287，20ll 一种微观漏洞数量预测模型 聂楚江1 赵险峰1 陈 恺h2 韩正清3 1(信息安全国家重点实验室(中国科学院软件研究所)北京 100190) 2(信息安全国家重点实验室(中国科学院研究生院)北京100049) 3(北方交大计算所北京100029) (niecj@is．iscas．ac．cn) AnSoftwVaurleneraNbiulmibtPeyrerdicMtoidoBneals0endMicr俨Parameters NieChujian9X1i，aZnhfaeon9K1a，iClh仙e，naHnadZnhengqin93 1(S细￡PLKnP6y0r口fory∥，咒，0砌口￡iD砣S盯“r的(J玎吡砧抛。厂So^伽"o，，吼Sic咒i￡朋scePA5∞)d，跏B1Py0巧0i1行g90) 2(S纽把KLP口y60rnof，ofq以如研ⅥfiD咒S吖“rf缈(Gr口d啪抛U行iw搿iA砂fo口，dC已，优loy，iS珂cPis棚Pf￡s)，BF1巧0i0玎0g49) 3(J雄5￡if越￡已o，(为m户越T￡■if咒IgIl玎ooZ，oNgDy，tIIlF，W．，iU口hDifD御恕，g百i￡y，BP巧1i0咒0g029) AbstmAcsthecosctausbeydsoftwvaurlenerabikleietpisnecsreasingp，apmyeorpalenemdore attentiotohneresearocnhtehsevulnerabiAllityh．oduigshcovevurlinegraibsidilfiftiycbuelctause ofthedefeocftvulneraabn订ailtysipsr，etdoitchtenumboefvrulnerab订isivteireuyssefiunlsome domaisnu，cahsinformasteicuornaistsyessmeAnttp．resentth，emaimnethotdoesstimtahtee densiofttyhevulnerabifloictuoisnetshemacr1eovelb，uthecyanotrefletchtessentoifal vulnerabiAliptrye．dicmtioodnebalseodnmicro-parisdpmreotpeortsoepdreditchtenumboefr vulnerabwilttihhtemyicro-paroafsmoefttewrasriete，xatnrdatchtetsypicmailcro—parafmreotmers somseoftwsaereifeosrthepurpofsdiescovethrerienlgatiobnesthiwpteheevnulneranbiulmibteyr anmdicro．paramWeittetrhseh．ypothoefvsuilsnerabiinlhietryitinpgr，etdhicemtoidoaneblstracts themicro．parafmreostmeorfstwaanrdterietsofinadlinerarelatiobnesthiwpetheevnulnerability numbaenrdsommeicro—paramTehtiemrsos．dealsgoiveasmethtoodpreditchtevulnerability numboefsroftwwairteihtsmicr0-paraanmdtehteveurlsnerabniulmibtoyefirtsprevivoeursions． Thimsethiosvderifwiietd7hsoftwsaereies，thaernedsulsthsotwheDredicmtoidoneelfifesctive． KewyoI。dsvulnerabpirleidtiycts；saonfatlwyasries；invhuelrnietreadb订ityv；uhlinsetroabriylity； microscpoapriacmeters 摘要全球每年因为软件漏洞造成的损失十分巨大，而软件漏洞分析方法的缺陷使得漏洞本身难以被 发现，因此大家开始对漏洞数量进行预测，预测软件的漏洞数量对信息安全评估有着重要的意义．目前 主要的估算方法是漏洞密度的方法，但此方法仅是宏观范围内估算，并不能反映漏洞软件本身的性质． 提出从软件的微观角度进行软件漏洞数量的估算通过提取软件典型微观参数，从而发现软件漏洞数量 与其微观参数的联系，相比漏洞密度的预测方法具有相当的优势．软件微观漏洞模型在提出漏洞继承假 设的基础上，认为软件的漏洞数量与它的某些微观参数之间存在线性关系。并给出了根据软件微观参数 以及其历史版本漏洞数据预测软件漏洞数量的方法．通过对7款软件进行验证，证明了软件微观漏洞模 型在预测软件漏洞数量时的有效性与准确性． 收穑日期：2009—12—31；修回日期：2011一05一06 基金项目：国家自然科学基金项目(61073179)I工信部科技重大专项基金项目(2011Zx03002∞05一02) 万方数据

}
}

@InProceedings{赵玉洁，汤战勇，王妮，房鼎益，顾元祥9825,
  Title                    = {代码混淆算法有效性评估},
  Author                   = {赵玉洁，汤战勇，王妮，房鼎益，顾元祥},
  Year                     = {9825},

  Doi                      = {：10．3724／SP．J．1001．2012．03h9t9t4p】：／／wwwjos．org．cn},
  File                     = {:article\\代码混淆算法有效性评估.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {软件学报ISS1N000—9825R，CUOXDEUNEW E-mail：jos@iscas．ac．cn JouronfaSloftware，2012，23(3)：700—711[doi：10．3724／SP．J．1001．2012．03h9t9t4p】：／／wwwjos．org．cn ◎中国科学院软件研究所版权所有． Tel／Fax：+08．66—21562563 代码混淆算法有效性评估木 赵玉洁1r，汤战勇1’，王妮1，一，房鼎益1，2+，顾元祥2，3 1(西北大学信息科学与技术学院，陕西西安710127) 2(西北大学．爱迪德信息安全联合实验室，陕西西安710127) 3(爱迪德技术(北京)有限公司，北京100125) EvaluaotfCiodnOebfuscTartainsgformation ZHAYuO—Jiel，2Z，ThAaNnG—Yon91，N2i，lW，A2N，GFDAiNngG—Yil，2Yu+a，nG-UXian92，3 1(SchoofoIlnforSmcaiteianoncnTedechnology，UNnoirvtehrswietsy7t，1X0i1’a2n7，China) 2(NWU—INredtewtork．InfSoercmuarJtioitioyLnnatboratory(NI7S1L0)1，2X7i，’Cahnina) 3(IrdAectcoeTseschnology(Beijing)1C0o0．1L2t5d，．C，hBeiinjai)ng +Correspaountdhoirn：gE-mail：dcyaf@nwu．edu ZhaYoJ，TaZnYg，WaNn，gFaDngY，GYuX．Evaluoafctoidooenbfusctatriangsformationf．Journal Software，2012，23(3)：700-711．http：／／www．jos．org．cn／1000—9825／3994．htm AbstracCto：doebfuscaistciuornreontnleoyfthemosvtiabmlethfoodprsrevenrteivnegernsegineeatrtiancgks Mankyindosfcodoebfuscattriaonnsfaorrewmisdeulsyeidnsoftwparoetection．Howareesvteilrnl,othere suffictiheenotrtioesvalutahteeffectivoefonbefsusscattrainosnforfma．cItn，mfewasureamreanvtaislable thaptrovidneformatbiooutnthecapabiolfiotbyfuscattorieodnuactetackers’effifceiwexnicsyt,ianngd theories，dwrhaiwucsphocnomplemxeittryifcrsosmoftweanrgeineericnogn，vairencinpga．pTeuhrsiesas differweantyoevalutahtedeiffictuhlatatyttackhearvsienunderstandmiondgifoybifnugscsaotfetdware throustgahtaincalysis，dyenbaumgiogcfirnevgeresnegineeritnhge，tnoaanbdstrsaoctmmeetrtiocqsuanttiofy whaetxtetnhtactodoebfusciasatbilotenomakaettamckosrdeiffitcoubletperformed． Kewyords：codoebfuscatione；nrgeivneeresreing；evaplluuagt·inosn；；cfIolDnoAtfwrloalttening 摘要： 代码混淆是一种能够有效增加攻击者逆向分析和攻击代价的软件保护技术．然而，混淆算法的有效性评 价和验证是代码混淆研究中亟待解决的重要问题．目前，对代码混淆有效性的研究大都是基于软件复杂性度量的，然 而代码混淆作为一种保护软件安全的技术，更需要从逆向攻击的角度进行评估一寺面向逆向工程的思想引入到代码 混淆算法评估中，通过理论证明和具体实验验证了其可行性．该评估方法能够为混淆算法提供有效证明，并对判别和 选择代码混淆算法具有指导意义，同时也有助于寻求更有效的代码混淆方法． 关键词： 代码混淆；逆向工程；评估；IDA插件；平展控制流 中图法分类号：TP309 文献标识码：A ·基金项目：国家自然科学基金(61070176，61170218)；陕西省教育厅产业化示范项目(2010JC24)；西安市科技计划项目 (CXY1l)0；1西北大学一爱迪德信息安全联合实验室项目(NISL一2009TR01)；西北大学研究生自主创新资助项目(10YZZl6) 收稿时间：2010—09—09；定稿时间：2011-01—21 万方数据

}
}

@MastersThesis{雷植洲2007,
  Title                    = {代码混淆技术及其在软件安全保护中的应用研究},
  Author                   = {雷植洲},
  Year                     = {2007},

  File                     = {:article\\代码混淆技术及其在软件安全保护中的应用研究.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {华中科技大学 硕士学位论文 代码混淆技术及其在软件安全保护中的应用研究 姓名：雷植洲 申请学位级别：硕士 专业：计算机应用技术 指导教师：徐海银 20070530

}
}

@InProceedings{（2012年）2012,
  Title                    = {中国计算机学会推荐国际学术会议和期刊目录},
  Author                   = {（2012年）},
  Year                     = {2012},

  File                     = {:article\\中国计算机学会推荐国际刊物会议列表2012.pdf:PDF},
  Groups                   = {Academic Information},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = { 中国计算机学会推荐国际学术会议和期刊目录 （2012年） 中国计算机学会 

}
}

@Other{,
  Title                    = {-An_Empirical_Study_on_Using_the_National_Vulnerability_Database_to_Predict_Software_Vulnerabilities.html},
  File                     = {:home/ccc/github/literature/article/-An_Empirical_Study_on_Using_the_National_Vulnerability_Database_to_Predict_Software_Vulnerabilities.html:URL}
}

@Other{,
  Title                    = {2012-06-08-ICSE-UnderstandingIntegerOverflow.html},
  File                     = {:home/ccc/github/literature/article/2012-06-08-ICSE-UnderstandingIntegerOverflow.html:URL}
}

@Other{,
  Title                    = {-Predicting_Defects_in_SAP_Java_Code_An_Experience_Report.html},
  File                     = {:home/ccc/github/literature/article/-Predicting_Defects_in_SAP_Java_Code_An_Experience_Report.html:URL}
}

@Article{,
  Title                    = {12341-sec13-paper_davidson.pdf},

  File                     = {:article\\12341-sec13-paper_davidson.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {An Experimental Evaluation of Deliberate Unsoundness in a Static Program Analyzer.pdf},

  File                     = {:article\\An Experimental Evaluation of Deliberate Unsoundness in a Static Program Analyzer.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Other{,
  Title                    = {Towards automatic vulnerability detection.ps},
  File                     = {:home/ccc/github/literature/article/Towards automatic vulnerability detection.ps:PostScript}
}

@Article{,
  Title                    = {Automatic Discovery and Patching of Buffer and Integer Overflow Errors MIT-CSAIL-TR-2015-018.pdf},

  File                     = {:article\\Automatic Discovery and Patching of Buffer and Integer Overflow Errors MIT-CSAIL-TR-2015-018.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Bytecode Model Checking_ An Experimental Analysis.pdf},

  File                     = {:article\\Bytecode Model Checking_ An Experimental Analysis.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Docker从入门到实践.pdf},

  File                     = {:article\\Docker从入门到实践.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Unpublished{,
  Title                    = {Lecture19.4up.pdf},

  File                     = {:article\\Lecture19.4up.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Rd                       = {N},
  Read                     = {未读}
}

@Article{,
  Title                    = {Fixing recurring crash bugs via analyzing Q\&A sites ASE15-Gao.pdf},

  File                     = {:article\\Fixing recurring crash bugs via analyzing Q&A sites ASE15-Gao.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {ICEIS 2014.pdf},

  File                     = {:article\\ICEIS 2014.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.10.23}
}

@Article{,
  Title                    = {issta12.pdf},

  File                     = {:article\\issta12.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Model Checking as Static Analysis_Revisited.pdf},

  File                     = {:article\\Model Checking as Static Analysis_Revisited.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Model Checking Programs.pdf},

  File                     = {:article\\Model Checking Programs.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {pldi2013.pdf},

  File                     = {:article\\pldi2013.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Principles.of.Program.Analysis,.Nielson,.Nielson,.Hankin,.1999,.only.ch.1-4.pdf},

  File                     = {:article\\Principles.of.Program.Analysis,.Nielson,.Nielson,.Hankin,.1999,.only.ch.1-4.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Program Analysis as Model Checking of Abstract Interpretations.pdf},

  File                     = {:article\\Program Analysis as Model Checking of Abstract Interpretations.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Prophet_ Automatic Patch Generation via Learning from Successful Human PatchesMIT-CSAIL-TR-2015-019.pdf},

  File                     = {:article\\Prophet_ Automatic Patch Generation via Learning from Successful Human PatchesMIT-CSAIL-TR-2015-019.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Semantic Importance Sampling for Statistical Model Checking.pdf},

  File                     = {:article\\Semantic Importance Sampling for Statistical Model Checking.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Software Model Checking_Searching for Computations in the Abstract or the Concrete.pdf},

  File                     = {:article\\Software Model Checking_Searching for Computations in the Abstract or the Concrete.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.12.21}
}

@Article{,
  Title                    = {Static Program Analysis(Anders Møller).pdf},

  File                     = {:article\\Static Program Analysis(Anders Møller).pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Static Program Analysis-plmw14.pdf},

  File                     = {:article\\Static Program Analysis-plmw14.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@Article{,
  Title                    = {Symbolic Execution Based Automated Static Bug Detection for Eclipse CDT.pdf},

  File                     = {:article\\Symbolic Execution Based Automated Static Bug Detection for Eclipse CDT.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2015.11.12}
}

@Article{,
  Title                    = {Targeted Automatic Integer Overflow Discovery Using asplos15.pdf},

  File                     = {:article\\Targeted Automatic Integer Overflow Discovery Using asplos15.pdf:PDF},
  Owner                    = {c},
  Timestamp                = {2016.01.18}
}

@InProceedings{,
  Title                    = {程序分析-原理 UCSB CS290C课程笔记},

  File                     = {:article\\程序分析-原理部分全.pdf:PDF},
  Review                   = {程序分析-原理 UCSB CS290C课程笔记 0 缘起 程序分析最传统、可能也是最重要的应用应该在编译优化。有关编译的材料，网上相当 多，其中最权威的包括龙书、虎书、鲸书和Rice出的那本Optimizing Compilers for Modern Architectures。龙书和虎书覆盖编译了很多基础方面，部分章节也讨论优化；而 后两者专門讲优化，程序分析的大部分内容其实都涉及到了。从编译的角度，这份材料可 以作为补充参考，此外这門课在两个方面还有点特色：(1) 对程序分析的体系梳理比较到 位，内容几乎都很直观，便于理解。可以看完这个材料再去看那几本书，很多内容会更清 晰；(2) 这門课的授课老师Ben Hardekopf在指针分析领域有几个重要的工作，对指针分 析介绍得比较全面深入。 这份材料主要的对象是那些对程序分析感兴趣但还没有找到合适材料的同仁。说实话，能 把这个领域的主要内容压缩在几十页的篇幅自己也有点惊讶，然而事实就是如此。有些内 容如果感兴趣希望扩展阅读，后面也列出的重要的参考文献。程序分析是一个相当大的领 域，发表的文章可谓浩如烟海，通过浓缩的方式把精要梳理出来，把重要的文献整理出 来，相信能帮助节省很多自己摸索的时间。通过这份材料或许能给你的工具箱添加一个新 的探索问题的工具。 必须说明这份材料仅仅只是笔记，里面的贴图都来自课程的ppt(网上可以下载)。由于水 平有限可能有错误之处，建议结合ppt对照使用。由于这个领域很大，相信有很多内容没 有涉及到，因此这个文档没有写完，只是由于自身能力局限，只能写到这里。恳请知道更 多内容且有时间的同仁继续补充，最终给大家提供一份这个领域比较全面的资料。 整个材料的提纲如下： 1 导言 2 控制流分析 3 数据流分析 4 稀疏分析和SSA 5 指针分析 6 过程间分析 7 集合约束和Andersen指针分析 8 类型约束和Steensgaard指针分析

}
}

@InProceedings{9825,
  Title                    = {基于Markov博弈模型的网络安全态势感知方法},
  Year                     = {9825},

  Doi                      = {r：nEg—mail：jz矗ang@zmail．ustc．edu．en},
  File                     = {:article\\基于Markov博弈模型的网络安全态势感知方法.pdf:PDF},
  Groups                   = {Network Security},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {软件学报ISS1N000．9825R，CUOXDEUNEW E-mail：jos@iiscas．ac．cn JournalofSoft．aye，2011,22(3)：495-508【doi：10．3724／SELl001．201ht1t．p0：3／／7w5w1w】．jos．org．cn @中国科学院软件研究所版权所有． Tel／Fax：+86．10．62562563 基于Markov博弈模型的网络安全态势感知方法幸 张勇+，谭小彬，崔孝林，奚宏生 (中国科学技术大学自动化系，安徽合肥230027) NetwSoerckurSityuaAtiwoanreAnpepsrsoBacsohendMarkGoavmMeodel ZHAYNoGng+，XTiaAoN．．BinX，iCaUoI--LHiong,-X一ISheng (DeparotfmAauattomation，UonfSicvieeransnciTdeteychnoolfCohgiyna，H2e3f0e0i27，Chimf) +Correspaoutnhdoir：nEg—mail：jz矗ang@zmail．ustc．edu．en ZhanY，gTaXnB，CXuLi，XHiS．NetsweocrukrsiittyuataiwoanreanepspsrobaacsheodnMarkgoavme model．JooufSronfatlware,2011，22(3)：495-508．http：／／www．jos．org．．cn／1000--9825／3751．htm Abstracatn：aTlyothzienfluoenfpcreopagaotnainoentwsoyrsktaenmadccuraetveallyusaytsetsecmurity, thipsapepropoasnaepsprotaoicmhprtohveaewareonfensestwsoerckurityo,nbthaeMseadrkGoavmMeodel IIMGM)．aTphpirsogaacihnastanddaartodafassets，threvautlsn，earnadbivliiaftuiseiasnvgarieotfsyystem securdiattycaollebctymeudlti一．senseorvse．rtFhyorreat．a，intalytzheersuloefpropagatnibdouniMatshreat propaganteitownork(TuPsNin)tgh．eGBaymTeheotroaynalythzbeehaviooftrhsreats，administrators，and ordinuasreyrse，istabliatshreepselayMeGrM．oIrndteormaktehevaluaptrioocnearsesal-otpiemreation，it optimithzreeslaatledgorithMm．GTchMaerdlynamiecvaallluysaytsetsemcursiittyuatainopdnrovithdbeest reinforcsecmheenfmtoratheadministraetvoarl．uTahotefiaosnpecinfiectwionrdkicattheatsheapproias ch suitafbolarereanletweonrvkironmentth．e，vaanlduarteisouinlsptreciasnedfficienrte．iTnhfeorsccemhenmta Caenffectcivuertlbhyepropagoaftthiroenats． Kewyords：nestecwuorsriitktyuataiwoanrenessp；rtohrpeaagtnateitownork；gMarmmkeoodevlI 摘要： 为了分析威胁传播对网络系统的影响，准确．全面地评估系统的安全性，并给出相应的加固方案，提出一种 基于Markov博弈分析的网络安全态势感知方法通过对多传感器检测到的安全数据进行融合，得到资产、威胁和脆 弱性的规范化数据；对每个威胁，分析其传播规律，建立相应的威胁传播网络；通过对威胁、管理员和普通用户的行为 进行博弈分析，建立三方参与的Markov博弈模型，并对相关算法进行优化分析，使得评估过程能够实时运行．Markov 博弈模型能够动态评估系统安全态势，并为管理员提供最佳的加固方案．通过对具体网络的测评分析表明，基于 Markov博弈分析的方法符合实际应用，评估结果准确、有效，提供的加固方案可有效抑制威胁的扩散． 关键词： 网络安全态势感知；威胁传播网络．；Markov博弈模型 中图法分类号：TP393 文献标识码：A 随着网络结构的日趋庞杂和各种新型攻击手段的大量涌现，网络安全问题越来越严峻，网络安伞技术也在 不断变革，从传统的入侵阻止、入侵检测发展到入侵容忍、可生存性研究，从关注信息的保密性发展到关注信 ·基金项目：国家高技术研究发展计划(863)(2006从Olz449)；中国博士后科学基金资助项[|(20070420738) 收稿时间：2009．06．-24；定稿时间：2009．10-．10 万方数据

}
}

@InProceedings{9825a,
  Title                    = {基于行为依赖特征的恶意代码相似性比较方法},
  Year                     = {9825},

  Doi                      = {ncretahseaeccuroafcoymp撕asnoadnnti-ja珊c嘶anpgabil1i0劬0tpahsree},
  File                     = {:article\\基于行为依赖特征的恶意代码相似性比较方法.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {软件学报ISS1N000．9825R，ICoⅨDUEENW E-mail：jos@iscas．ac．cn ．肋“，w口，矿5龟7’¨W耀，20l】0，2)2：(21438—24503．[3d7o2j4P．／：Js．l01 01．210．l03888】 bllp：／／wwwjos．o喀．cn @中国科学院软件研究所版权所有． Tel／Fax：+86．10—62562563 基于行为依赖特征的恶意代码相似性比较方法木 杨轶1，3+，苏璞睿1，应凌云1，冯登国1’2 1(信息安全国家重点实验室中国科学院软件研究所，北京 lool90) 2(信息安全国家重点实验室中国科学院研究生院，北京100049) 3(信息安全共性技术国家工程研究中心，北京 100190) DependencMya-lBwaSasiremeidIaCr0imtyparMiestohnOd YANYiGl，3+，SUPu．RuilY， INLiGng．Y嘶1F，ENDeGng．Gu01，2 1(stateKeyLaboratsoercyuroiftI)n，f。oImnastitoInlteofso脚are，TheChin1e0s0e1A90c，acdheimnyao)fsciences，Be幻ing 2(staKteLy西oratoofIrnyfoⅡmSteiocn谢ty，研audunaitverscih饥iTnhAecseadoefsmcyiences，Bleoq0i0Ⅱ4g9，China) 3(NatiEongalineR州cnsge盯CcehntfeorrInfo珊Saectlilornity，1B0e0司1in9g0，China) +co玎espoanutdhionr：gE—mail：y趾gyi@is．iscas．∞．cn YaⅡYg，SPuR，YiLnYg，FeDnGg．Dependenmcayl-wBsairsmieldacroimtpyarisoⅡmethod．．，bH棚村矿 蛳口比，20ll，22(10)：2438—2453．ht印：／／www．jos．org．cn／1000—9825／3888．h恤 AbstracMta：lwsairmeilacroitmyparisiosnoenftllbeasiwcoriknsmalwanraelysdiest锄edction．Presently， mosstimilari哆commeptahrnoi‘edsamostnalwasCreFoGrbehavsieoqruences．Maluwsaeorbmes谢ctaetrison， packerost柚hmederaIolfstechnitoqcuoenfuse仃adsiitmiiolnc撕aotlmyparmiestohnods．paTphpeirsopoases neawppmianicdhenti母tlhisenigmilarbietitewsmeaelnwsaarmeples，rwehloiynchontrdoelpendednactea粕d d印endence．Fidrsytn，锄tithaceillatnalyissipscIfb衄toeodbtacionn仃0d1ependreenlcaetiaonndsata dependreenlcaetions．cNoenxttrd，oealpendgern印chaendatdaependgernacpaerheconstIucted．Similarity infomaitsoibotnaibnyecdompatrhiensge撕t7y0peosfgraph．oIrndetortal【efulladvanotfatgheinherent behavoifmoarlicicoudsaenstdoincretahseaeccuroafcoymp撕asnoadnnti-ja珊c嘶anpgabil1i0劬0tpahsree recuetdh加nedlbbissrhemobvyemdeaonftshdeependgernacprhee—processirnegd，uwtchhiececsohmplexi哆 ofthesimil撕cto)rmparailsgoondatnhdimprotvhepserfornolfathnecaelgorithpmr．oTphoepsreodto哆pe systheamsbeeanppliteowdilmdalwacrolelectiorness．uTlshthseotwhattheaccllroaftchyemethod孤d comparciaspaobnilainthiaeVsaenobViaoduvsantage． Kewyords：malware；sicmoimlaprairtiyson；dynamicp锄raolpyasgiast；itoanint 摘要： 恶意代码相似性比较是恶意代码分析和检测的基础性工作之一，现有方法主要是基于代码结构或行为序 列进行比较．但恶意代码编写者常采用代码混淆、程序加壳等手段对恶意代码进行处理，导致传统的相似性比较方 法失效．提出了一种基于行为之间控制依赖关系和数据依赖关系的恶意代码相似性比较方法，该方法利用动态污点 传播分析识别恶意行为之间的依赖关系，然后，以此为基础构造控制依赖图和数据依赖图，根据两种依赖关系进行恶 意代码的相似性比较．该方法充分利用了恶意代码行为之间内在的关联性，提高了比较的准确性，具有较强的抗干扰 ·基金项目：国家自然科学基金(60703076)；国家高技术研究发展计划(863)(2007AAOIz45l，2009从OIZ435) 收稿时间：20lO—01-08；修改时间：2010—03—30；定稿时间：20lO—05-14 万方数据

}
}

@InProceedings{9825b,
  Title                    = {基于语义的恶意代码行为特征提取及检测方法},
  Year                     = {9825},

  Abstract                 = {：aTphepirsopoasesesmantic—abpapsreodtaocmhalwabreheavisoirganlateuxrteracatniodn
},
  Doi                      = {r：nEg·mail：wangrui@is．iscas．∽．∞},
  File                     = {:article\\基于语义的恶意代码行为特征提取及检测方法.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {软件学报ISS1N000．9825R．CUOXDEUNEW E·mail：jos@iseas．ae．cn JournalofSoftware，2012，23(2)：378—393【doi：10．3724／SP．J．1001．2012h．t0t3p：9／5／3w】ww．jos．org．cn o中国科学院软件研究所版权所有． Tel／Fax-+86．10．62562563 基于语义的恶意代码行为特征提取及检测方法· 王蕊1,2+9冯登国13，杨轶3，苏璞睿3 1(中国科学院研究生院，北京 100049) 2(信息安全国家重点实验室(中国科学院信息工程研究所)'j1E0京0029) 3(中国科学院软件研究所，北京100190) SemanticMs-aBiawsBaerdheavSiogrnaEtxutrreacatniDdoentecMtieotnhod WANRuGil，2+，DFeEnNgG．Guol，一Y，iY3A，NPGSuU-Rui3 1(GradUunaitveersiCthy，iTnhAeecseadoefmSyciences1，0B0e0U4i9n，gChina) 2(StKateeLyaboraotfoIrnyforSmeactuirointy(IonfsItniftoutreEmnagtiinoeneriCnhgi，nTAehsceeadoefmScyiences)1，0B0e0i2j9，ing China) 3(InstiotfSuotfetwarCeh，iTnhAecseadoefSmcyiences，1Be0i0j1i9n0g，China) +Correspaoutnhdoir：nEg·mail：wangrui@is．iscas．∽．∞ WanRgFenDgG，YaYn，gSuPR．Semanticmsa—iBwasaberdheavsiiogrnateuxrteracatnidoentection method．JofuSronftawlare,2012,23(2)：378-393．http：／／wwwdos．org．cn／1000—9825／3953．htm Abstractp：aTphepirsopoasesesmantic—abpapsreodtaocmhalwabreheavisoirganlateuxrteracatniodn detectiona．pTphrioseaxcthracrtistimcallwabreheaviaoswreslalsdependeanmcioentshgesbehaviors， integraitnisntgructiotani-anltneavleylasnibdsehaviors-elmeavnetalniaclsysis．Tahceqnu，iartnetsi·interference malwaberheavsiiogrnatusriensagnti-obfuesncgatitinooeindentsifeymanitrircelevaanndscemantically equivalence．pFruorttohtseyrps,etabeamsoendthissignaetxutrreacatnidoentectaipopnroisadcehvelaopnedd evaluabtymeudltimplaelwasarmeples．Experresiumlhetansvtdeaelmonsttrhatheemdalwsairgenatures extracstheodgwooadbilittoayntoibfuscatnidtohnedetectbiaosneodnthesseisgnatcuoruelsrdecognize malwvaarreiaenftfsectively． Kewyords：malware；semantiscisg；nbaethxurtaervaicotriond；emtaelcwtairoen 摘要： 提出一种基于语义的恶意代码行为特征提取及检测方法，通过结合指令层的污点传播分析与行为层的语 义分析，提取恶意代码的关键行为及行为阃的依赖关系；然后，利用抗混淆引擎识别语义无关及语义等价行为．获取 具有一定抗干扰能力的恶意代码行为特征．在此基础上，实现特征提取及检测原型系统．通过对多个恶意代码样本的 分析和检测，完成了对该系统的实验验证．实验结果表明，基于上述方法提取的特征具有抗干扰能力强等特点，基于 此特征的检测对恶意代码具有较好的识别能力． 关键词： 恶意代码；语义；行为特征提取；恶意代码检测 中图法分类号：TP309 文献标识码：A ·基金项目：国家自然科学基金(60703076，61073179)；国家高技术研究发展计划(863)(2007AA012451，2009AA012435) 收稿时间：2010．04．12；修改时间：2010．09-10；定稿时间：2010．10．II 万方数据

}
}

@InProceedings{,
  Year                     = {3456},

  File                     = {:home/ccc/github/literature/article/Android??????????.pdf:PDF},
  Review                   = {! !"#$%&'(
!"#$%&'())*++,$%%%-$./0&.%$(&.%$(%('.
!
" #! $
1234,56278293;<4=<+<54>?5,@!<A<629<,; (.$% .$B'.$'' .%$(
: :
+,-./01234
!"#$%&#
$ $
$ $/ $ .) . $ $ $ $
!"# $%& ' ( ')* +,- ./0 1 2 345 6 7
! ! ! ! ! ! ! ! ! ! !
$
" #
>)-.?@.)*!"#ABCDEF>G HI
$%$)%D
! !
.
" " # #
JKLMANOPQR<S)*TUVWX Y7Z[-<@. Y7
'$%%'$
! !
/
" #
)*!"#AB\]<S^N_`>G HI
$%%%.0
! !
)
" #
HIZ[-<.? HI
$%%%'%
! !
" #
E?5, 3>5+&5>&>,
FGH"
'($)* %,!"#$%&#-(."*$/0&.&1 2*1*31&%"
+ +
$ $
$ $/ $ .) . $ $
$ $ $ $ $ $ $
I?5, J3*, K5, I?<3, M5, N5* M5, I?**5, J3<O2,E?23 P*3Q*R3 O<J35,
F H F F L F F H F F
$ $
$
P*S*52* 5,@J5, T5,
H F F
$
" $ $ #
!"#$%&"'(%) +#,-!,#.%-/0&#-+1$%&2-%#,3#$%&(,&#,- 4&$5,-1$# % (8$&,1,93":,) % ;3$,&3,1 <,$$& $%$)%D
* 6 7 6 7 = >
.
" " #$ #
;#"#,?, @"A%-"#%- % 0&#,-"#,:;,-5$3,1!,#.%-/1 B$:$"&4&$5,-1$# B$C"&'$%%'$
6 6 7 > 6
/
" $ #
!"#$%&"'(%) +#,-!,#.%-/D),-,&3 E,1%&1,F,38&$3"'F,")(%%-:$&"#$%&(,&#,-% (8$&" <,$$& $%%%.0
!
* > 6 * 7 = >
)
" $ #
<,$$& D',3#-%&$3;3$,&3,"&:F,38&%'% 0&1#$#+#, <,$$& $%%%'%
= > >6 = >
!041$/31 U36,<45V*6*; 65+5>4*;*>56426<*,W,@42*@+<>34*;&X?<4<724<*;*+A<4 9<5,*,736;2@2
! G: G G G F
$
4<+<54>?2,A36,<45V*6*; @<;<>;*2,;<>?,*3<+ Y?*>?>5,<,?5,><W,@42*@+<>34*; 5,@ 42;<>;3+<4Z+
G H G :
$
4*A5> &#,;?*+ 5<4 Y<7*4+;6+39954;?<,39V<4;4<,@+5,@>5;<24*<+27W,@42*@A36,<45V*6*;*<+
: G :: G G F
7429.%%D;2.%$(&X?<,Y<5,56E<;?<4<+<54>? 424<++27W,@42*@+<>34*;7429.%$.;2.%$)5,@
G : F G
$
422+<5,2A<4A*<Y 27 W,@42*@A36,<45V*6*; @<;<>;*2,;<>?,*3<+&W7;<4;?5; Y<@<;5*6;?<
: : G H
$ $ $
;<>?,*3<+74<3<,;6 3+*, *,>344<,;4<+<54>?<+ +3>?5+;5*,;5,56+*+ 4<5>?5V6< 5;?@*+>2A<4
H H G F G : G
$
+9V26*><R<>3;*2,5,@73EE*, ;<+;&#,5@@*;*2, Y<56+272>3+2,;?<;<>?,*3<+>29V*,*, +;5;*>
G F H F
$
5,56+*+5,@@,59*>;<+;+3>?5+>2,>26*>;<+;*, 5,@@*4<>;<@73EE*, &W;65+; Y<>2,>63@<;?<+;5;3+
G G F F
$
325,@2<,+234><;226+*,W,@42*@A36,<45V*6*; @<;<>;*2, 5,@ 422+<A5635V6<*++3<+Y?*>?54<
H : G : :
Y24;?734;?<4+;3@*, &
G F
% % % %
5* 6%$#4 W,@42*@+<>34*; +34A< A36,<45V*6*; @<;<>;*2, +;5;*>5,56+*+ @,59*>5,56+*+
! G G G G G G
+
!
!"#$% &'(!")*+,-./0 12345678 &'!"#
5 6
W,@42*@ W,@42*@
! !
! " !
$ 9:;<=>?@A!") BCDE!"FGH(IJKLMN O5IJ(PQFRSTU V
&
# !
WX YZ [\#$]^_`Fabcdefg hijbjke !"Y
W,@42*@ .%%D .%$( W,@42*@
# !
Zlmnop [\(qLrsct %2uvp wxe #$78KL(gyz
.%$. .%$) & W,@42*@
! " " "
{ |}X#$78YZ*~D�(�����jk ����jk ���d ���KL
K3EE*,
F
!
cd���� �X�����dF�� �?����(KLcde�� �iX #
K3EE*, & W,@42*@
F
!
$78YZ(���Ocdeg� |�Qe��c����rs(!"��
&
$ $ $ $
!" �� #$78 ��jk ?�jk
789
W,@42*@
!
:;<=>?
X[/%0&$
!
! % !
!"#$ %&#$
C C C C
.%$(%B$( .%$(%D.B
!
! " $ #% " & ' #
'()* )*+,-./012 )*'34567891 '3:;< =
B$.'.)D$B$('.)B% .%$.$).)
!
?

}
}

@InProceedings{2348,
  Title                    = {Abstract interpretation: a semantics-based tool for program analysis},
  Year                     = {2348},

  File                     = {:article\\Abstract interpretation_ a semantics-based tool for program analysis.pdf:PDF},
  Groups                   = {Software Basic Theory},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/234803097 Abstractinterpretation:asemantics-basedtool forprogramanalysis ARTICLE·JANUARY1994 CITATIONS READS 143 117 2AUTHORS,INCLUDING: NeilD.Jones UniversityofCopenhagen 149PUBLICATIONS6,282CITATIONS SEEPROFILE Availablefrom:NeilD.Jones Retrievedon:22October2015

}
}

@Article{2015,
  Title                    = {Innovative Security Solutions for Information Technology and Communications},
  Journal                  = {Lecture Notes in Computer Science},
  Year                     = {2015},

  Doi                      = {10.1007/978-3-319-27179-8},
  File                     = {:home/ccc/github/literature/article/-Up-High to Down-Low\: Applying Machine Learning to an Exploit Database.pdf:PDF;2015（极其重要切实相关）craxfuzz.pdf:home/ccc/github/literature/article/2015（极其重要切实相关）craxfuzz.pdf:PDF;2015（函数摘要）Enhancing Symbolic Execution Method with a Taint Layer.pdf:home/ccc/github/literature/article/2015（函数摘要）Enhancing Symbolic Execution Method with a Taint Layer.pdf:PDF;2015以特定敏感函数为目标的符号执行系统.pdf:home/ccc/github/literature/article/2015以特定敏感函数为目标的符号执行系统.pdf:PDF;2015 A Guided Fuzzing Approach for Security Testing of 网络协议（污点分析） .pdf:home/ccc/github/literature/article/2015 A Guided Fuzzing Approach for Security Testing of 网络协议（污点分析） .pdf:PDF},
  Groups                   = {Predication Vulnerability},
  ISBN                     = {http://id.crossref.org/isbn/978-3-319-27179-8},
  ISSN                     = {1611-3349},
  Publisher                = {Springer International Publishing},
  Url                      = {http://dx.doi.org/10.1007/978-3-319-27179-8}
}

@Article{2015a,
  Title                    = {Memory Allocation and Vulnerability Analysis and Analysis and Optimization for C and Programs Based and on Formal and Methods},
  Journal                  = {Journal of Software},
  Year                     = {2015},

  Abstract                 = {The information security problems caused by the software vulnerabilities have became more and
},
  File                     = {:article\\Memory Allocation Vulnerability Analysis and Analysis Optimization for C Programs Based on Formal Methods.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Journal of Software Memory Allocation Vulnerability Analysis and Analysis Optimization for C Programs Based on Formal Methods Deng Hui*, Liu Hui, Guo Ying, Zhang Baofeng China Information Technology Security Evaluation Center, Beijing, 100085, China. * Corresponding author. Email: gcdh2014@126.com Manuscript submitted January 10, 2015; accepted April 20, 2015. doi: 10.17706/jsw.10.9.1079-1085 Abstract: The information security problems caused by the software vulnerabilities have became more and more complex. Among these vulnerabilities, the ones existing in memory allocations appear to be difficult to diagnose due to the absence of an appropriate method. In order to solve this problem, we introduce a methodology including four novel frameworks in this paper. The formalization for a program called algebraic transition system is proposed first. It aims to transform the data exchange process and its security attribute of a program into algebraic systems which are able to be considered as objection functions and constraint conditions, respectively. Based on the systems, the behavior and structure of formalization are optimized with bisimulation to reduce the computing cost in the subsequent processes. The determination of bisimulation is implemented by numerical and symbolic computation. Finally, the specific detection of the memory allocation vulnerability in the C program can be changed into a constraints solving problem called Max function which is able to be resolved with the filled function method. The experiment results represent that our approach is feasible. Key words: C program, memory allocation vulnerability, algebraic transition system, bisimulation, formal method. 1. Introduction The increasing development of the computer technology brings convenience for us. Unfortunately, the instability of software always brings information security problems. A tiny vulnerability of software could lead to a great harm [1]. A memory allocation known as a design vulnerability refers to performing manual memory management in C programs for dynamic memory allocation in C programming language via a group of function in C standard library, namely malloc( ), realloc( ), calloc( ) and free( ). This vulnerability will lead to information security problems when they are exploited, just like Dos which can cause execute arbitrary commands and so on [2], [3]. Unfortunately, there doesn't exist an appropriate method to analyze memory allocation vulnerability for C programs. In order to deal with this problem, this paper proposes a novel framework to analyze this kind of vulnerabilities on the basis of the formal methods, for instance, numerical and symbolic computation [4]-[6], constraints solving [7], [8] and so on. In this framework, an algebraic transition system is applied to describe all behavior and the structure of a C program [9], [10]. Besides, bisimulation is used to optimize system to make vulnerability analysis cost less computing efforts which has already been applied to optimize behavior and structure of dynamical systems [11]-[13]. On basis of formalization, all of the data exchange processes of a program are modeled by the algebraic systems and considered as objection 1079 Volume 10, Number 9, September 2015

}
}

@TechReport{2015b,
  Title                    = {Model Checking and as Static and Analysis},
  Year                     = {2015},
  Number                   = {Technical},

  File                     = {:article\\Model Checking as Static Analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Review                   = {Downloaded from orbit.dtu.dk on: Oct 23, 2015 Model Checking as Static Analysis Zhang, Fuyuan; Nielson, Flemming; Nielson, Hanne Riis Publication date: 2012 Document Version Publisher final version (usually the publisher pdf) Link to publication Citation (APA): Zhang, F., Nielson, F., & Nielson, H. R. (2012). Model Checking as Static Analysis. Kgs. Lyngby: Technical University of Denmark. (IMM-PHD-2012; No. 280). General rights Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. • Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal ? If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim. 

}
}

@InProceedings{2015c,
  Title                    = {A Generic and Approach to Automatic and Deobfuscation of Executable and Code},
  Booktitle                = {2015 IEEE Symposium on Security and Privacy},
  Year                     = {2015},
  Publisher                = {IEEE},

  Abstract                 = {Malicious software are usually obfuscated to avoid are unpacked at runtime [4] or if there are multiple layers of detection and resist analysis. When new malware is encountered, interpretation with distinct virtual program counters that are such obfuscations have to be penetrated or removed (“deobfus- difﬁcult to tease apart. The work of Coogan et al. [6] has cated”) in order to understand the internal logic of the code and devise countermeasures. This paper discusses a generic similar goals to us, but is based on equational reasoning about approach for deobfuscation of obfuscated executable code. Our assembly-level instruction semantics, which is technically very approach does not make any assumptions about the nature of the different from our work (see Section VI) and has the short- obfuscations used, but instead uses semantics-preserving program coming that controlling the equational reasoning process can transformations to simplify away obfuscation code. We have be challenging, making it difﬁcult to recover the logic of the applied a prototype implementation of our ideas to a variety of different kinds of obfuscation, including emulation-based underlying computation into a program representation such as obfuscation, emulation-based obfuscation with runtime code control ﬂow graphs. unpacking, and return-oriented programming. Our experimental results are encouraging and suggest that this approach can be A second class of programs that can be challenging to effective in extracting the internal logic from code obfuscated reverse-engineer are return-oriented programs (ROP) [7], [8]. using a variety of obfuscation techniques, including tools such as While originally devised to bypass defenses against code Themida that previous approaches could not handle. injection, this programming technique can result in highly
},
  Doi                      = {10.1109/SP.2015.47},
  File                     = {:article\\A Generic Approach to automatic Deobfuscation of Executable Code.pdf:PDF},
  Keywords                 = {Deobfuscation; Virtualization-Obfuscation; Return convoluted control ﬂow between many small gadgets, leading Oriented Programming to program logic that can be tricky to decipher. Other than the},
  Review                   = {2015 IEEE Symposium on Security and Privacy A Generic Approach to Automatic Deobfuscation of Executable Code Babak Yadegari Brian Johannesmeyer Benjamin Whitely Saumya Debray Department of Computer Science The University of Arizona Tucson, AZ 85721 {babaky, bjohannesmeyer, whitely, debray}@cs.arizona.edu Abstract—Malicious software are usually obfuscated to avoid are unpacked at runtime [4] or if there are multiple layers of detection and resist analysis. When new malware is encountered, interpretation with distinct virtual program counters that are such obfuscations have to be penetrated or removed (“deobfus- difﬁcult to tease apart. The work of Coogan et al. [6] has cated”) in order to understand the internal logic of the code and devise countermeasures. This paper discusses a generic similar goals to us, but is based on equational reasoning about approach for deobfuscation of obfuscated executable code. Our assembly-level instruction semantics, which is technically very approach does not make any assumptions about the nature of the different from our work (see Section VI) and has the short- obfuscations used, but instead uses semantics-preserving program coming that controlling the equational reasoning process can transformations to simplify away obfuscation code. We have be challenging, making it difﬁcult to recover the logic of the applied a prototype implementation of our ideas to a variety of different kinds of obfuscation, including emulation-based underlying computation into a program representation such as obfuscation, emulation-based obfuscation with runtime code control ﬂow graphs. unpacking, and return-oriented programming. Our experimental results are encouraging and suggest that this approach can be A second class of programs that can be challenging to effective in extracting the internal logic from code obfuscated reverse-engineer are return-oriented programs (ROP) [7], [8]. using a variety of obfuscation techniques, including tools such as While originally devised to bypass defenses against code Themida that previous approaches could not handle. injection, this programming technique can result in highly Keywords-Deobfuscation; Virtualization-Obfuscation; Return convoluted control ﬂow between many small gadgets, leading Oriented Programming to program logic that can be tricky to decipher. Other than the work of Lu et al. [9], there has been little work on automatic I. INTRODUCTION deobfuscation of ROPs. Malicious software are usually deployed in heavily ob- This paper describes a generic approach to deobfuscation of fuscated form, both to avoid detection and also to hinder executable code that is conceptually simpler and more general reverse engineering by security analysts. Much of the research than those described above. Obfuscation-speciﬁc approaches to date on automatic deobfuscation of code has focused on have the signiﬁcant limitation that they can only be effective obfuscation-speciﬁc approaches. While important and useful, against previously-seen obfuscations; they are, unfortunately, such approaches are of limited utility against obfuscations of limited utility when confronted by new kinds of obfusca- that are different from the speciﬁc ones they target, and tions or new combinations of obfuscations that violate their therefore against new obfuscations not previously encountered. assumptions. Our work on generic deobfuscation is motivated We aim to address this problem via a generic semantics-based by the need for deobfuscation techniques that can be effective approach to deobfuscation; in particular, this paper focuses even when applied to previously unseen obfuscations. The on two very different kinds of programming/obfuscation tech- underlying intuition is that the semantics of a program can be niques that can be challenging to reverse engineer: emulation- understood as a mapping, or transformation, from input values based obfuscation and return-oriented programming. to output values. Deobfuscation thus becomes a problem of In emulation-based obfuscation, the computation being ob- identifying and simplifying the code that effects this input- fuscated is implemented using an emulator for a custom- to-output transformation. We use taint propagation to track generated virtual machine together with a byte-code-like rep- the ﬂow of values from the program’s inputs to its outputs, resentation of the program’s logic [1]–[4]. Examination of the and semantics-preserving code transformations to simplify the obfuscated code reveals only the emulator’s logic, not that of logic of the instructions that operate on and transform values the emulated code. Existing techniques for reverse engineering through this ﬂow. We make few if any assumptions about emulation-obfuscated code ﬁrst reconstruct speciﬁcs of the the nature of the any obfuscation being used, whether that virtual machine emulator, then use this to decipher individual be emulation, or ROP, or anything else. Experiments using byte code instructions, and ﬁnally recover the logic embedded several emulation-obfuscation tools, including Themida, Code in the byte code program [5]. Such approaches typically make Virtualizer, VMProtect, and ExeCryptor, as well as a number strong assumptions about the structure and properties of the of return-oriented implementations of programs, suggest that emulator and may not work well if the analyzer’s assumptions the approach is helpful in reconstructing the logic of the do not ﬁt the code being analyzed, e.g., if parts of the emulator original program. © 2015, Babak Yadegari. Under license to IEEE. 674 DOI 10.1109/SP.2015.47

}
}

@InProceedings{2015d,
  Title                    = {SOURCE PUBLICATION and LIST FOR WEB OF SCIENCE 2015-08},
  Year                     = {2015},

  File                     = {:article\\WEB OF SCIENCE publist_sciex.pdf:PDF},
  Review                   = {REUTERS/Morteza Nikoubazl SOURCE PUBLICATION LIST FOR WEB OF SCIENCE® SCIENCE CITATION INDEX EXPANDED 

}
}

@InProceedings{2014,
  Title                    = {A survey of static code analysis methods for and security vulnerabilities detection},
  Year                     = {2014},

  Abstract                 = {Software security is becoming highly important documented more than 680 software weaknesses that can lead for universal acceptance of applications for many kinds of to impacts like modified data, read data, denial of service transactions. Automated code analyzers can be utilized to detect (DOS) attacks, resource consumption, execution of security vulnerabilities during the development phase. This unauthorized code, gain of privileges, protection bypass or the paper is aimed to provide a survey on Static code analysis and hiding of activities. how it can be used to detect security vulnerabilities. The most recent findings and publications are summarized and presented Many security focused methodologies like the Open in this paper. This paper provides an overview of the gains, flows Software Assurance Maturity Model (Open SAMM) [3],The and algorithms of static code analyzers. It can be considered a Building Security In Maturity Model (BSIMM) [4], or stepping stone for further research in this domain. Microsoft’s Security Development Lifecycle (SDL) [5] are},
  File                     = {:article\\A survey of static code analysis methods for security vulnerabilities detection.pdf:PDF},
  Keywords                 = {static code analysis; security; vulnerability; research provided by Cenzic showed that 99% of all reviewed},
  Review                   = {MIPRO 2014, 26-30 May 2014, Opatija, Croatia A survey of static code analysis methods for security vulnerabilities detection Melina Kulenovic, Dzenana Donko Faculty of Electrical Engineering, University of Sarajevo Sarajevo, Bosnia and Herzegovina E-mails: mk14746@etf.una.ba , ddonko@etf.unsa.ba Abstract—Software security is becoming highly important documented more than 680 software weaknesses that can lead for universal acceptance of applications for many kinds of to impacts like modified data, read data, denial of service transactions. Automated code analyzers can be utilized to detect (DOS) attacks, resource consumption, execution of security vulnerabilities during the development phase. This unauthorized code, gain of privileges, protection bypass or the paper is aimed to provide a survey on Static code analysis and hiding of activities. how it can be used to detect security vulnerabilities. The most recent findings and publications are summarized and presented Many security focused methodologies like the Open in this paper. This paper provides an overview of the gains, flows Software Assurance Maturity Model (Open SAMM) [3],The and algorithms of static code analyzers. It can be considered a Building Security In Maturity Model (BSIMM) [4], or stepping stone for further research in this domain. Microsoft’s Security Development Lifecycle (SDL) [5] are designed to enforce developers to build security in. Still, the Keywords—static code analysis; security; vulnerability; research provided by Cenzic showed that 99% of all reviewed survey; applications in 2012 had at least one serious security I. I vulnerability [6].Conveniently, tools that are able to scan NTRODUCTION millions of lines of source code and detect defects and Millions of users accomplish their daily tasks using web vulnerabilities in relatively short time are improving rapidly. A and desktop applications on different electronic devices. The variety of such utilities is now available as commercial or free prevalence of software enforces the software industry to think tools that can be used either as a standalone tool or as a plug-in of how to build quality in. Most aspects of software quality are for a development framework. Automated code analysis tools related to the skills and knowledge of the development team can be utilized during the development phase and as part of the [1]. Unfortunately, developers make mistakes that lead to build process. The detection of defects in such an early phase vulnerable and defect software. Exploited security can reduce costs significantly. vulnerabilities lead to unreliable software that can become harmful for the user and software provider. The main characteristics of static code analysis tools, their gains and flows are outlined in section II. Section II also The CWE (Common Weakness Enumerations) initiative provides information on how to measure the contribution of provides a unified, measurable set of software weaknesses static analysis to vulnerabilities detection. Section III presents which can lead to serious security vulnerabilities [2].The top 25 the techniques and algorithms static code analysis is based on. vulnerabilities list contains the most widespread and critical This section contains an overview of the most recent work and errors that can lead to security vulnerabilities that can be easy a summary of related researches. The Conclusion is presented exploited by an attacker. Table I presents an overview and in section IV. description of the most common and harmful attacks. CWE has Table I. Top 5 security attacks [2] Name Description Effects SQL injection SQL – Injection is one of the major threats for data rich software. Every non Attackers use the commands to bypass validated textbox that utilizes user input and uses it in a SQL context can be authentication mechanisms or insert tainted data into used by an attacker to inject SQL commands. the database. OS command Applications are considered vulnerable to the OS command injection attack if Attackers attempt to execute system level commands injection they utilize non validated user input in a system level command what can lead in order to achieve the desired goal. to the invocation of scripts injected by the attacker. Buffer overflow Buffer overflows is an anomaly where a program, while writing data to a Buffer overflow may result in erratic program buffer, overruns the buffer's boundary and overwrites adjacent memory. It can behavior, including memory access errors, incorrect be triggered by non-validated inputs that are designed to execute code. results, a crash, or a breach of system security. Cross-site scripting Cross-site scripting is typically found in Web applications and it enables Attackers use it to bypass access controls, steal user attackers to inject client-side scripts into Web pages viewed by other users. sessions or to redirect users to a different website. Missing Missing authentication is a security vulnerability that occurs in software that Exposing critical functionality. Provides an attacker authentication does not perform any authentication for functionalities that require a provable with the privilege level of that functionality. user identity or consume a significant amount of resources. 1381

}
}

@Article{2014a,
  Title                    = {Enabling Static Security Vulnerabilitiy Analysis in PHP Applications for Novice Developers with SSVChecker},
  Year                     = {2014},

  File                     = {:article\\Enabling static security vulnerability analysis in PHP applications for novice developers with SSVChecker.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {(QDEOLQJ6WDWLF6HFXULW\9XOQHUDELOLW\$QDO\VLVLQ3+3 $SSOLFDWLRQVIRU1RYLFH'HYHORSHUVZLWKSSVChecker 0LFKDHO6PLWK -RVK'HKOLQJHU 7RZVRQ8QLYHUVLW\ 7RZVRQ8QLYHUVLW\ <RUN5RDG <RUN5RDG 7RZVRQ0' 7RZVRQ0'   PVPLWK#VWXGHQWVWRZVRQHGX MGHKOLQJHU#WRZVRQHGX $%675$&7 VHFWRUV DUH VWLOO URXWLQHO\ IRXQG WR EH YXOQHUDEOH >@ WR WKHPRVW :HEEDVHG V\VWHPV SHUYDGH RXU VRFLHW\ VXSSRUWLQJ EXVLQHVV FRPPRQVHFXULW\YXOQHUDELOLWLHVLQFOXGLQJ64/,QMHFWLRQ64/, FULWLFDO DSSOLFDWLRQV IUHTXHQWO\ UHTXHVWLQJVWRULQJ FXVWRPHU¶V &URVV6LWH6FULSWLQJ;66DQGLQIRUPDWLRQOHDNDJH)RUH[DPSOH SHUVRQDO LQIRUPDWLRQ QHFHVVLWDWLQJ LQFUHDVLQJO\ KLJKHU OHYHOV RI >@ IRXQG WKDW  RI WKH ZHE DSSOLFDWLRQV WHVWHG FRQWDLQHG LQIRUPDWLRQDVVXUDQFH1RYLFHZHESURJUDPPHUVZLWKOLWWOHRUQR VHFXULW\ YXOQHUDELOLWLHV LQ WKH DSSOLFDWLRQ OD\HUZLWK WKH DYHUDJH VHFXUH SURJUDPPLQJ VNLOOV XQNQRZLQJO\ GHYHORS ZHE QXPEHU RI YXOQHUDELOLWLHV IRXQG SHU DSSOLFDWLRQ EHLQJ  7KHVH DSSOLFDWLRQVULSHZLWKVHFXULW\YXOQHUDELOLWLHVWKXVFRPSURPLVLQJ ZHE DSSOLFDWLRQ VHFXULW\YXOQHUDELOLWLHV DUH FRQWLQXDOO\ H[SORLWHG WKH LQWHJULW\ RI WKH DSSOLFDWLRQ $V D UHVXOW D QXPEHU RI VWDWLF E\PDOLFLRXVKDFNHUV WRREWDLQVHQVLWLYH LQIRUPDWLRQZLWKUHFHQW DQDO\VLV VHFXULW\ WRROV KDYH EHHQ GHYHORSHG WR IODJ SRWHQWLDO H[DPSOHVLQFOXGLQJ7DUJHW>@(YHUQRWH>@HWF VHFXULW\ YXOQHUDELOLWLHV <HW WKHVH WRROV DUH GLIILFXOW WR XVH $PDLQFDXVHIRUWKHSUHYDOHQFHRINQRZQZHOOGRFXPHQWHGZHE GLYRUFHGIURPWKHVRIWZDUHLQWHJUDWHGGHYHORSPHQWHQYLURQPHQWV DSSOLFDWLRQ VHFXULW\ YXOQHUDELOLWLHV HJ 64/, ;66 HWF,'( DQG UHPDLQ XQNQRZQ WR QRYLFH GHYHORSHUV 7KLV SDSHU SHUVLVWLQJ VWHPV IURP DO DFN RI HGXFDWLRQ DQG DZDUHQHVV RQ WKH FRQWULEXWHV DQ(FOLSVHSOXJLQ WKDW HQDEOHV VWDWLF DQDO\VLV RI3+3 SDUWRIZHEGHYHORSHUVDQGWKHLQDELOLW\IRUVRIWZDUHFRPSDQLHVWR VRXUFHFRGHXVLQJH[LVWLQJWRROVGLUHFWO\ZLWKLQDFRPPRQ,'(WR ³KLUH WUDLQ DQG UHWDLQ ZHE DSSOLFDWLRQ VHFXULW\ H[SHUWV´ >@ HQDEOHQRYLFHGHYHORSHUV WREXLOGPRUH VHFXUHZHEDSSOLFDWLRQV 6LPSO\ VWDWHG QRYLFH ZHE GHYHORSHUV ZLWK OLWWOH RU QR VHFXUH :H PDNH WZR FODLPV IRU WKH H[WHQVLRQ RI SSVChecker )LUVW LW SURJUDPPLQJ VNLOOV XQNQRZLQJO\ GHYHORSZHE DSSOLFDWLRQV ULSH VHDPOHVVO\HPEHGVLQWRDFRPPRQ,'(PDNLQJLWHDV\IDPLOLDUWR ZLWK VHFXULW\ YXOQHUDELOLWLHV WKXV FRPSURPLVLQJ WKH LQWHJULW\ RI XVH IRU QRYLFH GHYHORSHUV 6HFRQG LW SURYLGHV IXQFWLRQDOLW\ WKH DSSOLFDWLRQ >@ )XUWKHU >@ IRXQG WKDW DGGLWLRQDO FDXVDO OHYHUDJLQJ PXOWLSOH WRROV WR UHGXFH UHSRUWHG IDOVH SRVLWLYHV DQG IDFWRUV LQFOXGH   GHYHORSPHQW DQG EXVLQHVV SUHVVXUHV SXVK EHWWHUIRFXVQRYLFHGHYHORSHUVRQSRWHQWLDOVHFXULW\YXOQHUDELOLWLHV DVLGH VHFXULW\ SURFHVVHV  YHU\ IHZ FRPSDQLHV XWLOL]HV VHFXUH 7R GHPRQVWUDWH WKHVH FODLPV ZH XVHSSVChecker RQ D SRSXODU FRGLQJGHYHORSPHQW SUDFWLFHV DQG  GHYHORSHUV VWUXJJOH ZLWK RSHQ VRXUFH 3+3EDVHG ZHE DSSOLFDWLRQ ZLWK NQRZQ VHFXULW\ OHJDF\WRROVWKDWDUHQRWLQWHJUDWHGLQWRWKHLUH[LVWLQJGHYHORSPHQW YXOQHUDELOLWLHV HQYLURQPHQWDQGWKDWUHSRUWDKLJKQXPEHURIIDOVHSRVLWLYHV &DWHJRULHVDQG6XEMHFW'HVFULSWRUV $VDUHVXOWRIWKHJURZLQJQHHGIRUVHFXULW\LQZHEDSSOLFDWLRQVD '>6RIWZDUH(QJLQHHULQJ@6RIWZDUH3URJUDP9HULILFDWLRQ± QXPEHU VWDWLF DQDO\VLV VHFXULW\ WHVWLQJ WRROV HJ3+3/LQW >@ Validation ' >6RIWZDUH (QJLQHHULQJ@ 3URJUDPPLQJ 3KDQWP >@ DQG3L[\ >@KDYHEHHQGHYHORSHG WR IODJSRWHQWLDO (QYLURQPHQWV±Integrated environments.  VHFXULW\ YXOQHUDELOLWLHV LQ ZHE DSSOLFDWLRQ FRGH 6XFK VWDWLF DQDO\VLVVHFXULW\ WHVWLQJ WRROVFDQDLG LQ LGHQWLI\LQJVRPHNQRZQ *HQHUDO7HUPV VHFXULW\YXOQHUDELOLWLHV GXULQJ WKHGHYHORSPHQWSURFHVV EXW WKH\ 6HFXULW\ RIWHQ FUHDWH DO DUJH QXPEHU RI ³VXSHUIOXRXV LQIRUPDWLRQ DQG IUXVWUDWLQJO\KLJKIDOVHSRVLWLYHVZKLFKUHGXFHVWKHLUHIIHFWLYHQHVV´ .H\ZRUGV >@ DQG UDUHO\ SURYLGH UHPHGLDWLRQ IHHGEDFN WR GHYHORSHUV >@ 6WDWLFDQDO\VLVVHFXUHSURJUDPPLQJVHFXULW\DXGLWLQJ )XUWKHU WKHVHVWDWLFDQDO\VLVVHFXULW\ WHVWLQJWRROVDUHGLIILFXOW WR XVH GLYRUFHG IURP WKH VRIWZDUH LQWHJUDWHG GHYHORSPHQW  ,1752'8&7,21 HQYLURQPHQWV ,'( DQG UHPDLQ XQNQRZQ WR QRYLFH GHYHORSHUV :HEDSSOLFDWLRQVSHUYDGHRXU VRFLHW\ VXSSRUWLQJDSSOLFDWLRQV LQ >@ ,I KRZHYHU ZHE DSSOLFDWLRQ GHYHORSHUV ZHUH ZDUQHG RI JRYHUQPHQW EDQNLQJ KHDOWKFDUH PDQXIDFWXULQJ DQG SRWHQWLDO VHFXULW\ YXOQHUDELOLWLHV GXULQJ GHYHORSPHQW SURYLGHG WHOHFRPPXQLFDWLRQV VHFWRUV7KHVH V\VWHPVDUHEHFRPLQJFULWLFDO ZLWKDQH[SODQDWLRQRI WKHYXOQHUDELOLW\DQGPLWLJDWLRQUHPHGLHV WRDEXVLQHVVHVRSHUDWLRQDQGIUHTXHQWO\UHTXHVWVWRUHFXVWRPHU¶V JLYHQ FRQILGHQFH WKDW IODJJHG SRWHQWLDO YXOQHUDELOLWLHV DUH PRUH SHUVRQDO LQIRUPDWLRQ QHFHVVLWDWLQJ LQFUHDVLQJO\ KLJKHU OHYHOV RI OLNHO\WREHDFWXDOYXOQHUDELOLWLHVDQGQRWIDOVHSRVLWLYHVZLWKLQD LQIRUPDWLRQ DVVXUDQFH 'HVSLWH WKLV ZHE DSSOLFDWLRQV DFURVV DOO FRPPRQO\XVHG LQWHJUDWHG GHYHORSPHQW HQYLURQPHQW ,'(  PDQ\VHFXULW\YXOQHUDELOLWLHVFRXOGEHDYRLGHG 3HUPLVVLRQWRPDNHGLJLWDORUKDUGFRSLHVRIDOORUSDUWRIWKLVZRUNIRU 7R DVVXDJH WKLV SUREOHP WKLV SDSHU FRQWULEXWHV D VLJQLILFDQW SHUVRQDORUFODVVURRPXVHLVJUDQWHGZLWKRXWIHHSURYLGHGWKDWFRSLHVDUH H[WHQVLRQWRSSVChecker6WDWLF6HFXULW\9XOQHUDELOLW\&KHFNHU QRW PDGH RU GLVWULEXWHG IRU SURILW RU FRPPHUFLDO DGYDQWDJH DQG WKDW >@DQ(FOLSVHSOXJLQWKDWXQLILHVH[LVWLQJVWDWLFDQDO\VLVVHFXULW\ FRSLHVEHDU WKLV QRWLFH DQG WKH IXOO FLWDWLRQ RQ WKH ILUVW SDJH 7R FRS\ RWKHUZLVH RU UHSXEOLVK WR SRVW RQ VHUYHUV RU WR UHGLVWULEXWH WR OLVWV WRROV E\ SURYLGLQJ VWDWLF DQDO\VLV RI 3+3 VRXUFH FRGH XVLQJ UHTXLUHVSULRUVSHFLILFSHUPLVVLRQDQGRUDIHH H[LVWLQJ WRROV GLUHFWO\ ZLWKLQ D FRPPRQ ,'( WR HQDEOH QRYLFH RACS’142FWREHU±7RZVRQ0'86$ GHYHORSHUV WREXLOGPRUHVHFXUHZHEDSSOLFDWLRQV:HPDNH WZR &RS\ULJKW$&0« IXQGDPHQWDO FODLPV IRU WKH H[WHQVLRQ RI SSVChecker )LUVW LW '2,KWWSG[GRLRUJ 

}
}

@InProceedings{2013,
  Title                    = {Embedded Devices Security and Firmware Reverse Engineering},
  Booktitle                = {BlackHat USA 2013},
  Year                     = {2013},

  Abstract                 = {Keywords Embedded devices have become the usual presence in the embedded devices, firmware, security, reverse engineering, network of (m)any household(s), SOHO, enterprise or criti- exploitation, vulnerabilities, backdoors, static analysis, bi- cal infrastructure. nary analysis, firmware unpacking, firmware analysis, firmware
},
  File                     = {:article\\US-13-Zaddach-Workshop-on-Embedded-Devices-Security-and-Firmware-Reverse-Engineering-WP.pdf:PDF},
  Groups                   = {source code vulnerability},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {Embedded Devices Security and Firmware Reverse Engineering BH13US Workshop ∗ † Jonas Zaddach Andrei Costin FIRMWARE.RE FIRMWARE.RE jonas@firmware.re andrei@firmware.re ABSTRACT Keywords Embedded devices have become the usual presence in the embedded devices, firmware, security, reverse engineering, network of (m)any household(s), SOHO, enterprise or criti- exploitation, vulnerabilities, backdoors, static analysis, bi- cal infrastructure. nary analysis, firmware unpacking, firmware analysis, firmware The preached Internet of Things promises to gazillion- modification uple their number and heterogeneity in the next few years. However, embedded devices are becoming lately the usual 1. INTRODUCTION suspects in security breaches and security advisories and thus In the world of ever increasing interconnection of com- become the Achilles’ heel of one’s overall infrastructure se- puting, mobile and embedded devices, their security has be- curity. come critical. The security of embedded devices and their An important aspect is that embedded devices run on firmwares is the new differentiator in the embedded market. what’s commonly known as firmwares. To understand how The security requirements and expectations for computing to secure embedded devices, one needs to understand their devices are being constantly raised as the world moves to- firmware and how it works. wards the Internet of Things. This is especially true for em- This workshop aims at presenting a quick-start at how to bedded devices and their software counterpart – firmwares inspect firmwares and a hands-on presentation with exercises – which is also their weakest point as shown below. on real firmwares from a security analysis standpoint. On another hand, embedded devices still have much less to offer in terms of firmware security at this point. We can see General Terms almost daily security advisories related to embedded devices,many of them related to critical computer or cyber-physical Compuster System Security, Network and Distributed Sys- systems. It’s not accidental that anecdotical evidence vehic- tem Security, Embedded Devices, Firmware, Security, Re- ulate the term Embedded and Firmware Security - Back verse Engineering to The 90s!. It both shows how easy it is to find vulnera- bilities in embedded firmware, as well as how bad is the state ∗ of affairs in the firmware world from a security view-point.PhD candidate on With this whitepaper and workshop, we aim at presenting ”Development of novel binary analysis techniques for security applications” at a quick-start at how to inspect and analyze firmwares, deliv- EURECOM, Sophia-Antipolis, Biot, France, ering hands-on presentation on real firmwares and compiling jonas.zaddach@eurecom.fr exercises from a security analysis standpoint. This, on an- † other hand, should help speed-up the responsible disclosurePhD candidate on and fixing of those dormant vulnerabilities. ”Software security in embedded systems” at This paper is organized as follows: we start with presen- EURECOM, Sophia-Antipolis, Biot, France, tation of minimal required theory in Section 2; we continue andrei.costin@eurecom.fr with survey on previous work and state of the art in Section 3; we present most commont firmware formats, their chal- lenges and how to handle their unpacking end-to-end in Sec- tion 4; we reinforce the presented knowledge with hands-on exercises and solutions in Section 5; we conclude in Section Permission to make digital or hard copies of all or part of this work for 6. personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies 1.1 Workshop Outline bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific The workshop which supports this whitepaper, is orga- permission and/or a fee. nized according to the following outline: BH13US ’13 Las Vegas, USA . • what are the embedded systems

}
}

@Article{2012,
  Title                    = {SimFuzz_ Test case similarity directed deep fuzzing},
  Journal                  = {The Journal of Systems and Software 85 (2012)},
  Year                     = {2012},

  Doi                      = {10.1016/j.},
  File                     = {:article\\SimFuzz_ Test case similarity directed deep fuzzing.pdf:PDF;2012-06-08-ICSE-UnderstandingIntegerOverflow.html:home/ccc/github/literature/article/2012-06-08-ICSE-UnderstandingIntegerOverflow.html:URL},
  Groups                   = {source code vulnerability},
  Keywords                 = {iques Fuzzing Software testing test inputs and explore as many paths as possible. It is powerful to explore new program branches so as},
  Review                   = {The Journal of Systems and Software 85 (2012) 102–111 Contents lists available at ScienceDirect The Journal of Systems and Software journa l homepage: www.e lsev ier .c SimFuz zzi Dazhi Zh Csa Department of States a r t i c l Article history: ware Received 25 A inpu Received in re ntics Accepted 13 Ju ult, b Available onlin ram. t is of Keywords: iques Fuzzing Software testing test inputs and explore as many paths as possible. It is powerful to explore new program branches so as Software vulnerability to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difﬁcult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efﬁciently. The fuzzing process comprises two stages. At the ﬁrst stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the ﬁrst stage is selected based on the test case similarity 1. Introdu Software can cause p zations, an proposed re (Wagner et 2008; Cowa 2003). Secu bilities and 2009; Gode Fuzz test to test the security tes tools have b ∗ Correspon E-mail add (D. Liu), ylei@c (C. Csallner), n (W. Wang). 0164-1212/$ – doi:10.1016/j. metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e., inputs that just explore shallow program paths, are created at the ﬁrst stage, and more distinct test inputs, i.e., inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising. © 2011 Elsevier Inc. All rights reserved. ction Ganesh et al., 2009; Godefroid et al., 2008). The basic idea of fuzzing is quite simple. Generally, certain program inputs are mutated to security breaches cost billions of dollars each year and produce new ones to explore different paths of the program. Here, otentially devastating impact on individuals, organi- the mutation means to modify the inputs. Then, the program is d government agencies. Many approaches have been executed under these input mutations and the runtime behavior cently to detect or prevent vulnerabilities in programs is monitored. If an exception such as a segmentation fault is cap- al., 2000; Godefroid et al., 2008; Cadar et al., 2006, tured, a potential vulnerability is detected. In general, there are n et al., 1998; Newsome and Song, 2005; Bhatkar et al., two types of fuzzing approaches: blackbox fuzzing and whitebox rity testing can pro-actively detect program vulnera- fuzzing. has become an attractive research area (Ganesh et al., Blackbox fuzzing does not require program source code. It ran- froid et al., 2008). domly or systematically mutates well-formed inputs to generate ing (or fuzzing) was ﬁrst proposed in Miller et al. (1990) new inputs and feeds them to the program for testing. It can be robustness of UNIX utilities. Currently, it is a primary remarkably effective for ﬁnding vulnerabilities in input validation ting approach due to its cost-effectiveness, and many components of a program since many programs fail to check input een developed (Miller et al., 1990; Sutton et al., 2007; formats properly. However, it is hard for blackbox fuzzing to exten- sively test deep program paths or semantics. The reason is that most of the resulting input mutations are ill-formed. They cannot satisfy required program conditions to reach deep program states. ding author. They usually repetitively explore shallow program paths and are resses: dazhi.zhang@mavs.uta.edu (D. Zhang), dliu@cse.uta.edu rejected by the program. Domain knowledge such as input gram- se.uta.edu (Y. Lei), kung@cse.uta.edu (D. Kung), csallner@cse.uta.edu ystrom@cse.uta.edu (N. Nystrom), wenhua.wang@mavs.uta.edu mars can be used to direct the fuzzing process and mitigate this limitation (Röning et al., 2002; Sparks et al., 2007). However, it is see front matter © 2011 Elsevier Inc. All rights reserved. jss.2011.07.028 z: Test case similarity directed deep fu ang, Donggang Liu ∗, Yu Lei, David Kung, Christoph Computer Science and Engineering, University of Texas at Arlington, Arlington, United e i n f o a b s t r a c t Fuzzing is widely used to detect soft ugust 2010 source code. It mutates well-formed vised form 4 June 2011 do not exercise deep program sema ly 2011 a deep program state is low. As a res e 4 August 2011 input validation components of a prog mitigate these limitations. However, i fuzzing employs heavy analysis techn om/ locate / j ss ng llner, Nathaniel Nystrom, Wenhua Wang vulnerabilities. Blackbox fuzzing does not require program ts to produce new ones. However, these new inputs usually since the possibility that they can satisfy the conditions of lackbox fuzzing is often limited to identify vulnerabilities in Domain knowledge such as input speciﬁcations can be used to ten expensive to obtain such knowledge in practice. Whitebox , i.e., dynamic symbolic execution, to systematically generate

}
}

@MastersThesis{2012a,
  Title                    = {一种基于图灵机的代码混淆评价方法},
  Year                     = {2012},

  File                     = {:article\\一种基于图灵机的代码混淆评价方法.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {南开大学 硕士学位论文 一种基于图灵机的代码混淆评价方法 姓名：王冬 申请学位级别：硕士 专业：计算机应用技术 指导教师：贾春福 2012-05

}
}

@Article{2011,
  Title                    = {Proceedings of the 2011 Seventh International Conference on Networked Computing and Advanced Information Management (NCM)},
  Journal                  = {Proceedings of the 2011 Seventh International Conference on Networked Computing and Advanced Information Management (NCM)},
  Year                     = {2011},
  Pages                    = {IEEE},

  __markedentry            = {[ccc:6]},
  Abstract                 = {The following topics are dealt with: virtual machine creation characteristics; vulnerability estimation; quick coding method; sound source localization; image segmentation; reinforcement learning technique; dynamic policy management framework; service-oriented architecture; software complexity methodology; access control mechanism; data warehouse design; adaptive beamforming; ZigBee network layer; model-driven development method; ontology based Web information extraction; semantic dataWeb; future Internet; hand gesture recognition; agile scrum development; parsing chinese;text extraction; tree based data gathering protocol; wireless ad hoc networks; behavior-based audit modeling; KDD-data mining framework;intelligent traffic video monitoring system; license plate recognition; video coding parameters; self-healthcare management system; preference querying;image understanding; power line communications; Saas privacy; cognitive knowledge status; wordnet based information retrieval; online SQL learning system; EMD-based neural network and uFolio.},
  Be                       = {Youngsuk ChoEOLEOLKawata, S.EOLEOLKo, F.},
  Bn                       = {978-1-4577-0185-6},
  Cl                       = {Gyeongju, South Korea},
  Ct                       = {2011 Seventh International Conference on Networked Computing andEOLEOLAdvanced Information Management (NCM)},
  Cy                       = {21-23 June 2011},
  File                     = {2011_cwe_sans_top25.pdf:home/ccc/github/literature/article/2011_cwe_sans_top25.pdf:PDF},
  Groups                   = {Code Mining},
  Tc                       = {0},
  Ut                       = {INSPEC:12137938},
  Z8                       = {0},
  Z9                       = {0},
  Zb                       = {0},
  Zr                       = {0},
  Zs                       = {0}
}

@InProceedings{2011a,
  Title                    = {Evaluating complexity, code churn, developer activity metrics as indicators of software vulnerabilities},
  Year                     = {2011},
  Publisher                = {IEEE},

  Abstract                 = {Security inspection and testing requires experts in security who think like an attacker. Security experts need},
  File                     = {:home/ccc/github/literature/article/Evaluating complexity, code churn, developer activity metrics as indicators of software vulnerabilities.pdf:PDF},
  Review                   = {> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO 1 EDIT) < Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities Yonghee SHIN, Student Member, IEEE, Andrew MENEELY, Student Member, IEEE, Laurie WILLIAMS, Member, IEEE, and Jason OSBORNE Abstract—Security inspection and testing requires experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely-used open source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all the three types of metrics together predicted over 80% of the known vulnerable files with less than 25% false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71% and 28%, respectively, for both projects. Index Terms-- Software security, Software metrics, Vulnerability prediction, Fault prediction. I. INTRODUCTION A single exploited software vulnerability1 can cause severe damage to an organization. Annual world-wide losses caused from cyber attacks have been reported to be as high as $226 billion [2]. Loss in stock market value in the days after an attack is estimated at $50 million to $200 million per organization [2]. The importance of detecting and mitigating software vulnerabilities before software release is paramount. Experience indicates that the detection and mitigation of vulnerabilities is best done by engineers specifically trained in software security and who “think like an attacker” in their daily Manuscript received March 18, 2009. This work was supported in part by National Science Foundation Grant No. 0716176 and the U.S. Army Research Office (ARO) under grant W911NF-08-1-0105 managed by NCSU Secure Open Systems Initiative (SOSI). Y. Shin is with North Carolina State University, Raleigh, NC 27695 USA (telephone: 919-946-0781, e-mail: yonghee.shin@ ncsu.edu). A. Meneely is with North Carolina State University, Raleigh, NC 27695 USA (e-mail: apmeneel@ncsu.edu). L. Williams is with North Carolina State University, Raleigh, NC 27695 USA (e-mail: williams@csc.ncsu.edu). J. Osborne is with North Carolina State University, Raleigh, NC 27695 USA (e-mail: jaosborn@stat.ncsu.edu). 1 An instance of a [fault] in the specification, development, or configuration of software such that its execution can violate an [implicit or explicit] security policy [1]}
}

@InProceedings{,
  Title                    = {2011 International Symposium on Empirical Software Engineering and Measurement},
  Year                     = {2011},
  Publisher                = {IEEE},

  Doi                      = {10.1109/ESEM.2011.18},
  File                     = {:home/ccc/github/literature/article/One technique is not enough-a comparison of vulnerability discovery techniques.pdf:PDF},
  Review                   = {2011 International Symposium on Empirical Software Engineering and Measurement
  
  
    
 
!" #  " # #
 !  ""    $  ',           "    #  #   /  -/       (  ""$%&""     )0+1 2 '    "    $'%    3 ,     #   ("")"# .     ) "&"$  ' #4 $        ' "%  "         5     6   "" "" "$    '  .                       # -/ 7          '    # 8     *"""  (  -/     &""  '   '  '    " #   . #           +  + " "  $ ,              "% & "  "              ""    $ *           ""   ""  &   

" $ *  "    %                "   " &% &    (  ' .    "    $ -% & '9'       1 % "     "   8  :   5:6 *      ""   & " &8-0#%     $"    #$       "  " "  (       "" ;   ;    # "      " "  
   '    &$  '     #     
      <     '          
 '' ' '
. # $# $%&%$& %'     1
        '    '       '    (   
    (  (                )*+# %             
    '    
'   # #   '        '    8     
 #     '    .   '    '
 '      , 9 9 ' #

 (# -     .   '     '          . '     ' #     .   
    ' #          '   '  # *1== #=3==
01==##=
978-0-7695-4604-9/11 $26.00 � 2011 IEEE 97 DOI 10.1109/ESEM.2011.18}
}

@Article{2009,
  Title                    = {Code Injection Vulnerabilities in Web Applications-Exemplified at Cross-site Scripting},
  Year                     = {2009},

  Doi                      = {10.1524/itit.2011.0651·Source:DBLP},
  File                     = {:article\\Code Injection Vulnerabilities in Web Applications-Exemplified at Cross-site Scripting.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Seediscussions,stats,andauthorprofilesforthispublicationat:http://www.researchgate.net/publication/220622661 Code-injectionVulnerabilitiesinWeb Applications-ExemplifiedatCross-site Scripting. ARTICLEinIT-INFORMATIONTECHNOLOGY·SEPTEMBER2011 DOI:10.1524/itit.2011.0651·Source:DBLP CITATIONS READS 8 45 1AUTHOR: MartinJohns SAPResearch 40PUBLICATIONS238CITATIONS SEEPROFILE Availablefrom:MartinJohns Retrievedon:12October2015}
}

@Article{2007,
  Title                    = {如何从检索结果中快速找到某个学科的相关文献},
  Year                     = {2007},

  File                     = {:article\\教你如何使用Web_of_Science查SCI论文.pdf:PDF},
  Groups                   = {Academic Information},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {如何从检索结果中快速找到某个学科的相关 文献？ 您可能经常遇到检索结果太多但又不是您需要的资料的情况，怎样可以改变这种状况呢？ 其实利用 Web of Science提供强大的精确检索功能（Refine），您可以简便快速的从检 索结果中锁定您所关心的学科领域的文献。 1．访问Web of Science数据库检索课题 请访问：www.isiknowledge.com, 进入ISI Web of Knowledge平台； 选择Web of Science数据库，(以下 图示为WOK4.0版新界面)。 如：我们想快速了解 2007年诺贝尔物理奖获奖课题“巨磁电阻效应-Giant Magnetoresistance”在材料科学 -MATERIALS SCIENCE领域的全貌。 2．精确检索-Refine 在检索结果界面上，通过左侧的精确检索-Refine功能您可以快速的了解该课题的学科、文献类型、作者、 机构、国家等，甚至通过 Subject Areas选项锁定某一学科的相关文献。 

}
}

@InProceedings{,
  Year                     = {2007},

  Abstract                 = {Firsdtelfym,ivtaoirfsoinofsutswaaSrSUel'awleircdeiscussekde?pytrhoeperotfsioefstwaasrseurwanecre},
  File                     = {:home/ccc/github/literature/article/????????.pdf:PDF}
}

@Article{2005,
  Title                    = {Finding Security Vulnerabilities in Java and Applications and with Static Analysis},
  Year                     = {2005},
  Number                   = {Report},

  File                     = {:article\\Finding Security  Vulnerabilities  in Java Applications  with Static Analysis.pdf:PDF},
  Groups                   = {source code vulnerability},
  Keywords                 = {literature,article},
  Read                     = {未读},
  Review                   = {Finding Security Vulnerabilities in Java Applications with Static Analysis Benjamin Livshits and Monica S. Lam Computer Science Department Stanford University {livshits, lam}@cs.stanford.edu Technical Report September 25, 2005

}
}

@InProceedings{1231,
  Title                    = {软件防篡改技术综述},
  Year                     = {1231},

  File                     = {:article\\软件防篡改技术综述.pdf:PDF},
  Groups                   = {software protection},
  Rd                       = {N},
  Read                     = {未读},
  Review                   = {计算机研究与发展 ISS1N000—12319l／一C1N7771TP JournoafClompuRteesreaarncDdhevelopment 48(6)：923—9133，201 软件防篡改技术综述 王朝坤h3“ 付军宁1 王建民1’3“ 余志伟2 1(清华大学软件学院北京100084) 2(清华大学计算机科学与技术系北京 100084) 3(清华信息科学与技术同家实验宅北京100084) 4(信息安全教育部重点实验室(清华大学)北京100084) (chaokun@tsinghua．edu．en) SurvoefSyoftwTaarmepPerorofTiencghnique WanCghaokunl’3J”u，nFnuin91J，iWanamnignl·3Y一u，Zahnidwei2 1(SchoofoSloftware，TUsninvgerhsuiaty，1B0e0i0j8i4n)g 2(DeparotfCmoemnptuStceireamnFTdechnology，UTnsiivnegrhsiutay．1B0e0i0j8i4n)g 3(TsinNgahtuiaoLnaabloraftoorIrnyformaStcieoancneTdechnology1。0B0e0i8j4i)ng 4(KLeayboraftoorhrfyormaStyisotSneemc‘urity(UTnsivnegrhsuiaty)o．MfEidnuicsattriyon．1B0e0i0j8i4n)g AbstraWcittthhewiduesoefcomputteecrhnologiesh，asbsoefctowimanredeispeninsoaubrdlaeily lifeandthecorresposnedciunrigstsyuienssoftwsayrseteamrsemoraendmorperominent． Especialtloyd，ehsoiawgpnractipcraoltecstcihoneismqeuitiemportaannhdtagsresaitgnifiicnance thesoftwraerseeaarncdheveloipnmdeunsttrioenseo．ftAhsekeymethfoodrssoftwparroetection， thesoftwtaarmepperrooftiencghniaqtutreamctuscahttentfiroonrmesearcbhoetrahtshomaend abroianrdeceynetars．tSeuchniaqiumaestpreventthiecnrgitipcarlogrinafmormaftriotomhne unauthomroidziefdicatniduosness，alnsdaotgenerathienrgespononsceteshetamperising detected．Prientsheinsptaepdeisrarevioefwtheanalyosfissoftwtaarmepperroofinogu．rIn discussion，dtiafmfpeperrreonotfmientghoadrecslassiifniettdowocategoriestsa：ttihcaemper proofmientghobadseodnthecodoebfuscastwieolanlsthedynatmaimcpperroofmientghboadsed ontheverification—reasdpvoannsteaa．ngTdehisesadvantagesa，nsdwtereankgnteohsfstsheesse methaordpesresenintdedtailt．hIennd，thrtohuesguhrvoeftyhesteampperrooftiencghniques， asummaisorbytaiwnheidicnhclundoetosnltyhecharacterisatlistcohse，bxuitstpirnogblaenmds futuwroerokfthesoftwtaarmepperrooftiencghnique． Kewyordtsampperroofing；sporfottweacrteiont；asmtappterirocoftiencghnique；tdaymnpaemric prooftiencghnique；verificationb—fruesscpaotnisoen；code 摘要随着计算机软件的广泛使用，软件安全性问题日益突出．如何设计切实可行的软件保护方案已 成为必须直面的挑战．具有重要的现实意义．近年来，软件防篡改技术作为软件保护的重要手段之一受 到国内外研究者的重视．软件防篡改的目标在于阻止程序中的关键信息被非法修改或使用；检测篡改并 作出适当的响应．针对这两个目标。重点介绍了基于代码混淆的静态防篡改技术和基于检测一响应的动 态防篡改技术，对现有主流的软件防篡改技术进行分类，并分析和讨论了各类方法的优劣和局限性．最 后，总结软件防篡改领域存在的问题．并对其未来可能的发展与研究方向提出建议． 收稿日期：2010-01—26；修回日期：2010—08—11 基金项目：国家fj然科学基金项日(90718010．60803016)；闱家“九七i”重点基础研究计划基金项目(2007CB310802)；国家。八六i?高技术 研究发展计划基金项目(2008AA042301)；国家核高纂科技重大专项(2010ZX01042—002—002一01)；清华信息科学与技术国家实验 室(筹)学科交叉基金项目 万方数据}
}

@InProceedings{,
  Title                    = {????????????},
  Year                     = {1231},

  Doi                      = {tchtenumboefvrulnerab?isivteireuyssefiunlsome},
  File                     = {:home/ccc/github/literature/article/????????????.pdf:PDF},
  Review                   = {???????? lsS1N000?123191??c1N777?TP
JournoafClompuRteesreaarncIdh)evelopment 48(7)?1279?1287?20ll
????????????
???1 ???1 ? ?h2 ???3 1(???????????(??????????)?? 100190)
2(???????????(?????????)??100049)
3(?????????100029)
(niecj@is?iscas?ac?cn)
AnSoftwVaurleneraNbiulmibtPeyrerdicMtoidoBneals0endMicr?Parameters
NieChujian9X1i?aZnhfaeon9K1a?iClh?e?naHnadZnhengqin93
1(S??PLKnP6y0r?fory????0???iD?S??r?(J??????So^?"o???Sic?i??scePA5?)d??B1Py0?0i1?g90) 2(S??KLP?y60rnof?ofq????fiD?S??rf?(Gr?d??U?iw?iA?fo??dC???loy?iS?cPis?Pf?s)?BF1?0i0?0g49)
3(J?5?if???o?(?m??T??if?IgIl?ooZ?oNgDy?tIIlF?W??iU?hDifD???g?i?y?BP?1i0?0g029)
AbstmAcsthecosctausbeydsoftwvaurlenerabikleietpisnecsreasingp?apmyeorpalenemdore
attentiotohneresearocnhtehsevulnerabiAllityh?oduigshcovevurlinegraibsidilfiftiycbuelctause
ofthedefeocftvulneraabn?ailtysipsr?etdoitchtenumboefvrulnerab?isivteireuyssefiunlsome
domaisnu?cahsinformasteicuornaistsyessmeAnttp?resentth?emaimnethotdoesstimtahtee
densiofttyhevulnerabifloictuoisnetshemacr1eovelb?uthecyanotrefletchtessentoifal
vulnerabiAliptrye?dicmtioodnebalseodnmicro-parisdpmreotpeortsoepdreditchtenumboefr
vulnerabwilttihhtemyicro-paroafsmoefttewrasriete?xatnrdatchtetsypicmailcro?parafmreotmers
somseoftwsaereifeosrthepurpofsdiescovethrerienlgatiobnesthiwpteheevnulneranbiulmibteyr
anmdicro?paramWeittetrhseh?ypothoefvsuilsnerabiinlhietryitinpgr?etdhicemtoidoaneblstracts
themicro?parafmreostmeorfstwaanrdterietsofinadlinerarelatiobnesthiwpetheevnulnerability
numbaenrdsommeicro?paramTehtiemrsos?dealsgoiveasmethtoodpreditchtevulnerability
numboefsroftwwairteihtsmicr0-paraanmdtehteveurlsnerabniulmibtoyefirtsprevivoeursions?
Thimsethiosvderifwiietd7hsoftwsaereies?thaernedsulsthsotwheDredicmtoidoneelfifesctive?
KewyoI?dsvulnerabpirleidtiycts?saonfatlwyasries?invhuelrnietreadb?ityv?uhlinsetroabriylity?
microscpoapriacmeters
???????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????
????????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????
??????????????????????????7??????????????????
????????????????????
?????2009?12?31??????2011?05?06
???????????????(61073179)I?????????????(2011Zx03002?05?02)
????}
}

@InProceedings{,
  Title                    = {??????????????????},
  Year                     = {1070},

  File                     = {:home/ccc/github/literature/article/??????????????????.pdf:PDF},
  Review                   = {? ? 10701 ? ? 1009120574
??? TP309.7 ? ? ??
???????? ??????????????????
Research on Theory andApplications of Obfuscation in
Pubic Key Cryptography
? ? ? ? ?? ????????? ?? ??
? ? ? ? ??? ????? ???
?????? ????????

}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Software Basic Theory\;0\;;
1 ExplicitGroup:Predication Vulnerability\;0\;;
1 ExplicitGroup:Academic Information\;0\;;
1 ExplicitGroup:software security\;2\;;
2 ExplicitGroup:source code vulnerability\;0\;;
2 ExplicitGroup:binarary vulnerability\;0\;;
2 ExplicitGroup:software protection\;0\;;
2 ExplicitGroup:Network Security\;0\;;
1 ExplicitGroup:Code Mining\;0\;;
}

@Comment{jabref-meta: groupsversion:3;}
