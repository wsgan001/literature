% Encoding: UTF-8

@Article{Se1979,
  author        = {Se-, Vol and Ie, Jan},
  title         = {{2014 Index IEEE Transactions on Software Engineering}},
  year          = {1979},
  volume        = {586},
  number        = {1},
  pages         = {6--10},
  __markedentry = {[ccc:6]},
  file          = {:article\\\\\2014 Index IEEE Transactions on Software Engineering.pdf:PDF},
  keywords      = {book,machine learning,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {book,machine learning,predicte,web},
}

@Article{Landi1992,
  author        = {Landi, William},
  title         = {{Undecidability of static analysis}},
  journal       = {ACM Letters on Programming Languages and Systems},
  year          = {1992},
  volume        = {1},
  number        = {4},
  pages         = {323--337},
  __markedentry = {[ccc:6]},
  abstract      = {Tatic analysis of programs is indispensable to any software tool, environment, or system that requires compile-time information about the semantics of programs. With the emergence of languages like C and LISP, static analysis of programs with dynamic storage and recursive data structures has become a field of active research. Such analysis is difficult, and the static-analysis community has recognized the need for simplifying assumptions and approximate solutions. However, even under the common simplifying assumptions, such analyses are harder than previously recognized. Two fundamental static-analysis problems are may alias and must alias. The former is not recursive (is undecidable), and the latter is not recursively enumerable (is uncomputable), even when all paths are executable in the program being analyzed for languages with if statements, loops, dynamic storage, and recursive data structures.},
  doi           = {10.1145/161494.161501},
  file          = {:article\\\Undecidability of static analysis.pdf:pdf},
  issn          = {10574514},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Jones1994,
  author        = {Jones, Neil D. and Nielson, Flemming},
  title         = {{Abstract interpretation: a semantics-based tool for program analysis}},
  journal       = {Handbook of Logic in Computer Science},
  year          = {1994},
  number        = {JANUARY 1994},
  pages         = {527--629},
  __markedentry = {[ccc:6]},
  file          = {:article\\Abstract interpretation a semantics-based tool for program analysis.pdf:pdf},
  isbn          = {0-19-853780-8},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Demillo1997,
  author        = {Demillo, Richard A and Offutt, A Jefferson},
  title         = {{Constraint Based Automatic Test Data Generation}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {1997},
  volume        = {17},
  number        = {September 1991},
  pages         = {1--18},
  __markedentry = {[ccc:6]},
  abstract      = {This paper presents a new technique for automatically generating test data. The technique is based on mutation analysis and creates test data that approximates relative-adequacy. The technique is a fault-based technique that uses algebraic constraints to describe test cases designed to nd particular types of faults. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraints and solves them to create test cases for unit and module testing. Godzilla has been integrated with the Mothra testing system and has been used as an e ective way to generate test data that kills program mutants. The paper includes an initial list of constraints and discusses some of the problems that have been solved to develop the complete implementation of the technique.},
  doi           = {10.1109/32.92910},
  file          = {:article\\Constraint Based Automatic Test Data Generation.pdf:pdf},
  isbn          = {0098-5589},
  issn          = {0098-5589},
  keywords      = {fuzz},
  mendeley-tags = {fuzz},
  pmid          = {24174595},
}

@Article{Ghosh1998,
  author        = {a.K. Ghosh and O'Connor, T. and McGraw, G.},
  title         = {{An automated approach for identifying potential vulnerabilities in$\backslash$nsoftware}},
  journal       = {Proceedings. 1998 IEEE Symposium on Security and Privacy (Cat. No.98CB36186)},
  year          = {1998},
  __markedentry = {[ccc:6]},
  abstract      = {The paper presents results from analyzing the vulnerability of$\backslash$nsecurity-critical software applications to malicious threats and$\backslash$nanomalous events using an automated fault injection analysis approach.$\backslash$nThe work is based on the well understood premise that a large proportion$\backslash$nof security violations result from errors in software source code and$\backslash$nconfiguration. The methodology employs software fault injection to force$\backslash$nanomalous program states during the execution of software and observes$\backslash$ntheir corresponding effects on system security. If insecure behaviour is$\backslash$ndetected, the perturbed location that resulted in the violation is$\backslash$nisolated for further analysis and possibly retrofitting with fault$\backslash$ntolerant mechanisms},
  doi           = {10.1109/SECPRI.1998.674827},
  file          = {:article\\An automated approach for identifying potential vulnerabilities innsoftware.pdf:pdf},
  isbn          = {0-8186-8386-4},
  issn          = {1081-6011},
  keywords      = {binary,fuzz,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,predicte,web},
}

@Article{Reps1998,
  author        = {Reps, Thomas},
  title         = {{Program analysis via graph reachability}},
  journal       = {Information and Software Technology},
  year          = {1998},
  volume        = {40},
  number        = {11-12},
  pages         = {701--726},
  __markedentry = {[ccc:6]},
  abstract      = {This paper describes how a number of program-analysis problems can be solved by transforming them to graph-reachability problems. Some of the program-analysis problems that are amenable to this treatment include program slicing, certain dataflow-analysis problems, one version of the problem of approximating the possible “shapes” that heap-allocated structures in a program can take on, and flow-insensitive points-to analysis. Relationships between graph reachability and other approaches to program analysis are described. Some techniques that go beyond pure graph reachability are also discussed.},
  doi           = {10.1016/S0950-5849(98)00093-7},
  file          = {:article\\Program analysis via graph reachability.pdf:pdf},
  isbn          = {0262631806},
  issn          = {09505849},
  keywords      = {stat,static analysi,static analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584998000937},
}

@Article{Schmidt1998,
  author        = {Schmidt, David A and Steffen, Bernhard},
  title         = {{Program Analysis as Model Checking of Abstract Interpretations}},
  journal       = {Proc. SAS '98},
  year          = {1998},
  pages         = {351--380},
  __markedentry = {[ccc:6]},
  file          = {:article\\Program Analysis as Model Checking of Abstract Interpretations.pdf:pdf},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Schmidt1998a,
  author        = {Schmidt, David a.},
  title         = {{Data flow analysis is model checking of abstract interpretations}},
  journal       = {Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of programming languages - POPL '98},
  year          = {1998},
  pages         = {38--48},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/268946.268950},
  file          = {:article\\Data flow analysis is model checking of abstract interpretations.pdf:pdf},
  isbn          = {0897919793},
  keywords      = {stat,static analysi,static analysis},
  url           = {http://portal.acm.org/citation.cfm?doid=268946.268950},
}

@Article{Cowan1999,
  author        = {Cowan, Crispin and Wagle, Perry and Pu, Calton and Beattie, Steve and Walpole, Jonathan},
  title         = {{Buffer Overflows: Attacks and Defenses for the Vulnerability of the Decade}},
  year          = {1999},
  pages         = {1--11},
  __markedentry = {[ccc:6]},
  file          = {:article\\Buffer Overflows Attacks and Defenses for the Vulnerability of the Decade.pdf:pdf},
  keywords      = {stat,static analysi,static analysis,survey,web},
  mendeley-tags = {survey,web},
}

@Article{Edvardsson1999,
  author        = {Edvardsson, Jon},
  title         = {{A survey on automatic test data generation}},
  journal       = {Proceedings of the 2nd Conference on Computer Science and Engineering},
  year          = {1999},
  number        = {x},
  pages         = {21--28},
  __markedentry = {[ccc:6]},
  abstract      = {In order to reduce the high cost of manual software testing and at the same time to increase the reliability of the testing processes researchers and practitioners have tried to automate it. One of the most important components in a testing environment is an automatic test data generator a system that automatically generates test data for a given program. Through the years several attempts in automatic test data generations have been made. The focus of this article is program-based...},
  file          = {:article\\A survey on automatic test data generation.pdf:pdf},
  keywords      = {fuzz,stat,static analysi,static analysis,survey},
  mendeley-tags = {fuzz,survey},
  url           = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.963},
}

@Article{Viega2000,
  author        = {Viega, John and Bloch, J T and Kohno, Y and McGraw, Gary},
  title         = {{{\{}ITS4:{\}} {\{}A{\}} Static Vulnerability Scanner for {\{}C{\}} and {\{}C++{\}} Code}},
  journal       = {Acsac},
  year          = {2000},
  pages         = {257},
  __markedentry = {[ccc:6]},
  file          = {:article\\\ITS4  A Static Vulnerability Scanner for C and C++ Code.pdf:PDF;
                   :article\\ITS4 A Static Vulnerability Scanner for C and C Code.pdf:PDF},
  groups        = {imprortant},
  keywords      = {first select,fuzz,second select,source code,source code-important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,survey,web},
}

@Article{Wagner2000,
  author        = {Wagner, D and Foster, J S and Brewer, E a and Aiken, a},
  title         = {{A first step towards automated detection of buffer overrun vulnerabilities}},
  journal       = {Network and Distributed System Security Symposium},
  year          = {2000},
  pages         = {3--17},
  __markedentry = {[ccc:6]},
  abstract      = {We describe a new technique for finding potential buffer overrun vulnerabilities in security-critical C code. The key to success is to use static analysis: we formulate detection of buffer overruns as an integer range analysis problem. One major advantage of static analysis is that security bugs can be eliminated before code is deployed. We have implemented our design and used our prototype to find new remotely-exploitable vulnerabilities in a large, widely deployed software package. An earlier hand audit missed these bugs.},
  doi           = {citeulike-article-id:514510},
  file          = {:article\\A first step towards automated detection of buffer overrun vulnerabilities.pdf:pdf},
  groups        = {imprortant},
  isbn          = {189156207X,},
  keywords      = {first select,predicte,second select,source code,source code-important,stat,static analysi,static analysis},
  mendeley-tags = {first select,predicte,second select,source code,source code-important},
}

@Article{Barak2001,
  author        = {Barak, Boaz and Goldreich, Oded and Impagliazzo, Rusell},
  title         = {{On the (im) possibility of obfuscating programs}},
  journal       = {Advances in Cryptology {\ldots}},
  year          = {2001},
  number        = {Im},
  pages         = {1--18},
  __markedentry = {[ccc:6]},
  abstract      = {Informally, an obfuscator O is an (efficient, probabilistic) compiler that takes as input a program (or circuit) P and produces a new program O(P) that has the same functionality as P yet is unintel- ligible in some sense. Obfuscators, if they exist, would have a wide vari- ety of cryptographic and complexity-theoretic applications, ranging from software protection to homomorphic encryption to complexity-theoretic analogues of Rices theorem. Most of these applications are based on an interpretation of the unintelligibility condition in obfuscation as mean- ing that O(P) is a virtual black box, in the sense that anything one can efficiently compute given O(P), one could also efficiently compute given oracle access to P. In this work, we initiate a theoretical investigation of obfuscation. Our main result is that, even under very weak formalizations of the above in- tuition, obfuscation is impossible.We prove this by constructing a family of functions F that are inherently unobfuscatable in the following sense: there is a property $\pi$ : F 0, 1 such that (a) given any program that computes a function f F, the value $\pi$(f) can be efficiently computed, yet (b) given oracle access to a (randomly selected) function f F, no efficient algorithm can compute $\pi$(f) much better than random guessing. We extend our impossibility result in a number of ways, including even obfuscators that (a) are not necessarily computable in polynomial time, (b) only approximately preserve the functionality, and (c) only need to work for very restricted models of computation (TC0). We also rule out several potential applications of obfuscators, by constructing unob- fuscatable signature schemes, encryption schemes, and pseudorandom function families.},
  doi           = {10.1007/3-540-44647-8},
  file          = {:article\\On the (im) possibility of obfuscating programs.pdf:pdf},
  isbn          = {9783540424567},
  issn          = {00045411},
  keywords      = {01,a preliminary version of,bgi,binary,boaz,complexity theory,crypto,cryptography,cs,department of computer science,e-mail,edu,homomorphic encryption,nj 08540,obfuscate,orem,princeton,princeton university,pseudorandom functions,rice,s the-,software protection,software watermarking,stat,static analysi,static analysis,statistical zero knowledge,this paper appeared in},
  mendeley-tags = {binary,obfuscate},
  url           = {http://link.springer.com/chapter/10.1007/3-540-44647-8{\_}1},
}

@Article{Wagner2001,
  author        = {Wagner, David and Berkeley, U C and {\'{Y}}{\textordmasculine}, {\^{U}} {\"{O}} Ð and Dean, Drew and {\'{O}}{\~{n}}, {\`{O}} {\^{O}} {\"{O}} {\"{O}}{\'{o}}{\"{u}}{\textordmasculine}},
  title         = {{Intrusion Detection via Static Analysis}},
  year          = {2001},
  volume        = {9},
  number        = {C},
  __markedentry = {[ccc:6]},
  file          = {:article\\Intrusion Detection via Static Analysis.pdf:pdf},
  isbn          = {0769510469},
  keywords      = {machine learning,network,predicte,stat,static analysi,static analysis},
  mendeley-tags = {machine learning,network,predicte},
}

@Article{Larochelle2001,
  author        = {Larochelle, David and Evans, David},
  title         = {{Statically Detecting Likely Buffer Overflow Vulnerabilities}},
  journal       = {Science},
  year          = {2001},
  volume        = {10},
  pages         = {177--190},
  __markedentry = {[ccc:6]},
  abstract      = {Buffer overflow attacks may be today's single most important security threat. This paper presents a new approach to mitigating buffer overflow vulnerabilities by detecting likely vulnerabilities through an analysis of the program source code. Our approach exploits information provided in semantic comments and uses lightweight and efficient static analyses. This paper describes an implementation of our approach that extends the LCLint annotation-assisted static checking tool. Our tool is as fast as a compiler and nearly as easy to use. We present experience using our approach to detect buffer overflow vulnerabilities in two security-sensitive programs.},
  file          = {:article\\Statically Detecting Likely Buffer Overflow Vulnerabilities(2).pdf:pdf},
  url           = {http://portal.acm.org/citation.cfm?id=1267612.1267626},
}

@Article{Larochelle2001a,
  author        = {Larochelle, David and Evans, David},
  title         = {{Statically Detecting Likely Buffer Overflow Vulnerabilities}},
  journal       = {Science},
  year          = {2001},
  volume        = {10},
  pages         = {177--190},
  __markedentry = {[ccc:6]},
  abstract      = {Buffer overflow attacks may be today's single most important security threat. This paper presents a new approach to mitigating buffer overflow vulnerabilities by detecting likely vulnerabilities through an analysis of the program source code. Our approach exploits information provided in semantic comments and uses lightweight and efficient static analyses. This paper describes an implementation of our approach that extends the LCLint annotation-assisted static checking tool. Our tool is as fast as a compiler and nearly as easy to use. We present experience using our approach to detect buffer overflow vulnerabilities in two security-sensitive programs.},
  file          = {:article\\Statically Detecting Likely Buffer Overflow Vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://portal.acm.org/citation.cfm?id=1267612.1267626},
}

@Article{Shankar2001,
  author        = {Shankar, U and Talwar, K and Foster, J and Wagner, D},
  title         = {{Detecting Format-String Vulnerabilities with Type Qualifiers}},
  journal       = {Proceedings of the 10th USENIX Security Symposium},
  year          = {2001},
  __markedentry = {[ccc:6]},
  file          = {:article\\Detecting Format-String Vulnerabilities with Type Qualifiers.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@PhdThesis{Sidahmed2001,
  author        = {Sidahmed, Mohamed A.},
  title         = {{EMPIRICAL ANALYSIS OF HYBRID OPEN SOURCE SOFTWARE MODEL: DETERMINATION OF EFFICIENCY AND GOVERNANCE}},
  year          = {2001},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\EMPIRICAL ANALYSIS OF HYBRID OPEN SOURCE SOFTWARE MODEL DETERMINATION OF EFFICIENCY AND GOVERNANCE.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {icle,predicte},
  mendeley-tags = {predicte},
  pmid          = {25246403},
  volume        = {1},
}

@Article{Engler2001,
  author        = {Engler, Dawson and Chen, David Yu and Hallem, Seth and Chou, Andy and Chelf, Benjamin},
  title         = {{Bugs as deviant behavior: a general approach to inferring errors in systems code}},
  journal       = {ACM SIGOPS Operating Systems Review},
  year          = {2001},
  pages         = {57--72},
  __markedentry = {[ccc:6]},
  abstract      = {A major obstacle to finding program errors in a real sys- tem is knowing what correctness rules the system must obey. These rules are often undocumented or specified in an ad hoc manner. This paper demonstrates tech- niques that automatically extract such checking infor- mation from the source code itself, rather than the pro- grammer, thereby avoiding the need for a priori knowl- edge of system rules. The cornerstone of our approach is inferring pro- grammer "beliefs" that we then cross-check for contra- dictions. Beliefs are facts implied by code: a dereference of a pointer, p, implies a belief that p is non-null, a call to "tmlock(1)" implies that 1 was locked, etc. For be- liefs we know the programmer must hold, such as the pointer dereference above, we immediately flag contra- dictions as errors. For beliefs that the programmer may hold, we can assume these beliefs hold and use a sta- tistical analysis to rank the resulting errors from most to least likely. For example, a call to "spinlock" fol- lowed once by a call to "spintmlock" implies that the programmer may have paired these calls by coincidence. If the pairing happens 999 out of 1000 times, though, then it is probably a valid belief and the sole deviation a probable error. The key feature of this approach is that it requires no a priori knowledge of truth: if two beliefs contradict, we know that one is an error without knowing what the correct belief is. Conceptually, our checkers extract beliefs by tailor- ing rule "templates" to a system - for example, finding all functions that fit the rule template " must be paired with ." We have developed six checkers that follow this conceptual framework. They find hundreds of bugs in real systems such as Linux and OpenBSD. From our experience, they give a dramatic reduction in the manual effort needed to check a large system. Com- pared to our previous work 9, these template checkers find ten to one hundred times more rule instances and derive properties we found impractical to specify manually.},
  doi           = {10.1145/502034.502041},
  file          = {:article\\Bugs as deviant behavior a general approach to inferring errors in systems code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1581133898},
  issn          = {01635980},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://portal.acm.org/citation.cfm?id=502041},
}

@Article{Larochelle2001b,
  author        = {Larochelle, David and Evans, David},
  title         = {{Statically Detecting Likely Buffer Overflow Vulnerabilities}},
  journal       = {Science},
  year          = {2001},
  volume        = {10},
  pages         = {177--190},
  __markedentry = {[ccc:6]},
  abstract      = {Buffer overflow attacks may be today's single most important security threat. This paper presents a new approach to mitigating buffer overflow vulnerabilities by detecting likely vulnerabilities through an analysis of the program source code. Our approach exploits information provided in semantic comments and uses lightweight and efficient static analyses. This paper describes an implementation of our approach that extends the LCLint annotation-assisted static checking tool. Our tool is as fast as a compiler and nearly as easy to use. We present experience using our approach to detect buffer overflow vulnerabilities in two security-sensitive programs.},
  file          = {:article\\Statically Detecting Likely Buffer Overflow Vulnerabilities(2).pdf:pdf},
  url           = {http://portal.acm.org/citation.cfm?id=1267612.1267626},
}

@Article{Piessens2002,
  author        = {Piessens, Frank},
  title         = {{A taxonomy of causes of software vulnerabilities in Internet software}},
  journal       = {Supplementary Proceedings of the 13th International Symposium on Software Reliability Engineering},
  year          = {2002},
  pages         = {47--52},
  __markedentry = {[ccc:6]},
  abstract      = {At the root of almost every security incident on the Internet are one or more software vulnerabilities, i.e. security-related bugs in the software that can be exploited by an attacker to perform actions he should not be able to perform. Analysis of vulnerability alerts as distributed by organisations like CERT(CER) or SANS(SAN), and analysis of causes of actual incidents shows that many vulnerabilities can be traced back to a relatively small number of causes: software developers are making the same mistakes over and over again.},
  url           = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.6567{\&}rep=rep1{\&}type=pdf},
}

@Article{Basin2002,
  author        = {Basin, David and Friedrich, Stefan and Gawkowski, Marek and Posegga, Joachim},
  title         = {{Bytecode model checking: An experimental analysis}},
  journal       = {Model Checking Software},
  year          = {2002},
  pages         = {75--77},
  __markedentry = {[ccc:6]},
  file          = {:article\\Bytecode model checking An experimental analysis.pdf:pdf},
  isbn          = {3540434771},
  issn          = {16113349},
  keywords      = {stat,static analysi,static analysis,web},
  mendeley-tags = {web},
  url           = {http://www.springerlink.com/index/JAUGJ9HXVD57KTDA.pdf},
}

@Article{Evans2002,
  author        = {Evans, David and Larochelle, David},
  title         = {{Improving security using extensible lightweight static analysis}},
  journal       = {IEEE Software},
  year          = {2002},
  volume        = {19},
  number        = {1},
  pages         = {42--51},
  __markedentry = {[ccc:6]},
  abstract      = {Most security attacks exploit instances of well-known classes of$\backslash$nimplementation flaws. Developers could detect and eliminate many of$\backslash$nthese flaws before deploying the software, yet these problems persist$\backslash$nwith disturbing frequency-not because the security community doesn't$\backslash$nsufficiently understand them but because techniques for preventing them$\backslash$nhave not been integrated into the software development process. This$\backslash$narticle describes an extensible tool that uses lightweight static$\backslash$nanalysis to detect common security vulnerabilities (including buffer$\backslash$noverflows and format string vulnerabilities)},
  doi           = {10.1109/52.976940},
  file          = {:article\\Improving security using extensible lightweight static analysis.pdf:pdf},
  groups        = {vice-important},
  isbn          = {0740-7459},
  issn          = {07407459},
  keywords      = {binary,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important,web},
}

@Article{Bishop2002,
  author        = {Bishop, Matt},
  title         = {{Trends in academic research: vulnerabilities analysis and intrusion detection}},
  journal       = {Computer},
  year          = {2002},
  volume        = {21},
  number        = {02},
  pages         = {609--612},
  __markedentry = {[ccc:6]},
  file          = {:article\\Trends in academic research vulnerabilities analysis and intrusion detection.pdf:pdf},
  keywords      = {first select,predicte,source code,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,source code,web},
}

@Article{Visser2002,
  author        = {Visser, Willem},
  title         = {{[22] Model Checking Programs}},
  journal       = {Automated Software Engineering},
  year          = {2002},
  pages         = {1--36},
  __markedentry = {[ccc:6]},
  file          = {:article\\22 Model Checking Programs.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {abstraction,binary,first select,java,model checking,runtime analysis,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,symmetry},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
}

@Article{Chen2002,
  author        = {Chen, Hao and Wagner, David},
  title         = {{MOPS: an Infrastructure for Examining Security Properties of Software}},
  journal       = {CCS '02: Proceedings of the 9th ACM Conference on Computer and Communications Security},
  year          = {2002},
  pages         = {235--244},
  __markedentry = {[ccc:6]},
  abstract      = {We describe a formal approach for finding bugs in security-relevant software and verifying their absence. The idea is as follows: we identify rules of safe programming practice, encode them as safety properties, and verify whether these properties are},
  doi           = {10.1145/586110.586142},
  file          = {:article\\MOPS an Infrastructure for Examining Security Properties of Software.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1-58113-612-9},
  keywords      = {first select,model checking,second select,security,source code,source code-important,source code-vice important,stat,static analysi,static analysis,verification},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://portal.acm.org/citation.cfm?id=586110.586142},
}

@Article{Khurshid2003,
  author        = {Khurshid, Sarfraz and PĂsĂreanu, Corina S. and Visser, Willem},
  title         = {{Generalized Symbolic Execution for Model Checking and Testing}},
  journal       = {Tools and Algorithms for the Construction and Analysis of Systems},
  year          = {2003},
  volume        = {2619},
  pages         = {553--568},
  __markedentry = {[ccc:6]},
  abstract      = {Modern software systems, which often are concurrent and manipulate complex data structures must be extremely reliable. We present a novel framework based on symbolic execution, for automated checking of such systems. We provide a two-fold generalization of traditional symbolic execution based approaches. First, we define a source to source translation to instrument a program, which enables standard model checkers to perform symbolic execution of the program. Second, we give a novel symbolic execution algorithm that handles dynamically allocated structures (e.g., lists and trees), method preconditions (e.g., acyclicity), data (e.g., integers and strings) and concurrency. The program instrumentation enables a model checker to automatically explore different program heap configurations and manipulate logical formulae on program data (using a decision procedure). We illustrate two applications of our framework: checking correctness of multi-threaded programs that take inputs from unbounded domains with complex structure and generation of non-isomorphic test inputs that satisfy a testing criterion. Our implementation for Java uses the Java PathFinder model checker.},
  doi           = {10.1007/3-540-36577-X_40},
  file          = {:article\\Generalized Symbolic Execution for Model Checking and Testing.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9783540008989},
  issn          = {03029743},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://link.springer.com/10.1007/3-540-36577-X{\_}40},
}

@Article{Dor2003,
  author        = {Dor, Nurit and Rodeh, Michael and Sagiv, Mooly},
  title         = {{CSSV: Towards a Realistic Tool for Statically Detecting All Buffer Overflows in C}},
  journal       = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation},
  year          = {2003},
  volume        = {2},
  pages         = {155},
  __markedentry = {[ccc:6]},
  abstract      = {Erroneous string manipulations are a major source of software defects in C programs yielding vulnerabilities which are exploited by software viruses. We present C S tring S tatic V erifyer (CSSV), a tool that statically uncovers all string manipulation errors. Being a conservative tool, it reports all such errors at the expense of sometimes generating false alarms . Fortunately, only a small number of false alarms are reported, thereby proving that statically reducing software vulnerability is achievable. CSSV handles large programs by analyzing each procedure separately. To this end procedure contracts are allowed which are verified by the tool.We implemented a CSSV prototype and used it to verify the absence of errors in real code from EADS Airbus. When applied to another commonly used string intensive application, CSSV uncovered real bugs with very few false alarms.},
  doi           = {10.1145/781131.781149},
  file          = {:article\\CSSV Towards a Realistic Tool for Statically Detecting All Buffer Overflows in C.pdf:pdf},
  groups        = {vice-important},
  isbn          = {1581136625},
  issn          = {03621340},
  keywords      = {1,1 i,1r,6,6d,b,c5,d,d i,first select,i,q,source code,source code-vice important,t,u,u {\`{i}}i,vp5,{\'{i}}},
  mendeley-tags = {first select,source code,source code-vice important},
  url           = {http://portal.acm.org/citation.cfm?doid=781131.781149},
}

@Article{Chess2004,
  author        = {Chess, B and McGraw, G},
  title         = {{Static analysis for security}},
  journal       = {Security $\backslash${\&} Privacy, IEEE},
  year          = {2004},
  volume        = {2},
  number        = {6},
  pages         = {76--79},
  __markedentry = {[ccc:6]},
  abstract      = {All software projects are guaranteed to have one artifact in common$\backslash$nsource code. Together with architectural risk analysis, code review$\backslash$nfor security ranks very high on the list of software security best$\backslash$npractices. We look at how to automate source-code security analysis$\backslash$nwith static analysis tools.},
  doi           = {10.1109/MSP.2004.111},
  file          = {:article\\Static analysis for security.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1540-7993 VO  - 2},
  issn          = {1540-7993},
  keywords      = {first select,program diagnostics security of data software tool,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,survey},
}

@Article{Zitser2004,
  author        = {Zitser, Misha and Lippmann, Richard and Leek, Tim},
  title         = {{Testing static analysis tools using exploitable buffer overflows from open source code}},
  journal       = {ACM SIGSOFT Software Engineering Notes},
  year          = {2004},
  volume        = {29},
  number        = {6},
  pages         = {97},
  __markedentry = {[ccc:6]},
  abstract      = {Five modern static analysis tools (ARCHER, BOON, Poly-Space C Verifier, Splint, and UNO) were evaluated using source code examples containing 14 exploitable buffer overflow vulnerabilities found in various versions of Sendmail, BIND, and WU-FTPD. Each code example included a "BAD" case with and a "OK" case without buffer overflows. Buffer overflows varied and included stack, heap, bss and data buffers; access above and below buffer bounds; access using pointers, indices, and functions; and scope differences between buffer creation and use. Detection rates for the "BAD" examples were low except for Poly-Space and Splint which had average detection rates of 87{\%} and 57{\%}, respectively. However, average false alarm rates were high and roughly 50{\%} for these two tools. On patched programs these two tools produce one warning for every 12 to 46 lines of source code and neither tool appears able to accurately distinguished between vulnerable and patched code.},
  annote        = {静态分析工具的比较研究},
  doi           = {10.1145/1041685.1029911},
  file          = {:article\\Testing static analysis tools using exploitable buffer overflows from open source code.pdf:pdf},
  groups        = {vice-important},
  isbn          = {1581138555},
  issn          = {01635948},
  keywords      = {and,buffer overflow,by the advanced research,detection,evaluation,exploit,false alarm,first select,security,source code,source code-vice important,stat,static analysi,static analysis,test,this work was sponsored,web},
  mendeley-tags = {first select,source code,source code-vice important,web},
  url           = {http://users.ece.cmu.edu/{~}dawnsong/teaching/reading/mit-benchmark.pdf},
}

@Article{Johnson2004,
  author        = {Johnson, Rob and Wagner, David},
  title         = {{Finding User / Kernel Pointer Bugs With Type Inference}},
  journal       = {13th USENIX Security Symposium},
  year          = {2004},
  __markedentry = {[ccc:6]},
  file          = {:article\\Finding User Kernel Pointer Bugs With Type Inference.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Huang2004,
  author        = {Huang, Yao-Wen and Yu, Fang and Hang, Christian and Tsai, Chung-Hung and Lee, Der-Tsai and Kuo, Sy-Yen},
  title         = {{Securing web application code by static analysis and runtime protection}},
  journal       = {Proceedings of the 13th international conference on World Wide Web},
  year          = {2004},
  pages         = {40--52},
  __markedentry = {[ccc:6]},
  abstract      = {Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions, especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs. Many verification tools are discovering previously unknown vulnerabilities in legacy C programs, raising hopes that the same success can be achieved with Web applications. In this paper, we describe a sound and holistic approach to ensuring Web application security. Viewing Web application vulnerabilities as a secure information flow problem, we created a lattice-based static analysis algorithm derived from type systems and typestate, and addressed its soundness. During the analysis, sections of code considered vulnerable are instrumented with runtime guards, thus securing Web applications in the absence of user intervention. With sufficient annotations, runtime overhead can be reduced to zero. We also created a tool named.WebSSARI (Web application Security by Static Analysis and Runtime Inspection) to test our algorithm, and used it to verify 230 open-source Web application projects on SourceForge.net, which were selected to represent projects of different maturity, popularity, and scale. 69 contained vulnerabilities. After notifying the developers, 38 acknowledged our findings and stated their plans to provide patches. Our statistics also show that static analysis reduced potential runtime overhead by 98.4{\%}.},
  doi           = {10.1145/988672.988679},
  file          = {:article\\Securing web application code by static analysis and runtime protection.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1-58113-844-X},
  keywords      = {binary,first select,information flow,noninterference,predicte,program security,second select,security vulnerabilities,source code,source code-important,source code-vice important,stat,static analysi,static analysis,type systems,verification,web,web application security},
  mendeley-tags = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://doi.acm.org/10.1145/988672.988679},
}

@Article{EditoEditorF.&DeuelR.L.2004.NotJustaboutSecurity.r2004,
  author        = {{EditoEditor, F., {\&} Deuel, R. L. (2004). Not Just about Security.r}, Features and Deuel, Rebecca L},
  title         = {{Not Just about Security}},
  year          = {2004},
  __markedentry = {[ccc:6]},
  file          = {:article\\Not Just about Security.pdf:pdf},
  groups        = {vice-important},
  keywords      = {first select,source code,source code-vice important,web},
  mendeley-tags = {first select,source code,source code-vice important,web},
}

@Article{Keromytis2004,
  author        = {Keromytis, Angelos D.},
  title         = {{"Patch on demand" Saves even more time?}},
  journal       = {Computer},
  year          = {2004},
  volume        = {37},
  number        = {8},
  pages         = {94--96},
  __markedentry = {[ccc:6]},
  abstract      = { Zero-day attacks are those for which users receive no prior warning and thus have no preventive measures in place. We integrate the vulnerability discovery, patch generation, and patch application cycles into a system that automatically detects a new attack, analyzes its modus operandi, determines the best software patch, and applies it at the desired level of granularity LAN, enterprise, or Internet-wide. We develop a vaccination system that automatically generates patches to protect an application's source code.},
  doi           = {10.1109/MC.2004.71},
  file          = {:article\\Patch on demand Saves even more time.pdf:pdf},
  issn          = {00189162},
  keywords      = {around-vulnerability},
  mendeley-tags = {around-vulnerability},
}

@InProceedings{Wiik2004,
  author        = {Wiik, Johannes and Gonzalez, Jose and Lipson, Howard and Shimeal, Timothyl},
  title         = {{Dynamics of Vulnerability – Modeling the Life Cycle of Software Vulnerabilities}},
  booktitle     = {The 22nd International System Dynamics Conference},
  year          = {2004},
  __markedentry = {[ccc:6]},
  abstract      = {Many of the contributing factors to computer security problems are non-technical in nature – that is, they are dependent upon human and organizational actions and interactions in the political, social, legal, and economic realms. However, much of the research in computer security has had a predominantly technical focus. This paper represents a first attempt at using the concepts of system dynamics to model some of the human and organizational actions and interactions that impact the software vulnerability life cycle, which represents the relationship over time between the discovery of security vulnerabilities (i.e., flaws) in software and the occurrence of computer security incidents based on the exploitation of those vulnerabilities by attackers. Although our initial model relies on several simplifying assumptions, it points the way towards richer and more comprehensive models that can increase our capabilities and understanding in ways not possible through traditional computer security research approaches.},
}

@Article{Hovemeyer2004,
  author        = {Hovemeyer, David and Pugh, William},
  title         = {{Finding bugs is easy}},
  journal       = {ACM SIGPLAN Notices},
  year          = {2004},
  volume        = {39},
  number        = {12},
  pages         = {92},
  __markedentry = {[ccc:6]},
  abstract      = {Many techniques have been developed over the years to au- tomatically find bugs in software. Often, these techniques rely on formal methods and sophisticated program analysis. While these techniques are valuable, they can be difficult to apply, and they aren't always effective in finding real bugs. Bug patterns are code idioms that are often errors. We have implemented automatic detectors for a variety of bug patterns found in Java programs. In this paper, we describe how we have used bug pattern detectors to find serious bugs in several widely used Java applications and libraries. We have found that the effort required to implement a bug pat- tern detector tends to be low, and that even extremely sim- ple detectors find bugs in real applications. From our experience applying bug pattern detectors to real programs, we have drawn several interesting conclu- sions. First, we have found that even well tested code writ- ten by experts contains a surprising number of obvious bugs. Second, Java (and similar languages) have many language features and APIs which are prone to misuse. Finally, that simple automatic techniques can be effective at countering the impact of both ordinary mistakes and misunderstood language features.},
  doi           = {10.1145/1052883.1052895},
  file          = {:article\\Finding bugs is easy.pdf:pdf},
  isbn          = {1581138334},
  issn          = {03621340},
  keywords      = {bug checkers,bug patterns,bugs,stat,static analysi,static analysis},
}

@Book{2004,
  title         = {{Principles of Program Analysis}},
  year          = {2004},
  __markedentry = {[ccc:6]},
  booktitle     = {Wireless Networks},
  file          = {:article\\Principles of Program Analysis.pdf:pdf},
  keywords      = {book},
  mendeley-tags = {book},
  pages         = {802--802},
}

@Article{Visser2004,
  author        = {Visser, Willem and Pasareanu, Corina S. and Khurshid, Sarfraz},
  title         = {{Test Input Generation with Java PathFinder}},
  journal       = {ACM SIGSOFT Software Engineering Notes},
  year          = {2004},
  volume        = {29},
  number        = {4},
  pages         = {97},
  __markedentry = {[ccc:6]},
  abstract      = {We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java TreeMap library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.},
  annote        = {模型检测和符号执行的应用},
  doi           = {10.1145/1013886.1007526},
  file          = {:article\\Test Input Generation with Java PathFinder.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1581138202},
  issn          = {01635948},
  keywords      = {binary,coverage,first select,ing,model check-,red-black trees,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,symbolic execution,testing object-oriented programs},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
}

@Article{Larus2004,
  author        = {Larus, James R. and Ball, Thomas and Das, Manuvir and DeLine, Robert and F{\"{a}}hndrich, Manuel and Pincus, Jon and Rajamani, Sriram K. and Venkatapathy, Ramanathan},
  title         = {{Righting software}},
  journal       = {IEEE Software},
  year          = {2004},
  volume        = {21},
  number        = {3},
  pages         = {92--100},
  __markedentry = {[ccc:6]},
  abstract      = { What tools do we use to develop and debug software? Most of us rely on a full-screen editor to write code, a compiler to translate it, a source-level debugger to correct it, and a source-code control system to archive and share it. These tools originated in the 1970s, when the change from batch to interactive programming stimulated the development of innovative languages, tools, environments, and other utilities we take for granted. Microsoft Research has developed two generations of tools, some of which Microsoft developers already use to find and correct bugs. These correctness tools can improve software development by systematically detecting programming errors.},
  doi           = {10.1109/MS.2004.1293079},
  file          = {:article\\Righting software.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {07407459},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Horwitz2004,
  author        = {Horwitz, Susan and Reps, Thomas and Binkley, David},
  title         = {{Interprocedural slicing using dependence graphs}},
  journal       = {ACM SIGPLAN Notices},
  year          = {2004},
  volume        = {39},
  number        = {4},
  pages         = {229},
  __markedentry = {[ccc:6]},
  abstract      = {The notion of a program slice, originally automatic parallelization, introduced by Mark Weiser, is useful in program debugging, and program integration. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing-generating to incorporate collections programs. Our main result is an algorithm of procedures (with for interpro- a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations procedure calls) rather than just monolithic cedural slicing that uses the new representation. somewhat restricted kind of slice: rather than permitting program point p and an arbitrary defined or used at p.) The chief difficulty in interprocedural direct-dependence (It should be noted that our work concerns a a program to be sliced with respect to variable, a slice must be taken with respect to a variable that is slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data dependence edges that represent transitiue dependences due to the effects of procedure calls, in addition to the conventional of an attribute grammar. The step of computing the required transitive-dependendence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.},
  doi           = {10.1145/989393.989419},
  file          = {:article\\Interprocedural slicing using dependence graphs.pdf:pdf},
  isbn          = {0897912691},
  issn          = {03621340},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Lattner2004,
  author        = {Lattner, C and Adve, V S},
  title         = {{{\{}LLVM{\}}: A Compilation Framework for Lifelong Program Analysis and Transformation}},
  journal       = {Cgo},
  year          = {2004},
  number        = {4},
  pages         = {75--88},
  __markedentry = {[ccc:6]},
  file          = {:article\\{\{}LLVM{\}} A Compilation Framework for Lifelong Program Analysis and Transformation.pdf:pdf},
  isbn          = {0769521029},
  keywords      = {binary,stat,static analysi,static analysis},
  mendeley-tags = {binary},
}

@Article{Engler2004,
  author        = {Engler, Dawson and Musuvathi, Madanlal},
  title         = {{Static Analysis versus Software Model Checking for Bug Finding}},
  journal       = {Verification, Model Checking, and Abstract Interpretation},
  year          = {2004},
  volume        = {2937},
  pages         = {405--427},
  __markedentry = {[ccc:6]},
  abstract      = {This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better � it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong.},
  doi           = {10.1007/978-3-540-24622-0_17},
  file          = {:article\\Static Analysis versus Software Model Checking for Bug Finding.pdf:pdf},
  isbn          = {978-3-540-20803-7},
  issn          = {1611-3349},
  keywords      = {stat,static analysi,static analysis},
  url           = {http://www.springerlink.com/index/wx4ppjhwgt696dt8.pdf$\backslash$nhttp://dx.doi.org/10.1007/978-3-540-24622-0{\_}17},
}

@Article{Newsome2005,
  author        = {Newsome, James and Song, Dawn},
  title         = {{Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software}},
  journal       = {Analysis},
  year          = {2005},
  volume        = {44},
  number        = {May 2004},
  pages         = {2--3},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerabilities have had a devastating effect on the Internet. Worms such as CodeRed and Slammer can compromise hundreds of thousands of hosts within hours or even minutes, and cause millions of dollars of damage 26, 43. To successfully combat these fast automatic Internet attacks, we need fast automatic attack detection and filtering mechanisms. In this paper we propose dynamic taint analysis for automatic detection of overwrite attacks, which include most types of exploits. This approach does not need source code or special compilation for the monitored program, and hence works on commodity software. To demonstrate this idea, we have implemented TaintCheck, a mechanism that can perform dynamic taint analysis by performing binary rewriting at run time. We show that TaintCheck reliably detects most types of exploits. We found that TaintCheck produced no false positives for any of the many different programs that we tested. Further, we describe how TaintCheck could improve automatic signature generation in several ways.},
  doi           = {10.1.1.62.8372},
  file          = {:article\\Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity softw.pdf:pdf},
  isbn          = {1-891562-20-7, 1-891562-19-3},
  keywords      = {binary,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,web},
  url           = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.2141{\&}rep=rep1{\&}type=pdf$\backslash$nhttp://repository.cmu.edu/ece/3/?utm{\_}source=repository.cmu.edu/ece/3{\&}utm{\_}medium=PDF{\&}utm{\_}campaign=PDFCoverPages},
}

@Article{Newsome2005a,
  author        = {Newsome, James and Song, Dawn},
  title         = {{Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software}},
  journal       = {Analysis},
  year          = {2005},
  volume        = {44},
  number        = {May 2004},
  pages         = {2--3},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerabilities have had a devastating effect on the Internet. Worms such as CodeRed and Slammer can compromise hundreds of thousands of hosts within hours or even minutes, and cause millions of dollars of damage 26, 43. To successfully combat these fast automatic Internet attacks, we need fast automatic attack detection and filtering mechanisms. In this paper we propose dynamic taint analysis for automatic detection of overwrite attacks, which include most types of exploits. This approach does not need source code or special compilation for the monitored program, and hence works on commodity software. To demonstrate this idea, we have implemented TaintCheck, a mechanism that can perform dynamic taint analysis by performing binary rewriting at run time. We show that TaintCheck reliably detects most types of exploits. We found that TaintCheck produced no false positives for any of the many different programs that we tested. Further, we describe how TaintCheck could improve automatic signature generation in several ways.},
  doi           = {10.1.1.62.8372},
  file          = {:article\\Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity so(2).pdf:pdf},
  isbn          = {1-891562-20-7, 1-891562-19-3},
  keywords      = {binary},
  mendeley-tags = {binary},
  url           = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.2141{\&}rep=rep1{\&}type=pdf$\backslash$nhttp://repository.cmu.edu/ece/3/?utm{\_}source=repository.cmu.edu/ece/3{\&}utm{\_}medium=PDF{\&}utm{\_}campaign=PDFCoverPages},
}

@PhdThesis{JEJT2005,
  author        = {{Jay-Evan J.Tevis}},
  title         = {{Automatic Detection of Software Security Vulnerabilities in Executable Program Files}},
  year          = {2005},
  __markedentry = {[ccc:6]},
  abstract      = {applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {Ph.D. Dissertation},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\Automatic Detection of Software Security Vulnerabilities in Executable Program Files.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {binary,book,first select,icle,obfuscate,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,book,first select,obfuscate,source code,source code-vice important,web},
  pmid          = {25246403},
}

@Article{Livshits2005,
  author        = {Livshits, V Benjamin and Lam, Monica S},
  title         = {{Finding Security Vulnerabilities in Java Applications with Static Analysis}},
  journal       = {Architecture},
  year          = {2005},
  pages         = {18},
  __markedentry = {[ccc:6]},
  abstract      = {This paper proposes a static analysis technique for detecting many recently discovered application vulnerabilities such as SQL injections, cross-site scripting, and HTTP splitting attacks. These vulnerabilities stem from unchecked input, which is widely recognized as the most common source of security vulnerabilities in Web applications. We propose a static analysis approach based on a scalable and precise points-to analysis. In our system, user-provided specifications of vulnerabilities are automatically translated into static analyzers. Our approach finds all vulnerabilities matching a specification in the statically analyzed code. Results of our static analysis are presented to the user for assessment in an auditing interface integrated within Eclipse, a popular Java development environment. Our static analysis found 29 security vulnerabilities in nine large, popular open-source applications, with two of the vulnerabilities residing in widely-used Java libraries. In fact, all but one application in our benchmark suite had at least one vulnerability. Context sensitivity, combined with improved object naming, proved instrumental in keeping the number of false positives low. Our approach yielded very few false positives in our experiments: in fact, only one of our benchmarks suffered from false alarms.},
  doi           = {10.1.1.132.3096},
  file          = {:article\\Finding Security Vulnerabilities in Java Applications with Static Analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  pmid          = {7665465},
  url           = {http://portal.acm.org/citation.cfm?id=1251416},
}

@Article{Andersen2005,
  author        = {Andersen, Kim and Debenham, John and Wagner, Roland and Weippl, Edgar and Klemen, Markus and Linnert, Manfred and Fenz, Stefan and Goluch, Gernot and Tjoa, A.},
  title         = {{An Empirical Study on Using the National Vulnerability Database to Predict Software Vulnerabilities}},
  year          = {2005},
  volume        = {3588},
  number        = {March 2016},
  pages         = {586--595--595},
  __markedentry = {[ccc:6]},
  abstract      = {Desktop search tools are becoming more popular. They have to deal with increasing amounts of locally stored data. Another approach is to analyze the semantic relationship between collected data in order to preprocess the data semantically. The goal is to allow searches based on relationships between various objects instead of focusing on the name of objects. We introduce a database architecture based on an existing software prototype, which is capable of meeting the various demands for a semantic information manager. We describe the use of an association table which stores the relationships between events. It enables adding or removing data items easily without the need for schema modifications. Existing optimization techniques of RDBMS can still be used.},
  doi           = {10.1007/11546924},
  file          = {:article\\An Empirical Study on Using the National Vulnerability Database to Predict Software Vulnerabilities.pdf:pdf},
  isbn          = {978-3-540-28566-3},
  keywords      = {predicte},
  mendeley-tags = {predicte},
  url           = {http://www.springerlink.com/content/befylhmud71yy6cl/},
}

@Article{Cadar2005,
  author        = {Cadar, Cristian and Engler, Dawson},
  title         = {{Execution Generated Test Cases : How to Make Systems Code Crash Itself}},
  journal       = {Symposium A Quarterly Journal In Modern Foreign Literatures},
  year          = {2005},
  pages         = {2--23},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract. This paper presents a technique that uses code to automat- ically generate its own test cases at run-time by using a combination of symbolic and concrete (i.e., regular) execution. The input values to a program (or software component) provide the standard interface of any testing framework with the program it is testing, and generating input values that will explore all the interesting behavior in the tested pro- gram remains an important open problem in software testing research. Our approach works by turning the problem on its head: we lazily gener- ate, from within the program itself, the input values to the program (and values derived from input values) as needed. We applied the technique to real code and found numerous corner-case errors ranging from simple memory overflows and infinite loops to subtle issues in the interpretation of language standards.},
  annote        = {测试样本生成的研究},
  doi           = {http://dx.doi.org/10.1007/11537328},
  file          = {:article\\Execution Generated Test Cases How to Make Systems Code Crash Itself.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {3540281959},
  issn          = {03029743},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {http://www.springerlink.com/index/6H0RAJ6K9ARVKPY0.pdf},
}

@PhdThesis{Sotirov2005,
  author        = {Sotirov, AI},
  title         = {{Automatic vulnerability detection using static source code analysis}},
  year          = {2005},
  __markedentry = {[ccc:6]},
  abstract      = {We present a static source analysis technique for vulnerability detection in C programs. Our approach is based on a combination of taint analysis, a well known vulnerability detection method, and value range propagation, a technique previously used for compiler optimizations. We examine a sample set of vulnerabilities and develop a vulnerability classi- fication based on common source code patterns. We identify three common charac- teristics present in most software vulnerabilities: one, data is read from an untrusted source, two, untrusted data is insufficiently validated, and three, untrusted data is used in a potentially vulnerable function or a language construct. We develop a static source analysis that is able to identify execution paths with these three characteristics and report them as potential vulnerabilities. We present an efficient implementation of our approach as an extension to the GNU C Compiler. We discuss the benefits of integrating a vulnerability detection system in a compiler. Finally, we present experimental results indicating a high level of accuracy of our technique.},
  booktitle     = {Vasa},
  file          = {:article\\Automatic vulnerability detection using static source code analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.9646{\&}rep=rep1{\&}type=pdf},
}

@Article{Godefroid2005,
  author        = {Godefroid, Patrice and Klarlund, Nils},
  title         = {{Software model checking: Searching for computations in the abstract or the concrete}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2005},
  volume        = {3771 LNCS},
  pages         = {20--32},
  __markedentry = {[ccc:6]},
  abstract      = {Software Model Checking: Searching for Computations in the Abstract or the Concrete Patrice Godefroid 1 and Nils Klarlund 2'* 1 Bell Laboratories, Lucent Technologies 2 Google Abstract. We review and discuss the current},
  doi           = {10.1007/11589976_3},
  file          = {:article\\Software model checking Searching for computations in the abstract or the concrete.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {3540304924},
  issn          = {03029743},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,survey},
}

@Article{Pan2006,
  author        = {Pan, Kai and Kim, Sunghun and Whitehead, E. J. Jr.},
  title         = {{Bug Classification Using Program Slicing Metrics}},
  journal       = {International Workshop on Source Code Analysis and Manipulation},
  year          = {2006},
  pages         = {31--42},
  __markedentry = {[ccc:6]},
  abstract      = {In this paper, we introduce 13 program slicing metrics for C language programs. These metrics use program slice information to measure the size, complexity, coupling, and cohesion properties of programs. Compared with traditional code metrics based on code statements or code structure, program slicing metrics involve measures for program behaviors. To evaluate the program slicing metrics, we compare them with the Understand for C++ suite of metrics, a set of widely-used traditional code metrics, in a series of bug classification experiments. We used the program slicing and the Understand for C++ metrics computed for 887 revisions of the Apache HTTP project and 76 revisions of the Latex2rtf project to classify source code files or functions as either buggy or bug-free. We then compared their classification prediction accuracy. Program slicing metrics have slightly better performance than the Understand for C++ metrics in classifying buggy/bug-free source code. Program slicing metrics have an overall 82.6{\%} (Apache) and 92{\%} (Latex2rtf) accuracy at the file level, better than the Understand for C++ metrics with an overall 80.4{\%} (Apache) and 88{\%} (Latex2rtf) accuracy. The experiments illustrate that the program slicing metrics have at least the same bug classification performance as the Understand for C++ metrics.},
  annote        = {和软件度量有关},
  doi           = {10.1109/SCAM.2006.6},
  file          = {:article\\Bug Classification Using Program Slicing Metrics.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0769523536},
  keywords      = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4026853},
}

@Article{Kim2006,
  author        = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and {Jr. Whitehead}, E.},
  title         = {{Automatic Identification of Bug-Introducing Changes}},
  journal       = {21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)},
  year          = {2006},
  pages         = {81--90},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/ASE.2006.23},
  file          = {:article\\Automatic Identification of Bug-Introducing Changes.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0-7695-2579-2},
  keywords      = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4019564},
}

@Article{Howard2006,
  author        = {Howard, Michael A.},
  title         = {{A process for performing security code reviews}},
  journal       = {IEEE Security and Privacy},
  year          = {2006},
  volume        = {4},
  number        = {4},
  pages         = {74--79},
  __markedentry = {[ccc:6]},
  abstract      = {No one really likes reviewing source code for security vulnerabilities; its slow, tedious, and mind-numbingly boring. Yet, code review is a critical component of shipping secure software to customers. Neglecting it isn't an option},
  annote        = {代码审查的经验，可以作为一线人员是怎么寻找安全问题了，如何去降低他们的工作量。},
  doi           = {10.1109/MSP.2006.84},
  file          = {:article\\A process for performing security code reviews.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {15407993},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Jovanovic2006,
  author        = {Jovanovic, Nenad and Kruegel, Christopher and Kirda, Engin},
  title         = {{Pixy : A Static Analysis Tool for Detecting Web Application Vulnerabilities ( Short Paper )}},
  journal       = {proceedings of the IEEE on Security and Privacy},
  year          = {2006},
  pages         = {258--263},
  __markedentry = {[ccc:6]},
  abstract      = {The number and the importance of Web applications haveincreasedrapidlyoverthelastyears. Atthesametime, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code re- viewsaretime-consuming,error-proneandcostly,theneed forautomatedsolutionshasbecomeevident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use ﬂow-sensitive, interprocedural and context-sensitive data ﬂow analysis to discover vulnerable pointsinaprogram. Inaddition,aliasandliteralanalysis are employed to improve the correctness and precision of theresults. Thepresentedconceptsaretargetedatthegen- eralclassoftaint-stylevulnerabilitiesandcanbeappliedto the detection of vulnerability types such as SQL injection, cross-sitescripting,orcommandinjection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vul- nerabilitiesin PHP scripts. Using our tool, we discovered andreported15previouslyunknownvulnerabilitiesinthree web applications, and reconstructed 36 known vulnerabil- ities in three other web applications. The observed false positive rate is at around 50{\%} (i.e., one false positive for eachvulnerability)andtherefore,lowenoughtopermitef- fectivesecurityaudits},
  doi           = {10.1109/SP.2006.29},
  file          = {:article\\Pixy A Static Analysis Tool for Detecting Web Application Vulnerabilities ( Short Paper ).pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0-7695-2574-1},
  keywords      = {binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Chen2006,
  author        = {Chen, Shuo and Jun, X. U. and Kalbarczyk, Zbigniew and Iyer, Ravishankar K.},
  title         = {{Security vulnerabilities: From analysis to detection and masking techniques}},
  journal       = {Proceedings of the IEEE},
  year          = {2006},
  volume        = {94},
  number        = {2},
  pages         = {407--418},
  __markedentry = {[ccc:6]},
  abstract      = {This paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. A finite-state machine (FSM) approach is employed to decompose programs into multiple ele- mentary activities, making it possible to extract simple predicates to be ensured for security. The FSM analysis pinpoints common characteristics among a broad range of security vulnerabilities: predictable memory layout, unprotected control data, and pointer taintedness.We propose memory layout randomization and control data randomization to mask the vulnerabilities at runtime. We also propose a static analysis approach to detect potential security vulnerabilities using the notion of pointer taintedness.},
  doi           = {10.1109/JPROC.2005.862473},
  file          = {:article\\Security vulnerabilities From analysis to detection and masking techniques.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0018-9219},
  issn          = {00189219},
  keywords      = {Protection,Randomization,Security attack,Vulnerability,binary,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Louridas2006,
  author        = {Louridas, P.},
  title         = {{Static code analysis}},
  journal       = {IEEE Software},
  year          = {2006},
  volume        = {23},
  number        = {4},
  pages         = {58--61},
  __markedentry = {[ccc:6]},
  abstract      = {Programmers usually employ static checkers, it checks our programs for errors without executing them, in a process called static code analysis. In this way, it works with a program that has an initial indication of correctness (because it compiles) and try to avoid well-known traps and pitfalls before measuring it against its specifications (when it's tested). We use FindBugs, a popular open source static code checker for Java. Static code checkers in Java come in two flavors: those that work directly on the program source code and those that work on the compiled bytecode. Although each code checker works in its own way, most share some basic traits. They read the program and construct some model of it, a kind of abstract representation that they can use for matching the error patterns they recognize. They also perform some kind of data-flow analysis, trying to infer the possible values that variables might have at certain points in the program. Data-flow analysis is especially important for vulnerability checking, an increasingly important area for code checkers},
  doi           = {10.1109/MS.2006.114},
  file          = {:article\\Static code analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0769523544},
  issn          = {0740-7459},
  keywords      = {first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,web},
  pmid          = {17411999},
}

@Article{Sutherland2006,
  author        = {Sutherland, Iain and Kalb, George E. and Blyth, Andrew and Mulley, Gaius},
  title         = {{An empirical examination of the reverse engineering process for binary files}},
  journal       = {Computers and Security},
  year          = {2006},
  volume        = {25},
  number        = {3},
  pages         = {221--228},
  __markedentry = {[ccc:6]},
  abstract      = {Reverse engineering of binary code file has become increasingly easier to perform. The binary reverse engineering and subsequent software exploitation activities represent a significant threat to the intellectual property content of commercially supplied software products. Protection technologies integrated within the software products offer a viable solution towards deterring the software exploitation threat. However, the absence of metrics, measures, and models to characterize the software exploitation process prevents execution of quantitative assessments to define the extent of protection technology suitable for application to a particular software product. This paper examines a framework for collecting reverse engineering measurements, the execution of a reverse engineering experiment, and the analysis of the findings to determine the primary factors that affect the software exploitation process. The results of this research form a foundation for the specification of metrics, gathering of additional measurements, and development of predictive models to characterize the software exploitation process. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.cose.2005.11.002},
  file          = {:article\\An empirical examination of the reverse engineering process for binary files.pdf:pdf},
  issn          = {01674048},
  keywords      = {Binary code,Complexity metrics,Process metrics,Reverse engineering,Software protection,reverse,stat,static analysi,static analysis},
  mendeley-tags = {reverse},
}

@Article{MunsonJ.C.aNikora2006,
  author        = {{Munson J.C.a Nikora}, A.P.b Sherif J.S.c d},
  title         = {{Software faults: A quantifiable definition}},
  journal       = {Advances in Engineering Software},
  year          = {2006},
  volume        = {37},
  number        = {5},
  pages         = {327--333},
  __markedentry = {[ccc:6]},
  abstract      = {An important aspect of developing models relating the number and type$\backslash$nof faults in a software system to a set of structural measurement$\backslash$nis defining what constitutes a fault. By definition, a fault is a$\backslash$nstructural imperfection in a software system that may lead to the$\backslash$nsystem's eventually failing. A measurable and precise definition$\backslash$nof what faults are makes it possible to accurately identify and count$\backslash$nthem, which in turn allows the formulation of models relating fault$\backslash$ncounts and types to other measurable attributes of a software system.$\backslash$nUnfortunately, the most widely used definitions are not measurable$\backslash$n- there is no guarantee that two different individuals looking at$\backslash$nthe same set of failure reports and the same set of fault definitions$\backslash$nwill count the same number of underlying faults. The incomplete and$\backslash$nambiguous nature of current fault definitions adds a noise component$\backslash$nto the inputs used in modeling fault content. If this noise component$\backslash$nis sufficiently large, any attempt to develop a fault model will$\backslash$nproduce invalid results. In this paper, we base our recognition and$\backslash$nenumeration of software faults on the grammar of the language of$\backslash$nthe software system. By tokenizing the differences between a version$\backslash$nof the system exhibiting a particular failure behavior, and the version$\backslash$nin which changes were made to eliminate that behavior, we are able$\backslash$nto unambiguously count the number of faults associated with that$\backslash$nfailure. With modern configuration management tools, the identification$\backslash$nand counting of software faults can be automated. {\textcopyright} 2005 Elsevier$\backslash$nLtd. All rights reserved.},
  doi           = {10.1016/j.advengsoft.2005.07.003},
  file          = {:article\\Software faults A quantifiable definition.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09659978},
  keywords      = {defect detection,first select,machine learning,predicte,second select,software faults,software quality,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://www.scopus.com/inward/record.url?eid=2-s2.0-31044454098{\&}partnerID=40{\&}md5=53aee69af1c895b9b76cfe9ab232213e},
}

@Article{Byers2006,
  author        = {Byers, David and Ardi, Shanai and Shahmehri, Nahid and Duma, Claudiu},
  title         = {{Modeling Software VulnerabilitiesWith Vulnerability Cause Graphs}},
  journal       = {2006 22nd IEEE International Conference on Software Maintenance},
  year          = {2006},
  pages         = {411--422},
  __markedentry = {[ccc:6]},
  abstract      = {When vulnerabilities are discovered in software, which often happens after deployment, they must be addressed as part of ongoing software maintenance. A mature software development organization should analyze vulnerabilities in order to determine how they, and similar vulnerabilities, can be prevented in the future. In this paper we present a structured method for analyzing and documenting the causes of software vulnerabilities. Applied during software maintenance, the method generates the information needed for improving the software development process, to prevent similar vulnerabilities in future releases. Our approach is based on vulnerability cause graphs, a structured representation of causes of software vulnerabilities},
  doi           = {10.1109/ICSM.2006.40},
  file          = {:article\\Modeling Software VulnerabilitiesWith Vulnerability Cause Graphs.pdf:pdf},
  isbn          = {0-7695-2354-4},
  issn          = {1063-6773},
  keywords      = {Computer worms,Data security,Databases,Information analysis,Information science,Information security,National security,Programming,Software maintenance,Software performance,safety-critical software,software development organization,software maintenance,software security,software vulnerability modeling,vulnerability cause graphs,vulnerability modeling},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4021368},
}

@Book{MarkDowdJohnMcDonald2006,
  title         = {{The Art of Software Security Assessment: Identifying and Preventing Software Vulnerabilities}},
  year          = {2006},
  author        = {{Mark Dowd, John McDonald}, Justin Schuh},
  __markedentry = {[ccc:6]},
  abstract      = {The Definitive Insider's Guide to Auditing Software Security This is one of the most detailed, sophisticated, and useful guides to software security auditing ever written. The authors are leading security consultants and researchers who have personally uncovered vulnerabilities in applications ranging from sendmail to Microsoft Exchange, Check Point VPN to Internet Explorer. Drawing on their extraordinary experience, they introduce a start-to-finish methodology for “ripping apart” applications to reveal even the most subtle and well-hidden security flaws. The Art of Software Security Assessment covers the full spectrum of software vulnerabilities in both UNIX/Linux and Windows environments. It demonstrates how to audit security in applications of all sizes and functions, including network and Web software. Moreover, it teaches using extensive examples of real code drawn from past flaws in many of the industry's highest-profile applications. Coverage includes • Code auditing: theory, practice, proven methodologies, and secrets of the trade • Bridging the gap between secure software design and post-implementation review • Performing architectural assessment: design review, threat modeling, and operational review • Identifying vulnerabilities related to memory management, data types, and malformed data • UNIX/Linux assessment: privileges, files, and processes • Windows-specific issues, including objects and the filesystem • Auditing interprocess communication, synchronization, and state • Evaluating network software: IP stacks, firewalls, and common application protocols • Auditing Web applications and technologies},
  file          = {:article\\The Art of Software Security Assessment Identifying and Preventing Software Vulnerabilities.pdf:pdf},
  isbn          = {978-0-321-44442-4},
  keywords      = {book,survey},
  mendeley-tags = {book,survey},
  pages         = {1200},
  url           = {www.awprofessional.com$\backslash$n},
}

@Misc{Ganapathy2007,
  author        = {Ganapathy, Vinod and King, David and Jaeger, Trent and Jha, Somesh},
  title         = {{Mining security-sensitive operations in legacy code using concept analysis}},
  year          = {2007},
  __markedentry = {[ccc:6]},
  abstract      = {This paper presents an approach to statically retrofit legacy servers with mechanisms for authorization policy enforcement. The approach is based upon the observation that security-sensitive operations performed by a server are characterized by idiomatic resource manipulations, called fingerprints. Candidate fingerprints are automatically mined by clustering resource manipulations using concept analysis. These fingerprints are then used to identify security-sensitive operations performed by the server. Case studies with three real-world servers show that the approach can be used to identify security-sensitive operations with a few hours of manual effort and modest domain knowledge.},
  annote        = {安全敏感操作，此处概念需仔细辨认},
  booktitle     = {Proceedings - International Conference on Software Engineering},
  doi           = {10.1109/ICSE.2007.54},
  file          = {:article\\Mining security-sensitive operations in legacy code using concept analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0769528287},
  issn          = {02705257},
  keywords      = {first select,second select,security-sensitive operations,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  pages         = {458--467},
}

@Article{Alhazmi2007,
  author        = {Alhazmi, O. H. and Malaiya, Y. K. and Ray, I.},
  title         = {{Measuring, analyzing and predicting security vulnerabilities in software systems}},
  journal       = {Computers and Security},
  year          = {2007},
  volume        = {26},
  number        = {3},
  pages         = {219--228},
  __markedentry = {[ccc:6]},
  abstract      = {In this work we examine the feasibility of quantitatively characterizing some aspects of security. In particular, we investigate if it is possible to predict the number of vulnerabilities that can potentially be present in a software system but may not have been found yet. We use several major operating systems as representatives of complex software systems. The data on vulnerabilities discovered in these systems are analyzed. We examine the results to determine if the density of vulnerabilities in a program is a useful measure. We also address the question about what fraction of software defects are security related, i.e., are vulnerabilities. We examine the dynamics of vulnerability discovery hypothesizing that it may lead us to an estimate of the magnitude of the undiscovered vulnerabilities still present in the system. We consider the vulnerability discovery rate to see if models can be developed to project future trends. Finally, we use the data for both commercial and open-source systems to determine whether the key observations are generally applicable. Our results indicate that the values of vulnerability densities fall within a range of values, just like the commonly used measure of defect density for general defects. Our examination also reveals that it is possible to model the vulnerability discovery using a logistic model that can sometimes be approximated by a linear model. ?? 2006 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.cose.2006.10.002},
  file          = {:article\\Measuring, analyzing and predicting security vulnerabilities in software systems.pdf:pdf},
  isbn          = {0167-4048},
  issn          = {01674048},
  keywords      = {Defect density,Quantitative security modeling,Risk evaluation,Security holes,Vulnerabilities,predicte},
  mendeley-tags = {predicte},
}

@Misc{2007,
  title         = {如何从检索结果中快速找到某个学科的相关文献 ？},
  year          = {2007},
  __markedentry = {[ccc:6]},
  file          = {:article\\如何从检索结果中快速找到某个学科的相关文献 ？.pdf:pdf},
  keywords      = {book,web},
  mendeley-tags = {book,web},
}

@Article{Neuhaus2007,
  author        = {Neuhaus, Stephan and Zimmermann, Thomas and Holler, Christian and Zeller, Andreas},
  title         = {{Predicting vulnerable software components}},
  journal       = {Proceedings of the 14th ACM conference on Computer and communications security CCS 07},
  year          = {2007},
  pages         = {529},
  __markedentry = {[ccc:6]},
  abstract      = {Where do most vulnerabilities occur in software? Our Vul- ture tool automatically mines existing vulnerability databa- ses and version archives to map past vulnerabilities to com- ponents. The resulting ranking of the most vulnerable com- ponents is a perfect base for further investigations on what makes components vulnerable. In an investigation of theMozilla vulnerability history, we surprisingly found that components that had a single vulner- ability in the past were generally not likely to have further vulnerabilities. However, components that had similar im- ports or function calls were likely to be vulnerable. Based on this observation, we were able to extend Vul- ture by a simple predictor that correctly predicts about half of all vulnerable components, and about two thirds of all predictions are correct. This allows developers and project managers to focus their their efforts where it is needed most: We should look at nsXPInstallManager because it is likely to contain yet unknown vulnerabilities.},
  doi           = {10.1145/1315245.1315311},
  file          = {:article\\Predicting vulnerable software components.pdf:pdf},
  isbn          = {9781595937032},
  issn          = {15437221},
  keywords      = {all incidents,binary,databases do not,fuzz,however,machine learning,maintains a vulnerability database,mozilla project,predicte,prediction,software security,stance,stat,static analysi,static analysis,tell how,tracking past vulnerabilities,web,which records},
  mendeley-tags = {binary,fuzz,machine learning,predicte,web},
  url           = {http://portal.acm.org/citation.cfm?doid=1315245.1315311},
}

@Article{Black2007,
  author        = {Black, Paul E.},
  title         = {{SAMATE and evaluating static analysis tools}},
  journal       = {Ada User Journal},
  year          = {2007},
  volume        = {28},
  number        = {3},
  pages         = {184--188},
  __markedentry = {[ccc:6]},
  file          = {:article\\SAMATE and evaluating static analysis tools.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {13816551},
  keywords      = {Software assurance,Source code,Static analysis,Tool testing,binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Engler2007,
  author        = {Engler, Dawson and Dunbar, Daniel},
  title         = {{Under-constrained Execution: Making Automatic Code Destruction Easy and Scalable}},
  journal       = {Proceedings of the 2007 international symposium on Software testing and analysis},
  year          = {2007},
  volume        = {0},
  pages         = {1--4},
  __markedentry = {[ccc:6]},
  abstract      = {An abstract is not available.},
  annote        = {KLEE的原始想法，动静结合的分析方法。},
  doi           = {10.1145/1273463.1273464},
  file          = {:article\\Under-constrained Execution Making Automatic Code Destruction Easy and Scalable.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781595937346},
  keywords      = {bug finding,dynamic analysis,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,symbolic execution},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@PhdThesis{雷植洲2007,
  author        = {雷植洲},
  title         = {代码混淆技术及其在软件安全保护中的应用研究},
  year          = {2007},
  __markedentry = {[ccc:6]},
  file          = {:article\\代码混淆技术及其在软件安全保护中的应用研究.pdf:pdf},
  keywords      = {obfuscate},
  mendeley-tags = {obfuscate},
}

@Article{Udrea2008,
  author        = {Udrea, Octavian and Lumezanu, Cristian and Foster, Jeffrey S.},
  title         = {{Rule-based static analysis of network protocol implementations}},
  journal       = {Information and Computation},
  year          = {2008},
  volume        = {206},
  number        = {2-4},
  pages         = {130--157},
  __markedentry = {[ccc:6]},
  abstract      = {Today's software systems communicate over the Internet using standard protocols that have been heavily scrutinized, providing some assurance of resistance to malicious attacks and general robustness. However, the software that implements those protocols may still contain mistakes, and an incorrect implementation could lead to vulnerabilities even in the most well-understood protocol. The goal of this work is to close this gap by introducing a new technique for checking that a C implementation of a protocol matches its description in an RFC or similar standards document. We present a static (compile-time) source code analysis tool called Pistachio that checks C code against a rule-based specification of its behavior. Rules describe what should happen during each round of communication, and can be used to enforce constraints on ordering of operations and on data values. Our analysis is not guaranteed sound due to some heuristic approximations it makes, but has a low false negative rate in practice when compared to known bug reports. We have applied Pistachio to two different implementations of SSH2 and an implementation of RCP. Pistachio discovered a multitude of bugs, including security vulnerabilities, that we confirmed by hand and checked against each project's bug databases. ?? 2007 Elsevier Inc. All rights reserved.},
  doi           = {10.1016/j.ic.2007.05.007},
  file          = {:article\\Rule-based static analysis of network protocol implementations.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {08905401},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Balzarotti2008,
  author        = {Balzarotti, Davide and Cova, Marco and Felmetsger, Vika and Jovanovic, Nenad and Kirda, Engin and Kruegel, Christopher and Vigna, Giovanni},
  title         = {{Saner: Composing static and dynamic analysis to validate sanitization in web applications}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2008},
  pages         = {387--401},
  __markedentry = {[ccc:6]},
  abstract      = {Web applications are ubiquitous, perform mission- critical tasks, and handle sensitive user data. Unfortunately, web applications are often implemented by developers with limited security skills, and, as a result, they contain vulnerabilities. Most of these vulnerabilities stem from the lack of input validation. That is, web applications use malicious input as part of a sensitive operation, without having properly checked or sanitized the input values prior to their use. Past research on vulnerability analysis has mostly focused on identifying cases in which a web application directly uses external input in critical operations. However, little research has been performed to analyze the correctness of the sanitization process. Thus, whenever a web application applies some sanitization routine to potentially malicious input, the vulnerability analysis assumes that the result is innocuous. Unfortunately, this might not be the case, as the sanitization process itself could be incorrect or incomplete. In this paper, we present a novel approach to the analysis of the sanitization process. More precisely, we combine static and dynamic analysis techniques to identify faulty sanitization procedures that can be bypassed by an attacker. We implemented our approach in a tool, called Saner, and we applied it to a number of real-world applications. Our results demonstrate that we were able to identify several novel vulnerabilities that stem from erroneous sanitization procedures.},
  doi           = {10.1109/SP.2008.22},
  file          = {:article\\Saner Composing static and dynamic analysis to validate sanitization in web applications.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9780769531687},
  issn          = {10816011},
  keywords      = {binary,first select,fuzz,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,source code,source code-vice important,web},
}

@Article{Cadar2008,
  author        = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson R.},
  title         = {{KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs}},
  journal       = {Proceedings of the 8th USENIX conference on Operating systems design and implementation},
  year          = {2008},
  pages         = {209--224},
  __markedentry = {[ccc:6]},
  abstract      = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage - on average over 90{\%} per tool (median: over 94{\%}) - and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100{\%} coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  annote        = {在中间的字节码上运行},
  doi           = {10.1.1.142.9494},
  file          = {:article\\KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-931971-65-2},
  issn          = {{\textless}null{\textgreater}},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {http://portal.acm.org/citation.cfm?id=1855756},
}

@Article{Chess2008,
  author        = {Chess, Brian and West, Jacob},
  title         = {{Dynamic taint propagation: Finding vulnerabilities without attacking}},
  journal       = {Information Security Technical Report},
  year          = {2008},
  volume        = {13},
  number        = {1},
  pages         = {33--39},
  __markedentry = {[ccc:6]},
  abstract      = {We apply dynamic taint propagation to find input validation bugs using less effort than typical security testing. We monitor a target program as it executes in order to track untrusted user input. Our system works in conjunction with normal functional testing, so effort devoted to functional testing can be directly leveraged to uncover vulnerabilities. The result is that we achieve higher test coverage (and therefore find more bugs) than typical security testing techniques and make it practical for quality assurance organizations with no security experience to test the security of the software they examine. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.istr.2008.02.003},
  file          = {:article\\Dynamic taint propagation Finding vulnerabilities without attacking.pdf:pdf},
  isbn          = {1363-4127},
  issn          = {13634127},
  keywords      = {Quality assurance,Security,Software,Taint propagation,Vulnerability detection,binary,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,web},
}

@Article{Liao2008,
  author        = {Liao, Zhensong and Jin, Hai and Zou, Deqing},
  title         = {{A logic predicate based automated trust negotiation model}},
  journal       = {Proceedings of the Second International Conference on Communications and Networking in China, ChinaCom 2007},
  year          = {2008},
  pages         = {418--422},
  __markedentry = {[ccc:6]},
  abstract      = {Automated trust negotiation (ATN) is an important means to establish trust between strangers through the exchange of digital credentials and mobile access control policies specifying what combinations of credentials a stranger must submit. As an access control policy may contain sensitive information, the negotiation process becomes complicated in order to prevent information leakage. Furthermore, since ATN is a type of access control mechanism, it is difficult to be described by formalization languages, therefore it has a low security level. To solve these problems, a Logic Predicate Based ATN Model (LPBM) is proposed in this paper. LPBM decomposes a compound policy into met a policies and provides suitable rules to simplify them. While doing this, LPBM can check policy consistency. In addition, a set of logic predicates are presented to describe the negotiation process. Agent is introduced to perform the negotiation on behalf of negotiators. A use case is studied to show that LPBM is sound and reasonable.},
  doi           = {10.1109/CHINACOM.2007.4469417},
  file          = {:article\\A logic predicate based automated trust negotiation model.pdf:pdf},
  isbn          = {9781424410095},
  keywords      = {predicte},
  mendeley-tags = {predicte},
}

@TechReport{Assume2008,
  author        = {Assume, N},
  title         = {{Immediate Dominators}},
  year          = {2008},
  __markedentry = {[ccc:6]},
  file          = {:article\\Immediate Dominators.pdf:pdf},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Emanuelsson2008,
  author        = {Emanuelsson, P{\"{a}}r and Nilsson, Ulf},
  title         = {{A Comparative Study of Industrial Static Analysis Tools}},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2008},
  volume        = {217},
  number        = {C},
  pages         = {5--21},
  __markedentry = {[ccc:6]},
  abstract      = {Tools based on static analysis can be used to find defects in programs. Tools that do shallow analyses based on pattern matching have existed since the 1980's and although they can analyze large programs they have the drawback of producing a massive amount of warnings that have to be manually analyzed to see if they are real defects or not. Recent technology advances has brought forward tools that do deeper analyses that discover more defects and produce a limited amount of false warnings. These tools can still handle large industrial applications with millions lines of code. This article surveys the underlying supporting technology of three state-of-the-art static analysis tools. The survey relies on information in research articles and manuals, and includes the types of defects checked for (such as memory management, arithmetics, security vulnerabilities), soundness, value and aliasing analyses, incrementality and IDE integration. This survey is complemented by practical experiences from evaluations at the Ericsson telecom company. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.entcs.2008.06.039},
  file          = {:article\\A Comparative Study of Industrial Static Analysis Tools.pdf:pdf},
  groups        = {vice-important},
  issn          = {15710661},
  keywords      = {Static analysis,dataflow analysis,defects,first select,security vulnerabilities,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,source code,source code-vice important},
}

@Article{Guo2009,
  author        = {Guo, Philip J and Engler, Dawson},
  title         = {{Linux Kernel Developer Responses to Static Analysis Bug Reports}},
  journal       = {Proceedings of the 2009 Conference on USENIX Annual Technical Conference},
  year          = {2009},
  pages         = {22},
  __markedentry = {[ccc:6]},
  abstract      = {We present a study of how Linux kernel developers respond to bug reports issued by a static analysis tool. We found that developers prefer to triage reports in younger, smaller, and more actively-maintained files (2), first address easy-to-fix bugs and defer difficult (but possibly critical) bugs (3), and triage bugs in batches rather than individually (4). Also, although automated tools cannot find many types of bugs, they can be effective at directing developers' attentions towards parts of the codebase that contain up to 3X more user-reported bugs (5). Our insights into developer attitudes towards static analysis tools allow us to make suggestions for improving their usability and effectiveness. We feel that it could be effective to run static analysis tools continuously while programming and before committing code, to rank reports so that those most likely to be triaged are shown to developers first, to show the easiest reports to new developers, to perform deeper analysis on more actively-maintained code, and to use reports as indirect indicators of code quality and importance.},
  annote        = {有意思的研究点，linux内核开发者对待bug报告的反应。},
  file          = {:article\\Linux Kernel Developer Responses to Static Analysis Bug Reports.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://dl.acm.org/citation.cfm?id=1855807.1855829},
}

@Article{Wang2009,
  author        = {Wang, Yi Hsien and Wu, I. Chen},
  title         = {{Achieving high and consistent rendering performance of java AWT/Swing on multiple platforms}},
  journal       = {Software - Practice and Experience},
  year          = {2009},
  volume        = {39},
  number        = {7},
  pages         = {701--736},
  __markedentry = {[ccc:6]},
  abstract      = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727–745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4 to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
  archiveprefix = {arXiv},
  arxivid       = {1008.1900},
  doi           = {10.1002/spe},
  eprint        = {1008.1900},
  file          = {:article\\Achieving high and consistent rendering performance of java AWTSwing on multiple platforms.pdf:pdf},
  isbn          = {0000000000000},
  issn          = {00380644},
  keywords      = {CYC Window Toolkit,Directx,Linux,Mac OS x,OpenGL,Windows,binary,obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,obfuscate,predicte,web},
  pmid          = {20926156},
}

@PhdThesis{CHANG2009,
  author        = {CHANG, RAY-YAUNG},
  title         = {{Discovering neglected conditions in software by mining dependence graphs}},
  year          = {2009},
  __markedentry = {[ccc:6]},
  abstract      = {Neglected conditions are an important but difficult-to-find class of software defects. This paper presents a novel approach to revealing neglected conditions that integrates static program analysis and advanced data mining techniques to discover implicit conditional rules in a code base and to discover rule violations that indicate neglected conditions. The approach requires the user to indicate minimal constraints on the context of the rules to be sought, rather than specific rule templates. To permit this generality, rules are modeled as graph minors of enhanced procedure dependence graphs (EPDGs), in which control and data dependence edges are augmented by edges representing shared data dependences. A heuristic maximal frequent subgraph mining algorithm is used to extract candidate rules from EPDGs, and a heuristic graph matching algorithm is used to identify rule violations. We also report the results of an empirical study in which the approach was applied to four open source projects (openssl, make, procmail, amaya). These results indicate that the approach is effective and reasonably efficient.},
  booktitle     = {CASE WESTERN RESERVE UNIVERSITY},
  doi           = {10.1109/TSE.2008.24},
  file          = {:article\\Discovering neglected conditions in software by mining dependence graphs.pdf:pdf},
  groups        = {vice-important},
  issn          = {00985589},
  keywords      = {Automatic defect detection,Frequent item set mining,Frequent subgraph mining,Mining software repositories,Program dependence graphs,first select,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,source code,source code-vice important,web},
}

@Article{Penta2009,
  author        = {Penta, Massimiliano Di and Cerulo, Luigi and Aversano, Lerina},
  title         = {{The life and death of statically detected vulnerabilities: An empirical study}},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {10},
  pages         = {1469--1484},
  __markedentry = {[ccc:6]},
  abstract      = {Vulnerable statements constitute a major problem for developers and maintainers of networking systems. Their presence can ease the success of security attacks, aimed at gaining unauthorized access to data and functionality, or at causing system crashes and data loss. Examples of attacks caused by source code vulnerabilities are buffer overflows, command injections, and cross-site scripting. This paper reports on an empirical study, conducted across three networking systems, aimed at observing the evolution and decay of vulnerabilities detected by three freely available static analysis tools. In particular, the study compares the decay of different kinds of vulnerabilities, characterizes the decay likelihood through probability density functions, and reports a quantitative and qualitative analysis of the reasons for vulnerability removals. The study is performed by using a framework that traces the evolution of source code fragments across subsequent commits. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  annote        = {有意思的研究点，研究源码漏洞的产生到修补的生命期},
  doi           = {10.1016/j.infsof.2009.04.013},
  file          = {:article\\The life and death of statically detected vulnerabilities An empirical study.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0950-5849},
  issn          = {09505849},
  keywords      = {Empirical study,Mining software repositories,Software vulnerabilities,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2009.04.013},
}

@Article{Thummalapenta2009,
  author        = {Thummalapenta, Suresh and Xie, Tao and Tillmann, Nikolai and Halleux, Jonathan De and Schulte, Wolfram},
  title         = {{MSeqGen: object-oriented unit-test generation via mining source code}},
  journal       = {Foundations of Software Engineering},
  year          = {2009},
  pages         = {193--202},
  __markedentry = {[ccc:6]},
  abstract      = {An objective of unit testing is to achieve high structural coverage of the code under test. Achieving high structural overage of object-oriented code requires desirable method-call sequences that create and mutate objects. These sequences help generate target object states such as argument or receiver object states (in short as target states) of a method under test. Automatic generation of sequences for achieving target states is often challenging due to a large search space of possible sequences. On the other hand, code bases using object types (such as receiver or argument object types) include sequences that can be used to assist automatic test-generation approaches in achieving target states. In this paper, we propose a novel approach, called MSeqGen, that mines code bases and extracts sequences related to receiver or argument object types of a method under test. Our approach uses these extracted sequences to enhance two state-of-the-art test-generation approaches: random testing and dynamic symbolic execution. We conduct two evaluations to show the effectiveness of our approach. Using sequences extracted by our approach, we show that a random testing approach achieves 8.7{\%} (with a maximum of 20.0{\%} for one namespace) higher branch coverage and a dynamic-symbolic-execution-based approach achieves 17.4{\%} (with a maximum of 22.5{\%} for one namespace) higher branch coverage than without using our approach. Such an improvement is significant as the branches that are not covered by these state-of-the-art approaches are generally quite difficult to cover.},
  doi           = {10.1145/1595696.1595725},
  file          = {:article\\MSeqGen object-oriented unit-test generation via mining source code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781605580012},
  keywords      = {first select,object-oriented testing,second select,sequence mining,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
  url           = {http://portal.acm.org/citation.cfm?id=1595725},
}

@Article{Turhan2009,
  author        = {Turhan, Burak and Kocak, Gozde and Bener, Ayse},
  title         = {{Data mining source code for locating software bugs: A case study in telecommunication industry}},
  journal       = {Expert Systems with Applications},
  year          = {2009},
  volume        = {36},
  number        = {6},
  pages         = {9986--9990},
  __markedentry = {[ccc:6]},
  abstract      = {In a large software system knowing which files are most likely to be fault-prone is valuable information for project managers. They can use such information in prioritizing software testing and allocating resources accordingly. However, our experience shows that it is difficult to collect and analyze fine-grained test defects in a large and complex software system. On the other hand, previous research has shown that companies can safely use cross-company data with nearest neighbor sampling to predict their defects in case they are unable to collect local data. In this study we analyzed 25 projects of a large telecommunication system. To predict defect proneness of modules we trained models on publicly available Nasa MDP data. In our experiments we used static call graph based ranking (CGBR) as well as nearest neighbor sampling for constructing method level defect predictors. Our results suggest that, for the analyzed projects, at least 70{\%} of the defects can be detected by inspecting only (i) 6{\%} of the code using a Na??ve Bayes model, (ii) 3{\%} of the code using CGBR framework. ?? 2008 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.eswa.2008.12.028},
  file          = {:article\\Data mining source code for locating software bugs A case study in telecommunication industry.pdf:pdf},
  groups        = {vice-important},
  isbn          = {0957-4174},
  issn          = {09574174},
  keywords      = {Case study,Defect prediction,Software bugs,Software testing,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.eswa.2008.12.028},
}

@Article{Hanna2009,
  author        = {Hanna, Aiman and Ling, Hai Zhou and Yang, XiaoChun and Debbabi, Mourad},
  title         = {{A Synergy between Static and Dynamic Analysis for the Detection of Software Security Vulnerabilities}},
  year          = {2009},
  pages         = {815--832},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-642-05151-7_5},
  file          = {:article\\A Synergy between Static and Dynamic Analysis for the Detection of Software Security Vulnerabilities.pdf:pdf},
  groups        = {vice-important},
  keywords      = {binary,dynamic analysis,first select,predicte,security automata,security testing,source code,source code-vice important,stat,static analysi,static analysis,test-data generation},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important},
  url           = {http://link.springer.com/10.1007/978-3-642-05151-7{\_}5},
}

@Article{Catal2009,
  author        = {Catal, Cagatay and Diri, Banu},
  title         = {{A systematic review of software fault prediction studies}},
  journal       = {Expert Systems with Applications},
  year          = {2009},
  volume        = {36},
  number        = {4},
  pages         = {7346--7354},
  __markedentry = {[ccc:6]},
  abstract      = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.eswa.2008.10.027},
  file          = {:article\\A systematic review of software fault prediction studies.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0957-4174},
  issn          = {09574174},
  keywords      = {Automated fault prediction models,Expert systems,Machine learning,Method-level metrics,Public datasets,binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.eswa.2008.10.027},
}
{2009,
abstract = {{软件确保的概念进行了介绍，指出软件确保主要聚焦于软件的安全性(security)、保险性(safety)、 可靠性(reliability)和生存性(survivability)，并以此为基础，提出了s3R软件确保模型，然后在战略层面．{\}}：讨 论了软件确保的发展过程，重点阐述了美国国土安全部、美国国防部、美国宇航局的软件确保计划；在技术层面上 探讨了确保软件S3R性质的相关技术进展，阐述了正在制定中的软件确保标准。最后指出了软件确保的发展趋势。},
author = {方滨兴 and 陆天波 and 李超},
file = {:article\\软件确保研究进展.pdf.pdf:pdf},
journal = {通信学报},
keywords = {survey,安全,脆弱性,软件确保},
mendeley-tags = {survey},
title = {软件确保研究进展.pdf},
volume = {30},
year = {2009}
}

@Article{Caballero2010,
  author        = {Caballero, Juan and Johnson, Noah M. and Mccamant, Stephen and Song, Dawn},
  title         = {{Binary Code Extraction and Interface Identification for Security Applications}},
  journal       = {17th Annual Network and Distributed System Security Symposium},
  year          = {2010},
  __markedentry = {[ccc:6]},
  abstract      = {Binary code reutilization is the process of automatically identifying the interface and extracting the instructions and data dependencies of a code fragment from an executable program, so that it is selfcontained and can be reused by external code. Binary code reutilization is useful for a number of security applications, including reusing the proprietary cryptographic or unpacking functions from a malware sample and for rewriting a network dialog. In this paper we conduct the first systematic study of automated binary code reutilization and its security applications. The main challenge in binary code reutilization is understanding the code fragments interface. We propose a novel technique to identify the prototype of an undocumented code fragment directly from the programs binary, without access to source code or symbol information. Further, we must also extract the code itself from the binary so that it is self-contained and can be easily reused in another program. We design and implement a tool that uses a combination of dynamic and static analysis to automatically identify the prototype and extract the instructions of an assembly function into a form that can be reused by other C code. The extracted function can be run independently of the rest of the programs functionality and shared with other users. We apply our approach to scenarios that include extracting the encryption and decryption routines from malware samples, and show that these routines can be reused by a network proxy to decrypt encrypted traffic on the network. This allows the network proxy to rewrite the malwares encrypted traffic by combining the extracted encryption and decryption functions with the session keys and the protocol grammar. We also show that we can reuse a code fragment from an unpacking function for the unpacking routine for a different sample of the same family, even if the code fragment is not a complete function.},
  annote        = {太强大了，直接从二进制程序中提取代码重用！！！},
  file          = {:article\\Binary Code Extraction and Interface Identification for Security Applications.pdf:pdf},
  keywords      = {binary,stat,static analysi,static analysis},
  mendeley-tags = {binary},
  url           = {https://software.imdea.org/{~}juanca/papers/bcr{\_}ndss10.pdf},
}

@Article{李根2010,
  author        = {李根},
  title         = {面向高可信软件的整数溢出错误的自动化测试.pdf},
  journal       = {软件学报},
  year          = {2010},
  volume        = {21},
  number        = {2},
  pages         = {179--193},
  __markedentry = {[ccc:6]},
  abstract      = {摘要： 面向高可信软件提出了一种二进制级高危整数溢出错误的全自动测试方法(dynamic automatic integer-overflow detection and testing，简称DAIDT)．该方法无需任何源码甚至是符号表支持，即可对二进制应用程序 进行全面测试，并自动发现高危整数溢出错误．在理论上形式化证明了该技术对高危整数溢出错误测试与发掘的无 漏报性、零误报性与错误可重现特性．为了验证该方法的有效性，实现了IntHonter原型系统．IntHunter对3个最新版 本的高可信应用程序(微软公司Windows 2003和2000 Server的WINS服务、百度公司的即时通讯软件BaiDu Hi) 分别进行了24小时测试，共发现了4个高危整数溢出错误．其中3个错误可导致任意代码执行，其中两个由微软安全 响应中心分配漏洞编号CVE-2009-1923，CVE-2009-1924。另一个由百度公司分配漏洞编号CVE-2008-6444．},
  file          = {:article\\面向高可信软件的整数溢出错误的自动化测试.pdf.pdf:pdf},
  keywords      = {dynamic automatic test case generation,fuzz,integer overflow,integer overflow vuinerability,symbolic execution,taint analysis},
  mendeley-tags = {fuzz},
}

@PhdThesis{HZL2010,
  author        = {{Hai Zhou Ling}},
  title         = {{Towards the automation of vulnerability detection in source code}},
  year          = {2010},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerability detection, which involves security property specification and veri- fication, is essential in assuring the software security. However, the process of vulnera- bility detection is labor-intensive, time-consuming and error-prone if done manually. In this thesis, we present a hybrid approach, which utilizes the power of static and dynamic analysis for performing vulnerability detection in a systematic way. The key contributions of this thesis are threefold. First, a vulnerability detection framework, which supports se- curity property specification, potential vulnerability detection, and dynamic verification, is proposed. Second, an investigation of test data generation for dynamic verification is conducted. Third, the concept of reducing security property verification to reachability is introduced. m},
  file          = {:article\\Towards the automation of vulnerability detection in source code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9780494672457},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  number        = {December 2009},
}

@Article{Saxena2010,
  author        = {Saxena, Prateek and Akhawe, Devdatta and Hanna, Steve and Mao, Feng and McCamant, Stephen and Song, Dawn},
  title         = {{A Symbolic Execution Framework for JavaScript}},
  journal       = {2010 IEEE Symposium on Security and Privacy},
  year          = {2010},
  pages         = {513--528},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/SP.2010.38},
  file          = {:article\\A Symbolic Execution Framework for JavaScript.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4244-6894-2},
  keywords      = {-web security,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,string decision,symbolic execution,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5504700},
}

@Article{Bishop2010,
  author        = {Bishop, Matt and Howard, Damien and Engle, Sophie and Whalen, Sean},
  title         = {{A Taxonomy of Buffer Overflow Preconditions}},
  journal       = {Security},
  year          = {2010},
  volume        = {9},
  number        = {July 2006},
  pages         = {1--18},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Taxonomy of Buffer Overflow Preconditions.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {arrays,binary,buffer overflows,first select,index terms,machine learning,predicte,program verification,protection mechanisms,second select,security,security privacy,software,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,vulnerabilities,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey,web},
  url           = {http://www.cs.ucdavis.edu/research/tech-reports/2010/CSE-2010-1.pdf},
}

@Article{Zimmermann2010,
  author        = {Zimmermann, Thomas and Nagappan, Nachiappan and Williams, Laurie},
  title         = {{Searching for a needle in a haystack: Predicting security vulnerabilities for Windows Vista}},
  journal       = {ICST 2010 - 3rd International Conference on Software Testing, Verification and Validation},
  year          = {2010},
  pages         = {421--428},
  __markedentry = {[ccc:6]},
  abstract      = {Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally {\&}{\#}x201C;searching for a needle in a haystack.{\&}{\#}x201D; In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall.},
  doi           = {10.1109/ICST.2010.32},
  file          = {:article\\Searching for a needle in a haystack Predicting security vulnerabilities for Windows Vista.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9780769539904},
  issn          = {2159-4848},
  keywords      = {Churn,Complexity,Coverage,Dependencies,Metrics,Organizational structure,Prediction,Vulnerabilities,binary,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important,web},
}

@Article{Jovanovic2010,
  author        = {Jovanovic, Nenad and Kruegel, Christopher and Kirda, Engin},
  title         = {{Static analysis for detecting taint-style vulnerabilities in web applications}},
  journal       = {Journal of Computer Security},
  year          = {2010},
  volume        = {18},
  number        = {5},
  pages         = {861--907},
  __markedentry = {[ccc:6]},
  abstract      = {The number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition to the taint analysis at the core of our engine, we employ a precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by em- ploying an iterative two-phase algorithm for fast and precise resolution of file inclusions. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be easily applied to the de- tection of vulnerability types such as SQL injection, cross-site scripting (XSS), and command injection. We implemented the presented concepts in Pixy, a high-precision static analysis tool aimed at detecting cross-site scripting and SQL injection vulnerabilities in PHP programs. To demonstrate the effectiveness of our techniques, we analyzed a number of popular, open-source web applications and discovered hun- dreds of previously unknown vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.},
  doi           = {10.3233/JCS-2009-0385},
  file          = {:article\\Static analysis for detecting taint-style vulnerabilities in web applications.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {0926227X},
  keywords      = {PHP,Program analysis,SQL injection,alias analysis,binary,cross-site scripting,data flow analysis,first select,scripting languages security,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web,web application security},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Wang2010,
  author        = {Wang, Tielei and Wei, Tao and Gu, Guofei and Zou, Wei},
  title         = {{TaintScope: A checksum-aware directed fuzzing tool for automatic software vulnerability detection}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2010},
  pages         = {497--512},
  __markedentry = {[ccc:6]},
  abstract      = {Fuzz testing has proven successful in finding security vulnerabilities in large programs. However, traditional fuzz testing tools have a well-known common drawback: they are ineffective if most generated malformed inputs are rejected in the early stage of program running, especially when target programs employ checksum mechanisms to verify the integrity of inputs. In this paper, we present TaintScope, an automatic fuzzing system using dynamic taint analysis and symbolic execution techniques, to tackle the above problem. TaintScope has several novel contributions: 1) TaintScope is the first checksum-aware fuzzing tool to the best of our knowledge. It can identify checksum fields in input instances, accurately locate checksum-based integrity checks by using branch profiling techniques, and bypass such checks via control flow alteration. 2) TaintScope is a directed fuzzing tool working at X86 binary level (on both Linux and Window). Based on fine-grained dynamic taint tracing, TaintScope identifies which bytes in a well-formed input are used in security-sensitive operations (e.g., invoking system/library calls) and then focuses on modifying such bytes. Thus, generated inputs are more likely to trigger potential vulnerabilities. 3) TaintScope is fully automatic, from detecting checksum, directed fuzzing, to repairing crashed samples. It can fix checksum values in generated inputs using combined concrete and symbolic execution techniques. We evaluate TaintScope on a number of large real-world applications. Experimental results show that TaintScope can accurately locate the checksum checks in programs and dramatically improve the effectiveness of fuzz testing. TaintScope has already found 27 previously unknown vulnerabilities in several widely used applications, including Adobe Acrobat, Google Picasa, Microsoft Paint, and ImageMagick. Most of these severe vulnerabilities have been confirmed by Secunia and oCERT, and assigned CVE identifiers (such as CVE-2009-1882, CVE-200-$\backslash$n-$\backslash$n9-2688). Corresponding patches from vendors are released or in progress based on our reports.},
  doi           = {10.1109/SP.2010.37},
  file          = {:article\\TaintScope A checksum-aware directed fuzzing tool for automatic software vulnerability detection.pdf:pdf},
  isbn          = {9780769540351},
  issn          = {10816011},
  keywords      = {Dynamic taint analysis,Fuzzirig,Symbolic execution,binary,fuzz,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,web},
}

@Article{Lei2010,
  author        = {Lei, Bin and Li, Xuandong and Liu, Zhiming and Morisset, Charles and Stolz, Volker},
  title         = {{Robustness testing for software components}},
  journal       = {Science of Computer Programming},
  year          = {2010},
  volume        = {75},
  number        = {10},
  pages         = {879--897},
  __markedentry = {[ccc:6]},
  abstract      = {Component-based development allows one to build software from existing components and promises to improve software reuse and reduce costs. For critical applications, the user of a component must ensure that it fits the requirements of the application. To achieve this, testing is a well-suited means when the source code of the components is not available. Robustness testing is a testing methodology to detect the vulnerabilities of a component under unexpected inputs or in a stressful environment. As components may fail differently in different states, we use a state machine based approach to robustness testing. First, a set of paths is generated to cover transitions of the state machine, and it is used by the test cases to bring the component into a specific control state. Second, method calls with invalid inputs are fed to the component in different states to test the robustness. By traversing the paths, the test cases cover more states and transitions compared to stateless API testing. We apply our approach to several components, including open source software, and compare our results with existing approaches. ?? 2010 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.scico.2010.02.005},
  file          = {:article\\Robustness testing for software components.pdf:pdf},
  issn          = {01676423},
  keywords      = {Component,Contract,Robustness testing,State machine,binary,fuzz,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2010.02.005},
}

@Article{Stuart2010,
  author        = {Stuart, Colleen and Science, Computer},
  title         = {{Detecting and Preventing Control-Flow Hijacking Attacks in Commodity Software}},
  year          = {2010},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/UUST.1980.1158402},
  file          = {:article\\Detecting and Preventing Control-Flow Hijacking Attacks in Commodity Software.pdf:pdf},
  isbn          = {2777777777777},
  keywords      = {binary,malware,obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,malware,obfuscate,predicte,web},
}

@Book{Jajodia2010,
  title         = {{Cyber situational awareness: advances in information security}},
  year          = {2010},
  author        = {Jajodia, Sushil and Liu, Peng and Swarup, Vipin and Wang, Cliff},
  __markedentry = {[ccc:6]},
  abstract      = {Sushil Jajodia Peng Liu Vipin Swarup Cliff Wang Barford P, Dacier M, Dietterich TG et al (2010) Cyber SA: situational awareness for cyber defense. In: Jajodia S, Liu P, Swarup V, Wang C (eds) Cyber situational awareness: advances in infor- mation security 46. Springer, Berlin, pp 3–13. doi:10.1007/978- 1-4419-0140-8{\_}1 Branlat},
  doi           = {doi:10.1007/978- 1-4419-0140-8_1},
  file          = {:article\\Cyber situational awareness advances in information security.pdf:pdf},
  isbn          = {9780387887746},
  keywords      = {binary,book,machine learning,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,book,machine learning,predicte,web},
}

@Article{Huynh2010,
  author        = {Huynh, Toan and Miller, James},
  title         = {{An empirical investigation into open source web applications' implementation vulnerabilities}},
  journal       = {Empirical Software Engineering},
  year          = {2010},
  volume        = {15},
  number        = {5},
  pages         = {556--576},
  __markedentry = {[ccc:6]},
  abstract      = {AbstractCurrent web applications have many inherent vulnerabilities; in fact, in 2008, over 63{\%} of all documented vulnerabilities are for web applications. While many approaches have been proposed to address various web application vulnerability issues, there has not been a study to investigate whether these vulnerabilities share any common properties. In this paper, we use an approach similar to the Goal-Question-Metric approach to empirically investigate four questions regarding open source web applications vulnerabilities: What proportion of security vulnerabilities in web applications can be considered as Are these vulnerabilities the result of interactions between web applications and external systems? What is the proportion of vulnerable lines of code within a web application? Are implementation vulnerabilities caused by implicit or explicit data flows? The results from the investigation show that implementation vulnerabilities dominate. They are caused through interactions between web applications and external systems. Furthermore, these vulnerabilities only contain explicit data flows, and are limited to relatively small sections of the source code.},
  doi           = {10.1007/s10664-010-9131-y},
  file          = {:article\\An empirical investigation into open source web applications' implementation vulnerabilities.pdf:pdf},
  groups        = {vice-important},
  issn          = {13823256},
  keywords      = {Classification of vulnerabilities,Empirical evaluation,Injection,Security,Vulnerability,Web applications,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,source code,source code-vice important,web},
}

@Article{Nguyen2010,
  author        = {Nguyen, Viet Hung and Tran, Le Minh Sang},
  title         = {{Predicting vulnerable software components with dependency graphs}},
  journal       = {Proceedings of the 6th International Workshop on Security Measurements and Metrics - MetriSec '10},
  year          = {2010},
  pages         = {1},
  __markedentry = {[ccc:6]},
  abstract      = {Security metrics and vulnerability prediction for software have gained a lot of interests from the community. Many software security metrics have been proposed e.g., complex- ity metrics, cohesion and coupling metrics. In this paper, we propose a novel code metric based on dependency graphs to predict vulnerable components. To validate the efficiency of the proposed metric, we conduct a prediction model which targets the JavaScript Engine of Firefox. In this experiment, our predictionmodel has obtained a very good result in term of accuracy and recall rates. This empirical result is a good evidence showing dependency graphs are also a good option for early indicating vulnerability. Categories},
  doi           = {10.1145/1853919.1853923},
  file          = {:article\\Predicting vulnerable software components with dependency graphs.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781450303408},
  keywords      = {first select,fuzz,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,fuzz,machine learning,predicte,source code,source code-vice important},
  url           = {http://portal.acm.org/citation.cfm?doid=1853919.1853923},
}

@Article{Menzies2010,
  author        = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ayşe},
  title         = {{Defect prediction from static code features: Current results, limitations, new approaches}},
  journal       = {Automated Software Engineering},
  year          = {2010},
  volume        = {17},
  number        = {4},
  pages         = {375--407},
  __markedentry = {[ccc:6]},
  abstract      = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection AUC(pd, pf) ; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize AUC(effort, pd) find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
  doi           = {10.1007/s10515-010-0069-5},
  file          = {:article\\Defect prediction from static code features Current results, limitations, new approaches.pdf:pdf},
  groups        = {vice-important},
  issn          = {09288910},
  keywords      = {Defect prediction,Static code features,Which,binary,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important},
}

@PhdThesis{李根2010a,
  author        = {李根},
  title         = {基于动态测试用例生成的二进制软件缺陷自动发掘技术研究.},
  year          = {2010},
  __markedentry = {[ccc:6]},
  file          = {:article\\基于动态测试用例生成的二进制软件缺陷自动发掘技术研究.pdf:pdf},
  keywords      = {fuzz},
  mendeley-tags = {fuzz},
}

@Article{Permissions2010,
  author        = {Permissions, Unix},
  title         = {{Secure Coding in C and C ++}},
  year          = {2010},
  volume        = {7},
  number        = {49},
  pages         = {1--19},
  __markedentry = {[ccc:6]},
  file          = {:article\\Secure Coding in C and C.pdf:pdf},
  isbn          = {9780321822130},
  keywords      = {survey},
  mendeley-tags = {survey},
}

@Article{Khalid2010,
  author        = {Khalid, Shamsul Kamal Ahmad and Zimmermann, Jacob and Corney, Diane and Fidge, Colin},
  title         = {{Automatic generation of assertions to detect potential security vulnerabilities in C programs that use union and pointer types}},
  journal       = {Proceedings - 2010 4th International Conference on Network and System Security, NSS 2010},
  year          = {2010},
  pages         = {351--356},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/NSS.2010.63},
  file          = {:article\\Automatic generation of assertions to detect potential security vulnerabilities in C programs that use union and.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9780769541594},
  keywords      = {C,Polymorphic types,Program analysis,Runtime assertions,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Venkatesan2010,
  author        = {Venkatesan, S. and Chellappan, C. and Vengattaraman, T. and Dhavachelvan, P. and Vaish, Anurika},
  title         = {{Advanced mobile agent security models for code integrity and malicious availability check}},
  journal       = {Journal of Network and Computer Applications},
  year          = {2010},
  volume        = {33},
  number        = {6},
  pages         = {661--671},
  __markedentry = {[ccc:6]},
  abstract      = {Mobile agent technology is an emerging paradigm in distributed computing environment and it holds a potential status in the relevant research field due to its unique capabilities like flexibility, dynamic customization and robust interaction in unreliable networks. But the limited security perspectives and shortfalls of the mobile agent environments degrade its usage in a variety of application domains. Even though some of the protection models are available for protecting the environments, they are not efficient in handling the security issues. To make the mobile agent environment secure, this paper proposed advanced models to improve the efficiency of the existing Malicious Identification Police model for scanning the incoming agent to detect the malicious activities and to overcome the availability of vulnerabilities in the existing Root Canal algorithm for code integrity checks. The MIP model is extended with the policy to differentiate the agent owners in the distributed environment and the Root Canal algorithm is improved as eXtended Root Canal algorithm. The experimental results of the advanced models show that though these mechanisms take more time complexity than the existing malicious identification police model and Root Canal model, these models are efficient in protecting the agent code integrity and scanning the agent for malicious activities. Also the new models possess less time complexity compared to the other related existing models in the secure mobile agent environment. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.jnca.2010.03.010},
  file          = {:article\\Advanced mobile agent security models for code integrity and malicious availability check.pdf:pdf},
  issn          = {10848045},
  keywords      = {Attack code identification,Code integrity check,Mobile agent,Mobile agent platform protection,Mobile agent security,notcare,obfuscate},
  mendeley-tags = {notcare,obfuscate},
  publisher     = {Elsevier},
  url           = {http://dx.doi.org/10.1016/j.jnca.2010.03.010},
}

@PhdThesis{Sullivan2010,
  author        = {Sullivan, P ´ adraig O '},
  title         = {{PREVENTING BUFFER OVERFLOWS WITH BINARY REWRITING}},
  year          = {2010},
  __markedentry = {[ccc:6]},
  file          = {:article\\PREVENTING BUFFER OVERFLOWS WITH BINARY REWRITING.pdf:pdf},
  keywords      = {binary,stat,static analysi,static analysis},
  mendeley-tags = {binary},
}

@Article{Geer2011,
  author        = {Geer, Daniel E.},
  title         = {{Correlation is not causation}},
  journal       = {IEEE Security and Privacy},
  year          = {2011},
  volume        = {9},
  number        = {2},
  pages         = {93--94},
  __markedentry = {[ccc:6]},
  abstract      = {Vulnerability reporting rates don't seem overtly predictive of data loss events. Alternate, perhaps complex, hypotheses are needed if there is to be any further argument that data loss has as a causal component the existence of software vulnerabilities in the aggregate. This installment of For Good Measure looks deep into the statistics behind vulnerability reporting.},
  doi           = {10.1109/MSP.2011.26},
  file          = {:article\\Correlation is not causation.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0090-0036 (Print)$\backslash$n0090-0036 (Linking)},
  issn          = {15407993},
  keywords      = {data loss,first select,predicte,privacy,second select,security,source code,source code-important,source code-vice important,vulnerabilities},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important},
  pmid          = {9585761},
}

@Article{杨轶2011,
  author        = {杨轶 and 苏璞睿 and 应凌云 and 冯登国},
  title         = {基于行为依赖特征的恶意代码相似性比较方法},
  journal       = {软件学报},
  year          = {2011},
  volume        = {22},
  number        = {10},
  pages         = {2438--2453},
  __markedentry = {[ccc:6]},
  abstract      = {恶意代码相似性比较是恶意代码分析和检测的基础性工作之一，现有方法主要是基于代码结构或行为序 列进行比较．但恶意代码编写者常采用代码混淆、程序加壳等手段对恶意代码进行处理，导致传统的相似性比较方 法失效．提出了一种基于行为之间控制依赖关系和数据依赖关系的恶意代码相似性比较方法，该方法利用动态污点 传播分析识别恶意行为之间的依赖关系，然后，以此为基础构造控制依赖图和数据依赖图，根据两种依赖关系进行恶 意代码的相似性比较．该方法充分利用了恶意代码行为之间内在的关联性，提高了比较的准确性，具有较强的抗干扰能力：通过循环消除、垃圾行为删除等方法对依赖图进行预处理，降低了相似性比较算法的复杂度，加快了比较速度． 实验结果表明，与现有方法相比，该方法的准确性和抗干扰能力均呈现明显优势．},
  file          = {:article\\基于行为依赖特征的恶意代码相似性比较方法.pdf:pdf},
  keywords      = {malware},
  mendeley-tags = {malware},
}

@Article{Yamaguchi2011,
  author        = {Yamaguchi, Fabian and Lindner, Felix and Rieck, Konrad},
  title         = {{Vulnerability Extrapolation: Assisted Discovery of Vulnerabilities Using Machine Learning}},
  journal       = {Proceedings of the 5th USENIX Conference on Offensive Technologies},
  year          = {2011},
  pages         = {13},
  __markedentry = {[ccc:6]},
  abstract      = {Rigorous identiﬁcation of vulnerabilities in program code is a key to implementing and operating secure systems. Unfortunately, only some types of vulnerabilities can be detected automatically. While techniques from software testing can accelerate the search for security ﬂaws, in the general case discovery of vulnerabilities is a tedious process that requires signiﬁcant expertise and time. In this paper, we propose a method for assisted discovery of vulnerabilities in source code. Our method proceeds by embedding code in a vector space and automatically determining API usage patterns using machine learning. Starting from a known vulnerability, these patterns can be exploited to guide the auditing of code and to identify potentially vulnerable code with similar characteristics—a process we refer to as vulnerability extrapolation. We empirically demonstrate the capabilities of our method in different experiments. In a case study with the library FFmpeg, we are able to narrow the search for interesting code from 6,778 to 20 functions and discover two security ﬂaws, one being a known ﬂaw and the other constituting a zero-day vulnerability.},
  file          = {:article\\Vulnerability Extrapolation Assisted Discovery of Vulnerabilities Using Machine Learning.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,machine learning,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,machine learning,second select,source code,source code-important,source code-vice important},
  url           = {http://dl.acm.org/citation.cfm?id=2028052.2028065},
}

@Article{Austin2011,
  author        = {Austin, Andrew and Williams, Laurie},
  title         = {{One Technique is Not Enough: A Comparison of Vulnerability Discovery Techniques}},
  journal       = {2011 International Symposium on Empirical Software Engineering and Measurement},
  year          = {2011},
  pages         = {97--106},
  __markedentry = {[ccc:6]},
  abstract      = {Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. To ease this difficulty, many different types of techniques have been devised to aid developers in vulnerability discovery. The goal of this research is to improve vulnerability detection by comparing the effectiveness of vulnerability discovery techniques and to provide specific recommendations to improve vulnerability discovery with these techniques. We conducted a case study on two electronic health record systems to compare four discovery techniques: systematic and exploratory manual penetration testing, static analysis, and automated penetration testing. In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered almost no individual vulnerabilities with multiple discovery techniques. We also found that systematic manual penetration testing found the most design flaws, while static analysis found the most implementation bugs. Finally, we found the most effective vulnerability discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing. These results suggest that if one has limited time to preform vulnerability discovery one should conduct automated penetration testing to discover implementation bugs and systematic manual penetration testing to discover design flaws.},
  doi           = {10.1109/ESEM.2011.18},
  file          = {:article\\One Technique is Not Enough A Comparison of Vulnerability Discovery Techniques.pdf:pdf},
  isbn          = {978-0-7695-4604-9},
  issn          = {1938-6451},
  keywords      = {blackbox testing,penetration testing,security,stat,static analysi,static analysis,vulnerability,whitebox testing},
}

@Article{Li2011,
  author        = {Li, Guodong and Ghosh, Indradeep and Rajan, Sreeranga P.},
  title         = {{KLOVER: A symbolic execution and automatic test generation tool for C++ programs}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2011},
  volume        = {6806 LNCS},
  pages         = {609--615},
  __markedentry = {[ccc:6]},
  abstract      = {We present the first symbolic execution and automatic test generation tool for C++ programs. First we describe our effort in extending an existing symbolic execution tool for C programs to handle C++ programs. We then show how we made this tool generic, efficient and usable to handle real-life industrial applications. Novel features include extended symbolic virtual machine, library optimization for C and C++, object-level execution and reasoning, interfacing with specific type of efficient solvers, and semi-automatic unit and component testing. This tool is being used to assist the validation and testing of industrial software as well as publicly available programs written using the C++ language.},
  annote        = {将其转换为llvm中间码，再处理},
  doi           = {10.1007/978-3-642-22110-1_49},
  file          = {:article\\KLOVER A symbolic execution and automatic test generation tool for C programs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9783642221095},
  issn          = {03029743},
  keywords      = {first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Coordinators2011,
  author        = {Coordinators, Project and Editor, Document and Top, Sans and Dangerous, Most and Errors, Software and Top, The and Enumeration, Common Weakness and Security, Homeland and Cyber, National and Division, Security and Cwe, The and Mitigations, Monster and Listing, Brief and View, Category-based},
  title         = {{2011 CWE/SANS Top 25 Most Dangerous Software Errors}},
  journal       = {Most},
  year          = {2011},
  __markedentry = {[ccc:6]},
  file          = {:article\\2011 CWESANS Top 25 Most Dangerous Software Errors.pdf:pdf},
  keywords      = {survey},
  mendeley-tags = {survey},
}

@Article{Ramos2011,
  author        = {Ramos, David A. and Engler, Dawson R.},
  title         = {{Practical, low-effort equivalence verification of real code}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2011},
  volume        = {6806 LNCS},
  pages         = {669--685},
  __markedentry = {[ccc:6]},
  abstract      = {uc-klee},
  doi           = {10.1007/978-3-642-22110-1_55},
  file          = {:article\\Practical, low-effort equivalence verification of real code.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9783642221095},
  issn          = {03029743},
  keywords      = {binary,first select,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,source code,source code-vice important},
}

@Article{Hu2011,
  author        = {Hu, Jian Jim and Wen, Qiaoyan and Sui, Ai Fen},
  title         = {{An efficient code audit method for accurately detecting security vulnerabilities in source codes}},
  journal       = {International Conference on Communication Technology Proceedings, ICCT},
  year          = {2011},
  pages         = {698--702},
  __markedentry = {[ccc:6]},
  abstract      = {Currently code security audit/review or white-box security test is widely used to analyze the source codes and detect security vulnerabilities. In this paper we describe a more efficient code security audit method based on the reference tree with security properties which building on all manipulable entries in source codes. This method can The method in this invention can greatly reduce false positives and provides an efficient solution for automated secure auditing on source codes by only checking the exploitable security flows.},
  doi           = {10.1109/ICCT.2011.6157966},
  file          = {:article\\An efficient code audit method for accurately detecting security vulnerabilities in source codes.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781612843070},
  keywords      = {first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Jones2011,
  author        = {Jones, Micah and Hamlen, Kevin W.},
  title         = {{A service-oriented approach to mobile code security}},
  journal       = {Procedia Computer Science},
  year          = {2011},
  volume        = {5},
  pages         = {531--538},
  __markedentry = {[ccc:6]},
  abstract      = {Client software for modern service-oriented web architectures is often implemented as mobile code applets made available by service-providers. Protecting clients from malicious mobile code is therefore an important concern in these architectures; however, the burden of security enforcement is typically placed entirely on the client. This approach violates the service-oriented paradigm. A method of realizing mobile code security as a separate service in a service-oriented web architecture is proposed. The security service performs in-lined reference monitoring of untrusted Java binaries on-demand for client-specified security policies. An XML format for specifying these policies is outlined, and preliminary experiments demonstrate the feasibility of the approach. ?? 2011 Published by Elsevier Ltd.},
  doi           = {10.1016/j.procs.2011.07.069},
  file          = {:article\\A service-oriented approach to mobile code security.pdf:pdf},
  issn          = {18770509},
  keywords      = {In-lined reference monitors,Mobile code,Runtime monitors,binary,network,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,network,web},
  url           = {http://dx.doi.org/10.1016/j.procs.2011.07.069},
}

@PhdThesis{Joh2011,
  author        = {Joh, HyunChul},
  title         = {{Quantitative analyses of software vulnerabilities}},
  year          = {2011},
  __markedentry = {[ccc:6]},
  abstract      = {There have been numerous studies addressing computer security and software vulnerability management. Most of the time, they have taken a qualitative perspective. In many other disciplines, quantitative analyses have been indispensable for performance assessment, metric measurement, functional evaluation, or statistical modeling. Quantitative approaches can also help to improve software risk management by providing guidelines obtained by using actual data-driven analyses for optimal allocations of resources for security testing, scheduling, and development of security patches. Quantitative methods allow objective and more accurate estimates of future trends than qualitative manners only because a quantitative approach uses real datasets with statistical methods which have proven to be a very powerful prediction approach in several research fields. A quantitative methodology makes it possible for end-users to assess the risks posed by vulnerabilities in software systems, and potential breaches without getting burdened by details of every individual vulnerability. At the moment, quantitative risk analysis in information security systems is still in its infancy stage. However, recently, researchers have started to explore various software vulnerability related attributes quantitatively as the vulnerability datasets have now become large enough for statistical analyses. In this dissertation, quantitative analysis is presented dealing with i) modeling vulnerability discovery processes in major Web servers and browsers, ii) relationship between the performance of S-shaped vulnerability discovery models and the skew in vulnerability datasets examined, iii) linear vulnerability discovery trends in multi-version software systems, iv) periodic behavior in weekly exploitation and patching of vulnerabilities as well as long term vulnerability discovery process, and v) software security risk evaluation with respect to the vulnerability lifecycle and CVSS. Results show good superior vulnerability discovery model fittings and reasonable prediction capabilities for both time-based and effort-based models for datasets from Web servers and browsers. Results also show that AML and Gamma distribution based models perform better than other S-shaped models with skewed left and right datasets respectively. We find that code sharing among the successive versions cause a linear discovery pattern. We establish that there are indeed long and short term periodic patterns in software vulnerability related activities which have been only vaguely recognized by the security researchers. Lastly, a framework for software security risk assessment is proposed which can allow a comparison of software systems in terms of the risk and potential approaches for optimization of remediation.},
  booktitle     = {ProQuest Dissertations and Theses},
  file          = {:article\\Quantitative analyses of software vulnerabilities.pdf:pdf},
  isbn          = {9781267097033},
  keywords      = {Applied sciences,Communication and the arts,Information security systems,Modeling,Quantitative analysis,Risk,Security,Software vulnerabilities,Vulnerability discovery process,predicte,web},
  mendeley-tags = {predicte,web},
  url           = {http://search.proquest.com/docview/916925550?accountid=44888 LA  - English},
}

@Article{Hall2011,
  author        = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
  title         = {{A Systematic Review of Fault Prediction Performance in Software Engineering}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2011},
  volume        = {38},
  number        = {6},
  pages         = {1276--1304},
  __markedentry = {[ccc:6]},
  abstract      = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs and improve the quality of software. Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence the performance of fault prediction models. Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modelling techniques such as Na{\"{i}}ve Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology and performance comprehensively.},
  doi           = {10.1109/TSE.2011.103},
  file          = {:article\\A Systematic Review of Fault Prediction Performance in Software Engineering.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781612081656},
  issn          = {0098-5589},
  keywords      = {first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,survey,web},
}

@Article{Torchiano2011,
  author        = {Torchiano, Marco and Ricca, Filippo and Marchetto, Alessandro},
  title         = {{Are web applications more defect-prone than desktop applications?}},
  journal       = {International Journal on Software Tools for Technology Transfer},
  year          = {2011},
  volume        = {13},
  number        = {2},
  pages         = {151--166},
  __markedentry = {[ccc:6]},
  abstract      = {A lot of effort in the literature has been devoted to define and validate$\backslash$nfault taxonomies and models related to different domains, e.g. Service-oriented$\backslash$nand Web systems, and properties, e.g. software quality and security.$\backslash$nNevertheless, few attempts were carried out to understand the specific$\backslash$nnature of Web bugs and their distribution among the layers of a typical$\backslash$napplication{\^{a}}s architecture{\^{a}}presentation layer, business logic$\backslash$nand data logic. In this paper, we present an experimental investigation$\backslash$naimed at studying the distribution of bugs among different layers$\backslash$nof Web and Desktop applications. The experiment follows a well-defined$\backslash$nprocedure executed by six bachelor students. Overall, the analysis$\backslash$nconsiders 1,472 bugs belonging to 20 different applications. The$\backslash$nexperimental study provides strong evidence that the presentation$\backslash$nlayer in Web applications is more defect-prone than the analogous$\backslash$nlayer in Desktop applications. An additional factor influencing the$\backslash$ndistribution of defects is represented by the application domain.},
  doi           = {10.1007/s10009-010-0182-6},
  file          = {:article\\Are web applications more defect-prone than desktop applications.pdf:pdf},
  groups        = {vice-important},
  issn          = {14332779},
  keywords      = {Defect location,Distribution of defects,Empirical evaluation,Web and desktop applications,Well-defined experimental procedure,first select,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,predicte,source code,source code-vice important,web},
}

@Article{Catal2011,
  author        = {Catal, Cagatay},
  title         = {{Software fault prediction: A literature review and current trends}},
  journal       = {Expert Systems with Applications},
  year          = {2011},
  volume        = {38},
  number        = {4},
  pages         = {4626--4636},
  __markedentry = {[ccc:6]},
  abstract      = {Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.eswa.2010.10.024},
  file          = {:article\\Software fault prediction A literature review and current trends.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0878353259},
  issn          = {09574174},
  keywords      = {Automated fault prediction models,Expert systems,Machine learning,Software engineering,Software quality engineering,Statistical methods,binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.eswa.2010.10.024},
}

@Article{Heckman2011,
  author        = {Heckman, Sarah and Williams, Laurie},
  title         = {{A systematic literature review of actionable alert identification techniques for automated static code analysis}},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {4},
  pages         = {363--387},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice. Objective: The goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT). Method: Relevant studies about AAITs were gathered via a systematic literature review. Results: We selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics. Conclusion: The selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program's history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2010.12.007},
  file          = {:article\\A systematic literature review of actionable alert identification techniques for automated static code analys.pdf:pdf},
  groups        = {vice-important},
  isbn          = {0950-5849},
  issn          = {09505849},
  keywords      = {Actionable alert identification,Actionable alert prediction,Automated static analysis,Systematic literature review,Unactionable alert mitigation,Warning prioritization,first select,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,machine learning,predicte,source code,source code-vice important,survey,web},
}

@Article{Cadar2011,
  author        = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and Pasareanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
  title         = {{Symbolic execution for software testing in practice: preliminary assessment}},
  journal       = {2011 33rd International Conference on Software Engineering (ICSE)},
  year          = {2011},
  pages         = {1066--1071},
  __markedentry = {[ccc:6]},
  abstract      = {We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
  doi           = {10.1145/1985793.1985995},
  file          = {:article\\Symbolic execution for software testing in practice preliminary assessment.pdf:pdf},
  isbn          = {978-1-4503-0445-0},
  issn          = {0270-5257},
  keywords      = {binary,dynamic test generation,fuzz,generalized symbolic execution,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {binary,fuzz,survey,web},
}

@Article{Heckman2011a,
  author        = {Heckman, Sarah and Williams, Laurie},
  title         = {{A systematic literature review of actionable alert identification techniques for automated static code analysis}},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {4},
  pages         = {363--387},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice. Objective: The goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT). Method: Relevant studies about AAITs were gathered via a systematic literature review. Results: We selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics. Conclusion: The selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program's history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2010.12.007},
  file          = {:article\\A systematic literature review of actionable alert identification techniques for automated static code ana(2).pdf:pdf},
  groups        = {vice-important},
  isbn          = {0950-5849},
  issn          = {09505849},
  keywords      = {Actionable alert identification,Actionable alert prediction,Automated static analysis,Systematic literature review,Unactionable alert mitigation,Warning prioritization,binary,first select,fuzz,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2010.12.007},
}

@Article{Yu2011,
  author        = {Yu, Fang and Alkhalaf, Muath and Bultan, Tevfik},
  title         = {{Patching vulnerabilities with sanitization synthesis}},
  journal       = {2011 33rd International Conference on Software Engineering (ICSE)},
  year          = {2011},
  pages         = {251--260},
  __markedentry = {[ccc:6]},
  abstract      = {We present automata-based static string analysis techniques that automatically generate sanitization statements for patching vulnerable web applications. Our approach consists of three phases: Given an attack pattern we first conduct a vulnerability analysis to identify if strings that match the attack pattern can reach the security-sensitive functions. Next, we compute vulnerability signatures that characterize all input strings that can exploit the discovered vulnerability. Given the vulnerability signatures, we then construct sanitization statements that 1) check if a given input matches the vulnerability signature and 2) modify the input in a minimal way so that the modified input does not match the vulnerability signature. Our approach is capable of generating relational vulnerability signatures (and corresponding sanitization statements) for vulnerabilities that are due to more than one input.},
  annote        = {the security-sensitive functions.},
  doi           = {10.1145/1985793.1985828},
  file          = {:article\\Patching vulnerabilities with sanitization synthesis.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4503-0445-0},
  issn          = {0270-5257},
  keywords      = {automata,first select,sanitization synthesis,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,string analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Schryen2011,
  author        = {Schryen, Guido},
  title         = {{Is open source security a myth?}},
  journal       = {Communications of the ACM},
  year          = {2011},
  volume        = {54},
  number        = {5},
  pages         = {130},
  __markedentry = {[ccc:6]},
  abstract      = {What does vulnerability and patch data say?},
  doi           = {10.1145/1941487.1941516},
  file          = {:article\\Is open source security a myth.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0001-0782},
  issn          = {00010782},
  keywords      = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,second select,source code,source code-important,source code-vice important,web},
  pmid          = {60863987},
}

@Article{Shin2011,
  author        = {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},
  title         = {{Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2011},
  volume        = {37},
  number        = {6},
  pages         = {772--787},
  __markedentry = {[ccc:6]},
  abstract      = {Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects. 2006 IEEE.},
  doi           = {10.1109/TSE.2010.81},
  file          = {:article\\Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {2009030060},
  issn          = {00985589},
  keywords      = {Fault prediction,binary,first select,machine learning,predicte,second select,software metrics,software security,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerability prediction,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Ali2011,
  author        = {Ali, Abdul Bashah Mat and Shakhatreh, Ala Yaseen Ibrahim and Abdullah, Mohd Syazwan and Alostad, Jasem},
  title         = {{SQL-injection vulnerability scanning tool for automatic creation of SQL-injection attacks}},
  journal       = {Procedia Computer Science},
  year          = {2011},
  volume        = {3},
  pages         = {453--458},
  __markedentry = {[ccc:6]},
  abstract      = {Securing the web against frequent cyber attacks is a big concern as attackers usually intend to snitch private information, financial information, deface and damages websites to prove their hacking capabilities. This type of vandalism may drive many corporations that conduct their business through the web to suffer financial and reputation damages. One of the most dangerous cyber attacks is the Structured Query Language (SQL)-injection attack, whereby this type of attack can be launched through the web browsers. The vulnerability of SQL-injection attack can be attributed to inappropriate programming practice by the website developers, which leaves a lot of doors widely open for the attackers to exploit these and gaining access to confidential information that resides in the website server databases. In order to address this vulnerability, it must be feasible to detect the vulnerability and enhance the coding structure of the website to avoid being an easy victim to this type of cyber attacks. Detecting the SQL-injection vulnerability requires the development of a powerful tool that can automatically create SQL-injection attacks using efficient features (different attacking patters) to detect the vulnerability of the websites. This paper discuss the development of a new web scanning (MySQLlInjector) tool with enhanced features that will be able to conduct efficient penetration test on PHP (started as Personal Home Page but now widely used as Hypertext Preprocesses) base d websites to detect SQL injection vulnerabilities. This tool will automate the penetration test process, to make it easy even for those who are not aware familiar about hacking techniques. ?? 2010 Published by Elsevier Ltd.},
  doi           = {10.1016/j.procs.2010.12.076},
  file          = {:article\\SQL-injection vulnerability scanning tool for automatic creation of SQL-injection attacks.pdf:pdf},
  isbn          = {0604928475},
  issn          = {18770509},
  keywords      = {Database security,SQL injection,SQL vulnerability,predicte,web},
  mendeley-tags = {predicte,web},
  publisher     = {Elsevier},
  url           = {http://dx.doi.org/10.1016/j.procs.2010.12.076},
}

@Article{V??mel2011,
  author        = {V??mel, Stefan and Freiling, Felix C.},
  title         = {{A survey of main memory acquisition and analysis techniques for the windows operating system}},
  journal       = {Digital Investigation},
  year          = {2011},
  volume        = {8},
  number        = {1},
  pages         = {3--22},
  __markedentry = {[ccc:6]},
  abstract      = {Traditional, persistent data-oriented approaches in computer forensics face some limitations regarding a number of technological developments, e.g., rapidly increasing storage capabilities of hard drives, memory-resident malicious software applications, or the growing use of encryption routines, that make an in-time investigation more and more difficult. In order to cope with these issues, security professionals have started to examine alternative data sources and emphasize the value of volatile system information in RAM more recently. In this paper, we give an overview of the prevailing techniques and methods to collect and analyze a computer's memory. We describe the characteristics, benefits, and drawbacks of the individual solutions and outline opportunities for future research in this evolving field of IT security. ?? 2011 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.diin.2011.06.002},
  file          = {:article\\A survey of main memory acquisition and analysis techniques for the windows operating system.pdf:pdf},
  isbn          = {1742-2876},
  issn          = {17422876},
  keywords      = {Live forensics,Memory acquisition,Memory analysis,Memory forensics,Microsoft windows,survey},
  mendeley-tags = {survey},
}

@Article{Johns2011,
  author        = {Johns, Martin},
  title         = {{Code-injection Vulnerabilities in Web Applications — Exemplified at Cross-site Scripting}},
  journal       = {it - Information Technology},
  year          = {2011},
  volume        = {53},
  number        = {5},
  pages         = {256--260},
  __markedentry = {[ccc:6]},
  doi           = {10.1524/itit.2011.0651},
  file          = {:article\\Code-injection Vulnerabilities in Web Applications — Exemplified at Cross-site Scripting.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {1611-2776},
  keywords      = {2,3,4,code injection,d,first select,processors,programming languages,second select,security,software,software engineering,source code,source code-important,source code-vice important,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
  url           = {http://www.degruyter.com/doi/10.1524/itit.2011.0651},
}

@Article{王朝坤2011,
  author        = {王朝坤 and 付军宁 and 王建民 and 余志伟},
  title         = {软件防篡改技术综述.pdf},
  journal       = {计算机研究与发展},
  year          = {2011},
  volume        = {48},
  number        = {6},
  pages         = {923--933},
  __markedentry = {[ccc:6]},
  abstract      = {随着计算机软件的广泛使用，软件安全性问题日益突出．如何设计切实可行的软件保护方案已 成为必须直面的挑战．具有重要的现实意义．近年来，软件防篡改技术作为软件保护的重要手段之一受 到国内外研究者的重视．软件防篡改的目标在于阻止程序中的关键信息被非法修改或使用；检测篡改并 作出适当的响应．针对这两个目标。重点介绍了基于代码混淆的静态防篡改技术和基于检测一响应的动 态防篡改技术，对现有主流的软件防篡改技术进行分类，并分析和讨论了各类方法的优劣和局限性．最 后，总结软件防篡改领域存在的问题．并对其未来可能的发展与研究方向提出建议．},
  file          = {:article\\软件防篡改技术综述.pdf.pdf:pdf},
  keywords      = {stat,static analysi,static analysis,防篡改；软件保护；静态防篡改技术；动态防篡改技术；检剥一响应；代码混淆},
}

@PhdThesis{Mohosina2011,
  author        = {Mohosina, Amatul},
  title         = {{Deserve : a Framework for Detecting Program Security Vulnerability Exploitations}},
  school        = {Queen‟s University},
  year          = {2011},
  __markedentry = {[ccc:6]},
  file          = {:article\\Deserve a Framework for Detecting Program Security Vulnerability Exploitations.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9780494771013},
  keywords      = {binary,first select,fuzz,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,predicte,source code,source code-vice important,web},
}

@Article{Zhang2011,
  author        = {Zhang, Su and Caragea, Doina and Ou, Xinming},
  title         = {{An empirical study on using the national vulnerability database to predict software vulnerabilities}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2011},
  volume        = {6860 LNCS},
  number        = {PART 1},
  pages         = {217--231},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-642-23088-2_15},
  file          = {:article\\An empirical study on using the national vulnerability database to predict software vulnerabilities.pdf:pdf},
  isbn          = {9783642230875},
  issn          = {03029743},
  keywords      = {cyber-security,data mining,machine learning,predicte,vulnerability prediction,web},
  mendeley-tags = {machine learning,predicte,web},
}

@Article{王雷2011,
  author        = {王雷},
  title         = {基于约束分析和模型检测的代码安全漏洞检测方法研究},
  journal       = {计算机研究与发展},
  year          = {2011},
  __markedentry = {[ccc:6]},
  file          = {:article\\基于约束分析和模型检测的代码安全漏洞检测方法研究.caj:caj},
  groups        = {imprortant, vice-important},
  keywords      = {first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{De-Guang2011,
  author        = {KONG, De-Guang and TAN, Xiao-Bin and XI, Hong-Sheng and GONG, Tao and SHUAI, Jian-Mei},
  title         = {{Obfuscated Malware Detection Based on Boosting Multilevel Features}},
  journal       = {Journal of Software},
  year          = {2011},
  volume        = {22},
  number        = {3},
  pages         = {522--533},
  __markedentry = {[ccc:6]},
  doi           = {10.3724/SP.J.1001.2011.03727},
  file          = {:article\\Obfuscated Malware Detection Based on Boosting Multilevel Features.caj:caj},
  issn          = {1000-9825},
  keywords      = {machine learning,obfuscate,stat,static analysi,static analysis,web},
  mendeley-tags = {machine learning,obfuscate,web},
  url           = {http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1001.2011.03727},
}

@Article{Woo2011,
  author        = {Woo, Sung Whan and Joh, Hyunchul and Alhazmi, Omar H. and Malaiya, Yashwant K.},
  title         = {{Modeling vulnerability discovery process in Apache and IIS HTTP servers}},
  journal       = {Computers and Security},
  year          = {2011},
  volume        = {30},
  number        = {1},
  pages         = {50--62},
  __markedentry = {[ccc:6]},
  abstract      = {Vulnerability discovery models allow prediction of the number of vulnerabilities that are likely to be discovered in the future. Hence, they allow the vendors and the end users to manage risk by optimizing resource allocation. Most vulnerability discovery models proposed use the time as an independent variable. Effort-based modeling has also been proposed, which requires the use of market share data. Here, the feasibility of characterizing the vulnerability discovery process in the two major HTTP servers, Apache and IIS, is quantitatively examined using both time and effort-based vulnerability discovery models, using data spanning more than a decade. The data used incorporates the effect of software evolution for both servers. In addition to aggregate vulnerabilities, different groups of vulnerabilities classified using both the error types and severity levels are also examined. Results show that the selected vulnerability discovery models of both types can fit the data of the two HTTP servers very well. Results also suggest that separate modeling for an individual class of vulnerabilities can be done. In addition to the model fitting, predictive capabilities of the two models are also examined. The results demonstrate the applicability of quantitative methods to widely-used products, which have undergone evolution. ?? 2010 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.cose.2010.10.007},
  file          = {:article\\Modeling vulnerability discovery process in Apache and IIS HTTP servers.pdf:pdf},
  issn          = {01674048},
  keywords      = {Quantitative modeling,Risk evaluation,Security,Vulnerability discovery model (VDM),Web server,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2010.10.007},
}

@Article{聂楚江2011,
  author        = {聂楚江 and 赵险峰 and 陈恺 and 韩正清},
  title         = {一种微观漏洞数量预测模型.pdf},
  journal       = {计算机y研究与发展},
  year          = {2011},
  volume        = {48},
  number        = {7},
  pages         = {1279--1287},
  __markedentry = {[ccc:6]},
  abstract      = {全球每年因为软件漏洞造成的损失十分巨大，而软件漏洞分析方法的缺陷使得漏洞本身难以被 发现，因此大家开始对漏洞数量进行预测，预测软件的漏洞数量对信息安全评估有着重要的意义．目前 主要的估算方法是漏洞密度的方法，但此方法仅是宏观范围内估算，并不能反映漏洞软件本身的性质． 提出从软件的微观角度进行软件漏洞数量的估算通过提取软件典型微观参数，从而发现软件漏洞数量 与其微观参数的联系，相比漏洞密度的预测方法具有相当的优势．软件微观漏洞模型在提出漏洞继承假 设的基础上，认为软件的漏洞数量与它的某些微观参数之间存在线性关系。并给出了根据软件微观参数 以及其历史版本漏洞数据预测软件漏洞数量的方法．通过对7款软件进行验证，证明了软件微观漏洞模 型在预测软件漏洞数量时的有效性与准确性．},
  file          = {:article\\一种微观漏洞数量预测模型.pdf.pdf:pdf},
  keywords      = {predicte,漏洞预测；软件分析；漏洞继承；历史漏洞；微观参数},
  mendeley-tags = {predicte},
}

@Article{Zhang2012,
  author        = {Zhang, Fuyuan},
  title         = {{Model Checking as Static Analysis}},
  year          = {2012},
  __markedentry = {[ccc:6]},
  file          = {:article\\Model Checking as Static Analysis.pdf:pdf},
  keywords      = {predicte,stat,static analysi,static analysis},
  mendeley-tags = {predicte},
}

@Article{Bradley2012,
  author        = {Bradley, Mark and Cassez, Franck and Fehnker, Ansgar and Given-Wilson, Thomas and Huuck, Ralf},
  title         = {{High performance static analysis for industry}},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2012},
  volume        = {289},
  pages         = {3--14},
  __markedentry = {[ccc:6]},
  abstract      = {Static source code analysis for software bug detection has come a long way since its early beginnings as a compiler technology. However, with the introduction of more sophisticated algorithmic techniques, such as model checking and constraint solving, questions about performance are a major concern. In this work we present an empirical study of our industrial strength source code analysis tool Goanna that uses a model checking core for static analysis of C/C++ code. We present the core technology and abstraction mechanism with a focus on performance, as guided by experience from having analyzed millions of lines of code. In particular, we present results from our recent study within the NIST/DHS SAMATE program. The results show that, maybe surprisingly, formal verification techniques can be used successfully in practical industry applications scaling roughly linearly, even for millions of lines of code. Crown Copyright ?? 2012 Published by Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.entcs.2012.11.002},
  file          = {:article\\High performance static analysis for industry.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {15710661},
  keywords      = {C/C++,SAMATE,Validation,first select,model checking,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,tools,verification},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.entcs.2012.11.002},
}

@Article{Liu2012,
  author        = {Liu, Bingchang and Shi, Liang and Cai, Zhuhua and Li, Min},
  title         = {{Software vulnerability discovery techniques: A survey}},
  journal       = {Proceedings - 2012 4th International Conference on Multimedia and Security, MINES 2012},
  year          = {2012},
  pages         = {152--156},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerabilities are the root cause of computer security problem. How people can quickly discover vulnerabilities existing in a certain software has always been the focus of information security field. This paper has done research on software vulnerability techniques, including static analysis, Fuzzing, penetration testing. Besides, the authors also take vulnerability discovery models as an example of software vulnerability analysis methods which go hand in hand with vulnerability discovery techniques. The ending part of the paper analyses the advantages and disadvantages of each technique introduced here and talks about the future direction of this field.},
  doi           = {10.1109/MINES.2012.202},
  file          = {:article\\Software vulnerability discovery techniques A survey.pdf:pdf},
  isbn          = {9780769548524},
  issn          = {2162-8998},
  keywords      = {Fuzzing,Penetration testing,Software static analysis,Vulnerability,Vulnerability discovery model,fuzz,stat,static analysi,static analysis,survey},
  mendeley-tags = {fuzz,survey},
}

@Article{Cui2012,
  author        = {Cui, Weidong and Peinado, Marcus and Xu, Zhilei and Chan, Ellick},
  title         = {{Tracking Rootkit Footprints with a Practical Memory Analysis System}},
  journal       = {Presented as part of the 21st USENIX Security Symposium (USENIX Security 12)},
  year          = {2012},
  pages         = {601--615},
  __markedentry = {[ccc:6]},
  file          = {:article\\Tracking Rootkit Footprints with a Practical Memory Analysis System.pdf:pdf},
  isbn          = {978-931971-95-9},
  keywords      = {malware,stat,static analysi,static analysis},
  mendeley-tags = {malware},
  url           = {https://www.usenix.org/conference/usenixsecurity12/technical-sessions/presentation/cui},
}

@Article{Cha2012,
  author        = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
  title         = {{Unleashing Mayhem on binary code}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2012},
  pages         = {380--394},
  __markedentry = {[ccc:6]},
  abstract      = {In this paper we present MAYHEM, a new sys- tem for automatically finding exploitable bugs in binary (i.e., executable) programs. Every bug reported by MAYHEM is accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is security- critical and actionable. M AYHEM works on raw binary code without debugging information. To make exploit generation possible at the binary-level, MAYHEM addresses two major technical challenges: actively managing execution paths without exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user input. To this end, we propose two novel techniques: 1) hybrid symbolic execution for combining online and offline (concolic) execution to maximize the benefits of both techniques, and 2) index-based memory modeling, a technique that allows MAYHEM to efficiently reason about symbolic memory at the binary level. We used M AYHEM to find and demonstrate 29 exploitable vulnerabilities in both Linux and Windows programs, 2 of which were previously undocumented.},
  doi           = {10.1109/SP.2012.31},
  file          = {:article\\Unleashing Mayhem on binary code.pdf:pdf},
  isbn          = {9780769546810},
  issn          = {10816011},
  keywords      = {binary,exploit generation,fuzz,hybrid execution,index-based memory modeling,stat,static analysi,static analysis,symbolic memory},
  mendeley-tags = {binary,fuzz},
}

@Article{Joseph2012,
  author        = {Joseph, Anthony D and Laskov, Pavel and Roli, Fabio and Tygar, J Doug and Nelson, Blaine},
  title         = {{Machine Learning Methods for Computer Security Edited by}},
  journal       = {Manifesto from Dagstuhl Perspectivess Workshop 12371},
  year          = {2012},
  volume        = {3},
  number        = {1},
  pages         = {1--30},
  __markedentry = {[ccc:6]},
  doi           = {10.4230/DagRep.2.9.109},
  file          = {:article\\Machine Learning Methods for Computer Security Edited by.pdf:pdf},
  issn          = {2192-5283},
  keywords      = {machine learning},
  mendeley-tags = {machine learning},
}

@Article{Ding2012,
  author        = {Ding, Sun and Tan, Hee Beng Kuan and Liu, Kaiping and Chandramohan, Mahinthan and Zhang, Hongyu},
  title         = {{Detection of buffer overflow vulnerabilities in C/C++ with pattern based limited symbolic evaluation}},
  journal       = {Proceedings - International Computer Software and Applications Conference},
  year          = {2012},
  pages         = {559--564},
  __markedentry = {[ccc:6]},
  abstract      = {Buffer overflow vulnerability is one of the major security threats for applications written in C/C++. Among the existing approaches for detecting buffer overflow vulnerability, though flow sensitive based approaches offer higher precision but they are limited by heavy overhead and the fact that many constraints are unsolvable. We propose a novel method to efficiently detect vulnerable buffer overflows in any given control flow graph through recognizing two patterns. The proposed approach first uses syntax analysis to filter away those branches that cannot possibly comply with any of the two patterns before applying a limited symbolic evaluation for a precise matching against the patterns. The proposed approach only needs to evaluate a limited set of selected branch predicates according to the patterns and avoids the need to deal with a large number of general branch predicates. This significantly improves the scalability while not sacrificing the detection precision. Our experiments demonstrate the scalability and efficiency of the proposed method, which demonstrates its applicability.},
  doi           = {10.1109/COMPSACW.2012.103},
  file          = {:article\\Detection of buffer overflow vulnerabilities in CC with pattern based limited symbolic evaluation.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9780769547589},
  issn          = {07303157},
  keywords      = {Empirical study,Pattern recognition,Security,Symbolic evaluation,Verification,binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
}

@Article{赵玉洁2012,
  author        = {赵玉洁 and 汤战勇 and 王妮 and 方鼎益 and 顾元祥},
  title         = {代码混淆算法有效性评估},
  journal       = {软件学报},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {摘要： 代码混淆是一种能够有效增加攻击者逆向分析和攻击代价的软件保护技术．然而，混淆算法的有效性评 价和验证是代码混淆研究中亟待解决的重要问题．目前，对代码混淆有效性的研究大都是基于软件复杂性度量的，然 而代码混淆作为一种保护软件安全的技术，更需要从逆向攻击的角度进行评估一寺面向逆向工程的思想引入到代码 混淆算法评估中，通过理论证明和具体实验验证了其可行性．该评估方法能够为混淆算法提供有效证明，并对判别和 选择代码混淆算法具有指导意义，同时也有助于寻求更有效的代码混淆方法．},
  file          = {:article\\代码混淆算法有效性评估.pdf:pdf},
  keywords      = {obfuscate,代码混淆；逆向工程；评估；IDA插件；平展控制流},
  mendeley-tags = {obfuscate},
}

@Article{王蕊2012,
  author        = {王蕊 and 冯登国 and 杨轶 and 苏璞睿},
  title         = {基于语义的恶意代码行为特征提取及检测方法},
  journal       = {软件学报},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {提出一种基于语义的恶意代码行为特征提取及检测方法，通过结合指令层的污点传播分析与行为层的语 义分析，提取恶意代码的关键行为及行为阃的依赖关系；然后，利用抗混淆引擎识别语义无关及语义等价行为．获取 具有一定抗干扰能力的恶意代码行为特征．在此基础上，实现特征提取及检测原型系统．通过对多个恶意代码样本的 分析和检测，完成了对该系统的实验验证．实验结果表明，基于上述方法提取的特征具有抗干扰能力强等特点，基于 此特征的检测对恶意代码具有较好的识别能力．},
  file          = {:article\\基于语义的恶意代码行为特征提取及检测方法.pdf:pdf},
  keywords      = {malware,恶意代码；语义；行为特征提取；恶意代码检测},
  mendeley-tags = {malware},
}

@Article{Dewey2012,
  author        = {Dewey, David and Giffin, Jonathon},
  title         = {{Static detection of C++ vtable escape vulnerabilities in binary code}},
  journal       = {Proceedings of the ISOC Network {\&} Distributed System Security Symposium (NDSS)},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {Static binary code analysis is a longstanding technique used to find security defects in deployed proprietary software. The complexities of binary code compiled from object-oriented source languages (e.g. C++) has limited the utility of binary analysis to basic applications using simpler coding constructs, so vulnerabilities in object-oriented code remain undetected. In this paper, we present vtable escape bugs—a class of type confusion errors specific to C++ code present in real, deployed software including Adobe Reader, Microsoft Office, and theWindows subsystem DLLs. We developed automated binary code analyses able to statically detect vtable escape bugs by reconstructing high-level objects and analyzing the safety of their use. We implemented our analysis in our own general object code decompilation framework to demonstrate that classes of object-oriented vulnerabilities can be uncovered from compiled binaries. We successfully found vtable escape bugs in a collection of test samples that mimic publicly disclosed vulnerabilities in Adobe Reader and Microsoft Excel. With these new analyses, security analysts gain the ability to find common flaws introduced by applications compiled from C++.},
  file          = {:article\\Static detection of C vtable escape vulnerabilities in binary code.pdf:pdf},
  keywords      = {binary,obfuscate,stat,static analysi,static analysis},
  mendeley-tags = {binary,obfuscate},
  url           = {https://www.internetsociety.org/sites/default/files/P14{\_}2.pdf},
}

@PhdThesis{李超2012,
  author        = {李超},
  title         = {混淆在公钥密码体制中的理论与应用研究},
  year          = {2012},
  __markedentry = {[ccc:6]},
  file          = {:article\\混淆在公钥密码体制中的理论与应用研究.pdf:pdf},
  keywords      = {obfuscate},
  mendeley-tags = {obfuscate},
}

@Article{Doudalis2012,
  author        = {Doudalis, Ioannis and Clause, James and Venkataramani, Guru and Prvulovic, Milos and Orso, Alessandro},
  title         = {{Effective and efficient memory protection using dynamic tainting}},
  journal       = {IEEE Transactions on Computers},
  year          = {2012},
  volume        = {61},
  number        = {1},
  pages         = {87--100},
  __markedentry = {[ccc:6]},
  abstract      = {Programs written in languages allowing direct access to memory through pointers often contain memory-related faults, which cause nondeterministic failures and security vulnerabilities. We present a new dynamic tainting technique to detect illegal memory accesses. When memory is allocated, at runtime, we taint both the memory and the corresponding pointer using the same taint mark. Taint marks are then propagated and checked every time a memory address m is accessed through a pointer p; if the associated taint marks differ, an illegal access is reported. To allow always-on checking using a low overhead, hardware-assisted implementation, we make several key technical decisions. We use a configurable, low number of reusable taint marks instead of a unique mark for each allocated area of memory, reducing the performance overhead without losing the ability to target most memory-related faults. We also define the technique at the binary level, which helps handle applications using third-party libraries whose source code is unavailable. We created a software-only prototype of our technique and simulated a hardware-assisted implementation. Our results show that 1) it identifies a large class of memory-related faults, even when using only two unique taint marks, and 2) a hardware-assisted implementation can achieve performance overheads in single-digit percentages.},
  doi           = {10.1109/TC.2010.215},
  file          = {:article\\Effective and efficient memory protection using dynamic tainting.pdf:pdf},
  isbn          = {0018-9340 VO  - 61},
  issn          = {00189340},
  keywords      = {Computer systems organization,binary,hardware/software interfaces,monitors,processor architectures,stat,static analysi,static analysis},
  mendeley-tags = {binary},
}

@Article{NSA2012,
  author        = {NSA},
  title         = {{Juliet Test Suite v1.2 for Java User Guide}},
  year          = {2012},
  number        = {December},
  __markedentry = {[ccc:6]},
  file          = {:article\\Juliet Test Suite v1.2 for Java User Guide.pdf:pdf},
  keywords      = {book},
  mendeley-tags = {book},
}

@Article{Yamaguchi2012,
  author        = {Yamaguchi, Fabian and Lottmann, Markus and Rieck, Konrad},
  title         = {{Generalized Vulnerability Extrapolation using Abstract Syntax Trees}},
  journal       = {ACSAC '12 Proceedings of the 28th Annual Computer Security Applications Conference},
  year          = {2012},
  pages         = {359--368},
  __markedentry = {[ccc:6]},
  abstract      = {The discovery of vulnerabilities in source code is a key for securing computer systems. While specific types of security flaws can be identified automatically, in the general case the process of finding vulnerabilities cannot be automated and vulnerabilities are mainly discovered by manual analysis. In this paper, we propose a method for assisting a security analyst during auditing of source code. Our method proceeds by extracting abstract syntax trees from the code and determining structural patterns in these trees, such that each function in the code can be described as a mixture of these patterns. This representation enables us to decompose a known vulnerability and extrapolate it to a code base, such that functions potentially suffering from the same flaw can be suggested to the analyst. We evaluate our method on the source code of four popular open-source projects: LibTIFF, FFmpeg, Pidgin and Asterisk. For three of these projects, we are able to identify zero-day vulnerabilities by inspecting only a small fraction of the code bases.},
  doi           = {10.1145/2420950.2421003},
  file          = {:article\\Generalized Vulnerability Extrapolation using Abstract Syntax Trees.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450313124},
  issn          = {2229-6166},
  keywords      = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,web},
}

@Article{Shekhar2012,
  author        = {Shekhar, Shashi and Dietz, Michael and Wallach, Dan S.},
  title         = {{AdSplit: Separating smartphone advertising from applications}},
  journal       = {Proceedings of the 21st USENIX conference on Security symposium},
  year          = {2012},
  pages         = {28},
  __markedentry = {[ccc:6]},
  abstract      = {A wide variety of smartphone applications today rely on third-party advertising services, which provide libraries that are linked into the hosting application. This situation is undesirable for both the application author and the advertiser. Advertising libraries require additional permissions, resulting in additional permission requests to users. Likewise, a malicious application could simulate the behavior of the advertising library, forging the user's interaction and effectively stealing money from the advertiser. This paper describes AdSplit, where we extended Android to allow an application and its advertising to run as separate processes, under separate user-ids, eliminating the need for applications to request permissions on behalf of their advertising libraries. We also leverage mechanisms from Quire to allow the remote server to validate the authenticity of client-side behavior. In this paper, we quantify the degree of permission bloat caused by advertising, with a study of thousands of downloaded apps. AdSplit automatically recompiles apps to extract their ad services, and we measure minimal runtime overhead. We also observe that most ad libraries just embed an HTML widget within and describe how AdSplit can be designed with this in mind to avoid any need for ads to have native code.},
  archiveprefix = {arXiv},
  arxivid       = {1202.4030},
  eprint        = {1202.4030},
  file          = {:article\\AdSplit Separating smartphone advertising from applications.pdf:pdf},
  isbn          = {978-931971-95-9},
  keywords      = {android,binary,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,predicte,web},
  url           = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final101.pdf$\backslash$nhttp://arxiv.org/abs/1202.4030$\backslash$nhttp://arxiv.org/abs/1202.4030},
}

@Article{Xu2012,
  author        = {Xu, Rubin and Sa{\"{i}}di, Hassen and Anderson, Ross and Saıdi, Hassen},
  title         = {{Aurasium: Practical Policy Enforcement for Android Applications}},
  journal       = {Proceedings of the 21st USENIX conference {\ldots}},
  year          = {2012},
  pages         = {27},
  __markedentry = {[ccc:6]},
  abstract      = {The increasing popularity of Google's mobile platform Android makes it the prime target of the latest surge in mobile malware. Most research on enhancing the platform's security and privacy controls requires extensive modification to the operating system, which has significant usability issues and hinders efforts for widespread adoption. We develop a novel solution called Aurasium that bypasses the need to modify the Android OS while providing much of the security and privacy that users de- sire. We automatically repackage arbitrary applications to attach user-level sandboxing and policy enforcement code, which closely watches the application's behavior for security and privacy violations such as attempts to retrieve a user's sensitive information, send SMS covertly to premium numbers, or access malicious IP addresses. Aurasium can also detect and prevent cases of privilege escalation attacks. Experiments show that we can apply this solution to a large sample of benign and malicious applications with a near 100 percent success rate, with- out significant performance and space overhead. Aura- sium has been tested on three versions of the Android OS, and is freely available.},
  file          = {:article\\Aurasium Practical Policy Enforcement for Android Applications.pdf:pdf},
  isbn          = {978-931971-95-9},
  keywords      = {android,binary,fuzz,obfuscate,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,fuzz,obfuscate,web},
  url           = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final60.pdf$\backslash$nhttp://dl.acm.org/citation.cfm?id=2362793.2362820},
}

@Article{NSA2012a,
  author        = {NSA},
  title         = {{Juliet Test Suite v1.2 for Java User Guide}},
  year          = {2012},
  number        = {December},
  __markedentry = {[ccc:6]},
  file          = {:article\\Juliet Test Suite v1.2 for Java User Guide(2).pdf:pdf},
  keywords      = {obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {obfuscate,predicte,web},
}

@Article{Chabbi2012,
  author        = {Chabbi, Milind and Perianayagam, Somu and Andrews, Gregory and Debray, Saumya},
  title         = {{(***)Efficient Dynamic Taint Analysis Using Multicore Machines}},
  journal       = {Cs.Rice.Edu},
  year          = {2012},
  pages         = {1--10},
  __markedentry = {[ccc:6]},
  file          = {:article\\()Efficient Dynamic Taint Analysis Using Multicore Machines.pdf:pdf},
  keywords      = {binary,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,web},
  url           = {http://www.cs.rice.edu/{~}mc29/Milind/Publications{\_}files/EfficientDynamicTaintAnalysisUsingMulticores.pdf},
}

@Article{Walden2012,
  author        = {Walden, James and Doyle, Maureen},
  title         = {{SAVI: Static-Analysis vulnerability indicator}},
  journal       = {IEEE Security and Privacy},
  year          = {2012},
  volume        = {10},
  number        = {3},
  pages         = {32--39},
  __markedentry = {[ccc:6]},
  abstract      = {Open source software presents new opportunities for software acquisition but introduces risks. The selection of open source applications should take into account both features and security risks. Risks include security vulnerabilities, of which published vulnerabilities are only the tip of the iceberg. Having an application's source code lets us look deeper at its security. SAVI (Static-Analysis Vulnerability Indicator) is a metric for assessing risks of using software built by external developers. It combines several types of static-analysis data to rank application vulnerability.},
  doi           = {10.1109/MSP.2012.1},
  file          = {:article\\SAVI Static-Analysis vulnerability indicator.pdf:pdf},
  groups        = {vice-important},
  issn          = {15407993},
  keywords      = {Web application security,computer security,empirical software engineering,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,source code,source code-vice important,web},
}

@Article{Elberzhager2012,
  author        = {Elberzhager, Frank and Rosbach, Alla and M??nch, J??rgen and Eschbach, Robert},
  title         = {{Reducing test effort: A systematic mapping study on existing approaches}},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {10},
  pages         = {1092--1106},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50{\%} of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective: The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method: Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results: In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion: The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected. ?? 2012 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2012.04.007},
  file          = {:article\\Reducing test effort A systematic mapping study on existing approaches.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Efficiency improvement,Mapping study,Quality assurance,Software testing,Test effort reduction,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,survey,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2012.04.007},
}

@Article{Wang2012,
  author        = {Wang, Rui and Chen, Shuo and Wang, XiaoFeng},
  title         = {{Signing me onto your accounts through Facebook and Google: A traffic-guided security study of commercially deployed single-sign-on web services}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2012},
  pages         = {365--379},
  __markedentry = {[ccc:6]},
  abstract      = {With the boom of software-as-a-service and social networking, web-based single sign-on (SSO) schemes are being deployed by more and more commercial websites to safeguard many web resources. Despite prior research in formal verification, little has been done to analyze the security quality of SSO schemes that are commercially deployed in the real world. Such an analysis faces unique technical challenges, including lack of access to well-documented protocols and code, and the complexity brought in by the rich browser elements (script, Flash, etc.). In this paper, we report the first “field study” on popular web SSO systems. In every studied case, we focused on the actual web traffic going through the browser, and used an algorithm to recover important semantic information and identify potential exploit opportunities. Such opportunities guided us to the discoveries of real flaws. In this study, we discovered 8 serious logic flaws in high-profile ID providers and relying party websites, such as OpenID (including Google ID and PayPal Access), Facebook, JanRain, Freelancer, FarmVille, Sears.com, etc. Every flaw allows an attacker to sign in as the victim user. We reported our findings to affected companies, and received their acknowledgements in various ways. All the reported flaws, except those discovered very recently, have been fixed. This study shows that the overall security quality of SSO deployments seems worrisome. We hope that the SSO community conducts a study similar to ours, but in a larger scale, to better understand to what extent SSO is insecurely deployed and how to respond to the situation.},
  doi           = {10.1109/SP.2012.30},
  file          = {:article\\Signing me onto your accounts through Facebook and Google A traffic-guided security study of commercially depl.pdf:pdf},
  isbn          = {9780769546810},
  issn          = {10816011},
  keywords      = {Authentication,Logic Flaw,Secure Protocol,Single-Sign-On,Web Service,stat,static analysi,static analysis,web},
  mendeley-tags = {web},
}

@Article{Shar2012,
  author        = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  title         = {{Automated removal of cross site scripting vulnerabilities in web applications}},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {5},
  pages         = {467--478},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Cross site scripting (XSS) vulnerability is among the top web application vulnerabilities according to recent surveys. This vulnerability occurs when a web application uses inputs received from users in web pages without properly checking them. This allows an attacker to inject malicious scripts in web pages via such inputs such that the scripts perform malicious actions when a client visits the exploited web pages. Such an attack may cause serious security violations such as account hijacking and cookie theft. Current approaches to mitigate this problem mainly focus on effective detection of XSS vulnerabilities in the programs or prevention of real time XSS attacks. As more sophisticated attack vectors are being discovered, vulnerabilities if not removed could be exploited anytime. Objective: To address this issue, this paper presents an approach for removing XSS vulnerabilities in web applications. Method: Based on static analysis and pattern matching techniques, our approach identifies potential XSS vulnerabilities in program source code and secures them with appropriate escaping mechanisms which prevent input values from causing any script execution. Results: We developed a tool, saferXSS, to implement the proposed approach. Using the tool, we evaluated the applicability and effectiveness of the proposed approach based on the experiments on five Java-based web applications. Conclusion: Our evaluation has shown that the tool can be applied to real-world web applications and it automatically removed all the real XSS vulnerabilities in the test subjects. ?? 2011 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2011.12.006},
  file          = {:article\\Automated removal of cross site scripting vulnerabilities in web applications.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0-7695-2224-6},
  issn          = {09505849},
  keywords      = {Automated bug fixing,Character escaping,Cross site scripting,Encoding,Injection vulnerability,Web security,binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2011.12.006},
}

@Article{Shahmehri2012,
  author        = {Shahmehri, Nahid and Mammar, Amel and {Montes De Oca}, Edgardo and Byers, David and Cavalli, Ana and Ardi, Shanai and Jimenez, Willy},
  title         = {{An advanced approach for modeling and detecting software vulnerabilities}},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {997--1013},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Passive testing is a technique in which traces collected from the execution of a system under test are examined for evidence of flaws in the system. Objective: In this paper we present a method for detecting the presence of security vulnerabilities by detecting evidence of their causes in execution traces. This is a new approach to security vulnerability detection. Method: Our method uses formal models of vulnerability causes, known as security goal models and vulnerability detection conditions (VDCs). The former are used to identify the causes of vulnerabilities and model their dependencies, and the latter to give a formal interpretation that is suitable for vulnerability detection using passive testing techniques. We have implemented modeling tools for security goal models and vulnerability detection conditions, as well as TestInv-Code, a tool that checks execution traces of compiled programs for evidence of VDCs. Results: We present the full definitions of security goal models and vulnerability detection conditions, as well as structured methods for creating both. We describe the design and implementation of TestInv-Code. Finally we show results obtained from running TestInv-Code to detect typical vulnerabilities in several open source projects. By testing versions with known vulnerabilities, we can quantify the effectiveness of the approach. Conclusion: Although the current implementation has some limitations, passive testing for vulnerability detection works well, and using models as the basis for testing ensures that users of the testing tool can easily extend it to handle new vulnerabilities. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2012.03.004},
  file          = {:article\\An advanced approach for modeling and detecting software vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Automatic testing,Dynamic analysis,Secure software engineering,Security modelling,Software security,binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2012.03.004},
}

@Article{Sui2012,
  author        = {Sui, Yulei and Ye, Ding and Xue, Jingling},
  title         = {{Static Memory Leak Detection Using Full-sparse Value-flow Analysis}},
  journal       = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
  year          = {2012},
  pages         = {254--264},
  __markedentry = {[ccc:6]},
  abstract      = {We introduce a static detector, Saber, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, Saber is the first to use a full-sparse value-flow analysis for leak detection. Saber tracks the flow of values from allocation to free sites using a sparse value-flow graph (SVFG) that captures def-use chains and value flows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting field-, flow- and context-sensitivity during different phases of the analysis, Saber detects leaks in a program by solving a graph reachability problem on its SVFG. Saber, which is fully implemented in Open64, is effective at detecting 211 leaks in the 15 SPEC2000 C programs and five applications, while keeping the false positive rate at 18.5{\%}. We have also compared Saber with Fastcheck (which analyzes allocated objects flowing only into top-level pointers) and Sparrow (which handles all allocated objects using abstract interpretation) using the 15 SPEC2000 C programs. Saber is as accurate as Sparrow but is 14.2X faster and reports 40.7{\%} more bugs than Fastcheck at a slightly higher false positive rate but is only 3.7X slower.},
  doi           = {10.1145/2338965.2336784},
  file          = {:article\\Static Memory Leak Detection Using Full-sparse Value-flow Analysis.pdf:pdf},
  groups        = {vice-important},
  isbn          = {978-1-4503-1454-1},
  keywords      = {binary,first select,memory leaks,source code,source code-vice important,sparse value-flow analysis,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,source code,source code-vice important,web},
  url           = {http://doi.acm.org/10.1145/2338965.2336784},
}

@Article{Shar2012a,
  author        = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  title         = {{Predicting common web application vulnerabilities from input validation and sanitization code patterns}},
  journal       = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering - ASE 2012},
  year          = {2012},
  pages         = {310},
  __markedentry = {[ccc:6]},
  abstract      = {Software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. On the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. However, finding alternative solutions to address these risks remains an important research problem. As web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities�SQL injection and cross site scripting. In our experiments, vulnerability predictors built from the proposed attributes detected more than 80{\%} of the vulnerabilities in the test subjects at low false alarm rates. },
  doi           = {10.1145/2351676.2351733},
  file          = {:article\\Predicting common web application vulnerabilities from input validation and sanitization code patterns.pdf:pdf},
  isbn          = {9781450312042},
  keywords      = {defect prediction,empirical study,input validation and sanitization,machine learning,predicte,stat,static analysi,static analysis,static code attributes,vulnerabilities,web,web application},
  mendeley-tags = {machine learning,predicte,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2351676.2351733},
}

@Article{Holler2012,
  author        = {Holler, Christian and Herzig, Kim and Zeller, Andreas},
  title         = {{Fuzzing with Code Fragments}},
  journal       = {Usenix},
  year          = {2012},
  pages         = {38},
  __markedentry = {[ccc:6]},
  abstract      = {Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.},
  file          = {:article\\Fuzzing with Code Fragments.pdf:pdf},
  isbn          = {978-931971-95-9},
  keywords      = {binary,fuzz,security, security testing, fuzz testing, grammar,web},
  mendeley-tags = {binary,fuzz,web},
  url           = {http://dl.acm.org/citation.cfm?id=2362793.2362831},
}

@Article{Zhang2012a,
  author        = {Zhang, Dazhi and Liu, Donggang and Lei, Yu and Kung, David and Csallner, Christoph and Nystrom, Nathaniel and Wang, Wenhua},
  title         = {{SimFuzz: Test case similarity directed deep fuzzing}},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {1},
  pages         = {102--111},
  __markedentry = {[ccc:6]},
  abstract      = {Fuzzing is widely used to detect software vulnerabilities. Blackbox fuzzing does not require program source code. It mutates well-formed inputs to produce new ones. However, these new inputs usually do not exercise deep program semantics since the possibility that they can satisfy the conditions of a deep program state is low. As a result, blackbox fuzzing is often limited to identify vulnerabilities in input validation components of a program. Domain knowledge such as input specifications can be used to mitigate these limitations. However, it is often expensive to obtain such knowledge in practice. Whitebox fuzzing employs heavy analysis techniques, i.e.; dynamic symbolic execution, to systematically generate test inputs and explore as many paths as possible. It is powerful to explore new program branches so as to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difficult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efficiently. The fuzzing process comprises two stages. At the first stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the first stage is selected based on the test case similarity metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e.; inputs that just explore shallow program paths, are created at the first stage, and more distinct test inputs, i.e.; inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising. ?? 2011 Elsevier Inc.},
  doi           = {10.1016/j.jss.2011.07.028},
  file          = {:article\\SimFuzz Test case similarity directed deep fuzzing.pdf:pdf},
  issn          = {01641212},
  keywords      = {Fuzzing,Software testing,Software vulnerability,binary,fuzz,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,web},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jss.2011.07.028},
}

@PhdThesis{Guo2012,
  author        = {Guo, Pj},
  title         = {{Software tools to facilitate research programming}},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {Research programming is a type of programming activity where the goal is to write computer programs to obtain insights from data. Millions of professionals in fields ranging from science, engineering, business, finance, public policy, and journalism, as well as numerous students and computer hobbyists, all perform research programming on a daily basis. My thesis is that by understanding the unique challenges faced during research programming, it becomes possible to apply techniques from dynamic program analy- sis, mixed-initiative recommendation systems, and OS-level tracing to make research programmers more productive. cal challenges faced by research programmers, and presents five software tools that I have developed to address some key challenges. 1.) ProactiveWrangler is an interac- tive graphical tool that helps research programmers reformat and clean data prior to analysis. 2.) IncPy is a Python interpreter that speeds up the data analysis scripting cycle and helps programmers manage code and data dependencies. 3.) SlopPy is a Python interpreter that automatically makes existing scripts error-tolerant, thereby also speeding up the data analysis scripting cycle. 4.) Burrito is a Linux-based system that helps programmers organize, annotate, and recall past insights about their experiments. 5.) CDE is a software packaging tool that makes it easy to deploy, archive, and share research code. Taken together, these five tools enable re- search programmers to iterate and potentially discover insights faster by offloading the burdens of data management and provenance to the computer.},
  file          = {:article\\Software tools to facilitate research programming.pdf:pdf},
  keywords      = {book,machine learning,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {book,machine learning,predicte,web},
  number        = {May},
  pages         = {230},
  url           = {http://pgbovine.net/publications/Philip-Guo{\_}PhD-dissertation{\_}software-tools-for-research-programming.pdf},
}

@Article{Zhang2012b,
  author        = {Zhang, Ruoyu and Huang, Shiqiu and Qi, Zhengwei and Guan, Haibing},
  title         = {{Static program analysis assisted dynamic taint tracking for software vulnerability discovery}},
  journal       = {Computers and Mathematics with Applications},
  year          = {2012},
  volume        = {63},
  number        = {2},
  pages         = {469--480},
  __markedentry = {[ccc:6]},
  abstract      = {The evolution of computer science has exposed us to the growing gravity of security problems and threats. Dynamic taint analysis is a prevalent approach to protect a program from malicious behaviors, but fails to provide any information about the code which is not executed. This paper describes a novel approach to overcome the limitation of traditional dynamic taint analysis by integrating static analysis into the system and presents framework SDCF to detect software vulnerabilities with high code coverage. Our experiments show that SDCF is not only able to provide efficient runtime protection by introducing an overhead of 4.16× based on the taint tracing technique, but is also capable of discovering latent software vulnerabilities which have not been exploited, and achieve code coverage of more than 90{\%}. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.camwa.2011.08.001},
  file          = {:article\\Static program analysis assisted dynamic taint tracking for software vulnerability discovery.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {08981221},
  keywords      = {Code coverage,Data flow analysis,Software vulnerability,Taint analysis,binary,first select,fuzz,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,predicte,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.camwa.2011.08.001},
}

@Article{Frisby2012,
  author        = {Frisby, Wl and Moench, B and Recht, B and Ristenpart, T},
  title         = {{Security Analysis of Smartphone Point-of-Sale Systems.}},
  journal       = {Woot},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {We experimentally investigate the security of several smartphone point-of-sale (POS) systems that consist of a software application combined with an audio-jack magnetic stripe reader (AMSR). The latter is a small hardware dongle that reads magnetic stripes on payment cards, (sometimes) encrypts the sensitive card data, and transmits the result to the application. Our main technical result is a complete break of a feature-rich AMSR with encryption support. We show how an arbitrary application running on the phone can permanently disable the AMSR, extract the cryptographic keys it uses to protect cardholder data, or gain the privileged access needed to upload new firmware to it.},
  file          = {:article\\Security Analysis of Smartphone Point-of-Sale Systems.pdf:pdf},
  keywords      = {binary,fuzz,network,web},
  mendeley-tags = {binary,fuzz,network,web},
  url           = {https://www.usenix.org/system/files/conference/woot12/woot12-final25.pdf},
}

@Article{Zhang2012c,
  author        = {Zhang, Mingwei and Prakash, Aravind and Li, Xiaolei and Liang, Zhenkai and Yin, Heng},
  title         = {{Identifying and Analyzing Pointer Misuses for Sophisticated Memory-corruption Exploit Diagnosis}},
  journal       = {19th Annual Network and Distributed System Security Symposium (NDSS'12)},
  year          = {2012},
  __markedentry = {[ccc:6]},
  abstract      = {Software exploits are one of the major threats to the In- ternet security. A large family of exploits works by corrupt- ing memory of the victim process to execute malicious code. To quickly respond to these attacks, it is critical to automati- cally diagnose such exploits to find out how they circumvent existing defense mechanisms. Because of the complexity of the victim programs and sophistication of recent exploits, existing analysis techniques fall short: they either miss im- portant attack steps or report too much irrelevant informa- tion. In this paper, based on the observation that the key steps in memory corruption exploits often involve pointer misuses, we propose a novel solution, PointerScope, to use type inference on binary execution to detect the pointer mis- uses induced by an exploit. These pointer misuses high- light the important attack steps of the exploit, and there- fore convey valuable information about the exploit mecha- nisms. Our approach complements dependency-based solu- tions to perform more comprehensive diagnosis of sophis- ticated memory exploits. We prototyped PointerScope and evaluated it using real-world exploit samples and demon- strated that PointerScope can successfully capture the key attack steps, which significantly facilitates attack response},
  file          = {:article\\Identifying and Analyzing Pointer Misuses for Sophisticated Memory-corruption Exploit Diagnosis.pdf:pdf},
  keywords      = {binary,web},
  mendeley-tags = {binary,web},
}

@Article{Cao2012,
  author        = {Cao, Yan and Wei, Qiang and Wang, Qingxian},
  title         = {{The method for parallel approach to sensitive point based on dynamic symbolic execution}},
  journal       = {Proceedings of the 2012 8th International Conference on Computational Intelligence and Security, CIS 2012},
  year          = {2012},
  pages         = {661--665},
  __markedentry = {[ccc:6]},
  abstract      = {In order to improve the efficiency of symbolic execution in software vulnerability detection, we propose the sensitive point oriented test method for parallel approach in the paper. Static analysis is used for identification and location of sensitive points. Then based on dynamic symbolic execution, the algorithm of parallel approach to sensitive point is designed, in order to select for the preferential path and realize iterative approach test. Moreover, we present search algorithm of test cases to reuse forward information and reduce communication redundancy. Finally, the experiment results verify the improvement of symbolic execution efficiency and effectiveness of exception detection.},
  doi           = {10.1109/CIS.2012.152},
  file          = {:article\\The method for parallel approach to sensitive point based on dynamic symbolic execution.pdf:pdf},
  isbn          = {9780769548968},
  keywords      = {parallel symbolic execution,security test,sensitive point oriented,stat,static analysi,static analysis},
}

@Article{Eskandari2012,
  author        = {Eskandari, Mojtaba and Hashemi, Sattar},
  title         = {{A graph mining approach for detecting unknown malwares}},
  journal       = {Journal of Visual Languages and Computing},
  year          = {2012},
  volume        = {23},
  number        = {3},
  pages         = {154--162},
  __markedentry = {[ccc:6]},
  abstract      = {Nowadays malware is one of the serious problems in the modern societies. Although the signature based malicious code detection is the standard technique in all commercial antivirus softwares, it can only achieve detection once the virus has already caused damage and it is registered. Therefore, it fails to detect new malwares (unknown malwares). Since most of malwares have similar behavior, a behavior based method can detect unknown malwares. The behavior of a program can be represented by a set of called API's (application programming interface). Therefore, a classifier can be employed to construct a learning model with a set of programs' API calls. Finally, an intelligent malware detection system is developed to detect unknown malwares automatically. On the other hand, we have an appealing representation model to visualize the executable files structure which is control flow graph (CFG). This model represents another semantic aspect of programs. This paper presents a robust semantic based method to detect unknown malwares based on combination of a visualize model (CFG) and called API's. The main contribution of this paper is extracting CFG from programs and combining it with extracted API calls to have more information about executable files. This new representation model is called API-CFG. In addition, to have fast learning and classification process, the control flow graphs are converted to a set of feature vectors by a nice trick. Our approach is capable of classifying unseen benign and malicious code with high accuracy. The results show a statistically significant improvement over . n-grams based detection method. ?? 2012 Elsevier Ltd.},
  doi           = {10.1016/j.jvlc.2012.02.002},
  file          = {:article\\A graph mining approach for detecting unknown malwares.pdf:pdf},
  issn          = {1045926X},
  keywords      = {API,CFG,Detection,Malware,PE-file,Unknown malwares,binary,machine learning,malware,predicte,stat,static analysi,static analysis},
  mendeley-tags = {binary,machine learning,malware,predicte},
  publisher     = {Elsevier},
  url           = {http://dx.doi.org/10.1016/j.jvlc.2012.02.002},
}

@Article{Zhang2012d,
  author        = {Zhang, Fuyuan and Nielson, Flemming and Nielson, Hanne Riis},
  title         = {{Model checking as static analysis: Revisited}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2012},
  volume        = {7321 LNCS},
  pages         = {99--112},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-642-30729-4_8},
  file          = {:article\\Model checking as static analysis Revisited.pdf:pdf},
  isbn          = {9783642307287},
  issn          = {03029743},
  keywords      = {predicte,stat,static analysi,static analysis},
  mendeley-tags = {predicte},
}

@Article{Oz2012,
  author        = {Oz, Isil and Topcuoglu, Haluk Rahmi and Kandemir, Mahmut and Tosun, Oguz},
  title         = {{Thread vulnerability in parallel applications}},
  journal       = {Journal of Parallel and Distributed Computing},
  year          = {2012},
  volume        = {72},
  number        = {10},
  pages         = {1171--1185},
  __markedentry = {[ccc:6]},
  abstract      = {Continuously reducing transistor sizes and aggressive low power operating modes employed by modern architectures tend to increase transient error rates. Concurrently, multicore machines are dominating the architectural spectrum today in various application domains. These two trends require a fresh look at resiliency of multithreaded applications against transient errors from a software perspective. In this paper, we propose and evaluate a new metric called the Thread Vulnerability Factor (TVF). A distinguishing characteristic of TVF is that its calculation for a given thread (which is typically one of the threads of a multithreaded application) does not depend on its code alone, but also on the codes of the threads that share resources and data with that thread. As a result, we decompose TVF of a thread into two complementary parts: local and remote. While the former captures the TVF induced by the code of the target thread, the latter represents the vulnerability impact of the threads that interact with the target thread. We quantify the local and remote TVF values for three architectural components (register file, ALUs, and caches) using a set of ten multithreaded applications from the Parsec and Splash-2 benchmark suites. Our experimental evaluation shows that TVF values tend to increase as the number of cores increases, which means the system becomes more vulnerable as the core count rises. We further discuss how TVF metric can be employed to explore performance-reliability tradeoffs in multicores. Reliability-based analysis of compiler optimizations and redundancy-based fault tolerance are also mentioned as potential usages of our TVF metric. ?? 2012 Elsevier Inc. All rights reserved.},
  doi           = {10.1016/j.jpdc.2012.05.002},
  file          = {:article\\Thread vulnerability in parallel applications.pdf:pdf},
  issn          = {07437315},
  keywords      = {Fault tolerance,Multicores,Reliability,TVF,Thread Vulnerability,binary,predicte,stat,static analysi,static analysis},
  mendeley-tags = {binary,predicte},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jpdc.2012.05.002},
}

@Article{Punitha2013,
  author        = {Punitha, K and Chitra, S},
  title         = {{Software defect prediction using software metrics - A survey}},
  journal       = {2013 International Conference on Information Communication and Embedded Systems (ICICES)},
  year          = {2013},
  pages         = {555--558},
  __markedentry = {[ccc:6]},
  abstract      = {Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. {\textcopyright} 2013 IEEE.},
  doi           = {10.1109/ICICES.2013.6508369},
  file          = {:article\\A survey.pdf:pdf},
  isbn          = {978-1-4673-5788-3},
  keywords      = {fuzz,machine learning,predicte,survey},
  mendeley-tags = {fuzz,machine learning,predicte,survey},
  url           = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6508369$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6508369},
}

@PhdThesis{Tobergte2013,
  author        = {Tobergte, David R. and Curtis, Shirley},
  title         = {一种基于图灵机的代码混淆评价方法},
  year          = {2013},
  __markedentry = {[ccc:6]},
  abstract      = {applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {Journal of Chemical Information and Modeling},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\一种基于图灵机的代码混淆评价方法.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {icle,obfuscate},
  mendeley-tags = {obfuscate},
  number        = {9},
  pages         = {1689--1699},
  pmid          = {25246403},
  volume        = {53},
}

@Article{Snow2013,
  author        = {Snow, Kevin Z. and Monrose, Fabian and Davi, Lucas and Dmitrienko, Alexandra and Liebchen, Christopher and Sadeghi, Ahmad Reza},
  title         = {{Just-in-time code reuse: On the effectiveness of fine-grained address space layout randomization}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2013},
  pages         = {574--588},
  __markedentry = {[ccc:6]},
  abstract      = {Fine-grained address space layout randomization (ASLR) has recently been proposed as a method of efficiently mitigating runtime attacks. In this paper, we introduce the design and implementation of a framework based on a novel attack strategy, dubbed just-in-time code reuse, that undermines the benefits of fine-grained ASLR. Specifically, we derail the assumptions embodied in fine-grained ASLR by exploiting the ability to repeatedly abuse a memory disclosure to map an application's memory layout on-the-fly, dynamically discover API functions and gadgets, and JIT-compile a target program using those gadgets -- all within a script environment at the time an exploit is launched. We demonstrate the power of our framework by using it in conjunction with a real-world exploit against Internet Explorer, and also provide extensive evaluations that demonstrate the practicality of just-in-time code reuse attacks. Our findings suggest that fine-grained ASLR may not be as promising as first thought.},
  doi           = {10.1109/SP.2013.45},
  file          = {:article\\Just-in-time code reuse On the effectiveness of fine-grained address space layout randomization.pdf:pdf},
  isbn          = {9780769549774},
  issn          = {10816011},
  keywords      = {binary,fuzz,predicte,stat,static analysi,static analysis},
  mendeley-tags = {binary,fuzz,predicte},
}

@Article{Li2013,
  author        = {Li, Hongzhe and Kim, Taebeom and Bat-Erdene, Munkhbayar and Lee, Heejo},
  title         = {{Software vulnerability detection using backward trace analysis and symbolic execution}},
  journal       = {Proceedings - 2013 International Conference on Availability, Reliability and Security, ARES 2013},
  year          = {2013},
  pages         = {446--454},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerability has long been considered an important threat to the safety of software systems. When source code is accessible, we can get much help from the information of source code to detect vulnerabilities. Static analysis has been used frequently to scan code for errors that cause security problems when source code is available. However, they often generate many false positives. Symbolic execution has also been proposed to detect vulnerabilities and has shown good performance in some researches. However, they are either ineffective in path exploration or could not scale well to large programs. During practical use, since most of paths are actually not related to security problems and software vulnerabilities are usually caused by the improper use of security-sensitive functions, the number of paths could be reduced by tracing sensitive data backwardly from security-sensitive functions so as to consider paths related to vulnerabilities only. What's more, in order to leave ourselves free from generating bug triggering test input, formal reasoning could be used by solving certain program conditions. In this research, we propose backward trace analysis and symbolic execution to detect vulnerabilities from source code. We first find out all the hot spot in source code file. Based on each hot spot, we construct a data flow tree so that we can get the possible execution traces. Afterwards, we do symbolic execution to generate program constraint(PC) and get security constraint(SC) from our predefined security requirements along each execution trace. A program constraint is a constraint imposed by program logic on program variables. A security constraint(SC) is a constraint on program variables that must be satisfied to ensure system security. Finally, this hot spot will be reported as a vulnerability if there is an assignment of values to program inputs which could satisfy PC but violates SC, in other words, satisfy PC $\Lambda$ S̅C̅. We have - mplemented our approach and conducted experiments on test cases which we randomly choose from Juliet Test Suites provided by US National Security Agency(NSA). The results show that our approach achieves Precision value of 83.33{\%}, Recall value of 90.90{\%} and F1 Value of 86.95{\%} which gains the best performance among competing tools. Moreover, our approach can efficiently mitigate path explosion problem in traditional symbolic execution.},
  annote        = {将符号执行用在源码上，精读},
  doi           = {10.1109/ARES.2013.59},
  file          = {:article\\Software vulnerability detection using backward trace analysis and symbolic execution.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9780769550084},
  keywords      = {Program constraint,Static analysis,Symbolic execution,Vulnerability detection,binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important},
}

@Article{Tobergte2013a,
  author        = {Tobergte, David R. and Curtis, Shirley},
  title         = {{Using Semantic Templates to Study Vulnerabilities Recorded in Large Software Repositories}},
  journal       = {Journal of Chemical Information and Modeling},
  year          = {2013},
  volume        = {53},
  number        = {9},
  pages         = {1689--1699},
  __markedentry = {[ccc:6]},
  abstract      = {applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\Using Semantic Templates to Study Vulnerabilities Recorded in Large Software Repositories.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {fuzz,icle},
  mendeley-tags = {fuzz},
  pmid          = {25246403},
}

@Article{王雅文2013,
  author        = {王雅文},
  title         = {一种基于代码静态分析的缓冲区溢出检测算法},
  journal       = {Journal of Chemical Information and Modeling},
  year          = {2013},
  volume        = {53},
  number        = {9},
  pages         = {1689--1699},
  __markedentry = {[ccc:6]},
  abstract      = {applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\一种基于代码静态分析的缓冲区溢出检测算法.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {first select,icle,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  pmid          = {25246403},
}

@Article{Zhao2013,
  author        = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve},
  title         = {{Formal Verification of SSA-based Optimizations for LLVM}},
  journal       = {Acm Sigplan {\ldots}},
  year          = {2013},
  pages         = {175--186},
  __markedentry = {[ccc:6]},
  abstract      = {Modern compilers, such as LLVM and GCC, use a static single assignment(SSA) intermediate representation (IR) to simplify and enable many advanced optimizations. However, formally verifying the correctness of SSA-based optimizations is challenging because SSA properties depend on a function's entire control-flow graph. This paper addresses this challenge by developing a proof technique for proving SSA-based program invariants and compiler optimizations. We use this technique in the Coq proof assistant to create mechanized correctness proofs of several "micro" transformations that form the building blocks for larger SSA optimizations. To demonstrate the utility of this approach, we formally verify a variant of LLVM's mem2reg transformation in Vellvm, a Coq-based formal semantics of the LLVM IR. The extracted implementation generates code with performance comparable to that of LLVM's unverified implementation.},
  doi           = {10.1145/2491956.2462164},
  file          = {:article\\Formal Verification of SSA-based Optimizations for LLVM.pdf:pdf},
  isbn          = {978-1-4503-2014-6},
  issn          = {03621340},
  keywords      = {binary,coq,llvm,single static assignment,stat,static analysi,static analysis},
  mendeley-tags = {binary},
  url           = {http://doi.acm.org/10.1145/2491956.2462164$\backslash$nhttp://dl.acm.org/ft{\_}gateway.cfm?id=2462164{\&}type=pdf},
}

@Article{Wu2013,
  author        = {Wu, Lei and Grace, Michael and Zhou, Yajin and Wu, Chiachih and Jiang, Xuxian},
  title         = {{The impact of vendor customizations on android security}},
  journal       = {Ccs},
  year          = {2013},
  number        = {1},
  pages         = {623--634},
  __markedentry = {[ccc:6]},
  abstract      = {The smartphone market has grown explosively in recent years, as more and more consumers are attracted to the sensor-studded mul- tipurpose devices. Android is particularly ascendant; as an open platform, smartphone manufacturers are free to extend and modify it, allowing them to differentiate themselves from their competitors. However, vendor customizations will inherently impact overall An- droid security and such impact is still largely unknown. In this paper, we analyze ten representative stock Android im- ages from ﬁve popular smartphone vendors (with two models from each vendor). Our goal is to assess the extent of security issues that may be introduced from vendor customizations and further de- termine how the situation is evolving over time. In particular, we take a three-stage process: First, given a smartphone's stock im- age, we perform provenance analysis to classify each app in the image into three categories: apps originating from the AOSP, apps customized or written by the vendor, and third-party apps that are simply bundled into the stock image. Such provenance analysis allows for proper attribution of detected security issues in the ex- amined Android images. Second, we analyze permission usages of pre-loaded apps to identify overprivileged ones that unnecessarily request more Android permissions than they actually use. Finally, in vulnerability analysis, we detect buggy pre-loaded apps that can be exploited to mount permission re-delegation attacks or leak pri- vate information. Our evaluation results are worrisome: vendor customizations are signiﬁcant on stock Android devices and on the whole responsible for the bulk of the security problems we detected in each device. Speciﬁcally, our results show that on average 85.78{\%} of all pre- loaded apps in examined stock images are overprivileged with a majority of them directly from vendor customizations. In addition, 64.71{\%} to 85.00{\%} of vulnerabilities we detected in examined im- ages from every vendor (except for Sony) arose from vendor cus- tomizations. In general, this pattern held over time – newer smart- phones, we found, are not necessarily more secure than older ones.},
  doi           = {10.1145/2508859.2516728},
  file          = {:article\\The impact of vendor customizations on android security.pdf:pdf},
  isbn          = {9781450324779},
  issn          = {15437221},
  keywords      = {android,customization,obfuscate,provenance,stat,static analysi,static analysis,web},
  mendeley-tags = {android,obfuscate,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2508859.2516728},
}

@Article{Moshtari2013,
  author        = {Moshtari, Sara and Sami, Ashkan and Azimi, Mahdi},
  title         = {{Using complexity metrics to improve software security}},
  journal       = {Computer Fraud and Security},
  year          = {2013},
  volume        = {2013},
  number        = {5},
  pages         = {8--17},
  __markedentry = {[ccc:6]},
  abstract      = {Information technology is quickly spreading across critical infrastructures and software has become an inevitable part of industries and organisations. At the same time, many cyberthreats are the result of poor software coding. Stuxnet, which was the most powerful cyber-weapon used against industrial control systems, exploited zero-day vulnerabilities in Microsoft Windows.1 The US Department of Homeland Security (DHS) also announced that software vulnerabilities are among the three most common cyber-security vulnerabilities in Industrial Control Systems (ICSs).2 Therefore, improving software security has an important role in increasing the security level of computer-based systems.},
  doi           = {10.1016/S1361-3723(13)70045-9},
  file          = {:article\\Using complexity metrics to improve software security.pdf:pdf},
  groups        = {vice-important},
  issn          = {13613723},
  keywords      = {binary,first select,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/S1361-3723(13)70045-9},
}

@Article{Zhi-Hua2013,
  author        = {ZHOU, Zhi-Hua},
  title         = {{Disagreement-based Semi-supervised Learning}},
  journal       = {Acta Automatica Sinica},
  year          = {2013},
  volume        = {39},
  number        = {11},
  pages         = {1871},
  __markedentry = {[ccc:6]},
  doi           = {10.3724/SP.J.1004.2013.01871},
  file          = {:article\\Disagreement-based Semi-supervised Learning.pdf:pdf},
  issn          = {0254-4156},
  keywords      = {11,2013,39,acta automatica sinica,disagreement-based semi-supervised learning,machine learning,predicte,semi-supervised learning,unlabeled data,web},
  mendeley-tags = {machine learning,predicte,web},
  url           = {http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1004.2013.01871},
}

@PhdThesis{MER2013,
  author        = {{Michelle Elaine Ruse}},
  title         = {{Model checking techniques for vulnerability analysis of Web applications}},
  year          = {2013},
  __markedentry = {[ccc:6]},
  booktitle     = {Ph.D. Dissertation},
  file          = {:article\\Model checking techniques for vulnerability analysis of Web applications.pdf:pdf},
  groups        = {vice-important},
  keywords      = {binary,book,first select,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,book,first select,source code,source code-vice important,web},
}

@Article{Davidson2013,
  author        = {Davidson, Drew and Moench, Benjamin and Jha, Somesh and Ristenpart, Thomas},
  title         = {{FIE on Firmware: Finding Vulnerabilities in Embedded Systems using Symbolic Execution}},
  journal       = {Proceedings of the 22nd USENIX Security Symposium},
  year          = {2013},
  pages         = {463--478},
  __markedentry = {[ccc:6]},
  file          = {:article\\FIE on Firmware Finding Vulnerabilities in Embedded Systems using Symbolic Execution.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781931971034},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {https://www.usenix.org/conference/usenixsecurity13/technical-sessions/paper/davidson},
}

@Article{Tomas2013,
  author        = {Tomas, P. and Escalona, M. J. and Mejias, M.},
  title         = {{Open source tools for measuring the Internal Quality of Java software products. A survey}},
  journal       = {Computer Standards and Interfaces},
  year          = {2013},
  volume        = {36},
  number        = {1},
  pages         = {244--255},
  __markedentry = {[ccc:6]},
  abstract      = {Collecting metrics and indicators to assess objectively the different products resulting during the lifecycle of a software project is a research area that encompasses many different aspects, apart from being highly demanded by companies and software development teams. Focusing on software products, one of the most used methods by development teams for measuring Internal Quality is the static analysis of the source code. This paper works in this line and presents a study of the state-of-the-art open source software tools that automate the collection of these metrics, particularly for developments in Java. These tools have been compared according to certain criteria defined in this study. {\textcopyright} 2013 The Authors.},
  doi           = {10.1016/j.csi.2013.08.006},
  file          = {:article\\Open source tools for measuring the Internal Quality of Java software products. A survey.pdf:pdf},
  groups        = {vice-important},
  issn          = {09205489},
  keywords      = {Automation,Internal Quality,Java,Metrics,Open source,Software product,Source code,Static analysis,Tools,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,predicte,source code,source code-vice important,survey,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.csi.2013.08.006},
}

@Article{Radjenovic2013,
  author        = {Radjenovi{\'{c}}, Danijel and Heri{\v{c}}ko, Marjan and Torkar, Richard and {\v{Z}}ivkovi{\v{c}}, Ale{\v{s}}},
  title         = {{Software fault prediction metrics: A systematic literature review}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {8},
  pages         = {1397--1418},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Software metrics may be used in fault prediction models to improve software quality by predicting fault location. Objective: This paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. Method: This systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. Results: Object-oriented metrics (49{\%}) were used nearly twice as often compared to traditional source code metrics (27{\%}) or process metrics (24{\%}). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. Conclusion: More studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2013.02.009},
  file          = {:article\\Software fault prediction metrics A systematic literature review.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {09505849},
  issn          = {09505849},
  keywords      = {Software fault prediction,Software metric,Systematic literature review,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey,web},
}

@Article{孙浩2013,
  author        = {孙浩 and 李会朋 and 曾庆凯},
  title         = {基于信息流的整数漏洞插装和验证},
  journal       = {软件学报},
  year          = {2013},
  volume        = {24},
  number        = {12},
  pages         = {2767--2781},
  __markedentry = {[ccc:6]},
  abstract      = {为降低整数漏洞插装验证的运行开销，提出基于信息流的整数漏洞插装方法．从限定分析对象范围的角 度出发，将分析对象约减为污染信息流路径上的所有危险整数操作，以降低静态插装密度．在GCC平台上，实现了原 型系统DRIVER(detect and run．time check integer-based vulnerabilities with information flow)．实验结果表明，该方法 具有精度高、开销低、定位精确等优点．},
  file          = {:article\\基于信息流的整数漏洞插装和验证.pdf:pdf},
  groups        = {vice-important},
  keywords      = {first select,source code,source code-vice important,整数漏洞；信息流；污点分析；插装},
  mendeley-tags = {first select,source code,source code-vice important},
}

@Article{Yamaguchi2013,
  author        = {Yamaguchi, Fabian and Wressnegger, Christian and Gascon, Hugo and Rieck, Konrad},
  title         = {{Chucky: Exposing Misssing Checks in Source Code for Vulnerability Discovery}},
  journal       = {Proceedings of the 2013 ACM SIGSAC conference on Computer {\&} communications security - CCS '13},
  year          = {2013},
  number        = {October 2015},
  pages         = {499--510},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2508859.2516665},
  file          = {:article\\Chucky Exposing Misssing Checks in Source Code for Vulnerability Discovery.pdf:pdf},
  groups        = {imprortant},
  isbn          = {9781450324779},
  keywords      = {anomaly detection,binary,first select,fuzz,machine learning,second select,source code,source code-important,stat,static analysi,static analysis,vulnerabilities,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,second select,source code,source code-important,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2508859.2516665},
}

@Article{Han2013,
  author        = {Han, Jih and Yan, Qiang and Gao, Debin and Zhou, Jianying and Deng, Robert H},
  title         = {{Comparing Mobile Privacy Protection through Cross-Platform Applications}},
  journal       = {Network and Distributed System Security Symposium},
  year          = {2013},
  pages         = {1--15},
  __markedentry = {[ccc:6]},
  abstract      = {With the rapid growth of the mobile market, secu- rity of mobile platforms is receiving increasing attention from both research community as well as the public. In this paper, we make the first attempt to establish a baseline for security comparison between the two most popular mobile platforms. We investigate applications that run on both Android and iOS and examine the difference in the usage of their security sensitive APIs (SS-APIs). Our analysis over 2,600 applications shows that iOS applications consistently access more SS-APIs than their counterparts on Android. The additional privileges gained on iOS are often associated with accessing private resources such as device ID, camera, and users' contacts. A possible explanation for this difference in SS-API usage is that privileges obtained by an application on the current iOS platform are invisible to end users. Our analysis shows that: 1) third-party libraries (specifically advertising and an- alytic libraries) on iOS invoke more SS-APIs than those on Android; 2) Android application developers avoid requesting unnecessary privileges which will be shown in the permission list during application installation. Considering the fact that an Android application may gain additional privileges with privilege-escalation attacks and iOS provides a more restricted privilege set accessible by third-party applications, our results do not necessarily imply that Android provides better privacy protection than iOS. However, our evidence suggests that Apple's application vetting process may not be as effective as Android's privilege notification mechanism, particularly in protecting sensitive resources from third-party applications.},
  file          = {:article\\Comparing Mobile Privacy Protection through Cross-Platform Applications.pdf:pdf},
  keywords      = {android,binary,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,web},
  url           = {http://www.liaiqin.com/hanjin/$\backslash$npapers3://publication/uuid/EDE08F21-0175-4B99-B31B-86FC339DAFB4},
}

@Article{Garousi2013,
  author        = {Garousi, Vahid and Mesbah, Ali and Betin-Can, Aysu and Mirshokraie, Shabnam},
  title         = {{A systematic mapping study of web application testing}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {8},
  pages         = {1374--1396},
  __markedentry = {[ccc:6]},
  abstract      = {Context: The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field. Method We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study. Results Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2013.02.006},
  file          = {:article\\A systematic mapping study of web application testing.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Bibliometrics,Paper repository,Systematic mapping,Testing,Web application,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,survey,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2013.02.006},
}

@Article{CCF2013,
  author        = {CCF},
  title         = {中国计算机学会推荐国际学术会议和期刊目录（ 2012）},
  year          = {2013},
  volume        = {21},
  number        = {1},
  pages         = {25--32},
  __markedentry = {[ccc:6]},
  file          = {:article\\中国计算机学会推荐国际学术会议和期刊目录（ 2012）.pdf:pdf},
  keywords      = {book,fuzz,machine learning,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {book,fuzz,machine learning,survey,web},
}

@Article{Brauer2013,
  author        = {Brauer, J{\"{o}}rg and King, Andy and Kowalewski, Stefan},
  title         = {{Abstract interpretation of microcontroller code: Intervals meet congruences}},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {7},
  pages         = {862--883},
  __markedentry = {[ccc:6]},
  abstract      = {Bitwise instructions, loops and indirect data access present challenges to the verification of microcontroller programs. In particular, since registers are often memory mapped, it is necessary to show that an indirect store operation does not accidentally mutate a register. To prove this and related properties, this article advocates using the domain of bitwise linear congruences in conjunction with intervals to derive accurate range information. The paper argues that these two domains complement one another when reasoning about microcontroller code. The paper also explains how SAT solving, which applied with dichotomic search, can be used to recover branching conditions from binary code which, in turn, further improves interval analysis. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.scico.2012.06.001},
  file          = {:article\\Abstract interpretation of microcontroller code Intervals meet congruences.pdf:pdf},
  groups        = {vice-important},
  issn          = {01676423},
  keywords      = {Abstract interpretation,Binary code,Embedded systems,Intervals,Linear congruences,binary,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2012.06.001},
}

@Article{Diaz2013,
  author        = {D{\'{i}}az, Gabriel and Bermejo, Juan Ram{\'{o}}n},
  title         = {{Static analysis of source code security: Assessment of tools against SAMATE tests}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {8},
  pages         = {1462--1476},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Static analysis tools are used to discover security vulnerabilities in source code. They suffer from false negatives and false positives. A false positive is a reported vulnerability in a program that is not really a security problem. A false negative is a vulnerability in the code which is not detected by the tool. Objective: The main goal of this article is to provide objective assessment results following a well-defined and repeatable methodology that analyzes the performance detecting security vulnerabilities of static analysis tools. The study compares the performance of nine tools (CBMC, K8-Insight, PC-lint, Prevent, Satabs, SCA, Goanna, Cx-enterprise, Codesonar), most of them commercials tools, having a different design. Method: We executed the static analysis tools against SAMATE Reference Dataset test suites 45 and 46 for C language. One includes test cases with known vulnerabilities and the other one is designed with specific vulnerabilities fixed. Afterwards, the results are analyzed by using a set of well known metrics. Results: Only SCA is designed to detect all vulnerabilities considered in SAMATE. None of the tools detect "cross-site scripting" vulnerabilities. The best results for F-measure metric are obtained by Prevent, SCA and K8-Insight. The average precision for analyzed tools is 0.7 and the average recall is 0.527. The differences between all tools are relevant, detecting different kinds of vulnerabilities. Conclusions: The results provide empirical evidences that support popular propositions not objectively demonstrated until now. The methodology is repeatable and allows ranking strictly the analyzed static analysis tools, in terms of vulnerabilities coverage and effectiveness for detecting the highest number of vulnerabilities having few false positives. Its use can help practitioners to select appropriate tools for a security review process of code. We propose some recommendations for improving the reliability and usefulness of static analysis tools and the process of benchmarking. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2013.02.005},
  file          = {:article\\Static analysis of source code security Assessment of tools against SAMATE tests(2).pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Quality analysis and evaluation,Security development lifecycle,Security tools,Software/program verification,Vulnerability,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2013.02.005},
}

@PhdThesis{Tobergte2013b,
  author        = {Tobergte, David R. and Curtis, Shirley},
  title         = {{Model checking techniques for vulnerability analysis of Web applications}},
  school        = {Iowa State University},
  year          = {2013},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\Model checking techniques for vulnerability analysis of Web applications.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {binary,first select,icle,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,source code,source code-vice important,web},
  pmid          = {25246403},
}

@Article{Zhu2013,
  author        = {Zhu, Haiyan and Dillig, Thomas and Dillig, Isil},
  title         = {{Automated inference of library specifications for source-sink property verification}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {8301 LNCS},
  pages         = {290--306},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-319-03542-0_21},
  file          = {:article\\Automated inference of library specifications for source-sink property verification.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9783319035413},
  issn          = {03029743},
  keywords      = {binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Austin2013,
  author        = {Austin, Andrew and Holmgreen, Casper and Williams, Laurie},
  title         = {{A comparison of the efficiency and effectiveness of vulnerability discovery techniques}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {7},
  pages         = {1279--1288},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. A number of vulnerability discovery techniques are available, each with their own strengths. Objective: The objective of this research is to aid in the selection of vulnerability discovery techniques by comparing the vulnerabilities detected by each and comparing their efficiencies. Method: We conducted three case studies using three electronic health record systems to compare four vulnerability discovery techniques: exploratory manual penetration testing, systematic manual penetration testing, automated penetration testing, and automated static analysis. Results: In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered that the specific set of vulnerabilities identified by one tool was largely orthogonal to that of other tools. Systematic manual penetration testing found the most design flaws, while automated static analysis found the most implementation bugs. The most efficient discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing. Conclusion: The results show that employing a single technique for vulnerability discovery is insufficient for finding all types of vulnerabilities. Each technique identified only a subset of the vulnerabilities, which, for the most part were independent of each other. Our results suggest that in order to discover the greatest variety of vulnerability types, at least systematic manual penetration testing and automated static analysis should be performed. ?? 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2012.11.007},
  file          = {:article\\A comparison of the efficiency and effectiveness of vulnerability discovery techniques(2).pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0950-5849},
  issn          = {09505849},
  keywords      = {Black box testing,Penetration testing,Security,Static analysis,Vulnerability,White box testing,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2012.11.007},
}

@Article{Kamel2013,
  author        = {Kamel, Nassima and Lanet, Jean-Louis},
  title         = {{Risks induced by Web applications on smart cards}},
  journal       = {Journal of Information Security and Applications},
  year          = {2013},
  volume        = {18},
  number        = {2–3},
  pages         = {148--156},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract The evolution of new smart cards with improved processing power and memory size makes it possible to integrate a web server. This provides a way to simplify the integration of smart card to all existing equipments using standard protocols. However it opens up the possibilities to existing Web attacks that exploit Web application vulnerabilities. In this paper, we focus on the most common and dangerous attack named cross site scripting (XSS) and we propose solutions to prevent and check if the Web application is well developed by applying secured development methodology. },
  doi           = {http://dx.doi.org/10.1016/j.jisa.2013.09.002},
  file          = {:article\\Risks induced by Web applications on smart cards.pdf:pdf},
  issn          = {2214-2126},
  keywords      = {Data-tainting,Java Card 3,Smart card embedded Web server,Static analysis,fuzz,predicte,stat,static analysi,static analysis,web,{\{}XSS{\}} attacks},
  mendeley-tags = {fuzz,predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://www.sciencedirect.com/science/article/pii/S2214212613000483},
}

@Article{Zhou2013,
  author        = {Zhou, Xiaoyong and Demetriou, Soteris and He, Dongjing and Naveed, Muhammad and Pan, Xiaorui and Wang, XiaoFeng and Gunter, Carl a. and Nahrstedt, Klara},
  title         = {{Identity, location, disease and more: inferring your secrets from android public resources}},
  journal       = {Proceedings of the 2013 ACM SIGSAC conference on Computer {\&} communications security - CCS '13},
  year          = {2013},
  pages         = {1017--1028},
  __markedentry = {[ccc:6]},
  abstract      = {The design of Android is based on a set of unprotected shared resources, including those inherited from Linux (e.g., Linux public directories). However, the dramatic development in Android applications (app for short) makes available a large amount of public background information (e.g., social networks, public online services), which can potentially turn such originally harmless resource sharing into serious privacy breaches. In this paper, we report our work on this important yet understudied problem. We discovered three unexpected channels of information leaks on Android: per-app data-usage statistics, ARP information, and speaker status (on or off). By monitoring these channels, an app without any permission may acquire sensitive information such as smartphone user's identity, the disease condition she is interested in, her geo-locations and her driving route, from top-of-the-line Android apps. Furthermore, we show that using existing and new techniques, this zero-permission app can both determine when its target (a particular application) is running and send out collected data stealthily to a remote adversary. These findings call into question the soundness of the design assumptions on shared resources, and demand effective solutions. To this end, we present a mitigation mechanism for achieving a delicate balance between utility and privacy of such resources.},
  doi           = {10.1145/2508859.2516661},
  file          = {:article\\Identity, location, disease and more inferring your secrets from android public resources.pdf:pdf},
  isbn          = {9781450324779},
  issn          = {15437221},
  keywords      = {information leaks,mobile security,predicte,privacy,web},
  mendeley-tags = {predicte,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2508859.2516661},
}

@Article{Vanegue2013,
  author        = {Vanegue, Julien and Lahiri, Shuvendu K.},
  title         = {{Towards practical reactive security audit using extended static checkers}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2013},
  pages         = {33--47},
  __markedentry = {[ccc:6]},
  abstract      = {This paper describes our experience of performing reactive security audit of known security vulnerabilities in core operating system and browser COM components, using an extended static checker HAVOCLITE. We describe the extensions made to the tool to be applicable on such large C++ components, along with our experience of using an extended static checker in the large. We argue that the use of such checkers as a configurable static analysis in the hands of security auditors can be an effective tool for finding variations of known vulnerabilities. The effort has led to finding and fixing around 70 previously unknown security vulnerabilities in over 10 millions lines operating system and browser code.},
  doi           = {10.1109/SP.2013.12},
  file          = {:article\\Towards practical reactive security audit using extended static checkers.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9780769549774},
  issn          = {10816011},
  keywords      = {binary,extended static checking,first select,fuzz,obfuscate,program verification,second select,security audit,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,obfuscate,second select,source code,source code-important,source code-vice important,web},
}

@Article{Zaddach2013,
  author        = {Zaddach, Jonas and Costin, Andrei},
  title         = {{Embedded Devices Security and Firmware Reverse Engineering}},
  journal       = {Black Hat USA},
  year          = {2013},
  pages         = {9},
  __markedentry = {[ccc:6]},
  abstract      = {Embedded devices have become the usual presence in the network of (m)any household(s), SOHO, enterprise or criti- cal infrastructure. The preached Internet of Things promises to gazillion- uple their number and heterogeneity in the next few years. However, embedded devices are becoming lately the usual suspects in security breaches and security advisories and thus become the Achilles' heel of one's overall infrastructure se- curity. An important aspect is that embedded devices run on what's commonly known as firmwares. To understand how to secure embedded devices, one needs to understand their firmware and how it works. This workshop aims at presenting a quick-start at how to inspect firmwares and a hands-on presentation with exercises on real firmwares from a security analysis standpoint. General},
  file          = {:article\\Embedded Devices Security and Firmware Reverse Engineering.pdf:pdf},
  keywords      = {backdoors,bi-,binary,embedded devices,exploitation,firmware,firmware analysis,firmware unpacking,fuzz,nary analysis,obfuscate,reverse,reverse engineering,security,stat,static analysi,static analysis,vulnerabilities,web},
  mendeley-tags = {binary,fuzz,obfuscate,reverse,web},
  url           = {https://media.blackhat.com/us-13/US-13-Zaddach-Workshop-on-Embedded-Devices-Security-and-Firmware-Reverse-Engineering-WP.pdf},
}

@Article{Avancini2013,
  author        = {Avancini, Andrea and Ceccato, Mariano},
  title         = {{Comparison and integration of genetic algorithms and dynamic symbolic execution for security testing of cross-site scripting vulnerabilities}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {12},
  pages         = {2209--2222},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Cross-site scripting (XSS for short) is considered one of the major threat to the security of web applications. Static analysis supports manual security review in mitigating the impact of XSS-related issues, by suggesting a set of potential problems, expressed in terms of candidate vulnerabilities. A security problem spotted by static analysis, however, consists of a list of (possibly complicated) conditions that should be satisfied to concretely exploit a vulnerability. Static analysis, instead, does not provide examples of what input values must be used to make the application execute the (sometimes complex) execution path that causes a XSS vulnerability. Runnable test cases, however, consist of an executable and reproducible evidence of the vulnerability mechanics. Test cases represent a valuable support for developers who should concretely understand security problems in detail before fixing them. Objective: This paper evaluates various strategies to automatically generate security test cases, i.e. Test cases that expose a vulnerability by making the application control flow satisfy vulnerability conditions. Method: A combination of genetic algorithms and concrete symbolic execution is presented for the automatic generation of security test cases. This combined strategy is compared with genetic algorithms and with concrete symbolic execution alone, in terms of coverage and productivity on four case study web applications. Result: While genetic algorithms require less time to generate security test cases, those generated by concrete symbolic execution cover a higher number of vulnerabilities. The highest coverage, however, is achieved when the two approaches are combined and integrated. Conclusion: The integrated approach that we propose has shown to be effective for security testing. In fact, genetic algorithms have shown to be able to generate test cases only for few and simple vulnerabilities when not combined with other approaches. However, their contribution is fundamental to improve the coverage of test cases generated by concrete symbolic execution. ?? 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2013.08.001},
  file          = {:article\\Comparison and integration of genetic algorithms and dynamic symbolic execution for security testing of cross.pdf:pdf},
  groups        = {vice-important},
  issn          = {09505849},
  keywords      = {Dynamic analysis,Security testing,Static analysis,binary,first select,machine learning,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,source code,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2013.08.001},
}

@Article{Cankaya2013,
  author        = {Cankaya, Ebru Celikel and Nair, Suku and Cankaya, Hakki C},
  title         = {{Applying error correction codes to achieve security and dependability}},
  journal       = {Computer Standards {\&} Interfaces},
  year          = {2013},
  volume        = {35},
  number        = {1},
  pages         = {78--86},
  __markedentry = {[ccc:6]},
  abstract      = {We apply Linear Error Correction (LEC) code to a novel encoding scheme to assure two fundamental requirements for transmission channels and storage units: security and dependability. Our design has the capacity to adapt itself to different applications and their various characteristics such as availability, error rate, and vulnerabilities. Based on simple logic operations, our scheme affords fast encryption, scalability (dual or more column erasures), and flexibility (LEC encoder employed as a front end to any conventional compression scheme). Performance results are very promising: Experiments on dual erasures outperform conventional compression algorithms including Arithmetic Coding, Huffman, and LZ77. },
  doi           = {http://dx.doi.org/10.1016/j.csi.2012.06.009},
  file          = {:article\\Applying error correction codes to achieve security and dependability.pdf:pdf},
  issn          = {0920-5489},
  keywords      = {Compression,Dependability,Security,binary,web,{\{}LEC{\}} codes},
  mendeley-tags = {binary,web},
  publisher     = {Elsevier B.V.},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548912000864},
}

@Article{Sbirlea2013,
  author        = {Sb{\^{i}}rlea, Dragoş and Burke, Micheal G. and Guarnieri, Salvatore and Pistoia, Marco and Sarkar, Vivek},
  title         = {{Automatic Detection of Inter-application Permission Leaks in Android Applications}},
  journal       = {IBM Journal of Research and Development},
  year          = {2013},
  volume        = {57},
  number        = {6},
  pages         = {1--20},
  __markedentry = {[ccc:6]},
  abstract      = {The Android{\textregistered} operating system builds upon already well-established permission systems but complements them by allowing application components to be reused within and across applications through a single communication mechanism, called the Intent mechanism. In this paper, we describe techniques that we developed for statically detecting Android application vulnerability to attacks that obtain unauthorized access to permission-protected information. We address three kinds of such attacks, known as confused deputy, permission collusion, and Intent spoofing. We show that application vulnerability to these attacks can be detected using taint analysis. Based on this technique, we developed PermissionFlow, a tool for discovering vulnerabilities in the byte code and configuration of Android applications. To enable PermissionFlow analysis, we developed a static technique for automatic identification of permission-protected information sources in permission-based systems. This technique identifies application programming interfaces (APIs) whose execution leads to permission checking and considers these APIs to be sources of taint. Based on this approach, we developed Permission Mapper, a component of PermissionFlow that improves on previous work by performing fully automatic identification of such APIs for Android Java{\textregistered} code. Our automated analysis of popular applications found that 56{\%} of the most popular 313 Android applications actively use intercomponent information flows. Among the tested applications, PermissionFlow found four exploitable vulnerabilities. By helping ensure the absence of inter-application permission leaks, we believe that the proposed analysis will be highly beneficial to the Android ecosystem and other mobile platforms that may use similar analyses in the future.},
  doi           = {10.1147/JRD.2013.2284403},
  file          = {:article\\Automatic Detection of Inter-application Permission Leaks in Android Applications.pdf:pdf},
  groups        = {vice-important},
  issn          = {0018-8646},
  keywords      = {android,binary,first select,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,first select,source code,source code-vice important,web},
  url           = {http://www.cs.rice.edu/{~}vs3/PDF/PermissionFlow-TR.pdf},
}

@Article{Rahimi2013,
  author        = {Rahimi, Sanaz and Zargham, Mehdi},
  title         = {{Vulnerability scrying method for software vulnerability discovery prediction without a vulnerability database}},
  journal       = {IEEE Transactions on Reliability},
  year          = {2013},
  volume        = {62},
  number        = {2},
  pages         = {395--407},
  __markedentry = {[ccc:6]},
  abstract      = {Predicting software vulnerability discovery trends can help improve secure deployment of software applications and facilitate backup provisioning, disaster recovery, diversity planning, and maintenance scheduling. Vulnerability discovery models (VDMs) have been studied in the literature as a means to capture the underlying stochastic process. Based on the VDMs, a few vulnerability prediction schemes have been proposed. Unfortunately, all these schemes suffer from the same weaknesses: they require a large amount of historical vulnerability data from a database (hence they are not applicable to a newly released software application), their precision depends on the amount of training data, and they have significant amount of error in their estimates. In this work, we propose vulnerability scrying, a new paradigm for vulnerability discovery prediction based on code properties. Using compiler-based static analysis of a codebase, we extract code properties such as code complexity (cyclomatic complexity), and more importantly code quality (compliance with secure coding rules), from the source code of a software application. Then we propose a stochastic model which uses code properties as its parameters to predict vulnerability discovery. We have studied the impact of code properties on the vulnerability discovery trends by performing static analysis on the source code of four real-world software applications. We have used our scheme to predict vulnerability discovery in three other software applications. The results show that even though we use no historical data in our prediction, vulnerability scrying can predict vulnerability discovery with better precision and less divergence over time.},
  doi           = {10.1109/TR.2013.2257052},
  file          = {:article\\Vulnerability scrying method for software vulnerability discovery prediction without a vulnerability database.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {00189529},
  keywords      = {Code security,binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerability discovery model,vulnerability prediction,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Shar2013,
  author        = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
  title         = {{Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {10},
  pages         = {1767--1780},
  __markedentry = {[ccc:6]},
  abstract      = {Context SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure. Objective This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS. Method From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities. Results We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93{\%} recall and 11{\%} false alarm rate in predicting SQLI vulnerabilities, and 78{\%} recall and 6{\%} false alarm rate in predicting XSS vulnerabilities. Conclusion The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.infsof.2013.04.002},
  file          = {:article\\Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467310673},
  issn          = {09505849},
  keywords      = {Data mining,Empirical study,Input sanitization,Static code attributes,Vulnerability prediction,Web application vulnerability,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2013.04.002},
}

@Article{Zonouz2013,
  author        = {Zonouz, Saman and Houmansadra, Amir and Berthiera, Robin and Borisova, Nikita and Sanders, William},
  title         = {{Secloud: A cloud-based comprehensive and lightweight security solution for smartphones}},
  journal       = {Computers and Security},
  year          = {2013},
  volume        = {37},
  pages         = {215--227},
  __markedentry = {[ccc:6]},
  abstract      = {As smartphones are becoming more complex and powerful to provide better functionalities, concerns are increasing regarding security threats against their users. Since smart-phones use a software architecture similar to PCs, they are vulnerable to the same classes of security risks. Unfortunately, smartphones are constrained by their limited resources that prevent the integration of advanced security monitoring solutions that work with traditional PCs. We propose Secloud, a cloud-based security solution for smartphone devices. Secloud emulates a registered smartphone device inside a designated cloud and keeps it synchronized by continuously passing the device inputs and network connections to the cloud. This allows Secloud to perform a resource-intensive security analysis on the emulated replica that would otherwise be infeasible to run on the device itself. We demonstrate the practical feasibility of Secloud through a prototype for Android devices and illustrate its resource effectiveness by comparing it with on-device solutions. ?? 2013 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.cose.2013.02.002},
  file          = {:article\\Secloud A cloud-based comprehensive and lightweight security solution for smartphones.pdf:pdf},
  isbn          = {0167-4048},
  issn          = {01674048},
  keywords      = {Cloud computing,Energy-aware security,Intrusion detection,Real-time intrusion response,Smartphone security,android,stat,static analysi,static analysis,web},
  mendeley-tags = {android,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2013.02.002},
}

@Article{Glodek2013,
  author        = {Glodek, William and Harang, Richard},
  title         = {{Rapid permissions-based detection and analysis of mobile malware using random decision forests}},
  journal       = {Proceedings - IEEE Military Communications Conference MILCOM},
  year          = {2013},
  pages         = {980--985},
  __markedentry = {[ccc:6]},
  abstract      = {The explosion in mobile malware has led to the need for early, rapid detection mechanisms that can detect malware and identify risky applications prior to their deployment on end-user devices without the high cost of manual static and dynamic analysis. Previous work has shown that specific combinations of Android permissions, intents, broadcast receivers, native code and embedded applications can be effectively used to identify potentially malicious applications. We extend this work by using frequent combinations of such attributes as training features for random decision forest classification of malicious and benign applications. We demonstrate that using combinations of frequently-occuring permissions in this manner significantly improves previous results, and provides true positive rates in excess of 90{\%} while maintaining tractable false positive rates. This is true even with novel malware that is not reliably detected at the time of release by conventional anti-malware tools. In addition, the auxiliary information generated by the random decision forest algorithm provides useful insights into the key indicators of malicious activity and the functionality of the associated malware.},
  doi           = {10.1109/MILCOM.2013.170},
  file          = {:article\\Rapid permissions-based detection and analysis of mobile malware using random decision forests.pdf:pdf},
  isbn          = {9780769551241},
  keywords      = {Android,Machine learning,Mobile malware,Random decision forest,machine learning,stat,static analysi,static analysis},
  mendeley-tags = {machine learning},
}

@Article{Mou2013,
  author        = {Mou, Lili and Li, Ge and Liu, Yuxuan and Peng, Hao and Jin, Zhi and Xu, Yan and Zhang, Lu},
  title         = {{Knowledge Science, Engineering and Management}},
  year          = {2013},
  volume        = {8041},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1409.3358v1},
  doi           = {10.1007/978-3-642-39787-5},
  eprint        = {arXiv:1409.3358v1},
  file          = {:article\\Knowledge Science, Engineering and Management.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-3-642-39786-8},
  keywords      = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://link.springer.com/10.1007/978-3-642-39787-5},
}

@Article{Woo2013,
  author        = {Woo, Maverick and Cha, Sang Kil and Gottlieb, Samantha and Brumley, David},
  title         = {{Scheduling black-box mutational fuzzing}},
  journal       = {Proceedings of the 2013 ACM SIGSAC conference on Computer {\&} communications security - CCS '13},
  year          = {2013},
  pages         = {511--522},
  __markedentry = {[ccc:6]},
  abstract      = {Black-box mutational fuzzing is a simple yet effective tech- nique to find bugs in software. Given a set of program-seed pairs, we ask how to schedule the fuzzings of these pairs in order to maximize the number of unique bugs found at any point in time. We develop an analytic framework using a mathematical model of black-box mutational fuzzing and use it to evaluate 26 existing and new randomized online scheduling algorithms. Our experiments show that one of our new scheduling algorithms outperforms the multi-armed bandit algorithm in the current version of the CERT Basic Fuzzing Framework (BFF) by finding 1.5× more unique bugs in the same amount of time. Categories},
  doi           = {10.1145/2508859.2516736},
  file          = {:article\\Scheduling black-box mutational fuzzing.pdf:pdf},
  isbn          = {9781450324779},
  issn          = {15437221},
  keywords      = {binary,fuzz,fuzz configuration scheduling,machine learning,predicte,software security,web},
  mendeley-tags = {binary,fuzz,machine learning,predicte,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2508859.2516736},
}

@Article{李舟军2014,
  author        = {李舟军},
  title         = {软件安全漏洞检测技术},
  year          = {2014},
  __markedentry = {[ccc:6]},
  file          = {:article\\软件安全漏洞检测技术.caj:caj},
  keywords      = {survey},
  mendeley-tags = {survey},
}

@Article{Yamaguchi2014,
  author        = {Yamaguchi, Fabian and Golde, Nico and Arp, Daniel and Rieck, Konrad},
  title         = {{Modeling and discovering vulnerabilities with code property graphs}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2014},
  pages         = {590--604},
  __markedentry = {[ccc:6]},
  abstract      = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method for effectively mining large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
  doi           = {10.1109/SP.2014.44},
  file          = {:article\\Modeling and discovering vulnerabilities with code property graphs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479946860},
  issn          = {10816011},
  keywords      = {Graph Databases,Static Analysis,Vulnerabilities,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Rasthofer2014,
  author        = {Rasthofer, Siegfried and Arzt, Steven and Bodden, Eric},
  title         = {{A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks}},
  journal       = {Proceedings 2014 Network and Distributed System Security Symposium},
  year          = {2014},
  number        = {February},
  pages         = {23--26},
  __markedentry = {[ccc:6]},
  abstract      = {{\#}SUSI. They do static analysis (machine learning) to detect sources and sinks, and they categorized them. Features: method name, parameters, value type, parameter type (is interface?), modifiers, class modifiers, name, dataflow to return, dataflow to return, to sink, abstract sink, required permission. For category, class name, method invocation, body contents, parameter type and return value type.},
  doi           = {10.14722/ndss.2014.23039},
  file          = {:article\\A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks(2).pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1-891562-35-5},
  keywords      = {first select,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,machine learning,second select,source code,source code-important,source code-vice important},
  url           = {http://www.internetsociety.org/doc/machine-learning-approach-classifying-and-categorizing-android-sources-and-sinks},
}

@Article{Breuer2014,
  author        = {Breuer, Peter T. and Pickin, Simon},
  title         = {{Open source verification in an anonymous volunteer network}},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {91},
  number        = {PART B},
  pages         = {161--187},
  __markedentry = {[ccc:6]},
  abstract      = {An 'open' certification process is characterised here that is not based on any central agency, but rather on the option for any party to confirm any part of the certification process at will. The model for this paradigm has been a distributed, piece-wise, semantic audit carried out on the Linux kernel source code using a lightweight formal method. Our goal is a technology that allows open source developers to receive formally backed certifications for their project, in quid pro quo exchanges of resources and expertise with other developers within an amorphous and anonymous cloud of volunteers. To help ensure the integrity of the results, identifying details such as subroutine and variable names are not included in the data sent for analysis, each part of the computation is repeated many times at different sites, and checkpoint information is generated that enables independent checks to be carried out without starting from scratch each time. ?? 2013 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.scico.2013.08.010},
  file          = {:article\\Open source verification in an anonymous volunteer network.pdf:pdf},
  groups        = {vice-important},
  issn          = {01676423},
  keywords      = {Distributed computation,Formal methods,Open source,Software verification,Static analysis,first select,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,source code,source code-vice important},
}

@PhdThesis{Li2014,
  author        = {Li, Siliang},
  title         = {{Improving Quality of Software With Foreign Function Interfaces}},
  school        = {Lehigh University},
  year          = {2014},
  __markedentry = {[ccc:6]},
  file          = {:article\\Improving Quality of Software With Foreign Function Interfaces.pdf:pdf},
  keywords      = {binary,book,stat,static analysi,static analysis},
  mendeley-tags = {binary,book},
  number        = {May},
}

@Article{成荣2014,
  author        = {成荣 and 张方国},
  title         = {安全的程序混淆研究综述},
  journal       = {技术研究},
  year          = {2014},
  __markedentry = {[ccc:6]},
  file          = {:article\\安全的程序混淆研究综述.pdf:pdf},
  keywords      = {cryptography,obfuscate,program obfuscation,virtual black-box property},
  mendeley-tags = {obfuscate},
}

@Article{Zhang2014,
  author        = {Zhang, Dazhi and Liu, Donggang and Csallner, Christoph and Kung, David and Lei, Yu},
  title         = {{A distributed framework for demand-driven software vulnerability detection}},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {87},
  number        = {1},
  pages         = {60--73},
  __markedentry = {[ccc:6]},
  abstract      = {Security testing aims at detecting program security flaws through a set of test cases and has become an active area of research. The challenge is how to efficiently produce test cases that are highly effective in detecting security flaws. This paper presents a novel distributed demand-driven security testing system to address this challenge. It leverages how end users use the software to increase the coverage of essential paths for security testing. The proposed system consists of many client sites and one testing site. The software under test is installed at each client site. Whenever a new path is about to be exercised by a user input, it will be sent to the testing site for security testing. At the testing site, symbolic execution is used to check any potential vulnerability on this new path. If a vulnerability is detected, a signature is automatically generated and updated to all client sites for protection. The benefits are as follows. First, it allows us to focus testing on essential paths, i.e., the paths that are actually being explored by users or attackers. Second, it stops an attacker from exploiting an unreported vulnerability at the client site. A prototype system has been implemented to evaluate the performance of the proposed system. The results show that it is both effective and efficient in practice. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
  doi           = {10.1016/j.jss.2013.08.033},
  file          = {:article\\A distributed framework for demand-driven software vulnerability detection.pdf:pdf},
  issn          = {01641212},
  keywords      = {Security testing,Software vulnerability,Test decomposition,binary},
  mendeley-tags = {binary},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jss.2013.08.033},
}

@Article{Shepperd2014,
  author        = {Shepperd, Martin and Bowes, David and Hall, Tracy},
  title         = {{Researcher bias: The use of machine learning in software defect prediction}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2014},
  volume        = {40},
  number        = {6},
  pages         = {603--616},
  __markedentry = {[ccc:6]},
  abstract      = {Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect onpredictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build arandom effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.},
  doi           = {10.1109/TSE.2014.2322358},
  groups        = {imprortant, vice-important},
  issn          = {00985589},
  keywords      = {Software defect prediction,binary,first select,machine learning,malware,meta-analysis,predicte,researcher bias,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,machine learning,malware,predicte,second select,source code,source code-important,source code-vice important},
}

@Article{Kessentini2014,
  author        = {Kessentini, Wael and Kessentini, Marouane and Sahraoui, Houari and Bechikh, Slim and Ouni, Ali},
  title         = {{A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2014},
  volume        = {40},
  number        = {9},
  pages         = {841--861},
  __markedentry = {[ccc:6]},
  abstract      = {We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of 9 large open source systems where more than 85{\%} of precision and recall scores are obtained on a variety of 8 different types of code-smells.},
  doi           = {10.1109/TSE.2014.2331057},
  file          = {:article\\A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection.pdf:pdf},
  groups        = {vice-important},
  isbn          = {0098-5589},
  issn          = {00985589},
  keywords      = {Search-based software engineering,binary,code-smells,distributed evolutionary algorithms,first select,fuzz,machine learning,predicte,software quality,source code,source code-vice important},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,source code,source code-vice important},
}

@Article{Huang2014,
  author        = {Huang, Shih-kun and Huang, Min-hsiang and Huang, Po-yen and Lu, Han-lin and Lai, Chung-wei},
  title         = {{Software Crash Analysis for Automatic Exploit Generation on Binary Programs}},
  year          = {2014},
  volume        = {63},
  number        = {1},
  pages         = {270--289},
  __markedentry = {[ccc:6]},
  file          = {:article\\Software Crash Analysis for Automatic Exploit Generation on Binary Programs.pdf:pdf},
  keywords      = {binary},
  mendeley-tags = {binary},
}

@Article{Li2014a,
  author        = {Li, Hongzhe and Kwon, Hyuckmin and Kwon, Jonghoon and Lee, Heejo},
  title         = {{A scalable approach for vulnerability discovery based on security patches}},
  journal       = {Communications in Computer and Information Science},
  year          = {2014},
  volume        = {490},
  pages         = {109--122},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-662-45670-5},
  file          = {:article\\A scalable approach for vulnerability discovery based on security patches.pdf:pdf},
  groups        = {vice-important},
  issn          = {18650929},
  keywords      = {binary,first select,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,source code,source code-vice important},
}

@Article{李文明2014,
  author        = {李文明 and 陈哲 and 李绪蓉},
  title         = {缓冲区溢出研究与发展},
  journal       = {计算机应用研究},
  year          = {2014},
  volume        = {9},
  __markedentry = {[ccc:6]},
  abstract      = {为检测出缓存区溢出的发生和预防攻击者利用缓存区溢出漏洞进行攻击， 研究人员提出了各种各样的 检测和预防技术。首先介绍了缓存区溢出的四种攻击方式， 将主流的缓存区溢出检测和预防技术进行了分类， 介绍了每一类的原理、 发展历程和优缺点； 然后对这些检测和预防技术进行了综合讨论； 最后对缓存区溢出检测 和预防技术的未来发展趋势进行了分析与预测。},
  file          = {:article\\缓冲区溢出研究与发展.pdf:pdf},
  keywords      = {survey,缓存区溢出；缓存区溢出攻击；检测；预防},
  mendeley-tags = {survey},
}

@Article{Zhang2014a,
  author        = {Zhang, D.-L. and Jin, D.-H. and Gong, Y.-Z. and Wang, Q. and Dong, Y.-K. and Zhang, H.-L.},
  title         = {{Optimizing static analysis based on defect correlations}},
  journal       = {Ruan Jian Xue Bao/Journal of Software},
  year          = {2014},
  volume        = {25},
  number        = {2},
  pages         = {386--399},
  __markedentry = {[ccc:6]},
  abstract      = {Defect detection generally includes two stages: static analysis and defect inspection. A large number of defects reported may lead developers and managers to reject the use of static analysis tools as part of the development process due to the overhead of defect inspection. To help with the inspection tasks, this paper formally introduces defect correlation, a sound dependency relationship between defects. If the occurrence of one defect causes another defect to occur, the two defects are correlated. This paper presents a sound optimized method to static analysis that can classify the defects reported by static defect detection tool into different groups, in which all defects are false positives (true positives) if the dominant defect is false positives (true positives). The experimental results show a decrease of 22{\%} the time inspecting all defects and the capability and flexibility of this method to detect defects of large, critical or embedded systems. {\textcopyright} Copyright 2014, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
  doi           = {10.13328/j.cnki.jos.004538},
  file          = {:article\\Optimizing static analysis based on defect correlations.caj:caj},
  issn          = {10009825},
  keywords      = {Abstract interpretation,Defect correlation,Optimizing,State slicing,Static analysis,stat,static analysi,static analysis},
}

@Article{半斤八两2014,
  author        = {半斤八两},
  title         = {{纯手工秒杀 VM , SE 等虚拟机 Handle}},
  year          = {2014},
  __markedentry = {[ccc:6]},
  file          = {:article\\纯手工秒杀 VM , SE 等虚拟机 Handle.pdf:pdf},
  keywords      = {obfuscate},
  mendeley-tags = {obfuscate},
}

@Article{Rasthofer2014a,
  author        = {Rasthofer, Siegfried and Arzt, Steven and Bodden, Eric},
  title         = {{A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks}},
  journal       = {Proceedings 2014 Network and Distributed System Security Symposium},
  year          = {2014},
  number        = {February},
  pages         = {23--26},
  __markedentry = {[ccc:6]},
  abstract      = {{\#}SUSI. They do static analysis (machine learning) to detect sources and sinks, and they categorized them. Features: method name, parameters, value type, parameter type (is interface?), modifiers, class modifiers, name, dataflow to return, dataflow to return, to sink, abstract sink, required permission. For category, class name, method invocation, body contents, parameter type and return value type.},
  doi           = {10.14722/ndss.2014.23039},
  file          = {:article\\A Machine-learning Approach for Classifying and Categorizing Android Sources and Sinks.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1-891562-35-5},
  keywords      = {android,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {android,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://www.internetsociety.org/doc/machine-learning-approach-classifying-and-categorizing-android-sources-and-sinks},
}

@Article{Thung2014,
  author        = {Thung, Ferdian and Lucia and Lo, David and Jiang, Lingxiao and Rahman, Foyzur and Devanbu, Premkumar T.},
  title         = {{To what extent could we detect field defects? An extended empirical study of false negatives in static bug-finding tools}},
  journal       = {Automated Software Engineering},
  year          = {2014},
  volume        = {22},
  number        = {4},
  pages         = {561--602},
  __markedentry = {[ccc:6]},
  annote        = {对false positive的成因进行探讨},
  doi           = {10.1007/s10515-014-0169-8},
  file          = {:article\\To what extent could we detect field defects An extended empirical study of false negatives in static bug-finding.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {15737535},
  keywords      = {Empirical study,False negatives,Static bug-finding tools,first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  publisher     = {Springer US},
  url           = {http://dx.doi.org/10.1007/s10515-014-0169-8},
}

@Article{Rockai2014,
  author        = {Rockai, P. and Barnat, J. and Brim, L.},
  title         = {{Model Checking C++ with Exceptions}},
  year          = {2014},
  volume        = {70},
  __markedentry = {[ccc:6]},
  file          = {:article\\Model Checking C with Exceptions.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,earliest-deadline-f,first select,model-checking, task automata, earliest-deadline-f,second select,source code,source code-important,source code-vice important,task automata},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
}

@Article{Khedker2014,
  author        = {Khedker, Uday P.},
  title         = {{Buffer Overflow Analysis for C}},
  year          = {2014},
  __markedentry = {[ccc:6]},
  abstract      = {Buffer overflow detection and mitigation for C programs has been an important concern for a long time. This paper defines a string buffer overflow analysis for C programs. The key ideas of our formulation are (a) separating buffers from the pointers that point to them, (b) modelling buffers in terms of sizes and sets of positions of null characters, and (c) defining stateless functions to compute the sets of null positions and mappings between buffers and pointers.   This exercise has been carried out to test the feasibility of describing such an analysis in terms of lattice valued functions and relations to facilitate automatic construction of an analyser without the user having to write C/C++/Java code. This is facilitated by devising stateless formulations because stateful formulations combine features through side effects in states raising a natural requirement of C/C++/Java code to be written to describe them. Given the above motivation, the focus of this paper is not to build good static approximations for buffer overflow analysis but to show how given static approximations could be formalized in terms of stateless formulations so that they become amenable to automatic construction of analysers.},
  archiveprefix = {arXiv},
  arxivid       = {1412.5400},
  eprint        = {1412.5400},
  file          = {:article\\Buffer Overflow Analysis for C.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://arxiv.org/abs/1412.5400},
}

@Article{Satapathy2014,
  author        = {Satapathy, Suresh Chandra and Biswal, Bhabendra Narayan and Udgata, Siba K. and Mandal, J. K.},
  title         = {{Proceedings of the 3rd International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2014}},
  journal       = {Advances in Intelligent Systems and Computing},
  year          = {2014},
  volume        = {327},
  pages         = {113--122},
  __markedentry = {[ccc:6]},
  abstract      = {Diabetic Retinopathy (DR) is one of the major causes of blindness in diabetic patients. Early detection is required to reduce the visual impairment causing damage to eye. Microaneurysms are the first clinical sign of diabetic retinopathy. Robust detection of microaneurysms in retinal fundus images is critical in developing automated system. In this paper we present a new technique for detection and localization of microaneurysms using Dual tree complex wavelet transform and log Gabor features. Retinal blood vessels are eliminated using minor and major axis properties and correlation is performed on images with the Gabor features to detect the microaneurysms. Feature vectors are extracted from candidate regions based on texture properties. Support vector machine classifier classifies the detected regions to determine the findings as microaneurysms or not. Accuracy of the algorithm is evaluated using the sensitivity and specificity parameters.},
  doi           = {10.1007/978-3-319-11933-5},
  file          = {:article\\Proceedings of the 3rd International Conference on Frontiers of Intelligent Computing Theory and Applications.pdf:pdf},
  isbn          = {9783319119328},
  issn          = {21945357},
  keywords      = {aop,dynamic analysis,instrumentation,profiling,stat,static analysi,static analysis},
  pmid          = {17520397},
}

@InProceedings{Manohar2014,
  author        = {Manohar, Lakshmi and Velicheti, Rao and Feiock, Dennis C and Peiris, Manjula and Raje, Rajeev and Hill, James H},
  title         = {{Towards Modeling the Behavior of Static Code Analysis Tools}},
  booktitle     = {2014 9th Cyber and Information Security Research Conference},
  year          = {2014},
  __markedentry = {[ccc:6]},
  abstract      = {This paper presents preliminary results of an independent study to assess the performance of a static code analysis (SCA) tool's abil- ity to detect and identify weaknesses and vulnerabilities in source code. The goal of the study is to model the behavior of static code analysis tools, and predict what SCA tool, or set of SCA tools, should be applied against a given source code to identify weak- nesses and vulnerabilities.},
  file          = {:article\\Towards Modeling the Behavior of Static Code Analysis Tools.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450328128},
  keywords      = {behavior,evaluation,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,static code analysis tools},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important},
}

@PhdThesis{Wright2014,
  author        = {Wright, Jason L},
  title         = {{SOFTWARE VULNERABILITIES: LIFESPANS, METRICS, AND CASE STUDY}},
  year          = {2014},
  __markedentry = {[ccc:6]},
  abstract      = {It is difficult for end-users to judge the risk posed by software security vulnerabilities. This thesis examines three aspects of the software security vulnerability ecosystem to determine if commonly used metrics are based on sound engineering principles. First, the decision by several security research firms to decrease the grace period before publicly releasing vulnerability details was examined. No evidence was found to suggest periods less than 6 months are effective. Second, two new metrics are presented which are more easily computed, repeatable, and verifiable than previous metrics. Both metrics provide the ability to compare software packages based on number of vulnerabilities and vendor response time. Third, metrics based strictly on known vulnerabilities are brought into question. The number of bugs which represent vulnerabilities is estimated for a particular package and the estimated number of resulting vulnerabilities is found to be far greater than the currently known vulnerabilities.},
  file          = {:article\\SOFTWARE VULNERABILITIES LIFESPANS, METRICS, AND CASE STUDY.pdf:pdf},
  keywords      = {around-vulnerability},
  mendeley-tags = {around-vulnerability},
  number        = {May},
}

@Article{2014,
  title         = {{A Gentle Introduction to Program Analysis}},
  year          = {2014},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Gentle Introduction to Program Analysis.pdf:pdf},
  keywords      = {stat,static analysi,static analysis},
}

@InProceedings{Ia-Ferreira2014,
  author        = {Ia-Ferreira, Iv´ an Garc´ and Laorden, Carlos and Santos, Igor and ıa Bringas, Pablo Garc´},
  title         = {{A Survey on Static Analysis and Model Checking}},
  booktitle     = {International Joint Conference SOCO'14-CISIS'14},
  year          = {2014},
  volume        = {239},
  pages         = {761},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-319-01854-6},
  file          = {:article\\A Survey on Static Analysis and Model Checking.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-3-319-01853-9},
  issn          = {21945357},
  keywords      = {binary,cohen,first select,linear,packet header anomaly detection,regression analysis,s-d,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,statistical analysis,survey},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,survey},
  url           = {http://link.springer.com/10.1007/978-3-319-01854-6},
}

@PhdThesis{2014a,
  title         = {{Improving C ++ Software Quality with Static Code Analysis}},
  year          = {2014},
  __markedentry = {[ccc:6]},
  annote        = {对静态分析理论介绍比较全面},
  file          = {:article\\Improving C Software Quality with Static Code Analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
}

@Article{Chen2014,
  author        = {Chen, Kai and Zhang, Yingjun},
  title         = {{Statically-directed dynamic taint analysis}},
  journal       = {Chinese Journal of Electronics},
  year          = {2014},
  volume        = {23},
  number        = {1},
  pages         = {18--24},
  __markedentry = {[ccc:6]},
  file          = {:article\\Statically-directed dynamic taint analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {10224653},
  keywords      = {Binary code.,Dynamic analysis,Statically-directed,Taint analysis,binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
}

@Article{Evans2014,
  author        = {Evans, Nathan S. and Benameur, Azzedine and Elder, Matthew},
  title         = {{Large-Scale Evaluation of a Vulnerability Analysis Framework}},
  journal       = {7th Workshop on Cyber Security Experimentation and Test (CSET 14)},
  year          = {2014},
  pages         = {1--8},
  __markedentry = {[ccc:6]},
  file          = {:article\\Large-Scale Evaluation of a Vulnerability Analysis Framework.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,fuzz,obfuscate,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,obfuscate,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {https://www.usenix.org/conference/cset14/workshop-program/presentation/benameur},
}

@Article{Zhang2014b,
  author        = {Zhang, Mu and Duan, Yue and Yin, Heng and Zhao, Zhiruo},
  title         = {{Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs}},
  journal       = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security},
  year          = {2014},
  pages         = {1105--1116},
  __markedentry = {[ccc:6]},
  abstract      = {The drastic increase of Android malware has led to a strong interest in developing methods to automate the malware analysis process. Existing automated Android malware detection and classification methods fall into two general categories: 1) signature-based and 2) machine learning-based. Signature-based approaches can be easily evaded by bytecode-level transformation attacks. Prior learningbased works extract features from application syntax, rather than program semantics, and are also subject to evasion. In this paper, we propose a novel semantic-based approach that classifies Android malware via dependency graphs. To battle transformation attacks, we extract a weighted contextual API dependency graph as program semantics to construct feature sets. To fight against malware variants and zero-day malware, we introduce graph similarity metrics to uncover homogeneous application behaviors while tolerating minor implementation differences. We implement a prototype system, DroidSIFT, in 23 thousand lines of Java code. We evaluate our system using 2200 malware samples and 13500 benign samples. Experiments show that our signature detection can correctly label 93{\%} of malware instances; our anomaly detector is capable of detecting zero-day malware with a low false negative rate (2{\%}) and an acceptable false positive rate (5.15{\%}) for a vetting purpose},
  doi           = {10.1145/2660267.2660359},
  file          = {:article\\Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs.pdf:pdf},
  isbn          = {9781450329576},
  issn          = {15437221},
  keywords      = {all or part of,android,anomaly detection,binary,graph similar-,ity,machine learning,malware classification,obfuscate,or,or hard copies of,permission to make digital,semantics-aware,signature detection,stat,static analysi,static analysis,this work for personal,web},
  mendeley-tags = {android,binary,machine learning,obfuscate,web},
}

@Article{Enck2014,
  author        = {Enck, William and Gilbert, Peter and Chun, Byung-Gon and Cox, Landon P. and Jung, Jaeyeon and McDaniel, Patrick and Sheth, Anmol N.},
  title         = {{TaintDroid: An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones}},
  journal       = {Communications of the ACM},
  year          = {2014},
  volume        = {57},
  number        = {3},
  pages         = {99--106},
  __markedentry = {[ccc:6]},
  abstract      = {Today's smartphone operating systems frequently fail to provide users with adequate control over and visibility into how third-party applications use their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid provides realtime analysis by leveraging Android's virtualized execution environment. TaintDroid incurs only 14{\%} performance overhead on a CPU-bound micro-benchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, we found 68 instances of potential misuse of users' private information across 20 applications. Monitoring sensitive data with TaintDroid provides informed use of third-party applications for phone users and valuable input for smartphone security service firms seeking to identify misbehaving applications.},
  doi           = {10.1145/2494522},
  file          = {:article\\TaintDroid An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones.pdf:pdf},
  isbn          = {978-1-931971-79-9},
  issn          = {00010782},
  keywords      = {binary,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,predicte,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2566590.2494522},
}

@Article{Abal2014,
  author        = {Abal, Iago},
  title         = {{42 Variability Bugs in the Linux Kernel A Qualitative Study}},
  journal       = {(CCF Rank B)Proceedings of the 29th ACM/IEEE international conference on Automated software engineering(ASE'14)},
  year          = {2014},
  number        = {May},
  pages         = {421--432},
  __markedentry = {[ccc:6]},
  abstract      = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by vari-ability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides in-sights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
  doi           = {10.1145/2642937.2642990},
  file          = {:article\\42 Variability Bugs in the Linux Kernel A Qualitative Study.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9788779493186},
  keywords      = {bugs,feature interactions,first select,fuzz,linux,machine learning,second select,software variability,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,web},
}

@Article{Dahse2014,
  author        = {Dahse, Johannes and Holz, Thorsten},
  title         = {{Static Detection of Second-Order Vulnerabilities in Web Applications}},
  journal       = {23rd USENIX Security Symposium (USENIX Security 14)},
  year          = {2014},
  pages         = {989--1003},
  __markedentry = {[ccc:6]},
  abstract      = {Web applications evolved in the last decades from sim- ple scripts to multi-functional applications. Such com- plex web applications are prone to different types of se- curity vulnerabilities that lead to data leakage or a com- promise of the underlying web server. So called second- order vulnerabilities occur when an attack payload is first stored by the application on the web server and then later on used in a security-critical operation. In this paper, we introduce the first automated static code analysis approach to detect second-order vulnera- bilities and related multi-step exploits in web applica- tions. By analyzing reads and writes to memory loca- tions of the web server, we are able to identify unsani- tized data flows by connecting input and output points of data in persistent data stores such as databases or ses- sion data. As a result, we identified 159 second-order vulnerabilities in six popular web applications such as the conference management systems HotCRP and Open- Conf. Moreover, the analysis of web applications eval- uated in related work revealed that we are able to detect several critical vulnerabilities previously missed.},
  file          = {:article\\Static Detection of Second-Order Vulnerabilities in Web Applications.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-931971-15-7},
  keywords      = {first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  url           = {https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/dahse},
}

@Article{Collingbourne2014,
  author        = {Collingbourne, Peter and Cadar, Cristian and Kelly, Paul H J},
  title         = {{Symbolic crosschecking of data-parallel floating-point code}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2014},
  volume        = {40},
  number        = {7},
  pages         = {710--737},
  __markedentry = {[ccc:6]},
  abstract      = {We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number - f issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.},
  doi           = {10.1109/TSE.2013.2297120},
  file          = {:article\\Symbolic crosschecking of data-parallel floating-point code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450306348},
  issn          = {00985589},
  keywords      = {Data-parallel code,KLEE-CL,OpenCL,SIMD,binary,first select,floating point,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,symbolic execution,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
  pmid          = {6698391},
}

@Article{Massacci2014,
  author        = {Massacci, Fabio and Nguyen, Viet Hung},
  title         = {{An empirical methodology to evaluate vulnerability discovery models}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2014},
  volume        = {40},
  number        = {12},
  pages         = {1147--1162},
  __markedentry = {[ccc:6]},
  abstract      = {Vulnerability discovery models (VDMs) operate on known vulnerability data to estimate the total number of vulnerabilities that will be reported after a software is released. VDMs have been proposed by industry and academia, but there has been no systematic independent evaluation by researchers who are not model proponents. Moreover, the traditional evaluation methodology has some issues that biased previous studies in the field. In this work we propose an empirical methodology that systematically evaluates the performance of VDMs along two dimensions (quality and predictability) and addresses all identified issues of the traditional methodology. We conduct an experiment to evaluate most existing VDMs on popular web browsers' vulnerability data. Our comparison shows that the results obtained by the proposed methodology are more informative than those by the traditional methodology. Among evaluated VDMs, the simplest linear model is the most appropriate choice in terms of both quality and predictability for the first 6-12 months since a release date. Otherwise, logistics-based models are better choices.},
  doi           = {10.1109/TSE.2014.2354037},
  file          = {:article\\An empirical methodology to evaluate vulnerability discovery models.pdf:pdf},
  issn          = {00985589},
  keywords      = {Empirical evaluation,Software security,Vulnerability analysis,Vulnerability discovery model,predicte,web},
  mendeley-tags = {predicte,web},
}

@Article{Mokhov2014,
  author        = {Mokhov, Serguei A. and Paquet, Joey and Debbabi, Mourad},
  title         = {{The use of NLP techniques in static code analysis to detect weaknesses and vulnerabilities}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2014},
  volume        = {8436 LNAI},
  pages         = {326--332},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-319-06483-3_33},
  file          = {:article\\The use of NLP techniques in static code analysis to detect weaknesses and vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9783319064826},
  issn          = {16113349},
  keywords      = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Yu2014,
  author        = {Yu, Fang and Alkhalaf, Muath and Bultan, Tevfik and Ibarra, Oscar H.},
  title         = {{Automata-based symbolic string analysis for vulnerability detection}},
  journal       = {Formal Methods in System Design},
  year          = {2014},
  volume        = {44},
  number        = {1},
  pages         = {44--70},
  __markedentry = {[ccc:6]},
  abstract      = {Verifying string manipulating programs is a crucial problem in com- puter security. String operations are used extensively within web applications to manipulate user input, and their erroneous use is the most common cause of security vulnerabilities in web applications. We present an automata-based approach for symbolic analysis of string manipulating programs.We use deter- ministic finite automata (DFAs) to represent possible values of string variables. Using forward reachability analysis we compute an over-approximation of all possible values that string variables can take at each program point. Inter- secting these with a given attack pattern yields the potential attack strings if the program is vulnerable. Based on the presented techniques, we have imple- mented Stranger, an automata-based string analysis tool for detecting string- related security vulnerabilities in PHP applications. We evaluated Stranger on several open-source Web applications including one with 350,000+ lines of code. Stranger is able to detect known/unknown vulnerabilities, and, after in- serting proper sanitization routines, prove the absence of vulnerabilities with respect to given attack patterns.},
  doi           = {10.1007/s10703-013-0189-1},
  file          = {:article\\Automata-based symbolic string analysis for vulnerability detection.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1070301301891},
  issn          = {09259856},
  keywords      = {Automated verification,String analysis,Vulnerability analysis,Web application security,binary,first select,second select,source code,source code-important,source code-vice important,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Sun2014,
  author        = {Sun, Hao and Li, Hui-Peng and Zeng, Qing-Kai},
  title         = {{Statically Detect and Run-Time Check Integer-Based Vulnerabilities with Information Flow}},
  journal       = {Journal of Software},
  year          = {2014},
  volume        = {24},
  number        = {12},
  pages         = {2767--2781},
  __markedentry = {[ccc:6]},
  doi           = {10.3724/SP.J.1001.2013.04385},
  file          = {:article\\Statically Detect and Run-Time Check Integer-Based Vulnerabilities with Information Flow.caj:caj},
  issn          = {1000-9825},
  keywords      = {binary,information flow,instrumentation,integer-based vulnerability,stat,static analysi,static analysis,taint analysis,web,内存地址,在程序中常用于表示整数数值,循环,数组下标,整型变量是高级语言中常用的基本数据类型,整数运算结果失真,标志位或者参与算术运算,由于整数表示形式的局限性和 c 语言类型不安全的特点,计数},
  mendeley-tags = {binary,web},
  url           = {http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1001.2013.04385},
}

@Article{Huang2014a,
  author        = {Huang, Shih Kun and Huang, Min Hsiang and Huang, Po Yen and Lu, Han Lin and Lai, Chung Wei},
  title         = {{Software crash analysis for automatic exploit generation on binary programs}},
  journal       = {IEEE Transactions on Reliability},
  year          = {2014},
  volume        = {63},
  number        = {1},
  pages         = {270--289},
  __markedentry = {[ccc:6]},
  abstract      = {This paper presents a new method, capable of automatically generating attacks on binary programs from software crashes. We analyze software crashes with a symbolic failure model by performing concolic executions following the failure directed paths, using a whole system environment model and concrete address mapped symbolic memory in S2 E. We propose a new selective symbolic input method and lazy evaluation on pseudo symbolic variables to handle symbolic pointers and speed up the process. This is an end-to-end approach able to create exploits from crash inputs or existing exploits for various applications, including most of the existing benchmark programs, and several large scale applications, such as a word processor (Microsoft office word), a media player (mpalyer), an archiver (unrar), or a pdf reader (foxit). We can deal with vulnerability types including stack and heap overflows, format string, and the use of uninitialized variables. Notably, these applications have become software fuzz testing targets, but still require a manual process with security knowledge to produce mitigation-hardened exploits. Using this method to generate exploits is an automated process for software failures without source code. The proposed method is simpler, more general, faster, and can be scaled to larger programs than existing systems. We produce the exploits within one minute for most of the benchmark programs, including mplayer. We also transform existing exploits of Microsoft office word into new exploits within four minutes. The best speedup is 7,211 times faster than the initial attempt. For heap overflow vulnerability, we can automatically exploit the unlink() macro of glibc, which formerly requires sophisticated hacking efforts.},
  doi           = {10.1109/TR.2014.2299198},
  file          = {:article\\Software crash analysis for automatic exploit generation on binary programs.pdf:pdf},
  isbn          = {0018-9529},
  issn          = {00189529},
  keywords      = {Automatic exploit generation,binary,bug forensics,fuzz,predicte,software crash analysis,stat,static analysi,static analysis,symbolic execution,taint analysis,web},
  mendeley-tags = {binary,fuzz,predicte,web},
  pmid          = {332520700022},
}

@Article{Kulenovic2014,
  author        = {Kulenovic, Melina and Donko, Dzenana},
  title         = {{A survey of static code analysis methods for security vulnerabilities detection}},
  journal       = {2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2014 - Proceedings},
  year          = {2014},
  number        = {May},
  pages         = {1381--1386},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/MIPRO.2014.6859783},
  file          = {:article\\A survey of static code analysis methods for security vulnerabilities detection.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9789532330816},
  keywords      = {first select,predicte,second select,security,source code,source code-important,source code-vice important,stat,static analysi,static analysis,static code analysis,survey,vulnerability,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,survey,web},
}

@Article{Gupta2014,
  author        = {Gupta, Mukesh Kumar and Govil, M. C. and Singh, Girdhari},
  title         = {{Static analysis approaches to detect SQL injection and cross site scripting vulnerabilities in web applications: A survey}},
  journal       = {International Conference on Recent Advances and Innovations in Engineering, ICRAIE 2014},
  year          = {2014},
  pages         = {9--13},
  __markedentry = {[ccc:6]},
  abstract      = {Dependence on web applications is increasing very rapidly in recent time for social communications, health problem, financial transaction and many other purposes. Unfortunately, presence of security weaknesses in web applications allows malicious user's to exploit various security vulnerabilities and become the reason of their failure. Currently, SQL Injection (SQLI) and Cross-Site Scripting (XSS) vulnerabilities are most dangerous security vulnerabilities exploited in various popular web applications i.e. eBay, Google, Facebook, Twitter etc. Research on defensive programming, vulnerability detection and attack prevention techniques has been quite intensive in the past decade. Defensive programming is a set of coding guidelines to develop secure applications. But, mostly developers do not follow security guidelines and repeat same type of programming mistakes in their code. Attack prevention techniques protect the applications from attack during their execution in actual environment. The difficulties associated with accurate detection of SQLI and XSS vulnerabilities in coding phase of software development life cycle. This paper proposes a classification of software security approaches used to develop secure software in various phase of software development life cycle. It also presents a survey of static analysis based approaches to detect SQL Injection and cross-site scripting vulnerabilities in source code of web applications. The aim of these approaches is to identify the weaknesses in source code before their exploitation in actual environment. This paper would help researchers to note down future direction for securing legacy web applications in early phases of software development life cycle.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1109/ICRAIE.2014.6909173},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\Static analysis approaches to detect SQL injection and cross site scripting vulnerabilities in web applicat.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479940400},
  issn          = {`},
  keywords      = {SQL injection,cross site scripting,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,vulnerabilitie,web,web applicatio},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,survey,web},
  pmid          = {25246403},
}

@Article{Doǧan2014,
  author        = {Doǧan, Serdar and Betin-Can, Aysu and Garousi, Vahid},
  title         = {{Web application testing: A systematic literature review}},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  number        = {1},
  pages         = {174--201},
  __markedentry = {[ccc:6]},
  abstract      = {Context The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field. Methods We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area. Results Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community. {\textcopyright} 2014 Elsevier Inc.},
  doi           = {10.1016/j.jss.2014.01.010},
  file          = {:article\\Web application testing A systematic literature review.pdf:pdf},
  isbn          = {0164-1212},
  issn          = {01641212},
  keywords      = {Systematic literature review,Testing,Web application,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {survey,web},
}

@Article{Bajracharya2014,
  author        = {Bajracharya, Sushil and Ossher, Joel and Lopes, Cristina},
  title         = {{Sourcerer: An infrastructure for large-scale collection and analysis of open-source code}},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {79},
  pages         = {241--259},
  __markedentry = {[ccc:6]},
  abstract      = {A large amount of open source code is now available online, presenting a great potential resource for software developers. This has motivated software engineering researchers to develop tools and techniques to allow developers to reap the benefits of these billions of lines of source code. However, collecting and analyzing such a large quantity of source code presents a number of challenges. Although the current generation of open source code search engines provides access to the source code in an aggregated repository, they generally fail to take advantage of the rich structural information contained in the code they index. This makes them significantly less useful than Sourcerer for building state-of-the-art software engineering tools, as these tools often require access to both the structural and textual information available in source code. We have developed Sourcerer, an infrastructure for large-scale collection and analysis of open source code. By taking full advantage of the structural information extracted from source code in its repository, Sourcerer provides a foundation upon which state-of-the-art search engines and related tools can easily be built. We describe the Sourcerer infrastructure, present the applications that we have built on top of it, and discuss how existing tools could benefit from using Sourcerer. ?? 2011 Elsevier B.V. All rights reserved.},
  doi           = {10.1016/j.scico.2012.04.008},
  file          = {:article\\Sourcerer An infrastructure for large-scale collection and analysis of open-source code.pdf:pdf},
  groups        = {vice-important},
  isbn          = {0167-6423},
  issn          = {01676423},
  keywords      = {Data mining,Internet-scale code retrieval,Open source,Software information retrieval,Sourcerer,Static analysis,binary,first select,fuzz,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,source code,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2012.04.008},
}

@Article{Seo2014,
  author        = {Seo, Seung Hyun and Gupta, Aditi and Sallam, Asmaa Mohamed and Bertino, Elisa and Yim, Kangbin},
  title         = {{Detecting mobile malware threats to homeland security through static analysis}},
  journal       = {Journal of Network and Computer Applications},
  year          = {2014},
  volume        = {38},
  number        = {1},
  pages         = {43--53},
  __markedentry = {[ccc:6]},
  abstract      = {Recent years have seen the significant increase in the popularity of smartphones. This popularity has been accompanied with an equally alarming rise in mobile malware. Recently released mobile malware targeting Android devices have been found to specifically focus on root exploits to obtain root-level access and execute instructions from a remote server. Thus, this kind of mobile malware presents a significant threat to Homeland Security. This is possible because smartphones can serve as zombie devices which are then controlled by hackers' via a C{\&}C server. In this paper, we discuss the defining characteristics inherent in mobile malware and show mobile attack scenarios which are feasible against Homeland Security. We also propose a static analysis tool, DroidAnalyzer, which identifies potential vulnerabilities of Android apps and the presence of root exploits. Then, we analyze various mobile malware samples and targeting apps such as banking, flight tracking and booking, home{\&}office monitoring apps to examine potential vulnerabilities by applying DroidAnalyzer. ?? 2013 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.jnca.2013.05.008},
  file          = {:article\\Detecting mobile malware threats to homeland security through static analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1084-8045},
  issn          = {10848045},
  keywords      = {Android OS,Homeland security,Mobile malware,Smartphone,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier},
  url           = {http://dx.doi.org/10.1016/j.jnca.2013.05.008},
}

@Book{Srinivasan2014,
  title         = {{DeadDrop-in-a-Flash: Information Hiding at SSD NAND Flash Memory Physical Layer}},
  year          = {2014},
  author        = {Srinivasan, A and Wu, J},
  __markedentry = {[ccc:6]},
  booktitle     = {Securware {\ldots}},
  file          = {:article\\DeadDrop-in-a-Flash Information Hiding at SSD NAND Flash Memory Physical Layer.pdf:pdf},
  isbn          = {9781612083766},
  keywords      = {web},
  mendeley-tags = {web},
  url           = {http://www.researchgate.net/profile/Carlos{\_}Westphall/publication/275463281{\_}SECURWARE{\_}2014{\_}-{\_}The{\_}Eighth{\_}International{\_}Conference{\_}on{\_}Emerging{\_}Security{\_}Information{\_}Systems{\_}and{\_}Technologies/links/553d04b70cf245bdd7696e3d.pdf{\#}page=80},
}

@Article{Shahriar2014,
  author        = {Shahriar, Hossain and Weldemariam, Komminist and Zulkernine, Mohammad and Lutellier, Thibaud},
  title         = {{Effective detection of vulnerable and malicious browser extensions}},
  journal       = {Computers and Security},
  year          = {2014},
  volume        = {47},
  pages         = {66--84},
  __markedentry = {[ccc:6]},
  abstract      = {Unsafely coded browser extensions can compromise the security of a browser, making them attractive targets for attackers as a primary vehicle for conducting cyber-attacks. Among others, the three factors making vulnerable extensions a high-risk security threat for browsers include: i) the wide popularity of browser extensions, ii) the similarity of browser extensions with web applications, and iii) the high privilege of browser extension scripts. Furthermore, mechanisms that specifically target to mitigate browser extension-related attacks have received less attention as opposed to solutions that have been deployed for common web security problems (such as SQL injection, XSS, logic flaws, client-side vulnerabilities, drive-by-download, etc.). To address these challenges, recently some techniques have been proposed to defend extension-related attacks. These techniques mainly focus on information flow analysis to capture suspicious data flows, impose privilege restriction on API calls by malicious extensions, apply digital signatures to monitor process and memory level activities, and allow browser users to specify policies in order to restrict the operations of extensions. This article presents a model-based approach to detect vulnerable and malicious browser extensions by widening and complementing the existing techniques. We observe and utilize various common and distinguishing characteristics of benign, vulnerable, and malicious browser extensions. These characteristics are then used to build our detection models, which are based on the Hidden Markov Model constructs. The models are well trained using a set of features extracted from a number of browser extensions together with user supplied specifications. Along the course of this study, one of the main challenges we encountered was the lack of vulnerable and malicious extension samples. To address this issue, based on our previous knowledge on testing web applications and heuristics obtained from available vulnerable and malicious extensions, we have defined rules to generate training samples. The approach is implemented in a prototype tool and evaluated using a number of Mozilla Firefox extensions. Our evaluation indicated that the approach not only detects known vulnerable and malicious extensions, but also identifies previously undetected extensions with a negligible performance overhead.},
  doi           = {10.1016/j.cose.2014.06.005},
  file          = {:article\\Effective detection of vulnerable and malicious browser extensions.pdf:pdf},
  isbn          = {978-1-4799-0406-8},
  issn          = {01674048},
  keywords      = {Browser extensions,Hidden,JavaScript,Malware,Markov,Model,Web security,binary,machine learning,obfuscate,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,machine learning,obfuscate,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2014.06.005},
}

@Article{Tripp2014,
  author        = {Tripp, Omer and Rubin, Julia},
  title         = {{A Bayesian Approach to Privacy Enforcement in Smartphones}},
  journal       = {23rd USENIX Security Symposium (USENIX Security 14)},
  year          = {2014},
  pages         = {175--190},
  __markedentry = {[ccc:6]},
  abstract      = {Mobile apps often require access to private data, such as the device ID or location. At the same time, popular platforms like Android and iOS have limited support for user privacy. This frequently leads to unauthorized disclosure of private information by mobile apps, e.g. for advertising and analytics purposes. This paper addresses the problem of privacy enforcement in mobile systems, which we formulate as a classification problem: When arriving at a privacy sink (e.g., database update or outgoing web message), the runtime system must classify the sink's behavior as either legitimate or illegitimate. The traditional approach of information-flow (or taint) tracking applies “binary” classification, whereby information release is legitimate iff there is no data flow from a privacy source to sink arguments. While this is a useful heuristic, it also leads to false alarms. We propose to address privacy enforcement as a learning problem, relaxing binary judgments into a quantitative/ probabilistic mode of reasoning. Specifically, we propose a Bayesian notion of statistical classification, which conditions the judgment whether a release point is legitimate on the evidence arising at that point. In our concrete approach, implemented as the BAYESDROID system that is soon to be featured in a commercial product, the evidence refers to the similarity between the data values about to be released and the private data stored on the device. Compared to TaintDroid, a state-of-the-art taint-based tool for privacy enforcement, BAYESDROID is substantially more accurate. Applied to 54 top-popular Google Play apps, BAYESDROID is able to detect 27 privacy violations with only 1 false alarm.},
  file          = {:article\\A Bayesian Approach to Privacy Enforcement in Smartphones.pdf:pdf},
  isbn          = {978-1-931971-15-7},
  keywords      = {android,binary,fuzz,obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,fuzz,obfuscate,predicte,web},
  url           = {https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/tripp},
}

@Article{Scandariato2014,
  author        = {Scandariato, Riccardo and Walden, James and Hovsepyan, Aram and Joosen, Wouter},
  title         = {{Predicting Vulnerable Software Components via Text Mining}},
  year          = {2014},
  volume        = {40},
  number        = {10},
  pages         = {993--1006},
  __markedentry = {[ccc:6]},
  file          = {:article\\Predicting Vulnerable Software Components via Text Mining.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Zhu2014,
  author        = {Zhu, Feng and Wei, Jinpeng},
  title         = {{Static analysis based invariant detection for commodity operating systems}},
  journal       = {Computers and Security},
  year          = {2014},
  volume        = {43},
  pages         = {49--63},
  __markedentry = {[ccc:6]},
  abstract      = {Recent interest in runtime attestation requires modeling of a program's runtime behavior to formulate its integrity properties. In this paper, we study the possibility of employing static source code analysis to derive integrity models of a commodity operating systems kernel. We develop a precise and static analysis-based data invariant detection tool that overcomes several technical challenges: field-sensitivity, array-sensitivity, and pointer analysis. We apply our tool to Linux kernel 2.4.32 and Windows Research Kernel (WRK). For Linux kernel 2.4.32, our tool identifies 284,471 data invariants that are critical to its runtime integrity, e.g., we use them to detect ten real-world Linux rootkits. Furthermore, comparison with the result of a dynamic invariant detector reveals 17,182 variables that can cause false alarms for the dynamic detector in the constant invariants category. Our tool also works successfully for WRK and reports 202,992 invariants, which we use to detect nine real-world Windows malware and one synthetic Windows malware. When compared with a dynamic invariant detector, we see similar results in terms of false alarms. Our experience suggests that static analysis is a viable option for automated integrity property derivation, and it can have very low false positive rate and very low false negative rate (e.g., for the constant invariants of WRK, the false positive rate is one out of 100,822 and the false negative rate is 0.007{\%} or seven out of 100,822). ?? 2014 Elsevier Ltd. All rights reserved.},
  doi           = {10.1016/j.cose.2014.02.008},
  file          = {:article\\Static analysis based invariant detection for commodity operating systems.pdf:pdf},
  groups        = {vice-important},
  isbn          = {978-1-936968-32-9},
  issn          = {01674048},
  keywords      = {Integrity modeling,Invariants detection,Malware detection,Static analysis,Tools,binary,first select,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2014.02.008},
}

@Article{Fang2014,
  author        = {Fang, Ming and Hafiz, Munawar},
  title         = {{Discovering Buffer Overflow Vulnerabilities in the Wild: An Empirical Study}},
  journal       = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
  year          = {2014},
  pages         = {23:1----23:10},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Reporters of security vulnerabilities possess rich information about the security engineering process. Goal: We performed an empirical study on reporters of buffer overflow vulnerabilities to understand the methods and tools used during the discovery. Method: We ran the study in the form of an email questionnaire with open ended questions. The participants were reporters featured in the SecurityFocus repository during two six-month periods; we collected 58 responses. Results: We found that in spite of many apparent choices, reporters follow similar approaches. Most reporters typically use fuzzing, but their fuzzing tools are created ad hoc; they use a few debugging tools to analyze the crash introduced by a fuzzer; and static analysis tools are rarely used. We also found a serious problem in the vulnerability reporting process. Most reporters, especially the experienced ones, favor full-disclosure and do not collaborate with the vendors of vulnerable software. They think that the public disclosure, sometimes supported by a detailed exploit, will put pressure on vendors to fix the vulnerabilities. But, in practice, the vulnerabilities not reported to vendors are less likely to be fixed. Conclusions: The results are valuable for beginners exploring how to detect and report buffer overflows and for tool vendors and researchers exploring how to automate and fix the process. },
  doi           = {10.1145/2652524.2652533},
  file          = {:article\\Discovering Buffer Overflow Vulnerabilities in the Wild An Empirical Study.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4503-2774-9},
  issn          = {19493789},
  keywords      = {binary,empirical study,first select,fuzz,machine learning,predicte,second select,secure software engineering,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerability,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://doi.acm.org/10.1145/2652524.2652533},
}

@Article{Yu2014a,
  author        = {Yu, Tingting and Sung, Ahyoung and Srisa-An, Witawas and Rothermel, Gregg},
  title         = {{An approach to testing commercial embedded systems}},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {88},
  number        = {1},
  pages         = {207--230},
  __markedentry = {[ccc:6]},
  abstract      = {A wide range of commercial consumer devices such as mobile phones and smart televisions rely on embedded systems software to provide their functionality. Testing is one of the most commonly used methods for validating this software, and improved testing approaches could increase these devices' dependability. In this article we present an approach for performing such testing. Our approach is composed of two techniques. The first technique involves the selection of test data; it utilizes test adequacy criteria that rely on dataflow analysis to distinguish points of interaction between specific layers in embedded systems and between individual software components within those layers, while also tracking interactions between tasks. The second technique involves the observation of failures: it utilizes a family of test oracles that rely on instrumentation to record various aspects of a system's execution behavior, and compare observed behavior to certain intended system properties that can be derived through program analysis. Empirical studies of our approach show that our adequacy criteria can be effective at guiding the creation of test cases that detect faults, and our oracles can help expose faults that cannot easily be found using typical output-based oracles. Moreover, the use of our criteria accentuates the fault-detection effectiveness of our oracles. {\textcopyright} 2013 Elsevier Inc. All rihgts reserved.},
  doi           = {10.1016/j.jss.2013.10.041},
  file          = {:article\\An approach to testing commercial embedded systems.pdf:pdf},
  issn          = {01641212},
  keywords      = {Embedded systems,Software test adequacy criteria,Test oracles,android,binary,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,predicte,web},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jss.2013.10.041},
}

@Article{Statistik2014,
  author        = {Statistik, Badan Pusat},
  title         = {{No Title No Title}},
  journal       = {Katalog BPS},
  year          = {2014},
  volume        = {XXXIII},
  number        = {2},
  pages         = {81--87},
  __markedentry = {[ccc:6]},
  abstract      = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1007/s13398-014-0173-7.2},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\No Title No Title.pdf:pdf},
  isbn          = {9780874216561},
  issn          = {0717-6163},
  keywords      = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,web,youth},
  mendeley-tags = {web},
  pmid          = {15003161},
  url           = {http://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/15003161$\backslash$nhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991$\backslash$nhttp://www.scielo.cl/pd},
}

@Article{Pewny2014,
  author        = {Pewny, Jannik and Schuster, Felix and Bernhard, Lukas and Holz, Thorsten and Rossow, Christian},
  title         = {{Leveraging semantic signatures for bug search in binary programs}},
  journal       = {Annual Computer Security Applications Conference},
  year          = {2014},
  pages         = {406--415},
  __markedentry = {[ccc:6]},
  abstract      = {Software vulnerabilities still constitute a high security risk and there is an ongoing race to patch known bugs. However, especially in closed-source software, there is no straightforward way (in contrast to source code analysis) to find buggy code parts, even if the bug was publicly disclosed. To tackle this problem, we propose a method called Tree Edit Distance Based Equational Matching (TEDEM) to automatically identify binary code regions that are "similar" to code regions containing a reference bug. We aim to find bugs both in the same binary as the reference bug and in completely unrelated binaries (even compiled for different operating systems). Our method even works on proprietary software systems, which lack source code and symbols. The analysis task is split into two phases. In a preprocessing phase, we condense the semantics of a given binary executable by symbolic simplification to make our approach robust against syntactic changes across different binaries. Second, we use tree edit distances as a basic block-centric metric for code similarity. This allows us to find instances of the same bug in different binaries and even spotting its variants (a concept called vulnerability extrapolation). To demonstrate the practical feasibility of the proposed method, we implemented a prototype of TEDEM that can find real-world security bugs across binaries and even across OS boundaries, such as in MS Word and the popular messengers Pidgin (Linux) and Adium (Mac OS).},
  doi           = {10.1145/2664243.2664269},
  file          = {:article\\Leveraging semantic signatures for bug search in binary programs.pdf:pdf},
  isbn          = {9781450330053},
  keywords      = {binary,fuzz,machine learning,obfuscate,predicte},
  mendeley-tags = {binary,fuzz,machine learning,obfuscate,predicte},
  url           = {http://dl.acm.org/citation.cfm?doid=2664243.2664269},
}

@Article{张斌2014,
  author        = {张斌 and 唐朝京 and 李孟君 and 吴波},
  title         = {基于动态污点分析的二进制程序导向性模糊测试方法},
  journal       = {现代电子技术},
  year          = {2014},
  volume        = {37},
  __markedentry = {[ccc:6]},
  abstract      = {传统模糊测试中， 由于不同的输入可能重复测试相同的状态空间， 导致其效率严重低下。提出一种基于动态 污点分析与输入分域技术相结合的二进制程序导向性模糊测试技术， 可以对典型安全敏感操作与一般模块函数进行导向性 模糊测试， 很好地解决了传统模糊测试效率低下的问题。实现了二进制导向性模糊测试的原型系统TaintedFuzz， 实验证明， 该系统能够对二进制程序中存在的典型安全漏洞进行高效地发掘。},
  file          = {:article\\基于动态污点分析的二进制程序导向性模糊测试方法.caj:caj},
  keywords      = {binary,fuzz,安全漏洞；导向性模糊测试；动态污点分析；输入分域},
  mendeley-tags = {binary,fuzz},
}

@Article{Padmanabhuni2015,
  author        = {Padmanabhuni, Bindu Madhavi and Tan, Hee Beng Kuan},
  title         = {{Buffer Overflow Vulnerability Prediction from x86 Executables Using Static Analysis and Machine Learning}},
  journal       = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  year          = {2015},
  pages         = {450--459},
  __markedentry = {[ccc:6]},
  annote        = {本文是对预测二进制程序中的漏洞，参考下，并获取文中关于source code预测方法的信息和评价},
  doi           = {10.1109/COMPSAC.2015.78},
  file          = {:article\\Buffer Overflow Vulnerability Prediction from x86 Executables Using Static Analysis and Machine Learning.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4673-6564-2},
  issn          = {07303157},
  keywords      = {- binary static analysis,and data dependency,binary,buffer overflow,buffer usage pattern,control,disassembly,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,static code attributes,vulnerability prediction},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7273653},
}

@Article{Qiu2015,
  author        = {Qiu, Jing and Su, Xiaohong and Ma, Peijun},
  title         = {{Using Reduced Execution Flow Graph to Identify Library Functions in Binary Code}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2015},
  volume        = {6},
  number        = {1},
  pages         = {1--1},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/TSE.2015.2470241},
  file          = {:article\\Using Reduced Execution Flow Graph to Identify Library Functions in Binary Code.pdf:pdf},
  issn          = {0098-5589},
  keywords      = {binary,stat,static analysi,static analysis},
  mendeley-tags = {binary},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7210204},
}

@Article{Muntean2015,
  author        = {Muntean, Paul and Rahman, Mustafizur and Ibing, Andreas and Eckert, Claudia},
  title         = {{SMT-constrained symbolic execution engine for integer overflow detection in C code}},
  journal       = {2015 Information Security for South Africa (ISSA)},
  year          = {2015},
  number        = {3},
  pages         = {1--8},
  __markedentry = {[ccc:6]},
  abstract      = {{\textcopyright} 2015 IEEE. Integer overflow errors in C programs are difficult to detect since the C language specification rules which govern how one can cast or promote integer types are not accompanied by any unambiguous set of formal rules. Thus, making it difficult for the programmer to understand and use the rules correctly causing vulnerabilities or costly errors. Although there are many static and dynamic tools used for integer overflow detection, the tools lack the capacity of efficiently filtering out false positives and false negatives. Better tools are needed to be constructed which are more precise in regard to bug detection and filtering out false positives. In this paper, we present an integer overflow checker which is based on precise modeling of C language semantics and symbolic function models. We developed our checker as an Eclipse plug-in and tested it on the open source C/C++ test case CWE-190 contained in the National Institute of Standards and Technology (NIST) Juliet test suite for C/C++. We ran our checker systematically on 2592 programs having in total 340 KLOC with a true positive rate of 95.49{\%} for the contained C programs and with no false positives. We think our approach is effective to be applied in future to C++ programs as well, in order to detect other kinds of vulnerabilities related to integers.},
  doi           = {10.1109/ISSA.2015.7335070},
  file          = {:article\\SMT-constrained symbolic execution engine for integer overflow detection in C code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4799-7755-0},
  keywords      = {C code,C language semantics,C language specification rules,C programs,C++ language,C++ programs,CWE-190,Eclipse plug-in,KLOC,Lead,NIST Juliet test suite,National Institute of Standards and Technology,SMT-constrained symbolic execution engine,Yttrium,binary,bug detection,bug filtering,context-sensitive analysis,dynamic tools,false negatives,false positives,first select,formal rules,fuzz,information security,integer overflow checker,integer overflow error detection,integer types,open source C test case,open source C++ test case,program debugging,program testing,safety-critical software,second select,software vulnerability,source code,source code-important,source code-vice important,stat,static analysi,static analysis,static tools,symbolic function models},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7335070},
}

@Article{Wr2015,
  author        = {Wr, Dffruglqj and Vwdqgdugv, Fhuwdlq and Lwv, Ehiruh and Lq, Lqfoxvlrq},
  title         = {{Third-party Source Code Complance using Early Static Code Analysis}},
  year          = {2015},
  volume        = {66},
  pages         = {132--139},
  __markedentry = {[ccc:6]},
  file          = {:article\\Third-party Source Code Complance using Early Static Code Analysis.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781467376488},
  keywords      = {collaborative acceptance tests,first select,software,software compliance,software conformity,software quality,source code,source code-vice important,stat,static analysi,static analysis,static code analysis,verification},
  mendeley-tags = {first select,source code,source code-vice important},
}

@Article{Hui2015,
  author        = {Hui, Deng and Hui, Liu and Ying, Guo and Baofeng, Zhang},
  title         = {{Memory Allocation Vulnerability Analysis and Analysis Optimization for C Programs Based on Formal Methods}},
  journal       = {Journal of Software},
  year          = {2015},
  volume        = {10},
  number        = {9},
  pages         = {1079--1085},
  __markedentry = {[ccc:6]},
  doi           = {10.17706/jsw.10.9.1079-1085},
  file          = {:article\\Memory Allocation Vulnerability Analysis and Analysis Optimization for C Programs Based on Formal Methods.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {1796217X},
  keywords      = {algebraic transition system,bisimulation,c program,first select,formal,fuzz,memory allocation vulnerability,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important},
  url           = {http://www.jsoftware.us/index.php?m=content{\&}c=index{\&}a=show{\&}catid=156{\&}id=2525},
}

@Article{Malhotra2015,
  author        = {Malhotra, Ruchika},
  title         = {{A systematic review of machine learning techniques for software fault prediction}},
  journal       = {Applied Soft Computing Journal},
  year          = {2015},
  volume        = {27},
  pages         = {504--518},
  __markedentry = {[ccc:6]},
  abstract      = {Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work.},
  doi           = {10.1016/j.asoc.2014.11.023},
  file          = {:article\\A systematic review of machine learning techniques for software fault prediction.pdf:pdf},
  issn          = {15684946},
  keywords      = {Machine learning,Software fault proneness,Systematic literature review,machine learning,predicte},
  mendeley-tags = {machine learning,predicte},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.asoc.2014.11.023},
}

@Article{Caliskan-Islam2015,
  author        = {Caliskan-Islam, A and Harang, R and Liu, A},
  title         = {{De-anonymizing programmers via code stylometry}},
  journal       = {24th USENIX Security},
  year          = {2015},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {1512.08546},
  doi           = {10.1145/2665943.2665958},
  eprint        = {1512.08546},
  file          = {:article\\De-anonymizing programmers via code stylometry.pdf:pdf},
  isbn          = {978-1-931971-232},
  issn          = {2274-2042},
  url           = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/caliskan-islam},
}

@Article{Ting2015,
  author        = {Ting, D A I and Submitted, A Thesis and The, F O R and Of, Degree and Of, Doctor},
  title         = {{Detection and Prevention of Misuse of Software Components}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\Detection and Prevention of Misuse of Software Components.pdf:pdf},
  keywords      = {predicte},
  mendeley-tags = {predicte},
}

@Article{Ma2015,
  author        = {Ma, Jinxin and Dong, Guowei and Zhang, Puhan and Guo, Tao},
  title         = {{SymWalker: Symbolic execution in routines of binary code}},
  journal       = {Proceedings - 2014 10th International Conference on Computational Intelligence and Security, CIS 2014},
  year          = {2015},
  pages         = {694--698},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/CIS.2014.16},
  file          = {:article\\SymWalker Symbolic execution in routines of binary code.pdf:pdf},
  isbn          = {9781479974344},
  keywords      = {Control flow analysis,Security property,Symbolic execution,Vulnerabilities,binary},
  mendeley-tags = {binary},
}

@Article{Alrabaee2015,
  author        = {Alrabaee, Saed and Shirani, Paria and Wang, Lingyu and Debbabi, Mourad},
  title         = {{SIGMA: A semantic integrated graph matching approach for identifying reused functions in binary code}},
  journal       = {Digital Investigation},
  year          = {2015},
  volume        = {12},
  number        = {S1},
  pages         = {S61--S71},
  __markedentry = {[ccc:6]},
  abstract      = {The capability of efficiently recognizing reused functions for binary code is critical to many digital forensics tasks, especially considering the fact that many modern malware typically contain a significant amount of functions borrowed from open source software packages. Such a capability will not only improve the efficiency of reverse engineering, but also reduce the odds of common libraries leading to false correlations between unrelated code bases. In this paper, we propose SIGMA, a technique for identifying reused functions in binary code by matching traces of a novel representation of binary code, namely, the Semantic Integrated Graph (SIG). The SIG s enhance and merge several existing concepts from classic program analysis, including control flow graph, register flow graph, and function call graph into a joint data structure. Such a comprehensive representation allows us to capture different semantic descriptors of common functionalities in a unified manner as graph traces, which can be extracted from binaries and matched to identify reused functions, actions, or open source software packages. Experimental results show that our approach yields promising results. Furthermore, we demonstrate the effectiveness of our approach through a case study using two malware known to share common functionalities, namely, Zeus and Citadel.},
  doi           = {10.1016/j.diin.2015.01.011},
  file          = {:article\\SIGMA A semantic integrated graph matching approach for identifying reused functions in binary code.pdf:pdf},
  issn          = {17422876},
  keywords      = {Binary program analysis,Digital forensics,Function identification,Malware forensics,Reverse engineering,binary,obfuscate,stat,static analysi,static analysis},
  mendeley-tags = {binary,obfuscate},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.diin.2015.01.011},
}

@Article{Wang2015,
  author        = {Wang, Ping and Wang, Yu Shih},
  title         = {{Malware behavioural detection and vaccine development by using a support vector model classifier}},
  journal       = {Journal of Computer and System Sciences},
  year          = {2015},
  volume        = {81},
  number        = {6},
  pages         = {1012--1026},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract Most existing approaches for detecting viruses involve signature-based analyses to match the precise patterns of malware threats. However, the problem of classification accuracy regarding unspecified malware detection depends on correct extraction and completeness of training signatures. In practice, malware detection system uses the generalization ability of support vector models (SVMs) to guarantee a small classification error by machine learning. This study developed an automatic malware detection system by training an SVM classifier based on behavioural signatures. A cross-validation scheme was used for solving classification accuracy problems by using SVMs associated with 60 families of real malware. The experimental results reveal that the classification error decreases as the sizing of testing data is increased. For different sizing (N) of malware samples, the prediction accuracy of malware detection goes up to 98.7{\%} with N=100. The overall detection accuracy of the SVC is more than 85{\%} for unspecific mobile malware.},
  doi           = {10.1016/j.jcss.2014.12.014},
  file          = {:article\\Malware behavioural detection and vaccine development by using a support vector model classifier.pdf:pdf},
  issn          = {10902724},
  keywords      = {Behavioural detection,Digital vaccine,Malware detection system,Mobile security,Support vector model (SVM),machine learning,predicte},
  mendeley-tags = {machine learning,predicte},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jcss.2014.12.014},
}

@Article{Christakis2015,
  author        = {Christakis, Maria and M{\"{u}}ller, Peter and W{\"{u}}stholz, Valentin},
  title         = {{An Experimental Evaluation of Deliberate Unsoundness in a Static Program Analyzer}},
  journal       = {Vmcai},
  year          = {2015},
  pages         = {336--354},
  __markedentry = {[ccc:6]},
  abstract      = {Many practical static analyzers are not completely sound by design. Their designers trade soundness to increase automation, improve performance, and reduce the number of false positives or the annotation overhead. However, the impact of such design decisions on the effectiveness of an analyzer is not well understood. This paper reports on the first systematic effort to document and evaluate the sources of unsoundness in a static analyzer. We developed a code instrumentation that reflects the sources of deliberate unsoundness in the .NET static analyzer Clousot and applied it to code from six open-source projects. We found that 33{\%} of the instrumented methods were analyzed soundly. In the remaining methods, Clousot made unsound assumptions, which were violated in 2–26{\%} of the methods during concrete executions. Manual inspection of these methods showed that no errors were missed due to an unsound assumption, which suggests that Clousot's unsoundness does not compromise its effectiveness. Our findings can guide users of static analyzers in using them fruitfully, and designers in finding good trade-offs.},
  file          = {:article\\An Experimental Evaluation of Deliberate Unsoundness in a Static Program Analyzer.pdf:pdf},
  groups        = {vice-important},
  isbn          = {978-3-662-46080-1, 978-3-662-46081-8},
  issn          = {16113349},
  keywords      = {binary,first select,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,source code,source code-vice important},
  url           = {http://link.springer.com/chapter/10.1007/978-3-662-46081-8{\_}19$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-662-46081-8{\_}19$\backslash$nhttp://link.springer.com/content/pdf/10.1007/978-3-662-46081-8{\_}19.pdf},
}

@Article{Urma2015,
  author        = {Urma, Raoul Gabriel and Mycroft, Alan},
  title         = {{Source-code queries with graph databases - With application to programming language usage and evolution}},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {97},
  number        = {P1},
  pages         = {127--134},
  __markedentry = {[ccc:6]},
  abstract      = {Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA. The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held - with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses. We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or data-flow levels. We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.},
  doi           = {10.1016/j.scico.2013.11.010},
  file          = {:article\\With application to programming language usage and evolution.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {01676423},
  keywords      = {Graph databases,Programming language evolution,Source-code queries and DSLs,binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2013.11.010},
}

@Article{Caliskan-Islam2015a,
  author        = {Caliskan-Islam, Aylin},
  title         = {{Stylometric fingerprints and privacy behavior in textual data}},
  journal       = {ProQuest Dissertations and Theses},
  year          = {2015},
  number        = {May},
  pages         = {170},
  __markedentry = {[ccc:6]},
  abstract      = {Machine learning and natural language processing can be used to characterize and quantify aspects of human behavior expressed in language.  Linguistic features exhibited in any kind of text can be used to study individuals' behavior as well as to identify an author among thousands of authors. Studying aspects of human behavior can be automated by incorporating machine learning techniques and well-engineered features that represent behavior of interest. Human behavior analysis can be used to enhance security by detecting malware programmers, malicious users, or abusive multiple account holders in online networks. At the same time, such an automated analysis is a serious threat to privacy, especially to the privacy of persons that would like to remain anonymous. Nevertheless, privacy enhancing technologies can be built by first and foremost understanding privacy infringing methods in-depth to create countermeasures.   Authorship attribution through stylometry, the study of writing style, in translated or unconventional text yields as high accuracy as the state-of-the-art accuracy in authorship attribution in English prose. Applying stylometry to the more structured domain of programming languages is also possible through a robust and principled method introduced in this thesis. Code stylometry is able to de-anonymize thousands of programmers with high accuracy while providing insight into software engineering. Programmer de-anonymization can aid in forensic analysis, resolving plagiarism cases, or copyright investigations. On the other hand, de-anonymizing programmers constitutes a privacy threat for anonymous contributors of open source repositories. Bridging the gap between natural language processing and machine learning is a powerful step towards designing feature sets that represent aspects of human behavior. Features obtained through natural language processing methods can be used to study the privacy behavior of users in large social networks. Aggregate privacy analysis shows that people with similar privacy behavior appear in clusters. This knowledge can be used to design privacy nudges and effective privacy preserving technologies. Machine learning can be incorporated on any kind of textual data to automate human behavior extraction in large scale.},
  file          = {:article\\Stylometric fingerprints and privacy behavior in textual data.pdf:pdf},
  isbn          = {9781321785890},
  keywords      = {0984:Computer science,Applied sciences,Computer science,De-anonymization,Forensics,Privacy,Privacy behavior,Security,Stylometry,fuzz,machine learning,obfuscate,predicte,stat,static analysi,static analysis},
  mendeley-tags = {fuzz,machine learning,obfuscate,predicte},
  url           = {http://search.proquest.com/docview/1691134975?accountid=11054$\backslash$nhttp://jj2ec6wc6q.search.serialssolutions.com/?ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info:ofi/enc:UTF-8{\&}rfr{\_}id=info:sid/ProQuest+Dissertations+{\&}+Theses+Global{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:disserta},
}

@Article{Huuck2015,
  author        = {Huuck, Ralf},
  title         = {{Technology transfer: Formal analysis, engineering, and business value}},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {103},
  pages         = {3--12},
  __markedentry = {[ccc:6]},
  abstract      = {In this work we report on our experiences on developing and commercializing Goanna, a source code analyzer for detecting software bugs and security vulnerabilities in C/C++ code. Goanna is based on formal software analysis techniques such as model checking, static analysis and SMT solving. The commercial version of Goanna is currently deployed in a wide range of organizations around the world. Moreover, the underlying technology is licensed to an independent software vendor with tens of thousands of customers, making it possibly one of the largest deployments of automated formal methods technology. This paper explains some of the challenges as well as the positive results that we encountered in the technology transfer process. In particular, we provide some background on the design decisions and techniques to deal with large industrial code bases, we highlight engineering challenges and efforts that are typically outside of a more academic setting, and we address core aspects of the bigger picture for transferring formal techniques into commercial products, namely, the adoption of such technology and the value for purchasing organizations. While we provide a particular focus on Goanna and our experience with that underlying technology, we believe that many of those aspects hold true for the wider field of formal analysis and verification technology and its adoption in industry.},
  doi           = {10.1016/j.scico.2014.11.003},
  file          = {:article\\Technology transfer Formal analysis, engineering, and business value.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {01676423},
  keywords      = {Experience report,Industrial application,Model checking,SMT solving,Static analysis,binary,first select,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,machine learning,second select,source code,source code-important,source code-vice important},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2014.11.003},
}

@Article{Milewicz2015,
  author        = {Milewicz, Reed and Vanka, Rajesh and Tuck, James and Quinlan, Daniel and Pirkelbauer, Peter},
  title         = {{Lightweight runtime checking of C programs with RTC}},
  journal       = {Computer Languages, Systems and Structures},
  year          = {2015},
  volume        = {45},
  pages         = {191--203},
  __markedentry = {[ccc:6]},
  abstract      = {The C Programming Language is known for being an efficient language that can be compiled on almost any architecture and operating system. However the absence of dynamic safety checks and a relatively weak type system allows programmer oversights that are hard to spot. In this paper, we present RTC, a runtime monitoring tool that instruments unsafe code and monitors the program execution. RTC is built on top of the ROSE compiler infrastructure. RTC finds memory bugs and arithmetic overflows and underflows, and run-time type violations. Most of the instrumentations are directly added to the source file and only require a minimal runtime system. As a result, the instrumented code remains portable. In tests against known error detection benchmarks, RTC found 98{\%} of all memory related bugs and had zero false positives. In performance tests conducted with well known algorithms, such as binary search and MD5, we determined that our tool has an average run-time overhead rate of 9.7× and memory overhead rate of 3.5×.},
  doi           = {10.1016/j.cl.2016.01.001},
  file          = {:article\\Lightweight runtime checking of C programs with RTC.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781450331968},
  issn          = {14778424},
  keywords      = {C,C++,Runtime monitoring,Source code instrumentation,Static analysis,binary,first select,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important},
  publisher     = {Elsevier},
  url           = {http://dx.doi.org/10.1016/j.cl.2016.01.001},
}

@Article{Dewey2015,
  author        = {Dewey, David and Reaves, Bradley and Traynor, Patrick},
  title         = {{Uncovering use-after-free conditions in compiled code}},
  journal       = {Proceedings - 10th International Conference on Availability, Reliability and Security, ARES 2015},
  year          = {2015},
  pages         = {90--99},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/ARES.2015.61},
  file          = {:article\\Uncovering use-after-free conditions in compiled code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467365901},
  keywords      = {Binary Decompilation,Software Security,Static Analysis,binary,first select,obfuscate,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,obfuscate,second select,source code,source code-important,source code-vice important},
}

@Article{Melorose2015,
  author        = {Melorose, J. and Perroy, R. and Careas, S.},
  title         = {二进制代码安全性分析},
  journal       = {Statewide Agricultural Land Use Baseline 2015},
  year          = {2015},
  volume        = {1},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\二进制代码安全性分析.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {binary,icle},
  mendeley-tags = {binary},
  pmid          = {25246403},
}

@Article{Long2015,
  author        = {Long, Fan and Rinard, Martin},
  title         = {{Prophet: Automatic Patch Generation via Learning from Successful Patches}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {We present Prophet, a novel patch generation system that learns a probabilistic model over candidate patches from a database of past successful patches. Prophet defines the probabilistic model as the combination of a distribution over program points based on defect localization algorithms and a parametrized log-linear distribution over modification operations. It then learns the model parameters via maximum log-likelihood, which identifies important characteristics of the previous successful patches in the database. For a new defect, Prophet generates a search space that contains many candidate patches, applies the learned model to prioritize those potentially correct patches that are consistent with the identified successful patch characteristics, and then validates the candidate patches with a user supplied test suite. The experimental results indicate that these techniques enable Prophet to generate correct patches for 15 out of 69 real-world defects in eight open source projects. The previous state of the art generate and validate system, which uses a set of hand-code heuristics to prioritize the search, generates correct patches for 11 of these same 69 defects.},
  file          = {:article\\Prophet Automatic Patch Generation via Learning from Successful Patches.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781450335492},
  keywords      = {binary,first select,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,machine learning,predicte,source code,source code-vice important},
  url           = {http://dspace.mit.edu/handle/1721.1/97735},
}

@Article{Yeh2015,
  author        = {Yeh, Chao-Chun and Chung, Hsiang and Huang, Shih-Kun},
  title         = {{CRAXfuzz: Target-Aware Symbolic Fuzz Testing}},
  journal       = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  year          = {2015},
  number        = {March},
  pages         = {460--471},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/COMPSAC.2015.99},
  file          = {:article\\CRAXfuzz Target-Aware Symbolic Fuzz Testing.pdf:pdf},
  isbn          = {978-1-4673-6564-2},
  issn          = {07303157},
  keywords      = {-component,binary,fuzz,fuzz testing,software testing,symbolic execution,vulnerability},
  mendeley-tags = {binary,fuzz},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7273654},
}

@InProceedings{Dimitriou2015,
  author        = {Dimitriou, Tassos and Krontiris, Ioannis},
  title         = {{How Current Android Malware Seeks to Evade Automated Code Analysis}},
  booktitle     = {IFIP International Federation for Information Processing 2015},
  year          = {2015},
  pages         = {187--202},
  __markedentry = {[ccc:6]},
  abstract      = {First we report on a new threat campaign, underway in Korea, which infected around 20,000 Android users within two months. The campaign attacked mobile users with malicious applications spread via different channels, such as email attachments or SMS spam. A detailed investigation of the Android malware resulted in the identification of a new Android malware family Android/BadAccents. The family represents current state-of-the-art in mobile malware development for banking trojans. Second, we describe in detail the techniques this malware family uses and confront them with current state-of-the-art static and dynamic code-analysis techniques for Android applications. We highlight various challenges for automatic malware analysis frameworks that significantly hinder the fully automatic detection of malicious components in current Android malware. Furthermore, the malware exploits a previously unknown tapjacking vulnerability in the Android operating system, which we describe. As a result of this work, the vulnerability, affecting all Android versions, will be patched in one of the next releases of the Android Open Source Project.},
  doi           = {10.1007/978-3-319-24018-3},
  file          = {:article\\How Current Android Malware Seeks to Evade Automated Code Analysis.pdf:pdf},
  groups        = {vice-important},
  isbn          = {978-3-319-24017-6},
  issn          = {16113349},
  keywords      = {Incentive mechanisms,Mobile crowd sensing,Multi-attribute auctions,Security and privacy,android,first select,fuzz,machine learning,malware,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {android,first select,fuzz,machine learning,malware,predicte,source code,source code-vice important},
  url           = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84944704905{\&}partnerID=tZOtx3y1},
}

@Article{Yakdan2015,
  author        = {Yakdan, Khaled and Eschweiler, Sebastian and Gerhards-padilla, Elmar and Smith, Matthew},
  title         = {{No More Gotos : Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations}},
  journal       = {Ndss},
  year          = {2015},
  number        = {February},
  pages         = {8--11},
  __markedentry = {[ccc:6]},
  abstract      = {{\#}Dream. Solid paper. They have a new, pattern-independent algorithm to reverse the structure of a program. Interestingly, this algorithm resembles what I had in mind for solid: they first get the conditions for each instruction (in the paper, they call them "reaching condition", they compute a (boolean) formula, and they then minimize it. Their algorithms are design to not output gotos. The evaluation is strong: they beat hexrays and phoneix, and they indeed don't generate gotos, and it seems the output of their tool is recompilable C. I think this paper kills all possibilities for whatever you had in mind for java decompilation.},
  doi           = {http://dx.doi.org/10.14722/ndss.2015.23185},
  file          = {:article\\No More Gotos Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transf(2).pdf:pdf},
  isbn          = {189156238X},
  keywords      = {stat,static analysi,static analysis},
}

@PhdThesis{Testing2015,
  author        = {Testing, Scheduling Fuzz and Mutation, Symbolic},
  title         = {符号化变异之模糊测试拍程法},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\符号化变异之模糊测试拍程法.pdf:pdf},
  keywords      = {fuzz,stat,static analysi,static analysis},
  mendeley-tags = {fuzz},
}

@Article{Johnson2015,
  author        = {Johnson, Andrew and Waye, Lucas and Moore, Scott and Chong, Stephen},
  title         = {{Exploring and enforcing security guarantees via program dependence graphs}},
  journal       = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation - PLDI 2015},
  year          = {2015},
  pages         = {291--302},
  __markedentry = {[ccc:6]},
  abstract      = {We present PIDGIN, a program analysis and understanding tool that enables the specification and enforcement of precise application-specific information security guarantees. PIDGIN also allows developers to interactively explore the information flows in their applications to develop policies and investigate counter-examples. PIDGIN combines program dependence graphs (PDGs), which precisely capture the information flows in a whole application, with a custom PDG query language. Queries express properties about the paths in the PDG; because paths in the PDG correspond to information flows in the application, queries can be used to specify global security policies. PIDGIN is scalable. Generating a PDG for a 330k line Java application takes 90 seconds, and checking a policy on that PDG takes under 14 seconds. The query language is expressive, supporting a large class of precise, application-specific security guarantees. Policies are separate from the code and do not interfere with testing or development, and can be used for security regression testing. We describe the design and implementation of PIDGIN and report on using it: (1) to explore information security guarantees in legacy programs; (2) to develop and modify security policies concurrently with application development; and (3) to develop policies based on known vulnerabilities.},
  doi           = {10.1145/2737924.2737957},
  file          = {:article\\Exploring and enforcing security guarantees via program dependence graphs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450334686},
  issn          = {15232867},
  keywords      = {application-specific security,first select,graph,graph query language,program dependence,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://dl.acm.org/citation.cfm?doid=2737924.2737957},
}

@Article{Tsitsvero2015,
  author        = {Tsitsvero, M and Barbarossa, S and Lorenzo, P Di},
  title         = {{Signals on Graphs: Uncertainty Principle and Sampling}},
  journal       = {arXiv preprint arXiv:1507.08822},
  year          = {2015},
  number        = {2},
  pages         = {1--16},
  __markedentry = {[ccc:6]},
  abstract      = {In many applications of current interest, the observations are represented as a signal defined over a graph. The analysis of such signals requires the extension of standard signal processing tools. Building on the recently introduced Graph Fourier Transform, the first contribution of this paper is to provide an uncertainty principle for signals on graph. As a by-product of this theory, we show how to build a dictionary of maximally concentrated signals on vertex/frequency domains. Then, we establish a direct relation between uncertainty principle and sampling, which forms the basis for a sampling theorem for graph signals. Since samples location plays a key role in the performance of signal recovery algorithms, we suggest and compare a few alternative sampling strategies. Finally, we provide the conditions for perfect recovery of a useful signal corrupted by sparse noise, showing that this problem is also intrinsically related to vertex-frequency localization properties.},
  archiveprefix = {arXiv},
  arxivid       = {1507.08822{\#}},
  doi           = {10.1109/TSP.2016.2573748},
  eprint        = {1507.08822{\#}},
  file          = {:article\\Signals on Graphs Uncertainty Principle and Sampling.pdf:pdf},
  issn          = {1053-587X},
  keywords      = {stat,static analysi,static analysis},
  url           = {http://arxiv.org/abs/1507.08822},
}

@Article{Mangialardo2015,
  author        = {Mangialardo, R J and Duarte, J C},
  title         = {{Integrating Static and Dynamic Malware Analysis Using Machine Learning}},
  year          = {2015},
  volume        = {13},
  number        = {9},
  pages         = {3080--3087},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/TLA.2015.7350062},
  file          = {:article\\Integrating Static and Dynamic Malware Analysis Using Machine Learning.pdf:pdf},
  issn          = {15480992},
  keywords      = {binary,classifica{\c{c}}{\~{a}}o de,coleta est{\'{a}}tica e din{\^{a}}mica,de,dynamic analysis,em conjunto com algoritmos,information security,machine learning,malware,m{\'{a}}quina t{\^{e}}m sido utilizadas,para a identifica{\c{c}}{\~{a}}o e,stat,static analysi,static analysis,unified analysis},
  mendeley-tags = {binary,machine learning},
}

@Article{DeBoom2015,
  author        = {{De Boom}, Cedric and {Van Canneyt}, Steven and Bohez, Steven and Demeester, Thomas and Dhoedt, Bart},
  title         = {{Learning Semantic Similarity for Very Short Texts}},
  journal       = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
  year          = {2015},
  pages         = {1229--1234},
  __markedentry = {[ccc:6]},
  abstract      = {Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.},
  archiveprefix = {arXiv},
  arxivid       = {1512.00765},
  doi           = {10.1109/ICDMW.2015.86},
  eprint        = {1512.00765},
  file          = {:article\\Learning Semantic Similarity for Very Short Texts.pdf:pdf},
  isbn          = {978-1-4673-8493-3},
  keywords      = {machine learning,predicte},
  mendeley-tags = {machine learning,predicte},
  url           = {http://arxiv.org/abs/1512.00765},
}

@Article{Gao2015,
  author        = {Gao, Qing and Xiong, Yingfei and Mi, Yaqing and Zhang, Lu and Yang, Weikun and Zhou, Zhaoping and Xie, Bing and Mei, Hong},
  title         = {{Safe memory-leak fixing for C programs}},
  journal       = {Proceedings - International Conference on Software Engineering},
  year          = {2015},
  volume        = {1},
  number        = {1},
  pages         = {459--470},
  __markedentry = {[ccc:6]},
  abstract      = {Automatic bug fixing has become a promising direction for reducing manual effort in debugging. However, general approaches to automatic bug fixing may face some fundamental difficulties. In this paper, we argue that automatic fixing of specific types of bugs can be a useful complement. This paper reports our first attempt towards automatically fixing memory leaks in C programs. Our approach generates only safe fixes, which are guaranteed not to interrupt normal execution of the program. To design such an approach, we have to deal with several challenging problems such as inter-procedural leaks, global variables, loops, and leaks from multiple allocations. We propose solutions to all the problems and integrate the solutions into a coherent approach. We implemented our inter-procedural memory leak fixing into a tool named Leak Fix and evaluated Leak Fix on 15 programs with 522k lines of code. Our evaluation shows that Leak Fix is able to successfully fix a substantial number of memory leaks, and Leak Fix is scalable for large applications.},
  doi           = {10.1109/ICSE.2015.64},
  file          = {:article\\Safe memory-leak fixing for C programs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479919345},
  issn          = {02705257},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{王雷2015,
  author        = {王雷 and 陈归 and 金茂中},
  title         = {{Detection of Code Vulnerablilties via Constraint-Based Analysis and Model Checking}},
  journal       = {Statewide Agricultural Land Use Baseline 2015},
  year          = {2015},
  volume        = {1},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\基于约束分析与模型检测的代码安全漏洞检测方法研究.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {first select,icle,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  pmid          = {25246403},
}

@Article{Xiao-jun2015,
  author        = {Xiao-jun, Q I N and Zuo-ning, Chen and Lin-zhang, Wang},
  title         = {一种基于特征矩阵的软件脆弱性代码克隆检测方法},
  year          = {2015},
  volume        = {26},
  number        = {2},
  pages         = {348--363},
  __markedentry = {[ccc:6]},
  doi           = {10.13328/j.cnki.jos.004786},
  file          = {:article\\一种基于特征矩阵的软件脆弱性代码克隆检测方法.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {10009825},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Yakdan2015a,
  author        = {Yakdan, Khaled and Eschweiler, Sebastian and Gerhards-padilla, Elmar and Smith, Matthew},
  title         = {{No More Gotos : Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transformations}},
  journal       = {Ndss},
  year          = {2015},
  number        = {February},
  pages         = {8--11},
  __markedentry = {[ccc:6]},
  abstract      = {{\#}Dream. Solid paper. They have a new, pattern-independent algorithm to reverse the structure of a program. Interestingly, this algorithm resembles what I had in mind for solid: they first get the conditions for each instruction (in the paper, they call them "reaching condition", they compute a (boolean) formula, and they then minimize it. Their algorithms are design to not output gotos. The evaluation is strong: they beat hexrays and phoneix, and they indeed don't generate gotos, and it seems the output of their tool is recompilable C. I think this paper kills all possibilities for whatever you had in mind for java decompilation.},
  doi           = {http://dx.doi.org/10.14722/ndss.2015.23185},
  file          = {:article\\No More Gotos Decompilation Using Pattern-Independent Control-Flow Structuring and Semantics-Preserving Transfor.pdf:pdf},
  isbn          = {189156238X},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Carlini2015,
  author        = {Carlini, Nicolas and Barresi, Antonio and Z{\"{u}}rich, E T H and Payer, Mathias and Wagner, David and Gross, Thomas R and Z{\"{u}}rich, E T H and Carlini, Nicolas and Barresi, Antonio and Wagner, David and Gross, Thomas R},
  title         = {{Control-Flow Bending : On the Effectiveness of Control-Flow Integrity}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\Control-Flow Bending On the Effectiveness of Control-Flow Integrity.pdf:pdf},
  isbn          = {9781931971232},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Ma2015a,
  author        = {Ma, Jinxin and Zhang, Tao and Zhang, Puhan},
  title         = {{Enhancing symbolic execution method with a taint layer}},
  journal       = {2015 7th International Conference on Advanced Computational Intelligence, ICACI 2015},
  year          = {2015},
  pages         = {27--31},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/ICACI.2015.7184737},
  file          = {:article\\Enhancing symbolic execution method with a taint layer.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781479972579},
  keywords      = {Security,binary,first select,fuzz,obfuscate,predicte,source code,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,fuzz,obfuscate,predicte,source code,source code-vice important},
}

@PhdThesis{Ramos2015,
  author        = {Ramos, David A},
  title         = {{UNDER-CONSTRAINED SYMBOLIC EXECUTION:CORRECTNESS CHECKING FOR REAL CODE}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {Software defects pose a frequent challenge to developers, and their consequences are far- reaching. Despite advances in software engineering practices, programming language design, and debugging tools, bugs remain ubiquitous. Traditional testing techniques, while useful, have failed to prevent a significant number of bugs from affecting end users. One promising technique for automatically detecting bugs is dynamic symbolic execu- tion, which aims to test all possible execution paths through a program and identify inputs that cause the program to crash. Unfortunately, symbolic execution suffers from the well- known path explosion problem because the number of distinct execution paths through a program is, in the best case, exponential in the number of branch statements. Consequently, symbolic execution tools are typically ineffective for programs consisting of more than a few thousand lines of code, let alone large codebases with line counts in the millions. This dissertation presents a new, scalable approach to symbolic execution, under- constrained symbolic execution, that targets individual functions rather than whole pro- grams. This technique supports direct symbolic execution of arbitrary C functions and automatically synthesizes their inputs, even for complex, pointer-rich data structures. We demonstrate this technique's feasibility by thoroughly evaluating three use cases, although many others are possible. First, we use it to check the equivalence of library routines from different implementations that share a common interface (e.g., the C standard library). Second, we check whether code patches introduce new bugs by comparing two versions of the same function: before and after a patch is applied. Third, we use under- constrained symbolic execution to test a single version of a function, using a combination of heuristics to separate important errors from those likely to be false positives. In this dissertation, we describe UC-KLEE, a tool we built that implements under- constrained symbolic execution and supports the above use cases. We evaluate our tool on large, mature codebases including BIND, OpenSSL, and the Linux kernel and describe previously-unknown bugs we discovered in each of these codebases.},
  file          = {:article\\UNDER-CONSTRAINED SYMBOLIC EXECUTIONCORRECTNESS CHECKING FOR REAL CODE.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important},
  number        = {June},
}

@Article{Yeh2015a,
  author        = {Yeh, Chao-Chun and Chung, Hsiang and Huang, Shih-Kun},
  title         = {{CRAXfuzz: Target-Aware Symbolic Fuzz Testing}},
  journal       = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  year          = {2015},
  pages         = {460--471},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/COMPSAC.2015.99},
  file          = {:article\\CRAXfuzz Target-Aware Symbolic Fuzz Testing(2).pdf:pdf},
  isbn          = {978-1-4673-6564-2},
  issn          = {07303157},
  keywords      = {-component,fuzz,fuzz testing,software testing,symbolic execution,vulnerability},
  mendeley-tags = {fuzz},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7273654},
}

@Article{Blaschek2015,
  author        = {Blaschek, G},
  title         = {{Static program analysis}},
  journal       = {Elektronische Rechenanlagen},
  year          = {2015},
  volume        = {27},
  number        = {2},
  pages         = {89--95},
  __markedentry = {[ccc:6]},
  abstract      = {This paper deals with the state-of-the-art in the field of static program analysis in particular with the benefits and possible applications of complexity measures. Moreover, it contains the definition of a set of complexity numbers for hierarchic systems. It shows that static program analysis may cover more tasks during the software development process than just providing some help for quality assurance. Finally, it introduces two projects for implementation of tools for static program analysis, in particular for source language independent analyzers. (9 References).},
  file          = {:article\\Static program analysis.pdf:pdf},
  isbn          = {0013-5720},
  keywords      = {computational complexity,reviews,software engineering,stat,static analysi,static analysis},
  url           = {http://cs.au.dk/{~}amoeller/spa/spa.pdf},
}

@Article{Baier2015,
  author        = {Baier, Christel and Tinelli, Cesare},
  title         = {{Tools and Algorithms for the Construction and Analysis of Systems: 21st International Conference, TACAS 2015 Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015 London, UK, April 11-18, 2015 Proceedings}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2015},
  volume        = {9035},
  pages         = {III--IV},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-662-46681-0},
  file          = {:article\\Tools and Algorithms for the Construction and Analysis of Systems 21st International Conference, TACAS 2015 Held.pdf:pdf},
  isbn          = {9783662466803},
  issn          = {16113349},
  keywords      = {predicte},
  mendeley-tags = {predicte},
}

@Article{Singh2015,
  author        = {Singh, Pradeep Kumar and Agarwal, Dishti and Gupta, Aakriti},
  title         = {{A Systematic Review on Software Defect}},
  year          = {2015},
  pages         = {1793--1797},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Systematic Review on Software Defect.pdf:pdf},
  isbn          = {9789380544168},
  keywords      = {- software defect,defect density,faults,machine learning,predicte,prediction,software},
  mendeley-tags = {machine learning,predicte},
}

@Article{Min2015,
  author        = {Min, Changwoo and Kashyap, Sanidhya and Lee, Byoungyoung and Song, Chengyu and Kim, Taesoo},
  title         = {{Cross-checking semantic correctness}},
  journal       = {Proceedings of the 25th Symposium on Operating Systems Principles - SOSP '15},
  year          = {2015},
  pages         = {361--377},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2815400.2815422},
  file          = {:article\\Cross-checking semantic correctness.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450338349},
  keywords      = {binary,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,second select,source code,source code-important,source code-vice important,web},
  url           = {http://dl.acm.org/citation.cfm?id=2815400.2815422},
}

@InProceedings{Melorose2015a,
  author        = {Melorose, J. and Perroy, R. and Careas, S.},
  title         = {{International Journal on Advances in Security}},
  booktitle     = {International Journal on Anvances in Security},
  year          = {2015},
  volume        = {1},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\International Journal on Advances in Security.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {icle,machine learning,predicte,web},
  mendeley-tags = {machine learning,predicte,web},
  pmid          = {25246403},
}

@Article{Sampaio2015,
  author        = {Sampaio, Luciano and Garcia, Alessandro},
  title         = {{Exploring Context-Sensitive Data Flow Analysis for Early Vulnerability Detection}},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {113},
  pages         = {337--361},
  __markedentry = {[ccc:6]},
  abstract      = {Secure programming is the practice of writing programs that are resistant to attacks by malicious people or programs. Programmers of secure software have to be continuously aware of security vulnerabilities when writing their program statements. In order to improve programmers' awareness, static analysis techniques have been devised to find vulnerabilities in the source code. However, most of these techniques are built to encourage vulnerability detection a posteriori, only when developers have already fully produced (and compiled) one or more modules of a program. Therefore, this approach, also known as late detection, does not support secure programming but rather encourages posterior security analysis. The lateness of vulnerability detection is also influenced by the high rate of false positives, yielded by pattern matching, the underlying mechanism used by existing static analysis techniques. The goal of this paper is twofold. First, we propose to perform continuous detection of security vulnerabilities while the developer is editing each program statement, also known as early detection. Early detection can leverage his knowledge on the context of the code being created, contrary to late detection when developers struggle to recall and fix the intricacies of the vulnerable code they produced from hours to weeks ago. Second, we explore context-sensitive data flow analysis (DFA) for improving vulnerability detection and mitigate the limitations of pattern matching. DFA might be suitable for finding if an object has a vulnerable path. To this end, we have implemented a proof-of-concept Eclipse plugin for continuous DFA-based detection of vulnerabilities in Java programs. We also performed two empirical studies based on several industry-strength systems to evaluate if the code security can be improved through DFA and early vulnerability detection. Our studies confirmed that: (i) the use of context-sensitive DFA significantly reduces the rate of false positives when compared to existing techniques, without being detrimental to the detector performance, and (ii) early detection improves the awareness among developers and encourages programmers to fix security vulnerabilities promptly.},
  doi           = {10.1016/j.jss.2015.12.021},
  file          = {:article\\Exploring Context-Sensitive Data Flow Analysis for Early Vulnerability Detection.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {01641212},
  keywords      = {Data flow analysis,Secure programming,Security vulnerability,early detection,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier Inc.},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002873},
}

@Book{Hafiz2015,
  title         = {{Game of detections: how are security vulnerabilities discovered in the wild?}},
  publisher     = {Empirical Software Engineering},
  year          = {2015},
  author        = {Hafiz, Munawar and Fang, Ming},
  __markedentry = {[ccc:6]},
  booktitle     = {Empirical Software Engineering},
  doi           = {10.1007/s10664-015-9403-7},
  file          = {:article\\Game of detections how are security vulnerabilities discovered in the wild.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1066401594},
  issn          = {15737616},
  keywords      = {Empirical study,Secure software engineering,Vulnerability,binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://dx.doi.org/10.1007/s10664-015-9403-7},
}

@Article{Chen2015,
  author        = {Chen, Qi Alfred and Qian, Zhiyun and Jia, Yunhan Jack and Shao, Yuru and Mao, Z Morley},
  title         = {{Static Detection of Packet Injection Vulnerabilities – A Case for Identifying Attacker-controlled Implicit Information Leaks}},
  journal       = {Ccs '15},
  year          = {2015},
  pages         = {388--400},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2810103.2813643},
  file          = {:article\\Static Detection of Packet Injection Vulnerabilities – A Case for Identifying Attacker-controlled Implicit Inform.pdf:pdf},
  isbn          = {9781450338325},
  issn          = {15437221},
  keywords      = {all or part of,analysis,implicit information leakage,network,network protocol security,or,or hard copies of,permission to make digital,side channel detection,stat,static,static analysi,static analysis,this work for personal,web},
  mendeley-tags = {network,web},
}

@Article{Bakar2015,
  author        = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
  title         = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {106},
  pages         = {132--149},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
  doi           = {10.1016/j.jss.2015.05.006},
  file          = {:article\\Feature extraction approaches from natural language requirements for reuse in software product lines A s.pdf:pdf},
  groups        = {vice-important},
  isbn          = {01641212},
  issn          = {01641212},
  keywords      = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review,first select,fuzz,machine learning,source code,source code-vice important,web},
  mendeley-tags = {first select,fuzz,machine learning,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd.},
  url           = {http://dx.doi.org/10.1016/j.jss.2015.05.006},
}

@Article{Muntean2015a,
  author        = {Muntean, Paul and Rabbi, Adnan and Ibing, Andreas and Eckert, Claudia},
  title         = {{Automated Detection of Information Flow Vulnerabilities in UML State Charts and C Code}},
  journal       = {Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2015},
  year          = {2015},
  pages         = {128--137},
  __markedentry = {[ccc:6]},
  abstract      = {{\textcopyright} 2015 IEEE. Information flow vulnerabilities in UML statecharts and C code are detrimental as they can cause data leakagesor unexpected program behavior. Detecting such vulnerabilitieswith static code analysis techniques is challenging because codeis usually not available during the software design phase andprevious knowledge about what should be annotated and trackedis needed. In this paper we propose textual annotations used tointroduce information flow constraints in UML state charts andcode which are afterwards automatically loaded by informationflow checkers that check if imposed constraints hold or not. Weevaluated our approach on 6 open source test cases availablein the National Institute of Standards and Technology (NIST)Juliet test suite for C/C++. Our results show that our approachis effective and can be further applied to other types of UMLmodels and programming languages as well, in order to detectdifferent types of vulnerabilities.},
  doi           = {10.1109/QRS-C.2015.30},
  file          = {:article\\Automated Detection of Information Flow Vulnerabilities in UML State Charts and C Code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467395984},
  keywords      = {Information flow vulnerability,Model-based verification,Static code analysis,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Qian2015,
  author        = {Qian, Chenxiong and Luo, Xiapu and Le, Yu and Polytechnic, Hong Kong and Gu, Guofei},
  title         = {{VulHunter: Toward Discovering Vulnerabilities in Android Applications}},
  year          = {2015},
  pages         = {44--53},
  __markedentry = {[ccc:6]},
  file          = {:article\\VulHunter Toward Discovering Vulnerabilities in Android Applications.pdf:pdf},
  keywords      = {android,binary,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,web},
}

@Article{Shar2015,
  author        = {Shar, Lwin Khin and Briand, Lionel C. and Tan, Hee Beng Kuan},
  title         = {{Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning}},
  journal       = {IEEE Transactions on Dependable and Secure Computing},
  year          = {2015},
  volume        = {12},
  number        = {6},
  pages         = {688--707},
  __markedentry = {[ccc:6]},
  abstract      = {Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing.},
  doi           = {10.1109/TDSC.2014.2373377},
  file          = {:article\\Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {15455971},
  keywords      = {Vulnerability prediction,empirical study,first select,input validation and sanitization,machine learning,predicte,program analysis,second select,security measures,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Choi2015,
  author        = {Choi, Kwanghoon and Chang, Byeong-Mo},
  title         = {{A lightweight approach to component-level exception mechanism for robust android apps}},
  journal       = {Computer Languages, Systems {\&} Structures},
  year          = {2015},
  volume        = {44},
  pages         = {283--298},
  __markedentry = {[ccc:6]},
  abstract      = {Recent researches have reported that Android programs are vulnerable to unexpected exceptions. One reason is that the current design of Android platform solely depends on Java exception mechanism, which is unaware of the component-based structure of Android programs. This paper proposes a component-level exception mechanism for programmers to build robust Android programs with. With the mechanism, they can define an intra-component handler for each component to recover from exceptions, and they can propagate uncaught exceptions to caller component along the reverse of component activation flow. Theoretically, we have formalized an Android semantics with exceptions to prove the robustness property of the mechanism. In practice, we have implemented the mechanism with a domain-specific library that extends existing Android components. This lightweight approach does not demand the change of the Android platform. In our experiment with Android benchmark programs, the library is found to catch a number of runtime exceptions that would otherwise get the programs terminated abnormally. We also measure the overhead of using the library to show that it is very small. Our proposal is a new mechanism for defending Android programs from unexpected exceptions.},
  doi           = {10.1016/j.cl.2015.08.010},
  file          = {:article\\A lightweight approach to component-level exception mechanism for robust android apps.pdf:pdf},
  issn          = {14778424},
  keywords      = {Android,Component,Exception,Java,Semantics,android,binary,fuzz,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,fuzz,web},
  publisher     = {Elsevier},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842415300038},
}

@Article{Melorose2015b,
  author        = {Melorose, J. and Perroy, R. and Careas, S.},
  title         = {{SECURITY VULNERABILITIES: DISCOVERY, PREDICTION, EFFECT, ANDMITIGATION}},
  journal       = {Statewide Agricultural Land Use Baseline 2015},
  year          = {2015},
  volume        = {1},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\SECURITY VULNERABILITIES DISCOVERY, PREDICTION, EFFECT, ANDMITIGATION.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {binary,first select,icle,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,predicte,source code,source code-vice important,web},
  pmid          = {25246403},
}

@Article{Delaitre2015,
  author        = {Delaitre, Aurelien and Stivalet, Bertrand and Fong, Elizabeth and Okun, Vadim},
  title         = {{Evaluating bug finders - Test and measurement of static code analyzers}},
  journal       = {Proceedings - 1st International Workshop on Complex Faults and Failures in Large Software Systems, COUFLESS 2015},
  year          = {2015},
  pages         = {14--20},
  __markedentry = {[ccc:6]},
  abstract      = {—Software static analysis is one of many options for finding bugs in software. Like compilers, static analyzers take a program as input. This paper covers tools that examine source codewithout executing itand output bug reports. Static analysis is a complex and generally undecidable problem. Most tools resort to approximation to overcome these obstacles and it sometimes leads to incorrect results. Therefore, tool effectiveness needs to be evaluated. Several characteristics of the tools should be examined. First, what types of bugs can they find? Second, what proportion of bugs do they report? Third, what percentage of findings is correct? These questions can be answered by one or more metrics. But to calculate these, we need test cases having certain characteristics: statistical significance, ground truth, and relevance. Test cases with all three attributes are out of reach, but we can use combinations of only two to calculate the metrics. The results in this paper were collected during Static Analysis Tool Exposition (SATE) V, where participants ran 14 static analyzers on the test sets we provided and submitted their reports to us for analysis. Tools had considerably different support for most bug classes. Some tools discovered significantly more bugs than others or generated mostly accurate warnings, while others reported wrong findings more frequently. Using the metrics, an evaluator can compare candidates and select the tool that aligns best with his or her objectives. In addition, our results confirm that the bugs most commonly found by tools are among the most common and important bugs in software. We also observed that code complexity is a major hindrance for static analyzers and detailed which code constructs tools handle well and which impede their analysis.},
  doi           = {10.1109/COUFLESS.2015.10},
  file          = {:article\\Test and measurement of static code analyzers.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479919345},
  keywords      = {Software assurance,Software faults,Software vulnerability,Static analysis tools,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Shastry2015,
  author        = {Shastry, Bhargava and Yamaguchi, Fabian and Rieck, Konrad and Seifert, Jean-Pierre},
  title         = {{Towards Vulnerability Discovery Using Staged Program Analysis}},
  year          = {2015},
  pages         = {1--23},
  __markedentry = {[ccc:6]},
  abstract      = {Eliminating vulnerabilities from low-level code is vital for securing software. Static analysis is a promising approach for discovering vulnerabilities since it can provide developers early feedback on the code they write. But, it presents multiple challenges not the least of which is understanding what makes a bug exploitable and conveying this information to the developer. In this paper, we present the design and implementation of a practical vulnerability assessment framework, called Melange. Melange performs data and control flow analysis to diagnose potential security bugs, and outputs well-formatted bug reports that help developers understand and fix security bugs. Based on the intuition that real-world vulnerabilities manifest themselves across multiple parts of a program, Melange performs both local and global analyses. To scale up to large programs, global analysis is demand-driven. Our prototype detects multiple vulnerability classes in C and C++ code including type confusion, and garbage memory reads. We have evaluated Melange extensively. Our case studies show that Melange scales up to large codebases such as Chromium, is easy-to-use, and most importantly, capable of discovering vulnerabilities in real-world code. Our findings indicate that static analysis is a viable reinforcement to the software testing tool set.},
  archiveprefix = {arXiv},
  arxivid       = {1508.04627},
  eprint        = {1508.04627},
  file          = {:article\\Towards Vulnerability Discovery Using Staged Program Analysis.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,web},
  url           = {http://arxiv.org/abs/1508.04627},
}

@Article{Zhu2015,
  author        = {Zhu, Erzhou and Liu, Feng and Wang, Zuo and Liang, Alei and Zhang, Yiwen and Li, Xuejian and Li, Xuejun},
  title         = {{Dytaint: The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs}},
  journal       = {Computers and Security},
  year          = {2015},
  volume        = {52},
  pages         = {51--69},
  __markedentry = {[ccc:6]},
  abstract      = {By analyzing information flow at runtime, dynamic taint analysis can precisely detect a wide range of vulnerabilities of software. However, it suffers from substantial runtime overhead and is incapable of discovering potential threats. Yet, realistically, the interested analyst doesn't have access to the source code of the malware. Therefore, the task of software flaw tracking becomes rather complicated. In order to cope with these issues, this paper proposes Dytaint, a novel lightweight 3-state dynamic taint analysis framework, for diagnosing more software vulnerabilities with lower runtime overhead. The framework works for the x86 binary executables and requires no special hardware assistance. Besides the tainted and the untainted states that are discussed by many popularly used taint analysis tools, the third state, controlled-taint state, is proposed to detect more types of software vulnerabilities. The new Chaining Hash Table which reduces the space for storing taint information without increasing the accessing time is also incorporated in the framework. Furthermore, two mechanisms, namely, the irrelevant API filtering based on the function recognition method and basic block handling, are introduced to optimize the runtime performance of our framework. The testing results by running SPEC CINT2006 benchmarks and various popular software have demonstrated that Dytaint is efficient which incurs only 3.1 times overhead to the native on average and practical which is able to discover not only all the real threats but also most of the potential ones.},
  doi           = {10.1016/j.cose.2015.03.008},
  file          = {:article\\Dytaint The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs.pdf:pdf},
  issn          = {01674048},
  keywords      = {Chaining Hash Table,Controlled-taint,Dynamic binary taint analysis,Irrelevant API filtration,Software security,binary,fuzz,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2015.03.008},
}

@Article{Gupta2015,
  author        = {Gupta, Mukesh Kumar and Govil, Mahesh Chandra and Singh, Girdhari},
  title         = {{Predicting Cross-Site Scripting (XSS) security vulnerabilities in web applications}},
  journal       = {Proceedings of the 2015 12th International Joint Conference on Computer Science and Software Engineering, JCSSE 2015},
  year          = {2015},
  pages         = {162--167},
  __markedentry = {[ccc:6]},
  abstract      = {Recently, machine-learning based vulnerability prediction models are gaining popularity in web security space, as these models provide a simple and efficient way to handle web application security issues. Existing state-of-art Cross-Site Scripting (XSS) vulnerability prediction approaches do not consider the context of the user-input in output-statement, which is very important to identify context-sensitive security vulnerabilities. In this paper, we propose a novel feature extraction algorithm to extract basic and context features from the source code of web applications. Our approach uses these features to build various machine-learning models for predicting context-sensitive Cross-Site Scripting (XSS) security vulnerabilities. Experimental results show that the proposed features based prediction models can discriminate vulnerable code from non-vulnerable code at a very low false rate.},
  doi           = {10.1109/JCSSE.2015.7219789},
  file          = {:article\\Predicting Cross-Site Scripting (XSS) security vulnerabilities in web applications.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479919659},
  keywords      = {context-sensitive,cross-site scripting vulnerability,first select,input validation,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web,web application security},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Choi2015a,
  author        = {Choi, Junho and Sung, Woon and Choi, Chang and Kim, Pankoo},
  title         = {{Personal information leakage detection method using the inference-based access control model on the Android platform}},
  journal       = {Pervasive and Mobile Computing},
  year          = {2015},
  volume        = {24},
  pages         = {138--149},
  __markedentry = {[ccc:6]},
  abstract      = {The web services used on desktop can be accessed through a smartphone due to the development of smart devices. As the usage of smartphones increases, the importance of personal information security inside the smartphone is emphasized. The openness features of Android platform make a lot easier to develop an application and also deploying malicious codes into application is an easy task for hackers. The security practices are also growing rapidly as the number of malicious code increases exponentially. According to these circumstances, new methods for detecting and protecting the behavior of leaked personal information are needed to manage the personal information within a smartphone.In this paper, we study the permission access category in order to detect the malicious code, which discloses the personal information on Android environment such as equipment and location information, address book and messages, and solve the problem related to Resource access of Random Access Control method in conventional Android file system to detect the new malware or malicious code via the context ontology reasoning of permission access and API resource information which the personal information are leaked through. Then we propose an inference-based access control model, which can be enabled to access the proactive security. There is more improvement accuracy than existing malicious detecting techniques and effectiveness of access control model is verified through the proposal of inference-based access control model.},
  doi           = {10.1016/j.pmcj.2015.06.005},
  file          = {:article\\Personal information leakage detection method using the inference-based access control model on the Android platfor.pdf:pdf},
  issn          = {15741192},
  keywords      = {Access control model,Android security,Information leakage detection,Ontology inference,android,binary,stat,static analysi,static analysis,web},
  mendeley-tags = {android,binary,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.pmcj.2015.06.005},
}

@Article{Bahamdain2015,
  author        = {Bahamdain, Salem S.},
  title         = {{Open source software (OSS) quality assurance: A survey paper}},
  journal       = {Procedia Computer Science},
  year          = {2015},
  volume        = {56},
  number        = {1},
  pages         = {459--464},
  __markedentry = {[ccc:6]},
  abstract      = {Open source software (OSS) is a software product with the source code made public so that anyone can read, analyze, and change or improve the code. The use of this software is under a license, like Apache, GNU, MIT, Mozilla Public, and Eclipse Public License. Open source software development (OSSD) provides high quality assurance through user testing and peer reviews. The quality of these products depends on the size of the product community. This paper discusses the stakeholders of the OSS community, the quality assurance frameworks and models proposed in some studies, some statistics about OSS, the problems that affect the quality of OSSD, and the advantages and disadvantages of OSS compared to closed source software. This allows us to understand how we can achieve and improve the quality assurance and quality control of OSSD.},
  doi           = {10.1016/j.procs.2015.07.236},
  file          = {:article\\Open source software (OSS) quality assurance A survey paper.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {0-7695-2977-1},
  issn          = {18770509},
  keywords      = {OSS, open source,OSSD,Open source development model,Quality assurance,first select,second select,source code,source code-important,source code-vice important,survey,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,survey,web},
  pmid          = {21444166},
  publisher     = {Elsevier Masson SAS},
  url           = {http://dx.doi.org/10.1016/j.procs.2015.07.236},
}

@Article{Feizollah2015,
  author        = {Feizollah, Ali and Anuar, Nor Badrul and Salleh, Rosli and Wahab, Ainuddin Wahid Abdul},
  title         = {{A review on feature selection in mobile malware detection}},
  journal       = {Digital Investigation},
  year          = {2015},
  volume        = {13},
  pages         = {22--37},
  __markedentry = {[ccc:6]},
  abstract      = {The widespread use of mobile devices in comparison to personal computers has led to a new era of information exchange. The purchase trends of personal computers have started decreasing whereas the shipment of mobile devices is increasing. In addition, the increasing power of mobile devices along with portability characteristics has attracted the attention of users. Not only are such devices popular among users, but they are favorite targets of attackers. The number of mobile malware is rapidly on the rise with malicious activities, such as stealing users data, sending premium messages and making phone call to premium numbers that users have no knowledge. Numerous studies have developed methods to thwart such attacks. In order to develop an effective detection system, we have to select a subset of features from hundreds of available features. In this paper, we studied 100 research works published between 2010 and 2014 with the perspective of feature selection in mobile malware detection. We categorize available features into four groups, namely, static features, dynamic features, hybrid features and applications metadata. Additionally, we discuss datasets used in the recent research studies as well as analyzing evaluation measures utilized.},
  doi           = {10.1016/j.diin.2015.02.001},
  file          = {:article\\A review on feature selection in mobile malware detection.pdf:pdf},
  issn          = {17422876},
  keywords      = {Android,Feature selection,Mobile malware,Mobile operating system,Review paper,android,fuzz,machine learning,predicte,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {android,fuzz,machine learning,predicte,survey,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.diin.2015.02.001},
}

@Article{Hydara2015,
  author        = {Hydara, Isatou and Sultan, Abu Bakar Md and Zulzalil, Hazura and Admodisastro, Novia},
  title         = {{Current state of research on cross-site scripting (XSS) - A systematic literature review}},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {58},
  pages         = {170--186},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications. Objective: To conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks. Method: We followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings. Results: Research on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS. Conclusion: XSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment.},
  doi           = {10.1016/j.infsof.2014.07.010},
  file          = {:article\\A systematic literature review.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Cross-site scripting,Security,Systematic literature review,Web applications,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,survey,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2014.07.010},
}

@Article{Catherine2015,
  author        = {Catherine, S. Monica and George, Geogen},
  title         = {{S-compiler: A code vulnerability detection method}},
  journal       = {International Conference on Electrical, Electronics, Signals, Communication and Optimization, EESCO 2015},
  year          = {2015},
  pages         = {2--5},
  __markedentry = {[ccc:6]},
  abstract      = {Nowadays, security breaches are greatly increasing in number. This is one of the major threats that are being faced by most organisations which usually lead to a massive loss. The major cause for these breaches could potentially be the vulnerabilities in software products. There are many tools available to detect such vulnerabilities but detection and correction of vulnerabilities during development phase would be more beneficial. Though there are many standard secure coding practices to be followed in development phase, software developers fail to utilize them and this leads to an unsecured end product. The difficulty in manual analysis of vulnerabilities in source code is what leads to the evolution of automated analysis tools. Static and dynamic analyses are the two complementary methods used to detect vulnerabilities in development phase. Static analysis scans the source code which eliminates the need of execution of the code but it has many false positives and false negatives. On the other hand, dynamic analysis tests the code by running it along with the test cases. The proposed approach integrates static and dynamic analysis. This eliminates the false positives and false negatives problem of the existing practices and helps developers to correct their code in the most efficient way. It deals with common buffer overflow vulnerabilities and vulnerabilities from Common Weakness Enumeration (CWE). The whole scenario is implemented as a web interface.},
  doi           = {10.1109/EESCO.2015.7254018},
  file          = {:article\\S-compiler A code vulnerability detection method.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479976768},
  keywords      = {Buffer overflow,Dynamic analysis,Secure coding,Static analysis,first select,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Yamaguchi2015,
  author        = {Yamaguchi, Fabian and Maier, Alwin and Gascon, Hugo and Rieck, Konrad},
  title         = {{Automatic inference of search patterns for taint-style vulnerabilities}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2015},
  volume        = {2015-July},
  pages         = {797--812},
  __markedentry = {[ccc:6]},
  abstract      = {Taint-style vulnerabilities are a persistent problem in software development, as the recently discovered “Heartbleed” vulnerability strikingly illustrates. In this class of vulnerabil- ities, attacker-controlled data is passed unsanitized from an input source to a sensitive sink. While simple instances of this vulnerability class can be detected automatically, more subtle defects involving data flow across several functions or project- specific APIs are mainly discovered by manual auditing. Different techniques have been proposed to accelerate this process by searching for typical patterns of vulnerable code. However, all of these approaches require a security expert to manually model and specify appropriate patterns in practice. In this paper, we propose a method for automatically inferring search patterns for taint-style vulnerabilities in C code. Given a security-sensitive sink, such as a memory function, our method automatically identifies corresponding source-sink systems and constructs patterns that model the data flow and sanitization in these systems. The inferred patterns are expressed as traversals in a code property graph and enable efficiently searching for unsanitized data flows—across several functions as well as with project-specific APIs. We demonstrate the efficacy of this approach in different experiments with 5 open-source projects. The inferred search patterns reduce the amount of code to inspect for finding known vulnerabilities by 94.9{\%} and also enable us to uncover 8 previously unknown vulnerabilities.},
  doi           = {10.1109/SP.2015.54},
  file          = {:article\\Automatic inference of search patterns for taint-style vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467369497},
  issn          = {10816011},
  keywords      = {Clustering,Graph Databases,Vulnerabilities,binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,second select,source code,source code-important,source code-vice important,web},
}

@Article{Mokhov2015,
  author        = {Mokhov, Serguei A. and Paquet, Joey and Debbabi, Mourad},
  title         = {{MARFCAT : Fast Code Analysis for Defects and Vulnerabilities}},
  journal       = {Proceedings of the 1st International Workshop on Software Analytics (SWAN)},
  year          = {2015},
  pages         = {35--38},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/SWAN.2015.7070488},
  file          = {:article\\MARFCAT Fast Code Analysis for Defects and Vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467369237},
  keywords      = {binary,first select,fuzz,machine learning,marfcat,predicte,second select,signal processing,source code,source code-important,source code-vice important,stat,static analysi,static analysis,static code analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Book{Guo2015,
  title         = {{The Ph.D. Grind}},
  year          = {2015},
  author        = {Guo, Philip J},
  __markedentry = {[ccc:6]},
  file          = {:article\\The Ph.D. Grind.pdf:pdf},
  keywords      = {book,web},
  mendeley-tags = {book,web},
}

@Article{Gosain2015,
  author        = {Gosain, Anjana and Sharma, Ganga},
  title         = {{Static Analysis: A Survey of Techniques and Tools}},
  journal       = {Advances in Intelligent Systems and Computing},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {Static program analysis has shown tremendous surge from basic compiler optimization technique to becoming a major role player in correctness and verification of software. Because of its rich theoretical background, static analysis is in a good position to help produce quality software. This paper provides an overview of the existing static analysis techniques and tools. Further, it gives a critique of static analysis approach over six attributes, namely precision, efficiency, coverage, modularity, scalability, and automation.},
  doi           = {10.1007/978-81-322-2268-2},
  file          = {:article\\Static Analysis A Survey of Techniques and Tools.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-81-322-2267-5},
  issn          = {21945357},
  keywords      = {Feature selection,Machine learning,Movie domain,Sentiment analysis,Sentiment classification,first select,machine learning,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,second select,source code,source code-important,source code-vice important,web},
  url           = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84924058158{\&}partnerID=tZOtx3y1},
}

@Article{Shar2015a,
  author        = {Shar, Lwin Khin and Briand, Lionel C and Tan, Hee Kuan and Member, Senior},
  title         = {{Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning}},
  journal       = {IEEE Transactions on Dependable and Secure Computing},
  year          = {2015},
  volume        = {12},
  number        = {6},
  pages         = {688--707},
  __markedentry = {[ccc:6]},
  abstract      = {Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (staticþdynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing},
  file          = {:article\\Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {Vulnerability prediction,empirical study,first select,input validation and sanitization,machine learning,predicte,program analysis,second select,security measures,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Tang2015,
  author        = {Tang, Yaming and Zhao, Fei and Yang, Yibiao and Lu, Hongmin and Zhou, Yuming and Xu, Baowen},
  title         = {{Predicting Vulnerable Components via Text Mining or Software Metrics? An Effort-Aware Perspective}},
  journal       = {2015 IEEE International Conference on Software Quality, Reliability and Security},
  year          = {2015},
  pages         = {27--36},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/QRS.2015.15},
  file          = {:article\\Predicting Vulnerable Components via Text Mining or Software Metrics An Effort-Aware Perspective.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-1-4673-7989-2},
  keywords      = {-software metrics,binary,effort-aware,first select,machine learning,predicte,prediction,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7272911},
}

@Article{Ramos2015a,
  author        = {Ramos, David a and Engler, Dawson},
  title         = {{Under-Constrained Symbolic Execution : Correctness Checking for Real Code}},
  journal       = {USENIX Security Symposium},
  year          = {2015},
  pages         = {49--64},
  __markedentry = {[ccc:6]},
  file          = {:article\\Under-Constrained Symbolic Execution Correctness Checking for Real Code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781931971232},
  keywords      = {binary,first select,obfuscate,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,obfuscate,second select,source code,source code-important,source code-vice important,web},
}

@Article{Chen2015a,
  author        = {Chen, Kai and Zhang, Yingjun and Liu, Peng},
  title         = {{Dynamically Discovering Likely Memory Layout to Perform Accurate Fuzzing}},
  journal       = {IEEE Transactions on Reliability},
  year          = {2015},
  pages         = {1--15},
  __markedentry = {[ccc:6]},
  file          = {:article\\Dynamically Discovering Likely Memory Layout to Perform Accurate Fuzzing.pdf:pdf},
  keywords      = {Dynamic testing,binary,fuzz,fuzzing,memory layout,stat,static analysi,static analysis,vulner- ability,web,white-box},
  mendeley-tags = {binary,fuzz,web},
}

@Article{Roumani2015,
  author        = {Roumani, Yaman and Nwankpa, Joseph K and Roumani, Yazan F},
  title         = {{Time series modeling of vulnerabilities}},
  journal       = {Computers {\&} Security},
  year          = {2015},
  volume        = {51},
  number        = {0},
  pages         = {32--40},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract Vulnerability prediction models forecast future vulnerabilities and can be used to assess security risks and estimate the resources needed for handling potential security breaches. Although several vulnerability prediction models have been proposed, such models have shortcomings and do not consider trend, level, and seasonality components of vulnerabilities. Through time series analysis, this study built predictive models for five popular web browsers: Chrome, Firefox, Internet Explorer, Safari and Opera and for all reported vulnerabilities elsewhere. Results showed that time series models provide a good fit to our vulnerability datasets and can be useful for vulnerability prediction. Results also suggested that the level of the series is the best estimator of the prediction models.},
  doi           = {http://dx.doi.org/10.1016/j.cose.2015.03.003},
  file          = {:article\\Time series modeling of vulnerabilities.pdf:pdf},
  groups        = {vice-important},
  issn          = {0167-4048},
  keywords      = {ARIMA,Exponential smoothing,Forecasting,Time series modeling,Vulnerabilities,Web browsers,first select,machine learning,predicte,source code,source code-vice important,web},
  mendeley-tags = {first select,machine learning,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167404815000358},
}

@Article{Sidiroglou-Douskos2015,
  author        = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin},
  title         = {{Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement}},
  journal       = {Architectural Support for Programming Languages and Operating Systems},
  year          = {2015},
  pages         = {473--486},
  __markedentry = {[ccc:6]},
  abstract      = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.},
  doi           = {10.1145/2694344.2694389},
  file          = {:article\\Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781450328357},
  issn          = {03621340},
  keywords      = {binary,first select,fuzz,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,source code,source code-vice important,web},
  url           = {http://dl.acm.org/citation.cfm?doid=2694344.2694389},
}

@Article{Fuchs2015,
  author        = {Fuchs, Per},
  title         = {{DoS Detection in NodeRED}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\DoS Detection in NodeRED.pdf:pdf},
  keywords      = {machine learning,network,obfuscate,stat,static analysi,static analysis,web},
  mendeley-tags = {machine learning,network,obfuscate,web},
  url           = {https://web.sec.uni-passau.de/projects/compose/papers/Fuchs{\_}DoS{\_}in{\_}nodeRED.pdf},
}

@Article{Pan2015,
  author        = {Pan, Jinkun and Mao, Xiaoguang and Li, Weishi},
  title         = {{Analyst-oriented taint analysis by taint path slicing and aggregation}},
  journal       = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
  year          = {2015},
  volume        = {2015-Novem},
  pages         = {145--148},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/ICSESS.2015.7339024},
  file          = {:article\\Analyst-oriented taint analysis by taint path slicing and aggregation.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781479983520},
  issn          = {23270594},
  keywords      = {analyst,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,taint analysis,taint path,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Sidiroglou-douskos2015,
  author        = {Sidiroglou-douskos, Stelios and Lahtinen, Eric and Rinard, Martin and Sidiroglou-douskos, Stelios and Lahtinen, Eric and Rinard, Martin},
  title         = {{Automatic Discovery and Patching of Buffer and Integer Overflow Errors}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\Automatic Discovery and Patching of Buffer and Integer Overflow Errors.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,second select,source code,source code-important,source code-vice important,web},
}

@Article{He2015,
  author        = {He, Boyuan and Rastogi, Vaibhav and Cao, Yinzhi and Chen, Yan and Venkatakrishnan, V. N. and Yang, Runqing and Zhang, Zhenrui},
  title         = {{Vetting SSL usage in applications with SSLINT}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2015},
  volume        = {2015-July},
  pages         = {519--534},
  __markedentry = {[ccc:6]},
  abstract      = {Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols have become the security backbone of the Web and Internet today. Many systems including mobile and desktop applications are protected by SSL/TLS protocols against network attacks. However, many vulnerabilities caused by incorrect use of SSL/TLS APIs have been uncovered in recent years. Such vulnerabilities, many of which are caused due to poor API design and inexperience of application developers, often lead to confidential data leakage or man-in-the-middle attacks. In this paper, to guarantee code quality and logic correctness of SSL/TLS applications, we design and implement SSLINT, a scalable, automated, static analysis system for detecting incorrect use of SSL/TLS APIs. SSLINT is capable of performing automatic logic verification with high efficiency and good accuracy. To demonstrate it, we apply SSLINT to one of the most popular Linux distributions -- Ubuntu. We find 27 previously unknown SSL/TLS vulnerabilities in Ubuntu applications, most of which are also distributed with other Linux distributions.},
  doi           = {10.1109/SP.2015.38},
  file          = {:article\\Vetting SSL usage in applications with SSLINT.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467369497},
  issn          = {10816011},
  keywords      = {first select,fuzz,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Mirsky2015,
  author        = {Mirsky, Yisroel and Cohen, Noam and Shabtai, Asaf},
  title         = {{Up-High to Down-Low: Applying Machine Learning to an Exploit Database}},
  year          = {2015},
  volume        = {9522},
  pages         = {253--266},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-319-27179-8},
  file          = {:article\\Up-High to Down-Low Applying Machine Learning to an Exploit Database.pdf:pdf},
  groups        = {vice-important},
  isbn          = {978-3-319-27178-1},
  keywords      = {binary,countermeasure,differential power analysis attack,first select,implementation and algorithm level,machine learning,predicte,source code,source code-vice important,trivium,web},
  mendeley-tags = {binary,first select,machine learning,predicte,source code,source code-vice important,web},
  url           = {http://link.springer.com/10.1007/978-3-319-27179-8},
}

@Article{Zhu2015a,
  author        = {Zhu, Erzhou and Liu, Feng and Wang, Zuo and Liang, Alei and Zhang, Yiwen and Li, Xuejian and Li, Xuejun},
  title         = {{Dytaint: The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs}},
  journal       = {Computers and Security},
  year          = {2015},
  volume        = {52},
  pages         = {51--69},
  __markedentry = {[ccc:6]},
  abstract      = {By analyzing information flow at runtime, dynamic taint analysis can precisely detect a wide range of vulnerabilities of software. However, it suffers from substantial runtime overhead and is incapable of discovering potential threats. Yet, realistically, the interested analyst doesn't have access to the source code of the malware. Therefore, the task of software flaw tracking becomes rather complicated. In order to cope with these issues, this paper proposes Dytaint, a novel lightweight 3-state dynamic taint analysis framework, for diagnosing more software vulnerabilities with lower runtime overhead. The framework works for the x86 binary executables and requires no special hardware assistance. Besides the tainted and the untainted states that are discussed by many popularly used taint analysis tools, the third state, controlled-taint state, is proposed to detect more types of software vulnerabilities. The new Chaining Hash Table which reduces the space for storing taint information without increasing the accessing time is also incorporated in the framework. Furthermore, two mechanisms, namely, the irrelevant API filtering based on the function recognition method and basic block handling, are introduced to optimize the runtime performance of our framework. The testing results by running SPEC CINT2006 benchmarks and various popular software have demonstrated that Dytaint is efficient which incurs only 3.1 times overhead to the native on average and practical which is able to discover not only all the real threats but also most of the potential ones.},
  doi           = {10.1016/j.cose.2015.03.008},
  file          = {:article\\Dytaint The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary program(2).pdf:pdf},
  issn          = {01674048},
  keywords      = {Chaining Hash Table,Controlled-taint,Dynamic binary taint analysis,Irrelevant API filtration,Software security,binary,fuzz,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/j.cose.2015.03.008},
}

@Article{Goseva-Popstojanova2015,
  author        = {Goseva-Popstojanova, Katerina and Perhinschi, Andrei},
  title         = {{On the capability of static code analysis to detect security vulnerabilities}},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {68},
  pages         = {18--33},
  __markedentry = {[ccc:6]},
  abstract      = {Context: Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection. Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings. Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source programs. Results: Our experiment showed that 27{\%} of C/C++ vulnerabilities and 11{\%} of Java vulnerabilities were missed by all three tools. Some vulnerabilities were detected by only one or combination of two tools; 41{\%} of C/C++ and 21{\%} of Java vulnerabilities were detected by all three tools. More importantly, static code analysis tools did not show statistically significant difference in their ability to detect security vulnerabilities for both C/C++ and Java. Interestingly, all tools had median and mean of the per CWE recall values and overall recall across all CWEs close to or below 50{\%}, which indicates comparable or worse performance than random guessing. While for C/C++ vulnerabilities one of the tools had better performance in terms of probability of false alarm than the other two tools, there was no statistically significant difference among tools' probability of false alarm for Java test cases. Conclusions: Despite recent advances in methods for static code analysis, the state-of-the-art tools are not very effective in detecting security vulnerabilities.},
  doi           = {10.1016/j.infsof.2015.08.002},
  file          = {:article\\On the capability of static code analysis to detect security vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {09505849},
  keywords      = {Case studies,Common Weakness Enumeration (CWE),Experiment,Security vulnerabilities,Static code analysis evaluation,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Elsevier Ltd.},
  url           = {http://dx.doi.org/10.1016/j.infsof.2015.08.002},
}

@Article{Perl2015,
  author        = {Perl, Henning and Smith, Matthew and Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad and Fahl, Sascha},
  title         = {{VCCFinder : Finding Potential Vulnerabilities in Open-Source Projects to Assist Code}},
  journal       = {Ccs '15},
  year          = {2015},
  pages         = {426--437},
  __markedentry = {[ccc:6]},
  abstract      = {Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. How-ever, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag poten-tially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding poten-tially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We com-bine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 {\%} at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.},
  doi           = {10.1145/2810103.2813604},
  file          = {:article\\VCCFinder Finding Potential Vulnerabilities in Open-Source Projects to Assist Code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450338325},
  issn          = {15437221},
  keywords      = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerabilities,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Kazman2015,
  author        = {Kazman, Rick and Goldenson, Dennis and Monarch, Ira and Nichols, William and Valetto, Giuseppe},
  title         = {{Evaluating the Effects of Architectural Documentation : A Case Study of a Large Scale Open Source Project}},
  journal       = {Transactions on SOftware Engineering},
  year          = {2015},
  volume        = {PP},
  number        = {99},
  pages         = {1--29},
  __markedentry = {[ccc:6]},
  abstract      = {Sustaining large open source development efforts requires recruiting new participants; however, a lack of architectural documentation might inhibit new participants since large amounts of project knowledge are unavailable to newcomers. We present the results of a multitrait, multimethod analysis of the effects of introducing architectural documentation into a substantial open source project—the Hadoop Distributed File System (HDFS). HDFS had only minimal architectural documentation, and we wanted to discover whether the putative benefits of architectural documentation could be observed over time. To do this, we created and publicized an architecture document and then monitored its usage and effects on the project. The results were somewhat ambiguous: by some measures the architecture documentation appeared to effect the project but not by others. Perhaps of equal importance is our discovery that the project maintained, in its web-accessible JIRA archive of software issues and fixes, enough architectural discussion to support architectural thinking and reasoning. This “emergent” architecture documentation served an important purpose in recording core project members' architectural concerns and resolutions. However, this emergent architecture documentation did not serve all project members equally well; it appears that those on the periphery of the project—newcomers and adopters—still require explicit architecture documentation, as we will show.},
  doi           = {10.1109/TSE.2015.2465387},
  file          = {:article\\Evaluating the Effects of Architectural Documentation A Case Study of a Large Scale Open Source Project.pdf:pdf},
  issn          = {0098-5589},
  keywords      = {documentation,open source software,software architecture,web},
  mendeley-tags = {web},
}

@Article{Cook2015,
  author        = {Cook, D and Choe, Y R and {Hamilton Jr}, J A},
  title         = {{Finding bugs in source code using commonly available development metadata}},
  journal       = {Workshop on Cyber Security Experiment and Test},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {Developers and security analysts have been using static analysis for a long time to analyze programs for defects and vulnerabilities. Generally a static analysis tool is run on the source code for a given program, flagging areas of code that need to be further inspected by a human an- alyst. These tools tend to work fairly well – every year they find many important bugs. These tools are more impressive considering the fact that they only examine the source code, which may be very complex. Now con- sider the amount of data available that these tools do not analyze. There are many additional pieces of informa- tion available that would prove useful for finding bugs in code, such as a history of bug reports, a history of all changes to the code, information about committers, etc. By leveraging all this additional data, it is possible to find more bugs with less user interaction, as well as track useful metrics such as number and type of defects injected by committer. This paper provides a method for leveraging development metadata to find bugs that would otherwise be difficult to find using standard static analy- sis tools. We showcase two case studies that demonstrate the ability to find new vulnerabilities in large and small software projects by finding new vulnerabilities in the cpython and Roundup open source projects.},
  file          = {:article\\Finding bugs in source code using commonly available development metadata.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
  url           = {https://www.usenix.org/conference/cset15/workshop-program/presentation/cook$\backslash$npapers3://publication/uuid/08AEBA4B-48AA-4D4A-ABE7-267304976AD3},
}

@Article{Yamaguchi2015a,
  author        = {Yamaguchi, Fabian},
  title         = {{Pattern-Based Vulnerability Discovery}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\Pattern-Based Vulnerability Discovery.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{ThomsonReuters2015,
  author        = {{Thomson Reuters}},
  title         = {{Source publication list for Web of Science: Science Citation Index Expanded}},
  year          = {2015},
  __markedentry = {[ccc:6]},
  file          = {:article\\Source publication list for Web of Science Science Citation Index Expanded.pdf:pdf},
  keywords      = {fuzz,machine learning,web},
  mendeley-tags = {fuzz,machine learning,web},
  url           = {http://ip-science.thomsonreuters.com/mjl/publist{\_}sciex.pdf},
}

@Article{He2015a,
  author        = {He, Peng and Li, Bing and Liu, Xiao and Chen, Jun and Ma, Yutao},
  title         = {{An empirical study on software defect prediction with a simplified metric set}},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {59},
  pages         = {170--190},
  __markedentry = {[ccc:6]},
  abstract      = {Context Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. Objective The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. Method First, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests. Results The study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 12. Conclusion The experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Na{\"{i}}ve Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1402.3873v2},
  doi           = {10.1016/j.infsof.2014.11.006},
  eprint        = {arXiv:1402.3873v2},
  file          = {:article\\An empirical study on software defect prediction with a simplified metric set.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9789881925237},
  issn          = {09505849},
  keywords      = {Defect prediction,Metric set simplification,Software metrics,Software quality,binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important},
}

@Article{Nagappan2015,
  author        = {Nagappan, Meiyappan and Shihab, Emad},
  title         = {{Future Trends in Software Engineering Research for Mobile Apps}},
  journal       = {Saner'15},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {—There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.},
  doi           = {10.1109/SANER.2016.88},
  file          = {:article\\Future Trends in Software Engineering Research for Mobile Apps.pdf:pdf},
  isbn          = {9781509018550},
  keywords      = {survey},
  mendeley-tags = {survey},
  url           = {http://www.se.rit.edu/{~}mei/publications/pdfs/Future-Trends-in-Software-Engineering-Research-for-Mobile-Apps.pdf},
}

@Article{张玉清2015,
  author        = {张玉清},
  title         = {{Andoid安全漏洞挖掘技术综述}},
  journal       = {计算机研究与发展},
  year          = {2015},
  volume        = {53},
  number        = {9},
  pages         = {1689--1699},
  __markedentry = {[ccc:6]},
  abstract      = {applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1017/CBO9781107415324.004},
  eprint        = {arXiv:1011.1669v3},
  file          = {:article\\Andoid安全漏洞挖掘技术综述.pdf:pdf},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {icle,survey},
  mendeley-tags = {survey},
  pmid          = {25246403},
}

@Article{Yadegari2015,
  author        = {Yadegari, Babak and Johannesmeyer, Brian and Whitely, Ben and Debray, Saumya},
  title         = {{A generic approach to automatic deobfuscation of executable code}},
  journal       = {Proceedings - IEEE Symposium on Security and Privacy},
  year          = {2015},
  volume        = {2015-July},
  pages         = {674--691},
  __markedentry = {[ccc:6]},
  abstract      = {Malicious software are usually obfuscated to avoid detection and resist analysis. When new malware is encountered, such obfuscations have to be penetrated or removed ("deobfuscated") in order to understand the internal logic of the code and devise countermeasures. This paper discusses a generic approach for deobfuscation of obfuscated executable code. Our approach does not make any assumptions about the nature of the obfuscations used, but instead uses semantics-preserving program transformations to simplify away obfuscation code. We have applied a prototype implementation of our ideas to a variety of different kinds of obfuscation, including emulation-based obfuscation, emulation-based obfuscation with runtime code unpacking, and return-oriented programming. Our experimental results are encouraging and suggest that this approach can be effective in extracting the internal logic from code obfuscated using a variety of obfuscation techniques, including tools such as Themida that previous approaches could not handle.},
  doi           = {10.1109/SP.2015.47},
  file          = {:article\\A generic approach to automatic deobfuscation of executable code.pdf:pdf},
  isbn          = {9781467369497},
  issn          = {10816011},
  keywords      = {Deobfuscation,Return Oriented Programming,Virtualization-Obfuscation,obfuscate},
  mendeley-tags = {obfuscate},
}

@Book{Connolly2015,
  title         = {{ICT Systems Security and Privacy Protection}},
  year          = {2015},
  author        = {Connolly, Lena and Lang, Michael and Tygar, J. D.},
  volume        = {455},
  __markedentry = {[ccc:6]},
  abstract      = {At a time of rapid business globalisation, it is necessary to understand employee security behaviour within diverse cultural settings. While general deterrence theory has been extensively used in Behavioural Information Security research with the aim to explain the effect of deterrent factors on employees' security actions, these studies provide inconsistent and even contradictory findings. Therefore, a further examination of deterrent factors in the security context is required. The aim of this study is to contribute to the emerging field of Behavioural Information Security research by investigating how a combination of security countermeasures and cultural factors impact upon employee security behaviour in organisations. A particular focus of this project is to explore the effect of national culture and organisational culture on employee actions as regards information security. Preliminary findings suggest that organisational culture, national culture, and security countermeasures do have an impact upon employee security behaviour.},
  booktitle     = {IFIP Advances in Information and Communication Technology},
  doi           = {10.1007/978-3-319-18467-8},
  file          = {:article\\ICT Systems Security and Privacy Protection.pdf:pdf},
  isbn          = {978-3-319-18466-1},
  issn          = {18684238},
  keywords      = {Employee security behavior,National culture,Organisational culture,Security countermeasures,book},
  mendeley-tags = {book},
  pages         = {283--296},
  url           = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84942582480{\&}partnerID=tZOtx3y1},
}

@Article{Shijin2015,
  author        = {Shijin, Zhang and Zhaowei, Shang},
  title         = {{Cppcheck 的软件缺陷模式分析与定位}},
  journal       = {计算机工程与应用},
  year          = {2015},
  __markedentry = {[ccc:6]},
  abstract      = {能通过编译的C/C++程序代码可能依然隐含安全、 设计或风格上缺陷， 从而导致运行时出现内存泄露、 运行 异常等现象， 难以完成软件需求所预期的目标。针对开源软件缺陷检测工具Cppcheck软件存在的不足， 主要分析了 Cppcheck架构、 缺陷模式表示与实现， 在对已收集350个缺陷模式分析总结基础上， 对其完善， 提高缺陷检测能力。 通过两组实验验证改进Cppcheck工作的有效性},
  file          = {:article\\Cppcheck 的软件缺陷模式分析与定位.pdf:pdf},
  keywords      = {cppcheck,defect pattern,software defect,stat,static analysi,static analysis},
}

@Article{Yeh2015b,
  author        = {Yeh, Chao-Chun and Chung, Hsiang and Huang, Shih-Kun},
  title         = {{CRAXfuzz: Target-Aware Symbolic Fuzz Testing}},
  journal       = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  year          = {2015},
  pages         = {460--471},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/COMPSAC.2015.99},
  groups        = {imprortant},
  isbn          = {978-1-4673-6564-2},
  issn          = {07303157},
  keywords      = {-component,fuzz testing,software testing,source code,source code-important,symbolic execution,vulnerability},
  mendeley-tags = {source code,source code-important},
  url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7273654},
}

@Article{Alshammari2016,
  author        = {Alshammari, Bandar M and Fidge, Colin J and Corney, Diane},
  title         = {{Developing Secure Systems: A Comparative Study of Existing Methodologies}},
  year          = {2016},
  volume        = {4},
  number        = {2},
  pages         = {7763},
  __markedentry = {[ccc:6]},
  doi           = {10.7763/LNSE.2016.V4.239},
  file          = {:article\\Developing Secure Systems A Comparative Study of Existing Methodologies.pdf:pdf},
  issn          = {23013559},
  keywords      = {survey},
  mendeley-tags = {survey},
}

@Article{Liang2016,
  author        = {Liang, Bin and Bian, Pan and Zhang, Yan and Shi, Wenchang and You, Wei},
  title         = {{AntMiner : Mining More Bugs by Reducing Noise Interference}},
  journal       = {Icse},
  year          = {2016},
  __markedentry = {[ccc:6]},
  abstract      = {Detecting bugs with code mining has proven to be an effective approach. However, the existing methods suffer from reporting serious false positives and false negatives. In this paper, we de- veloped an approach called AntMiner to improve the precision of code mining by carefully preprocessing the source code. Specifi- cally, we employ the program slicing technique to decompose the original source repository into independent sub-repositories, tak- ing critical operations (automatically extracted from source code) as slicing criteria. In this way, the statements irrelevant to a criti- cal operation are excluded from the corresponding sub-repository. Besides, various semantics-equivalent representations are normal- ized into a canonical form. Eventually, the mining process can be performed on a refined code database, and false positives and false negatives can be significantly pruned. We have implemented AntMiner and applied it to detect bugs in the Linux kernel. It re- ported 52 violations that have been either confirmed as real bugs by the kernel development community or fixed in new kernel versions. Among them, 41 cannot be detected by a widely used representative analysis tool Coverity. Besides, the result of a com- parative analysis shows that our approach can effectively improve the precision of code mining and detect subtle bugs that have previously been missed. CCS},
  doi           = {10.1145/2884781.2884870},
  file          = {:article\\AntMiner Mining More Bugs by Reducing Noise Interference.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450339001},
  keywords      = {bug detection,code mining,first select,program slicing,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Johnson2016,
  author        = {Johnson, Pontus and Gorton, Dan and Lagerstr{\"{o}}m, Robert and Ekstedt, Mathias},
  title         = {{Time between vulnerability disclosures: A measure of software product vulnerability}},
  journal       = {Computers {\&} Security},
  year          = {2016},
  volume        = {62},
  pages         = {278--295},
  __markedentry = {[ccc:6]},
  doi           = {10.1016/j.cose.2016.08.004},
  file          = {:article\\Time between vulnerability disclosures A measure of software product vulnerability.pdf:pdf},
  issn          = {01674048},
  keywords      = {around-vulnerability,software vulnerability,time between vulnerability},
  mendeley-tags = {around-vulnerability},
  publisher     = {Elsevier Ltd},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S0167404816300955},
}

@InProceedings{Li2016,
  author        = {Li, Hongzhe and Oh, Jaesang and Oh, Hakjoo and Lee, Heejo},
  title         = {{Automated Source Code Instrumentation for Verifying Potential Vulnerabilities}},
  booktitle     = {IFIP International Federation for Information Processing 2016},
  year          = {2016},
  pages         = {211--226},
  __markedentry = {[ccc:6]},
  abstract      = {With a rapid yearly growth rate, software vulnerabilities are making great threats to the system safety. In theory, detecting and removing vulnerabilities before the code gets ever deployed can greatly ensure the quality of software released. However, due to the enormous amount of code being developed as well as the lack of human resource and expertise, severe vulnerabilities still remain concealed or cannot be revealed effectively. Current source code auditing tools for vulnerability discovery either generate too many false positives or require overwhelming manual efforts to report actual software flaws. In this paper, we propose an automatic verification mechanism to discover and verify vulnerabilities by using program source instrumentation and concolic testing. In the beginning, we leverage CIL to statically analyze the source code including extracting the program CFG, locating the security sinks and backward tracing the sensitive variables. Subsequently, we perform automated program instrumentation to insert security probes ready for the vulnerability verification. Finally, the instrumented program source is passed to the concolic testing engine to verify and report the existence of an actual vulnerability. We demonstrate the efficacy and efficiency of our mechanism by implementing a prototype system and perform experiments with nearly 4000 test cases from Juliet Test Suite. The results show that our system can verify over 90 {\%} of test cases and it reports buffer overflow flaws with P recision = 100 {\%} (0 FP) and Recall = 94.91 {\%}. In order to prove the practicability of our system working in real world programs, we also apply our system on 2 popular Linux utilities, Bash and Cpio. As a result, our system finds and verifies vulnerabilities in a fully automatic way with no false positives.},
  doi           = {10.1007/978-3-319-18467-8},
  file          = {:article\\Automated Source Code Instrumentation for Verifying Potential Vulnerabilities.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {978-3-319-18466-1},
  issn          = {18684238},
  keywords      = {Employee security behavior,National culture,Organisational culture,Security countermeasures,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84942582480{\&}partnerID=tZOtx3y1},
}

@InProceedings{Yun2016,
  author        = {Yun, Insu and Min, Changwoo and Si, Xujie and Jang, Yeongjin and Kim, Taesoo and Naik, Mayur},
  title         = {{APIS an : Sanitizing API Usages through Semantic Cross-Checking}},
  booktitle     = {Proceedings - 2016 25th USENIX Security Symposium},
  year          = {2016},
  __markedentry = {[ccc:6]},
  abstract      = {API misuse is a well-known source of bugs. Some of them (e.g., incorrect use of SSL API, and integer overflow of memory allocation size) can cause serious security vulnerabilities (e.g., man-in-the-middle (MITM) attack, and privilege escalation). Moreover, modern APIs, which are large, complex, and fast evolving, are error-prone. However, existing techniques to help finding bugs require manual effort by developers (e.g., providing specification or model) or are not scalable to large real-world software comprising millions of lines of code. In this paper, we present APISAN, a tool that automat- ically infers correct API usages from source code without manual effort. The key idea in APISAN is to extract likely correct usage patterns in four different aspects (e.g., causal relation, and semantic relation on arguments) by considering semantic constraints. APISAN is tailored to check various properties with security implications. We applied APISAN to 92 million lines of code, includ- ing Linux Kernel, and OpenSSL, found 76 previously unknown bugs, and provided patches for all the bugs. 1},
  file          = {:article\\APIS an Sanitizing API Usages through Semantic Cross-Checking.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781931971324},
  keywords      = {first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{El-licy2016,
  author        = {El-licy, Fatma A},
  title         = {{Paired Scrum for Large Projects}},
  year          = {2016},
  volume        = {40},
  number        = {January},
  pages         = {1--14},
  __markedentry = {[ccc:6]},
  file          = {:article\\Paired Scrum for Large Projects.pdf:pdf},
  keywords      = {agile development,around-vulnerability,large projects,requirement modulation,scaling scrum framework,scrum for,software engineering,testing and deployment},
  mendeley-tags = {around-vulnerability},
}

@Article{Yan2016,
  author        = {Yan, Hua},
  title         = {{Automated Memory Leak Fixing on Value-Flow Slices for C Programs}},
  year          = {2016},
  pages         = {1386--1393},
  __markedentry = {[ccc:6]},
  file          = {:article\\Automated Memory Leak Fixing on Value-Flow Slices for C Programs.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450337397},
  keywords      = {first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Wang2016,
  author        = {Wang, Song and Chollak, Devin and Movshovitz-Attias, Dana and Tan, Lin},
  title         = {{Bugram: bug detection with n-gram language models}},
  journal       = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering - ASE 2016},
  year          = {2016},
  pages         = {708--719},
  __markedentry = {[ccc:6]},
  abstract      = {To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs. In this paper, we propose a new approach—Bugram—that lever-ages n-gram language models instead of rules to detect bugs. Bu-gram models program tokens sequentially, using the n-gram lan-guage model. Token sequences from the program are then assessed according to their probability in the learned model, and low prob-ability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware. We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as cor-rect, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph-and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bu-gram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is comple-mentary to existing rule-based bug detection approaches.},
  doi           = {10.1145/2970276.2970341},
  file          = {:article\\Bugram bug detection with n-gram language models(2).pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450338455},
  keywords      = {Keywords Bug Detection,N-gram Language Model,Software testing and debugging,Static Code Analysis,first select,second select,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
  url           = {http://dl.acm.org/citation.cfm?doid=2970276.2970341},
}

@Article{Geng2016,
  author        = {Geng, Jinkun and Ye, Daren and Luo, Ping},
  title         = {{Predicting Severity of Software Vulnerability Based on Grey Theory}},
  year          = {2016},
  number        = {March},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/978-3-319-27161-3},
  file          = {:article\\Predicting Severity of Software Vulnerability Based on Grey Theory.pdf:pdf},
  isbn          = {9783319271606},
  issn          = {16113349},
  keywords      = {machine learning,predicte},
  mendeley-tags = {machine learning,predicte},
}

@Article{Grieco2016,
  author        = {Grieco, Gustavo and Grinblat, Guillermo Luis and Uzal, Lucas and Rawat, Sanjay and Feist, Josselin and Mounier, Laurent},
  title         = {{Toward large-scale vulnerability discovery using Machine Learning}},
  journal       = {Proceedings of the ACM Conference on Data and Application Security and Privacy (CODASPY)},
  year          = {2016},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2857705.2857720},
  file          = {:article\\Toward large-scale vulnerability discovery using Machine Learning.pdf:pdf},
  isbn          = {9781450339353},
  keywords      = {machine learning},
  mendeley-tags = {machine learning},
}

@Article{Cui2016,
  author        = {Cui, Baojiang and Wang, Fuwei and Hao, Yongle and Chen, Xiaofeng},
  title         = {{WhirlingFuzzwork: a taint-analysis-based API in-memory fuzzing framework}},
  journal       = {Soft Computing},
  year          = {2016},
  pages         = {1--14},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/s00500-015-2017-6},
  file          = {:article\\WhirlingFuzzwork a taint-analysis-based API in-memory fuzzing framework.pdf:pdf},
  issn          = {14337479},
  keywords      = {Control-flow hijacking,Fuzz testing,Software testing,Taint analysis,fuzz},
  mendeley-tags = {fuzz},
  publisher     = {Springer Berlin Heidelberg},
  url           = {"http://dx.doi.org/10.1007/s00500-015-2017-6},
}

@Article{Patel2016,
  author        = {Patel, Krishna A and Prajapati, Prof Rohan C},
  title         = {{A Survey-Vulnerability Classification of Bug Reports using Multiple Machine Learning Approach}},
  journal       = {COMPUSOFT, An international journal of advanced computer technology},
  year          = {2016},
  volume        = {5},
  number        = {Iii},
  pages         = {2320},
  __markedentry = {[ccc:6]},
  abstract      = {As critical and sensitive systems increasingly rely on complex software systems, identifying software vulnerabilities is becoming increasingly important. It has been suggested in previous work that some bugs are only identified as vulnerabilities long after the bug has been made public. These bugs are known as Hidden Impact Bugs (HIBs). This paper presents a hidden impact bug identification methodology by means of text mining bug databases. The presented methodology utilizes the textual description of the bug report for extracting textual information. The text mining process extracts syntactical information of the bug reports and compresses the information for easier manipulation and divided into frequency base and context base bug then give bug ranking.},
  file          = {:article\\A Survey-Vulnerability Classification of Bug Reports using Multiple Machine Learning Approach.pdf:pdf},
  keywords      = {bug database mining,classification,machine learning,survey,text mining},
  mendeley-tags = {machine learning,survey},
}

@Article{Gao2016,
  author        = {Gao, Qing and Zhang, Hansheng and Wang, Jie and Xiong, Yingfei and Zhang, Lu and Mei, Hong},
  title         = {{Fixing recurring crash bugs via analyzing Q{\&}A sites}},
  journal       = {Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
  year          = {2016},
  pages         = {307--318},
  __markedentry = {[ccc:6]},
  abstract      = {Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q{\&}A sites. By extracting queries from crash traces and retrieving a list of Q{\&}A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches.},
  doi           = {10.1109/ASE.2015.81},
  file          = {:article\\Fixing recurring crash bugs via analyzing Q{\&}A sites.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781509000241},
  keywords      = {first select,fuzz,source code,source code-vice important,web},
  mendeley-tags = {first select,fuzz,source code,source code-vice important,web},
}

@Article{Younis2016,
  author        = {Younis, Awad and {Yashwant Malaiya} and Anderson, Charles and Ray, Indrajit},
  title         = {{To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Function with an Existing Exploit}},
  journal       = {Proceedings of the ACM Conference on Data and Application Security and Privacy (CODASPY)},
  year          = {2016},
  pages         = {97--104},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2857705.2857750},
  file          = {:article\\To Fear or Not to Fear That is the Question Code Characteristics of a Vulnerable Function with an Existing Exploi.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450339353},
  keywords      = {binary,data mining and machine,exploitability,exploits,feature selection,first select,fuzz,learning,machine learning,predicte,prediction,second select,software metrics,software security,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerabilities severity,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Pittenger2016,
  author        = {Pittenger, Mike},
  title         = {{Know your open source code}},
  journal       = {Network Security},
  year          = {2016},
  volume        = {2016},
  number        = {5},
  pages         = {11--15},
  __markedentry = {[ccc:6]},
  doi           = {10.1016/S1353-4858(16)30048-4},
  file          = {:article\\Know your open source code.pdf:pdf},
  groups        = {vice-important},
  issn          = {13534858},
  keywords      = {first select,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S1353485816300484},
}

@Article{Younis2016a,
  author        = {Younis, Awad and Malaiya, Yashwant K. and Ray, Indrajit},
  title         = {{Assessing vulnerability exploitability risk using software properties}},
  journal       = {Software Quality Journal},
  year          = {2016},
  volume        = {24},
  number        = {1},
  pages         = {159--202},
  __markedentry = {[ccc:6]},
  abstract      = {Attacks on computer systems are now attracting increased attention. While the current trends in software vulnerability discovery indicate that the number of newly discovered vulnerabilities continues to be significant, the time between the public disclosure of vulnerabilities and the release of an automated exploit is shrinking. Thus, assessing the vulnerability exploitability risk is critical because this allows decision-makers to prioritize among vulnerabilities, allocate resources to patch and protect systems from these vulnerabilities, and choose between alternatives. Common vulnerability scoring system (CVSS) metrics have become the de facto standard for assessing the severity of vulnerabilities. However, the CVSS exploitability measures assign subjective values based on the views of experts. Two of the factors in CVSS, Access Vector and Authentication, are the same for almost all vulnerabilities. CVSS does not specify how the third factor, Access Complexity, is measured, and hence it is unknown whether it considers software properties as a factor. In this work, we introduce a novel measure, Structural Severity, which is based on software properties, namely attack entry points, vulnerability location, the presence of the dangerous system calls, and reachability analysis. These properties represent metrics that can be objectively derived from attack surface analysis, vulnerability analysis, and exploitation analysis. To illustrate the proposed approach, 25 reported vulnerabilities of Apache HTTP server and 86 reported vulnerabilities of Linux Kernel have been examined at the source code level. The results show that the proposed approach, which uses more detailed information, can objectively measure the risk of vulnerability exploitability and results can be different from the CVSS base scores.},
  doi           = {10.1007/s11219-015-9274-6},
  file          = {:article\\Assessing vulnerability exploitability risk using software properties.pdf:pdf},
  groups        = {vice-important},
  issn          = {15731367},
  keywords      = {Attack surface,CVSS metrics,Risk assessment,Software security metrics,Software vulnerability,Source code analysis,first select,fuzz,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,machine learning,predicte,source code,source code-vice important,web},
}

@Article{Mansfield-Devine2016,
  author        = {Mansfield-Devine, Steve},
  title         = {{The secure way to use open source}},
  journal       = {Computer Fraud {\&} Security},
  year          = {2016},
  volume        = {2016},
  number        = {5},
  pages         = {15--20},
  __markedentry = {[ccc:6]},
  doi           = {10.1016/S1361-3723(16)30046-X},
  file          = {:article\\The secure way to use open source.pdf:pdf},
  groups        = {vice-important},
  issn          = {13613723},
  keywords      = {binary,first select,fuzz,predicte,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,fuzz,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S136137231630046X},
}

@Article{Khalili2016,
  author        = {Khalili, Abdullah and Sami, Ashkan and Azimi, Mahdi and Moshtari, Sara and Salehi, Zahra and Ghiasi, Mahboobe and Safavi, Ali Akbar},
  title         = {{Employing secure coding practices into industrial applications: a case study}},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {1},
  pages         = {4--16},
  __markedentry = {[ccc:6]},
  abstract      = {Industrial Control Systems (ICS) are the vital part of modern critical infrastructures. Recent attacks to ICS indicate that these systems have various types of vulnerabilities. A large number of vulnerabilities are due to secure coding problems in industrial applications. Several international and national organizations like: NIST, DHS, and US-CERT have provided extensive documentation on securing ICS; however proper details on securing software application for industrial setting were not presented. The notable point that makes securing a difficult task is the contradictions between security priorities in ICS and IT systems. In addition, none of the guidelines highlights the implications on modification of general IT security solutions to industrial settings. Moreover based on the best of our knowledge, steps to develop a successful real-world secure industrial application have not been reported. In this paper, the first attempts to employ secure coding best practices into a real world industrial application (Supervisory Control and Data Acquisition) called OpenSCADA is presented. Experiments indicate that resolving the vulnerabilities of OpenSCADA in addition to possible improvement in its availability, does not jeopardize other dimensions of security. In addition, all experiments are backed up with proper statistical tests to see whether or not, improvements are statistically significant.},
  doi           = {10.1007/s10664-014-9341-9},
  file          = {:article\\Employing secure coding practices into industrial applications a case study.pdf:pdf},
  issn          = {15737616},
  keywords      = {Availability,Industrial control system,Memory leak,SCADA,Secure coding,Time critical process,predicte,web},
  mendeley-tags = {predicte,web},
}

@Article{Follner2016,
  author        = {Follner, Andreas and Bodden, Eric},
  title         = {{ROPocop — Dynamic mitigation of code-reuse attacks}},
  journal       = {Journal of Information Security and Applications},
  year          = {2016},
  volume        = {29},
  pages         = {16--26},
  __markedentry = {[ccc:6]},
  doi           = {10.1016/j.jisa.2016.01.002},
  file          = {:article\\ROPocop — Dynamic mitigation of code-reuse attacks.pdf:pdf},
  issn          = {22142126},
  keywords      = {binary,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S221421261600017X},
}

@Article{Ray2016,
  author        = {Ray, Baishakhi and Bacchelli, Alberto},
  title         = {{On the “ Naturalness ” of Buggy Code}},
  year          = {2016},
  __markedentry = {[ccc:6]},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1506.01159v2},
  eprint        = {arXiv:1506.01159v2},
  file          = {:article\\On the “ Naturalness ” of Buggy Code.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450339001},
  keywords      = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Kar2016,
  author        = {Kar, Debabrata and Panigrahi, Suvasini and Sundararajan, Srikanth},
  title         = {{SQLiGoT: Detecting SQL Injection Attacks using Graph of Tokens and SVM}},
  journal       = {Computers {\&} Security},
  year          = {2016},
  volume        = {60},
  pages         = {206--225},
  __markedentry = {[ccc:6]},
  abstract      = {SQL injection attacks have been predominant on web databases since last 15 years. Exploiting input validation flaws, attackers inject SQL code through the front-end of websites and steal data from the back-end databases. Detection of SQL injection attacks has been a challenging problem due to extreme heterogeneity of the attack vectors. In this paper, we present a novel approach to detect injection attacks by modeling SQL queries as graph of tokens and using the centrality measure of nodes to train a Support Vector Machine (SVM). We explore different methods of creating token graphs and propose alternative designs of the system comprising of single and multiple SVMs. The system is designed to work at the database firewall layer and can protect multiple web applications in a shared hosting scenario. Though we focus primarily on web applications developed with PHP and MySQL, the approach can be easily ported to other platforms. The experimental results demonstrate that this technique can effectively identify malicious SQL queries with negligible performance overhead.},
  doi           = {10.1016/j.cose.2016.04.005},
  file          = {:article\\SQLiGoT Detecting SQL Injection Attacks using Graph of Tokens and SVM.pdf:pdf},
  groups        = {vice-important},
  issn          = {01674048},
  keywords      = {binary,first select,obfuscate,source code,source code-vice important,sql injection attack,sql injection detection,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,first select,obfuscate,source code,source code-vice important,web},
  publisher     = {Elsevier Ltd},
  url           = {http://linkinghub.elsevier.com/retrieve/pii/S0167404816300451},
}

@Article{Murtaza2016,
  author        = {Murtaza, Syed Shariyar and Khreich, Wael and Hamou-Lhadj, Abdelwahab and Bener, Ayse Basar},
  title         = {{Mining trends and patterns of software vulnerabilities}},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {218--228},
  __markedentry = {[ccc:6]},
  abstract      = {Zero-day vulnerabilities continue to be a threat as they are unknown to vendors; when attacks occur, vendors have zero days to provide remedies. New techniques for the detection of zero-day vulnerabilities on software systems are being developed but they have their own limitations; e.g., anomaly detection techniques are prone to false alarms. To better protect software systems, it is also important to understand the relationship between vulnerabilities and their patterns over a period of time. The mining of trends and patterns of vulnerabilities is useful because it can help software vendors prepare solutions ahead of time for vulnerabilities that may occur in a software application. In this paper, we investigate the use of historical patterns of vulnerabilities in order to predict future vulnerabilities in software applications. In addition, we examine whether the trends of vulnerabilities in software applications have any significant meaning or not. We use the National Vulnerability Database (NVD) as the main resource of vulnerabilities in software applications. We mine vulnerabilities of the last six years from 2009 to 2014 from NVD. Our results show that sequences of the same vulnerabilities (e.g., buffer errors) may occur 150 times in a software product. Our results also depict that the number of SQL injection vulnerabilities have decreased in the last six years while cryptographic vulnerabilities have seen an important increase. However, we have not found any statistical significance in the trends of the occurrence of vulnerabilities over time. The most interesting finding is that the sequential patterns of vulnerability events follow a first order Markov property; that is, we can predict the next vulnerability by using only the previous vulnerability with a recall of approximately 80{\%} and precision of around 90{\%}.},
  doi           = {10.1016/j.jss.2016.02.048},
  file          = {:article\\Mining trends and patterns of software vulnerabilities.pdf:pdf},
  groups        = {vice-important},
  issn          = {01641212},
  keywords      = {Software vulnerabilities,Vulnerability prediction,Vulnerability trends,first select,machine learning,predicte,source code,source code-vice important,web},
  mendeley-tags = {first select,machine learning,predicte,source code,source code-vice important,web},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jss.2016.02.048},
}

@Article{Kim2016,
  author        = {Kim, Joon-Ho and Ma, Myung-Chul and Park, Jae-Pyo},
  title         = {{An analysis on secure coding using symbolic execution engine}},
  journal       = {Journal of Computer Virology and Hacking Techniques},
  year          = {2016},
  volume        = {12},
  number        = {3},
  pages         = {177--184},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/s11416-016-0263-5},
  file          = {:article\\An analysis on secure coding using symbolic execution engine.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {2263-8733},
  keywords      = {first select,fuzz,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,fuzz,second select,source code,source code-important,source code-vice important,web},
  publisher     = {Springer Paris},
  url           = {http://link.springer.com/10.1007/s11416-016-0263-5},
}

@Article{Cai2016,
  author        = {Cai, Jun and Zou, Peng and Ma, Jinxin and He, Jun},
  title         = {{SwordDTA: A dynamic taint analysis tool for software vulnerability detection}},
  journal       = {Wuhan University Journal of Natural Sciences},
  year          = {2016},
  volume        = {21},
  number        = {1},
  pages         = {10--20},
  __markedentry = {[ccc:6]},
  doi           = {10.1007/s11859-016-1133-1},
  file          = {:article\\SwordDTA A dynamic taint analysis tool for software vulnerability detection.pdf:pdf},
  issn          = {1007-1202},
  keywords      = {binary,dynamic taint analysis,fuzz,information security,software vulnerability detec-,stat,static analysi,static analysis,tion,use-after-free,web},
  mendeley-tags = {binary,fuzz,web},
  url           = {http://link.springer.com/10.1007/s11859-016-1133-1},
}

@Article{Younis2016b,
  author        = {Younis, Awad and {Yashwant Malaiya} and Anderson, Charles and Ray, Indrajit},
  title         = {{To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Function with an Existing Exploit}},
  journal       = {Proceedings of the ACM Conference on Data and Application Security and Privacy (CODASPY)},
  year          = {2016},
  pages         = {97--104},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2857705.2857750},
  file          = {:article\\To Fear or Not to Fear That is the Question Code Characteristics of a Vulnerable Function with an Existing Exp(2).pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781450339353},
  keywords      = {binary,data mining and machine,exploitability,exploits,feature selection,first select,fuzz,learning,machine learning,predicte,prediction,second select,software metrics,software security,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerabilities severity,web},
  mendeley-tags = {binary,first select,fuzz,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Zhou2016,
  author        = {Zhou, Zhi Quan and Xiang, Shaowen and Chen, Tsong Yueh},
  title         = {{Metamorphic Testing for Software Quality Assessment : A Study of Search Engines}},
  year          = {2016},
  volume        = {42},
  number        = {3},
  pages         = {264--284},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/TSE.2015.2478001},
  file          = {:article\\Metamorphic Testing for Software Quality Assessment A Study of Search Engines.pdf:pdf},
  issn          = {0098-5589},
  keywords      = {machine learning,predicte,web},
  mendeley-tags = {machine learning,predicte,web},
}

@Article{Shoshitaishvili2016,
  author        = {Shoshitaishvili, Yan and Wang, Ruoyu and Salls, Christopher and Stephens, Nick and Polino, Mario and Dutcher, Andrew and Grosen, John and Feng, Siji and Hauser, Christophe and Kruegel, Christopher and Vigna, Giovanni},
  title         = {{(State of) The Art of War: Offensive Techniques in Binary Analysis}},
  journal       = {S{\&}P},
  year          = {2016},
  pages         = {138--157},
  __markedentry = {[ccc:6]},
  abstract      = {Finding and exploiting vulnerabilities in binary code is a challenging task. The lack of high-level, semantically rich information about data structures and control constructs makes the analysis of program properties harder to scale. However, the importance of binary analysis is on the rise. In many situations binary analysis is the only possible way to prove (or disprove) properties about the code that is actually executed. In this paper, we present a binary analysis framework that implements a number of analysis techniques that have been proposed in the past. We present a systematized implementation of these techniques, which allows other researchers to compose them and develop new approaches. In addition, the implementation of these techniques in a unifying framework allows for the direct comparison of these apporaches and the identification of their advantages and disadvantages. The evaluation included in this paper is performed using a recent dataset created by DARPA for evaluating the effectiveness of binary vulnerability analysis techniques. Our framework has been open-sourced and is available to the security community.},
  annote        = {开源的平台，棒！},
  doi           = {10.1109/SP.2016.17},
  file          = {:article\\(State of) The Art of War Offensive Techniques in Binary Analysis.pdf:pdf},
  isbn          = {978-1-5090-0824-7},
  keywords      = {binary,fuzz,obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,obfuscate,predicte,web},
}

@Article{Alqahtani2016,
  author        = {Alqahtani, Sultan S. and Eghan, Ellis E. and Rilling, Juergen},
  title         = {{Tracing known security vulnerabilities in software repositories - A Semantic Web enabled modeling approach}},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {121},
  pages         = {153--175},
  __markedentry = {[ccc:6]},
  abstract      = {The introduction of the Internet has revolutionized not only our society but also transformed the software industry, with knowledge and information sharing becoming a central part of software development processes. The resulting globalization of the software industry has not only increased software reuse, but also introduced new challenges. Among the challenges, arising from the knowledge sharing is Information Security, which has emerged to become a major threat to the software development community, since not only source code but also its vulnerabilities are shared across project boundaries. Developers are unaware of such security vulnerabilities in their projects, often until a vulnerability is either exploited by attackers or made publicly available by independent security advisory databases. In this research, we present a modeling approach, which takes advantage of Semantic Web technologies, to establish traceability links between security advisory repositories and other software repositories. More specifically, we establish a unified ontological representation, which supports bi-directional traceability links between knowledge captured in software build repositories and specialized vulnerability database. These repositories can be considered trusted information silos that are typically not directly linked to other resources, such as source code repositories containing the reported instances of these problems. The novelty of our approach is that it allows us to overcome some of these traditional information silos and transform them into information hubs, which promote sharing of knowledge across repository boundaries. We conducted several experiments to illustrate the applicability of our approach by tracing existing vulnerabilities to projects which might directly or indirectly be affected by vulnerabilities inherited from other projects and libraries.},
  doi           = {10.1016/j.scico.2016.01.005},
  file          = {:article\\A Semantic Web enabled modeling app.pdf:pdf},
  groups        = {vice-important},
  issn          = {01676423},
  keywords      = {Impact analysis,Semantic knowledge modelling,Semantic web,Software security vulnerabilities,Software traceability,first select,machine learning,source code,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,machine learning,source code,source code-vice important,web},
  publisher     = {Elsevier B.V.},
  url           = {http://dx.doi.org/10.1016/j.scico.2016.01.005},
}

@Article{Furnell2016,
  author        = {Furnell, Steve},
  title         = {{Vulnerability management: Not a patch on where we should be?}},
  journal       = {Network Security},
  year          = {2016},
  volume        = {2016},
  number        = {4},
  pages         = {5--9},
  __markedentry = {[ccc:6]},
  abstract      = {Effective vulnerability management, particularly in the context of end-user systems, is inextricably linked to the timely application of software updates and patches. Vulnerabilities continue to be discovered, even in established software, and so impose a continual demand for our attention. The most recent findings from Secunia indicate a 55{\%} increase in the five-year trend, with an 18{\%} increase on the previous year (with 15,435 vulnerabilities detected in the latter period).1 Many of these will have led to resultant updates, which must be applied if systems are to remain protected against exploitation. Many network breaches continue to occur because systems are running with known security vulnerabilities, which in turn highlights the importance of updating software. However, despite various moves to raise awareness and automate the process, many users (and indeed organisations) appear to leave themselves vulnerable as a result of lax maintenance practices. Steve Furnell examines evidence of these poor practices across both system and application software updates, as well as some of the factors that can lead to updates being delayed or ignored.},
  doi           = {10.1016/S1353-4858(16)30036-8},
  file          = {:article\\Vulnerability management Not a patch on where we should be.pdf:pdf},
  issn          = {13534858},
  keywords      = {web},
  mendeley-tags = {web},
  publisher     = {Elsevier Ltd},
  url           = {http://dx.doi.org/10.1016/S1353-4858(16)30036-8},
}

@Article{Eschweiler2016,
  author        = {Eschweiler, Sebastian and Yakdan, Khaled and Gerhards-padilla, Elmar},
  title         = {{ discovRE: Efficient Cross-Architecture Identification of Bugs in Binary Code }},
  journal       = {Ndss},
  year          = {2016},
  number        = {February},
  pages         = {21--24},
  __markedentry = {[ccc:6]},
  abstract      = {—The identification of security-critical vulnerabilities is a key for protecting computer systems. Being able to perform this process at the binary level is very important given that many software projects are closed-source. Even if the source code is available, compilation may create a mismatch between the source code and the binary code that is executed by the processor, causing analyses that are performed on source code to fail at detecting certain bugs and thus potential vulnerabilities. Existing approaches to find bugs in binary code 1) use dynamic analysis, which is difficult for firmware; 2) handle only a single architecture; or 3) use semantic similarity, which is very slow when analyzing large code bases. In this paper, we present a new approach to efficiently search for similar functions in binary code. We use this method to identify known bugs in binaries as follows: starting with a vulnerable binary function, we identify similar functions in other binaries across different compilers, optimization levels, operating systems, and CPU architectures. The main idea is to compute similarity between functions based on the structure of the corresponding control flow graphs. To minimize this costly computation, we employ an efficient pre-filter based on numeric features to quickly identify a small set of candidate functions. This allows us to efficiently search for similar functions in large code bases. We have designed and implemented a prototype of our approach, called discovRE, that supports four instruction set architectures (x86, x64, ARM, MIPS). We show that discovRE is four orders of magnitude faster than the state-of-the-art academic approach for cross-architecture bug search in binaries. We also show that we can identify Heartbleed and POODLE vulnerabilities in an Android system image that contains over 130,000 native ARM functions in about 80 milliseconds.},
  doi           = {10.14722/ndss.2016.23185},
  file          = {:article\\discovRE Efficient Cross-Architecture Identification of Bugs in Binary Code.pdf:pdf},
  isbn          = {189156241X},
  keywords      = {binary,fuzz,machine learning,obfuscate,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,machine learning,obfuscate,predicte,web},
}

@Article{Medeiros2016,
  author        = {Medeiros, Iberia and Neves, Nuno and Correia, Miguel},
  title         = {{Detecting and Removing Web Application Vulnerabilities with Static Analysis and Data Mining}},
  journal       = {IEEE Transactions on Reliability},
  year          = {2016},
  volume        = {65},
  number        = {1},
  pages         = {54--69},
  __markedentry = {[ccc:6]},
  abstract      = {Although a large research effort on web application security has been going on for more than a decade, the security of web applications continues to be a challenging problem. An important part of that problem derives from vulnerable source code, often written in unsafe languages like PHP. Source code static analysis tools are a solution to find vulnerabilities, but they tend to generate false positives, and require considerable effort for programmers to manually fix the code. We explore the use of a combination of methods to discover vulnerabilities in source code with fewer false positives. We combine taint analysis, which finds candidate vulnerabilities, with data mining, to predict the existence of false positives. This approach brings together two approaches that are apparently orthogonal: humans coding the knowledge about vulnerabilities (for taint analysis), joined with the seemingly orthogonal approach of automatically obtaining that knowledge (with machine learning, for data mining). Given this enhanced form of detection, we propose doing automatic code correction by inserting fixes in the source code. Our approach was implemented in the WAP tool, and an experimental evaluation was performed with a large set of PHP applications. Our tool found 388 vulnerabilities in 1.4 million lines of code. Its accuracy and precision were approximately 5{\%} better than PhpMinerII's and 45{\%} better than Pixy's.},
  doi           = {10.1109/TR.2015.2457411},
  file          = {:article\\Detecting and Removing Web Application Vulnerabilities with Static Analysis and Data Mining.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {00189529},
  keywords      = {Automatic protection,binary,data mining,false positives,first select,input validation vulnerabilities,machine learning,predicte,second select,software security,source code,source code static analysis,source code-important,source code-vice important,stat,static analysi,static analysis,web,web applications},
  mendeley-tags = {binary,first select,machine learning,predicte,second select,source code,source code-important,source code-vice important,web},
}

@Article{Saravanan2016,
  author        = {Saravanan, A and Ahmed, M S Irfan},
  title         = {{A SURVEY ON EXPOSED VULNERABILITIES IN WEB PPLICATIONS}},
  journal       = {Asia Pacific Journal of Research},
  year          = {2016},
  number        = {Xxxv},
  pages         = {84--89},
  __markedentry = {[ccc:6]},
  abstract      = {Internet becomes more and more integrated in our society and our offline time continually decreases. However, the number of reported web application vulnerabilities is increasing dramatically. Security vulnerabilities in web applications may result in stealing of confidential data, breaking of data integrity or affect web application availability. So, it is clear that these vulnerabilities are complex and widespread. Thus, the task of securing web applications is not only important but also needs immediate attention, since for most people, Internet and the web are utilities that have become as common as food and water. In this paper, we explore some security breaches in web applications which needs immediate attention. We describe some of the attacks that enable an attacker to impersonate a victim},
  file          = {:article\\A SURVEY ON EXPOSED VULNERABILITIES IN WEB PPLICATIONS.pdf:pdf},
  keywords      = {confidentiality,survey,vulnerabilities,web,web applications},
  mendeley-tags = {survey,web},
}

@Article{Lingzi2016,
  author        = {Lingzi, Xiang and Zhi, Lin},
  title         = {{An Overview of Source Code Audit}},
  journal       = {Proceedings - 2015 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration, ICIICII 2015},
  year          = {2016},
  pages         = {26--29},
  __markedentry = {[ccc:6]},
  doi           = {10.1109/ICIICII.2015.94},
  file          = {:article\\An Overview of Source Code Audit.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {9781467383127},
  keywords      = {first select,information security,source code,source code review,source code-important,source code-vice important,stat,static analysi,static analysis,survey,web},
  mendeley-tags = {first select,source code,source code-important,source code-vice important,survey,web},
}

@Article{Shameli-Sendi2016,
  author        = {Shameli-Sendi, Alireza and Aghababaei-Barzegar, Rouzbeh and Cheriet, Mohamed},
  title         = {{Taxonomy of Information Security Risk Assessment (ISRA)}},
  journal       = {Computers {\&} Security},
  year          = {2016},
  volume        = {57},
  pages         = {14--30},
  __markedentry = {[ccc:6]},
  abstract      = {Abstract Information is a perennially significant business asset in all organizations. Therefore, it must be protected as any other valuable asset. This is the objective of information security, and an information security program provides this kind of protection for a company's information assets and for the company as a whole. One of the best ways to address information security problems in the corporate world is through a risk-based approach. In this paper, we present a taxonomy of security risk assessment drawn from 125 papers published from 1995 to May 2014. Organizations with different size may face problems in selecting suitable risk assessment methods that satisfy their needs. Although many risk-based approaches have been proposed, most of them are based on the old taxonomy, avoiding the need for considering and applying the important criteria in assessing risk raised by rapidly changing technologies and the attackers knowledge level. In this paper, we discuss the key features of risk assessment that should be included in an information security management system. We believe that our new risk assessment taxonomy helps organizations to not only understand the risk assessment better by comparing different new concepts but also select a suitable way to conduct the risk assessment properly. Moreover, this taxonomy will open up interesting avenues for future research in the growing field of security risk assessment.},
  doi           = {http://dx.doi.org/10.1016/j.cose.2015.11.001},
  file          = {:article\\Taxonomy of Information Security Risk Assessment (ISRA).pdf:pdf},
  issn          = {0167-4048},
  keywords      = {Information security,Risk analysis,Risk assessment,Risk management,Threat,Vulnerability,binary,fuzz,machine learning,predicte,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,fuzz,machine learning,predicte,web},
  publisher     = {Elsevier Ltd},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167404815001650},
}

@Book{Russo2016,
  title         = {{Benefits of open source software in defense environments}},
  year          = {2016},
  author        = {Russo, Daniel},
  volume        = {422},
  __markedentry = {[ccc:6]},
  abstract      = {Even though the use of Open Source Software (OSS) might seem paradoxical in Defense environments, this has been proven to be wrong. The use of OSS does not harm security; on the contrary, it enhances it. Even with some drawbacks, OSS is highly reliable and maintained by a huge software community, thus decreasing implementation costs and increasing reliability. Moreover, it allows military software engineers to move away from proprietary applications and single-vendor contracts. Furthermore, it decreases the cost of long-term development and lifecycle management, besides avoiding vendor's lock in. Nevertheless, deploying OSS deserves an appropriate organization of its life cycle and maintenance, which has a relevant impact on the project's budget that cannot be overseen. In this paper, we will describe some of the major trends in OSS in Defense environments. The community for OSS has a pivotal role, since it is the core development unit. With Agile and the newest DevOps methodologies, government officials could leverage OSS capabilities, decreasing the Design (or Technical) Debt. Software for Defense purposes could perform better, increase the number of the releases, enhance coordination through the different IT Departments (and the community), and increase release automation, decreasing the probability of errors.},
  booktitle     = {Communications in Computer and Information Science},
  doi           = {10.1007/978-3-319-27896-4_11},
  file          = {:article\\Benefits of open source software in defense environments.pdf:pdf},
  isbn          = {9783319278940},
  issn          = {18650929},
  keywords      = {predicte,web},
  mendeley-tags = {predicte,web},
  pages         = {123--131},
}

@Article{Mirakhorli2016,
  author        = {Mirakhorli, Mehdi and Cleland-Huang, Jane},
  title         = {{Detecting, Tracing, and Monitoring Architectural Tactics in Code}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2016},
  volume        = {42},
  number        = {3},
  pages         = {206--221},
  __markedentry = {[ccc:6]},
  abstract      = {Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system. {\textcopyright} 2015 IEEE.},
  doi           = {10.1109/TSE.2015.2479217},
  file          = {:article\\Detecting, Tracing, and Monitoring Architectural Tactics in Code.pdf:pdf},
  groups        = {vice-important},
  issn          = {00985589},
  keywords      = {Architecture,binary,first select,machine learning,predicte,source code,source code-vice important,stat,static analysi,static analysis,tactics,traceability,traceability information models,web},
  mendeley-tags = {binary,first select,machine learning,predicte,source code,source code-vice important,web},
}

@Article{Sui2016,
  author        = {Sui, Y and Xue, J},
  title         = {{SVF: interprocedural static value-flow analysis in LLVM}},
  journal       = {Proceedings of the 25th International Conference on {\ldots}},
  year          = {2016},
  __markedentry = {[ccc:6]},
  doi           = {10.1145/2892208.2892235},
  file          = {:article\\SVF interprocedural static value-flow analysis in LLVM.pdf:pdf},
  isbn          = {9781450342414},
  keywords      = {binary,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,web},
  url           = {http://dl.acm.org/citation.cfm?id=2892235},
}

@Article{Melleg??rd2016,
  author        = {Melleg??rd, Niklas and Ferwerda, Adry and Lind, Kenneth and Heldal, Rogardt and Chaudron, Michel R V},
  title         = {{Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study}},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2016},
  volume        = {42},
  number        = {3},
  pages         = {248--263},
  __markedentry = {[ccc:6]},
  abstract      = {Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.},
  doi           = {10.1109/TSE.2015.2479221},
  file          = {:article\\Impact of Introducing Domain-Specific Modelling in Software Maintenance An Industrial Case Study.pdf:pdf},
  isbn          = {0098-5589 VO  - PP},
  issn          = {00985589},
  keywords      = {Empirical investigation,maintenance measurement,predicte,process measurement,productivity,software maintenance,web},
  mendeley-tags = {predicte,web},
}

@Article{Chen2016,
  author        = {Chen, Daming Dominic and Egele, Manuel and Woo, Maverick and Brumley, David},
  title         = {{Towards Fully Automated Dynamic Analysis for Embedded Firmware}},
  journal       = {Network and Distributed System Security Symposium},
  year          = {2016},
  number        = {February},
  pages         = {21--24},
  __markedentry = {[ccc:6]},
  abstract      = {Commercial-off-the-shelf (COTS) network-enabled embedded devices are usually controlled by vendor firmware to perform integral functions in our daily lives. From home and small office networking equipment, such as wireless routers, over network attached storage, and surveillance cameras these devices are operated by proprietary firmware. For example, wireless home routers are often the first and only line of defense that separates a home user's personal computing and information devices from the Internet. Such a vital and privileged position in the user's network requires that these devices operate securely. Unfortunately, recent research and anecdotal evidence suggest that such security assumptions are not at all upheld by the devices deployed around the world. A first step to assess the security of such embedded device firmware is the accurate identification of vulnerabilities. However, the market offers a large variety of these embedded devices, which severely impacts the scalability of existing approaches in this area. In this paper, we present FIRMADYNE, the first automated dynamic analysis system that specifically targets Linux-based firmware on network-connected COTS devices in a scalable manner. We identify a series of challenges inherent to the dynamic analysis of COTS firmware, and discuss how our design decisions address them. At its core, FIRMADYNE relies on software-based full system emulation with an instrumented kernel to achieve the scalability necessary to analyze thousands of firmware binaries automatically. We evaluate FIRMADYNE on a real-world dataset of 23,035 firmware images across 42 device vendors gathered by our system. Using a sample of 74 exploits on the 9,486 firmware images that our system can successfully extract, we discover that 895 firmware images spanning at least 90 distinct products are vulnerable to one or more of the sampled exploit(s). This includes 14 previously-unknown vulnerabilities that were discovered with the aid of our framework, which affect 86 firmware images spanning at least 14 distinct products. Furthermore, our results show that 11 of our tested attacks affect firmware images from more than one vendor, suggesting that code-sharing and common upstream manufacturers (OEMs) are quite prevalent.},
  doi           = {http://dx.doi.org/10.14722/ndss.2016.23415},
  file          = {:article\\Towards Fully Automated Dynamic Analysis for Embedded Firmware.pdf:pdf},
  isbn          = {189156241X},
  keywords      = {binary,machine learning,obfuscate,stat,static analysi,static analysis,web},
  mendeley-tags = {binary,machine learning,obfuscate,web},
  url           = {https://www.dcddcc.com/docs/2016{\_}paper{\_}firmadyne.pdf},
}

@Article{Bagheri2016,
  author        = {Bagheri, Hamid and Garcia, Joshua and Sadeghi, Alireza and Malek, Sam and Medvidovic, Nenad},
  title         = {{Software Architectural Principles in Contemporary Mobile Software : from Conception to Practice}},
  journal       = {The Journal of Systems {\&} Software},
  year          = {2016},
  volume        = {119},
  pages         = {31--44},
  __markedentry = {[ccc:6]},
  doi           = {10.1016/j.jss.2016.05.039},
  file          = {:article\\Software Architectural Principles in Contemporary Mobile Software from Conception to Practice.pdf:pdf},
  issn          = {01641212},
  keywords      = {Software architecture,Android,Architectural styles,android,software architecture,stat,static analysi,static analysis,web},
  mendeley-tags = {android,web},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.jss.2016.05.039},
}

@Article{Hardekopf,
  author        = {Hardekopf, Ben},
  title         = {{ucsb - Program Analysis Notes}},
  __markedentry = {[ccc:6]},
  file          = {:article\\40 Variability Bugs in the Linux Kernel.pdf:PDF;:article\\ Mining Security-Sensitive Operations in Legacy Code Using Concept Analysis.pdf:PDF;:article\\Program Analysis Notes.pdf:PDF},
  keywords      = {stat,static analysi,static analysis},
}

@Misc{,
  title         = {{A Survey-Vulnerability Classification of Bug Reports using Multiple.pdf}},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Survey-Vulnerability Classification of Bug Reports using Multiple.pdf.pdf:pdf},
  keywords      = {survey},
  mendeley-tags = {survey},
}

@Misc{,
  title         = {{Analysis of Security and Survivability As Software Quality Attributes.Pdf}},
  __markedentry = {[ccc:6]},
  file          = {:article\\Analysis of Security and Survivability As Software Quality Attributes.Pdf.pdf:pdf},
  keywords      = {binary},
  mendeley-tags = {binary},
}

@Misc{,
  title         = {{[虎书][现代编译原理C语言描述](Andrew.W.Appel).pdf}},
  __markedentry = {[ccc:6]},
  file          = {:article\\虎书现代编译原理C语言描述(Andrew.W.Appel).pdf.pdf:pdf},
  keywords      = {book,stat,static analysi,static analysis},
  mendeley-tags = {book},
}

@Article{Walden,
  author        = {Walden, James and Stuckman, Jeff and Scandariato, Riccardo},
  title         = {{Predicting Vulnerable Components : Software Metrics vs Text Mining}},
  __markedentry = {[ccc:6]},
  file          = {:article\\Predicting Vulnerable Components Software Metrics vs Text Mining.pdf:pdf},
  keywords      = {predicte},
  mendeley-tags = {predicte},
}

@Article{,
  author        = {金茂忠, 王 雷 陈 归 and {Wang Lei}, Chen Gui, and Jin Maozhong},
  title         = {基于约束分析与模型检测的代码安全漏洞检测方法研究},
  journal       = {计算机研究与发展},
  volume        = {48},
  number        = {9},
  pages         = {1659--1666},
  __markedentry = {[ccc:6]},
  file          = {:article\\基于约束分析与模型检测的代码安全漏洞检测方法研究.pdf:pdf},
  groups        = {imprortant, vice-important},
  issn          = {1000-1239},
  keywords      = {constraint-based analysis,first select,model checking,program slicing,second select,security vulnerability,source code,source code-important,source code-vice important,static analysis,安全漏洞,模型检测,程序切片,约束分析,静态分析},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Cai,
  author        = {Cai, Jun and Zou, Peng and Xiong, Dapeng and He, Jun},
  title         = {{A Guided Fuzzing Approach for Security Testing of Network Protocol Software}},
  pages         = {0--3},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Guided Fuzzing Approach for Security Testing of Network Protocol Software.pdf:pdf},
  keywords      = {-software security testing,binary,dynamic,fuzz,guided fuzzing,security sensitive function,stat,static analysi,static analysis},
  mendeley-tags = {binary,fuzz,security sensitive function},
}

@Article{Yarochkin,
  author        = {Yarochkin, Fyodor},
  title         = {{Hunting the Shadows: In Depth Analysis of Escalated APT attacks}},
  __markedentry = {[ccc:6]},
  file          = {:article\\Hunting the Shadows In Depth Analysis of Escalated APT attacks.pdf:pdf},
  keywords      = {network},
  mendeley-tags = {network},
}

@Article{王旭,
  author        = {王旭 and 范文庆 and 黄玮},
  title         = {二进制代码混淆关键技术研究},
  __markedentry = {[ccc:6]},
  file          = {:article\\二进制代码混淆关键技术研究.pdf:pdf},
  keywords      = {code obfuscation,obfuscate,program control flow obfuscation,reverse analysis},
  mendeley-tags = {obfuscate},
}

@Misc{,
  author        = {Writing a literature review: six steps to get you from start to finish},
  title         = {{Writing a literature review: six steps to get you from start to finish | Wiley}},
  __markedentry = {[ccc:6]},
  abstract      = {Writing a literature review i},
  url           = {https://hub.wiley.com/community/exchanges/discover/blog/2015/07/02/writing-a-literature-review-six-steps-to-get-you-from-start-to-finish},
}

@Misc{,
  title         = {{基于Markov博弈模型的网络安全态势感知方法.pdf}},
  __markedentry = {[ccc:6]},
  file          = {:article\\基于Markov博弈模型的网络安全态势感知方法.pdf.pdf:pdf},
  keywords      = {network,网络安全态势感知；威胁传播网络．；Markov博弈模型},
  mendeley-tags = {network},
}

@Article{Sezer,
  author        = {Sezer and C., Emre and Kil, Chongkyung and Ning, Peng},
  title         = {{Automated Software Vulnerability Analysis}},
  __markedentry = {[ccc:6]},
  abstract      = {Despite decades of research, software continues to have vulnerabilities. Successful ex- ploitations of these vulnerabilities by attackers cost millions of dollars to businesses and individ- uals. Unfortunately, most effective defensive measures, such as patching and intrusion prevention systems, require an intimate knowledge of the vulnerabilities. Many systems for detecting attacks have been proposed. However, the analysis of the exploited vulnerabilities is left to security experts and programmers. Both the human effort involved and the slow analysis process are unfavorable for timely defensive measure to be deployed. The problem is exacerbated by zero-day attacks. This chapter presents two recent research efforts, named MemSherlock and CBones, for au- tomatically aiding experts in identifying and analyzing unknown vulnerabilities. Both methods rely on monitoring user applications during their runtime and checking for inconsistencies in their memory or memory access patterns. MemSherlock is a post-mortem analysis tool that monitors an application's memory operations to determine malicious ones, indicative of an ongoing attack. It produces valuable information regarding the vulnerability and the attack vector. CBones takes snapshots of the memory and looks for inconsistencies by identifying invariants for an applica- tion's memory and verifying them at runtime. Experimental evaluation shows that both methods are capable of providing critical information about vulnerabilities and attack vectors.},
  doi           = {10.1007/978-1-4419-0140-8},
  file          = {:article\\Automated Software Vulnerability Analysis.pdf:pdf},
  isbn          = {9781441901392},
  issn          = {15682633},
  keywords      = {binary,stat,static analysi,static analysis},
  mendeley-tags = {binary},
}

@Misc{Paul,
  author        = {Paul, Santanu},
  title         = {{A Framework for Source Code Search using Program Patterns 2 Comparison with Other Tools}},
  __markedentry = {[ccc:6]},
  file          = {:article\\A Framework for Source Code Search using Program Patterns 2 Comparison with Other Tools.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {first select,ing,pattern match-,program understanding,query language,reverse engineering,second select,software maintenance,software reengineering,source code,source code-important,source code-vice important},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important},
}

@Article{Eal,
  author        = {Eal, D E Montr and Lavoie, Thierry M},
  title         = {{Leveraging Software Clones for Software Comprehension :}},
  __markedentry = {[ccc:6]},
  file          = {:article\\Leveraging Software Clones for Software Comprehension.pdf:pdf},
  keywords      = {stat,static analysi,static analysis},
}

@Misc{,
  title         = {{ICEIS 2014.pdf}},
  __markedentry = {[ccc:6]},
  file          = {:article\\ICEIS 2014.pdf.pdf:pdf},
  keywords      = {book},
  mendeley-tags = {book},
}

@Article{Christey,
  author        = {Christey, Steven M and Kenderdine, Janis E and Mazella, John M and Miles, Brendan and Martin, Robert a},
  title         = {{CWE{\_}v2.9}},
  journal       = {Security},
  __markedentry = {[ccc:6]},
  file          = {:article\\CWE{\_}v2.9.pdf:pdf},
  keywords      = {around-vulnerability},
  mendeley-tags = {around-vulnerability},
}

@Article{,
  title         = {{Vulnerability of DM watermarking of non-iid host signals to attacks utilising the statistics of independent components}},
  journal       = {STEGANOGRAPHY AND DIGITAL WATERMARKING Vulnerability},
  __markedentry = {[ccc:6]},
  file          = {:article\\Vulnerability of DM watermarking of non-iid host signals to attacks utilising the statistics of independent componen.pdf:pdf},
  keywords      = {notcare},
  mendeley-tags = {notcare},
}

@Article{韩伟,
  author        = {韩伟 and 何叶平},
  title         = {{Static Analysis of TOCTTOU Vulnerabilities in Unix-Style File System}},
  journal       = {计算机研究与发展},
  number        = {1000-1239/CN 11-1777/TP},
  pages         = {1430--1437},
  __markedentry = {[ccc:6]},
  file          = {:article\\Static Analysis of TOCTTOU Vulnerabilities in Unix-Style File System.caj:caj},
  keywords      = {stat,static analysi,static analysis},
}

@Comment{jabref-meta: databaseType:bibtex;}

{,
  title         = {{{\~{\$}Bugram: Bug Detection with N-gram Language Models.pdf}},
  __markedentry = {[ccc:6]},
  file          = {:home/ccc/github/literature/article/{\~{\$}Bugram\backslash: Bug Detection with N-gram Language Models.pdf:pdf},
}

@Article{Brown,
  author        = {Brown, Fraser and Andres, N},
  title         = {{How to Build Static Checking Systems Using Orders of Magnitude Less Code}},
  pages         = {143--157},
  __markedentry = {[ccc:6]},
  annote        = {现代的static bug checkers太复杂，针对的语言也很单一，文中设计一种新的static bug checkers编写方式，用了2500多行代码就实现了一个可以针对多种语言，效果良好的static bug checkers。},
  doi           = {10.1145/2872362.2872364},
  file          = {:article\\How to Build Static Checking Systems Using Orders of Magnitude Less Code.pdf:pdf},
  groups        = {vice-important},
  isbn          = {9781450340915},
  issn          = {01635980},
  keywords      = {bug finding,first select,micro-grammars,parsing,source code,source code-vice important,stat,static,static analysi,static analysis},
  mendeley-tags = {first select,source code,source code-vice important},
  url           = {http://web.stanford.edu/{~}mlfbrown/paper.pdf},
}

@Article{DEMM,
  author        = {{Dawson Engler and Madanlal Musuvathi}},
  title         = {{Static Analysis versus Software Model Checking for Bug Finding}},
  pages         = {3653},
  __markedentry = {[ccc:6]},
  file          = {:article\\Static Analysis versus Software Model Checking for Bug Finding.pdf:pdf},
  keywords      = {stat,static analysi,static analysis},
}

@Article{Lechtaler,
  author        = {Lechtaler, Antonio Castro and Liporace, Julio C{\'{e}}sar and Cipriano, Marcelo and Maiorano, Ariel and Malvacio, Eduardo and Tapia, N{\'{e}}stor},
  title         = {{Automated Analysis of Source Code Patches using Machine Learning Algorithms}},
  number        = {January 2011},
  __markedentry = {[ccc:6]},
  file          = {:article\\Automated Analysis of Source Code Patches using Machine Learning Algorithms.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {analysis,automated,binary,first select,machine learning,patch,second select,software quality,source code,source code analysis,source code review,source code-important,source code-vice important,stat,static analysi,static analysis,text mining,web},
  mendeley-tags = {binary,first select,machine learning,second select,source code,source code-important,source code-vice important,web},
}

@PhdThesis{玄跻峰,
  author        = {玄跻峰},
  title         = {{面向软件 Bug 仓库的数据分析及其应用}},
  school        = {大连理工大学},
  __markedentry = {[ccc:6]},
  abstract      = {软件bug仓库是软件开发与维护中的大规模数据库。 面向Bug仓库的数据分析是软 件工程和数据分析的交叉领域，用以应对bug仓库中遇到的复杂问题。Bug仓库数据分 析的目标是通过分析bug数据解决bug仓库相关的软件问题。本论文专注于bug仓库数 据分析及其应用。本文的主要贡献可分为三个层次，分别是挖掘与分析bug仓库（半监 督bug分派算法设计、bug质量与数据归约）、理解与提高软件开发（软件债务高发bug 建模、开发者优先级识别算法设计）和应用bug仓库分析（需求仓库迁移）。 ① 半监督 bug 分派算法设计。Bug 分派是软件开发的典型任务，旨在为新 bug 分 配合适的开发者。本工作设计了基于期望最大化的半监督分派方法，融合无标签bug报 告，进而提高bug分派。实验表明半监督方法可以提高典型分类器的bug分派准确率。 ② Bug 质量与数据归约。Bug 报告的质量影响了修复 bug 的过程。由于 bug 报告 多由开发者以自然语言填写，因此存在噪声和冗余。本部分工作展示了如何应用特征选 择和实例选择技术，归约bug数据规模。为了决定应用特征选择和实例选择的顺序，bug 历史数据集的属性用于构建分类器。归约后的bug数据集可用于提高bug分派。 ③ 软件债务高发 bug 建模。技术债务是软件质量和项目计划之间的一种权衡。本 工作提出债务高发bug的概念。债务高发bug扩展了软件维护中的技术债务，包括三种 类型，即标签bug、重开启bug和副本bug。基于Mozilla的实例研究显示软件产品平均 bug修复时间与债务高发bug的属性有关联。监测债务高发bug有利于检验软件质量。 ④ 开发者优先级识别算法设计。开发者优先级用于区分 bug 仓库中开发者的活跃 程度。本工作设计了获得开发者优先级的算法，并证明其收敛性。在bug评论的基础上 构建的社会网络分析，生成开发者优先级。基于Eclipse和Mozilla的实验，本工作分析 了开发者优先级的特性，并藉此改进bug仓库的三个典型任务，即bug分派、严重程度 识别和重开启bug预测。 实验表明开发者优先级有助于提高软件任务， 尤其是bug分派。 ⑤ 需求仓库迁移。Bug 仓库的数据分析可应用于软件开发维护之外的领域。本工 作将bug仓库分析的知识，迁移至需求数据库，以弥补开放且大规模的需求数据不足的 问题。需求工程中的典型问题，即软件下一版本发布问题，用于检验实验结果；在基于 数据迁移的实例上， 设计了基于骨架的多级算法求解该问题， 并给出了相关性质及证明。 本论文的工作，即bug仓库数据分析及其应用，改进了数据分析方法，并有助于理 解开发过程和开发者合作行为。该研究的经验可以推广到软件工程的其它领域。},
  file          = {:article\\面向软件 Bug 仓库的数据分析及其应用.pdf:pdf},
  keywords      = {around-vulnerability,binary,fuzz,machine learning,predicte,web,数据分析,数据归约；技术债务；开发者优先级；需求仓库迁移},
  mendeley-tags = {around-vulnerability,binary,fuzz,machine learning,predicte,web},
}

@Article{Grossman,
  author        = {Grossman, Dan},
  title         = {{Cyclone: A Type-Safe Dialect of C}},
  __markedentry = {[ccc:6]},
  file          = {:article\\Cyclone A Type-Safe Dialect of C.pdf:pdf},
  groups        = {imprortant, vice-important},
  isbn          = {1-880446-00-6},
  keywords      = {first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Goichi,
  author        = {Goichi, Nakamura and Kyoko, Makino and Ichiro, Murase},
  title         = {{2-4 Buffer-Overflow Detection in C Program}},
  pages         = {35--41},
  __markedentry = {[ccc:6]},
  file          = {:article\\2-4 Buffer-Overflow Detection in C Program.pdf:pdf},
  groups        = {imprortant, vice-important},
  keywords      = {buffer{\_}overflow,first select,second select,source code,source code-important,source code-vice important,stat,static analysi,static analysis,vulnerability detection,web},
  mendeley-tags = {first select,second select,source code,source code-important,source code-vice important,web},
}

@Article{Irfxv,
  author        = {Irfxv, Ehwwhu and Ghyhorshuv, Qrylfh and Srwhqwldo, R Q and Yxoqhudelolwlhv, Vhfxulw and Ri, Sduw and Ghyhorshuv, Z H E and Wkh, D Q G and Iru, Lqdelolw and Frpsdqlhv, Vriwzduh},
  title         = {{Enabling Static Security Vulnerability Analysis in PHP Applications for novice Developers with SSVChecks}},
  pages         = {3--8},
  __markedentry = {[ccc:6]},
  file          = {:article\\Enabling Static Security Vulnerability Analysis in PHP Applications for novice Developers with SSVChecks.pdf:pdf},
  keywords      = {secure programming,security auditing,stat,static analysi,static analysis,web},
  mendeley-tags = {web},
}

@Article{Fh,
  author        = {Fh, C and Rqc, I and Idvo, F C Q O and Yqc, I and Cqqchf, I and Boqid, T and Gdtd, T Rhpo and Rqc, I},
  title         = {混淆算法研究综述},
  journal       = {同级大学自然科学},
  volume        = {0},
  __markedentry = {[ccc:6]},
  file          = {:article\\混淆算法研究综述.pdf:pdf},
  keywords      = {obfuscate,stat,static analysi,static analysis},
  mendeley-tags = {obfuscate},
}

@Article{Of,
  author        = {Of, Lumped-model Analysis and Vulnerability, Microcircuit and Raymond, J P and Chang, W W and Budris, R E and Laboratories, Northrop Corporate},
  title         = {{LUMPED-MODEL ANALYSIS OF MICROCIRCUIT VULNERABILITY}},
  pages         = {271--278},
  __markedentry = {[ccc:6]},
  file          = {:article\\LUMPED-MODEL ANALYSIS OF MICROCIRCUIT VULNERABILITY.pdf:pdf},
  keywords      = {around-vulnerability},
  mendeley-tags = {around-vulnerability},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:source code\;0\;;
2 ExplicitGroup:imprortant\;0\;;
2 ExplicitGroup:vice-important\;0\;;
}
